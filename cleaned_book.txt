1.1 
Definition of AI and AI Effect 
The term artificial intelligence (AI) dates back to the 1950s and refers to the objective of building and 
programming “intelligent” machines capable of imitating human beings.  The definition today has 
evolved significantly, and the following definition captures the concept : 
The capability of an engineered system to acquire, process and apply knowledge and skills.  
The way in which people understand the meaning of AI depends on their current perception.  In the 
1970s the idea of a computer system that could beat a human at chess was somewhere in the future 
and most considered this to be AI.  Now, over twenty years after the computer-based system Deep 
Blue beat world chess champion Garry Kasparov, the “brute force” approach implemented in that 
system is not considered by many to be true artificial intelligence (i.e., the system did not learn from 
data and was not capable of self-learning).  Similarly, the expert systems of the 1970s and 1980s 
incorporated human expertise as rules which could be run repeatedly without the expert being 
present. These were considered to be AI then, but are not considered as such now.   
The changing perception of what constitutes AI is known as the “AI Effect” .  As the perception of 
AI in society changes, so does its definition. As a result, any definition made today is likely to change 
in the future and may not match those from the past. 
1.2 
Narrow, General and Super AI 
At a high level, AI can be broken into three categories: 
• 
Narrow AI (also known as weak AI) systems are programmed to carry out a specific task with 
limited context.  Currently this form of AI is widely available.  For example, game-playing 
systems, spam filters, test case generators and voice assistants. 
• 
General AI (also known as strong AI) systems have general (wide-ranging) cognitive abilities 
similar to humans.  These AI-based systems can reason and understand their environment as 
humans do, and act accordingly.  As of 2021, no general AI systems have been realized. 
• 
Super AI systems are capable of replicating human cognition (general AI) and make use of 
massive processing power, practically unlimited memory and access to all human knowledge 
(e.g., through access to the web). It is thought that super AI systems will quickly become 
wiser than humans. The point at which AI-based systems transition from general AI to super 
AI is commonly known as the technological singularity . 
1.3 
AI-Based and Conventional Systems 
In a typical conventional computer system, the software is programmed by humans using an 
imperative language, which includes constructs such as if-then-else and loops.  It is relatively easy for 
humans to understand how the system transforms inputs into outputs.  In an AI-based system using 
machine learning (ML), patterns in data are used by the system to determine how it should react in 
the future to new data .  For example, an AI-based 
image processor designed to identify images of cats is trained with a set of images known to contain 
cats.  The AI determines on its own what patterns or features in the data can be used to identify cats.  
These patterns and rules are then applied to new images in order to determine if they contain cats.  In 
many AI-based systems, this results in the prediction-making procedure being less easy to 
understand by humans .

In practice, AI-based systems can be implemented by a variety of technologies , and 
the “AI Effect”  may determine what is currently considered to be an AI-based 
system and what is considered to be a conventional system. 
1.4 
AI Technologies 
AI can be implemented using a wide range of technologies , such as: 
• 
Fuzzy logic 
• 
Search algorithms  
• 
Reasoning techniques 
- 
Rule engines 
- 
Deductive classifiers  
- 
Case-based reasoning 
- 
Procedural reasoning  
• 
Machine learning techniques 
- 
Neural networks 
- 
Bayesian models 
- 
Decision trees 
- 
Random forest 
- 
Linear regression 
- 
Logistic regression 
- 
Clustering algorithms 
- 
Genetic algorithms 
- 
Support vector machine (SVM) 
AI-based systems typically implement one or more of these technologies. 
1.5 
AI Development Frameworks 
There are many AI development frameworks available, some of which are focused on specific 
domains. These frameworks support a range of activities, such as data preparation, algorithm 
selection, and compilation of models to run on various processors, such as central processing units 
(CPUs), graphical processing units (GPUs) or Cloud Tensor Processing Units (TPUs). The selection 
of a particular framework may also depend on particular aspects such as the programming language 
used for the implementation and its ease of use.  The following frameworks are some of the most 
popular (as of April 2021): 
• 
Apache MxNet: A deep learning open-source framework used by Amazon for Amazon Web 
Services (AWS) . 
• 
CNTK: The Microsoft Cognitive Toolkit (CNTK) is an open-source deep-learning toolkit . 
• 
IBM Watson Studio: A suite of tools that support the development of AI solutions .

• 
Keras: A high-level open-source API, written in the Python language, capable of running on 
top of TensorFlow and CNTK . 
• 
PyTorch: An open-source ML library operated by Facebook and used for apps applying 
image processing and natural language processing (NLP).  Support is provided for both 
Python and C++ interfaces .   
• 
Scikit-learn: An open-source machine ML library for the Python programming language .  
• 
TensorFlow: An open-source ML framework based on data flow graphs for scalable machine 
learning, provided by Google .  
Note that these development frameworks are constantly evolving, sometimes combining, and 
sometimes being replaced by new frameworks. 
1.6 
Hardware for AI-Based Systems 
A variety of hardware is used for ML model training  and model implementation.  For 
example, a model that performs speech recognition may run on a low-end smartphone, although 
access to the power of cloud computing may be needed to train it.  A common approach used when 
the host device is not connected to the internet is to train the model in the cloud and then deploy it to 
the host device. 
ML typically benefits from hardware that supports the following attributes: 
• 
Low-precision arithmetic: This uses fewer bits for computation (e.g., 8 instead of 32 bits, 
which is usually all that is needed for ML). 
• 
The ability to work with large data structures (e.g., to support matrix multiplication). 
• 
Massively parallel (concurrent) processing.   
General-purpose CPUs provide support for complex operations that are not typically required for ML 
applications and only provide a few cores.  As a result, their architecture is less efficient for training 
and running ML models when compared to GPUs, which have thousands of cores and which are 
designed to perform the massively parallel but relatively simple processing of images.  As a 
consequence, GPUs typically outperform CPUs for ML applications, even though CPUs typically have 
faster clock speeds.  For small-scale ML work, GPUs generally offer the best option. 
Some hardware is specially intended for AI, such as purpose-built Application-Specific Integrated 
Circuits (ASICs) and System on a Chip (SoC). These AI-specific solutions have features such as 
multiple cores, special data management and the ability to perform in-memory processing.  They are 
most suitable for edge computing, while the training of the ML model is done in the cloud. 
Hardware with specific AI architectures is currently (as of April 2021) under development. This 
includes neuromorphic processors , which do not use the traditional von Neumann architecture, 
but rather an architecture that loosely mimics brain neurons.  
Examples of AI hardware providers and their processors include (as of April 2021): 
• 
NVIDIA: They provide a range of GPUs and AI-specific processors, such as the Volta . 
• 
Google: They have developed application-specific integrated circuits for both training and 
inferencing.  Google TPUs (Cloud Tensor Processing Units)  can be accessed by users 
on the Google Cloud, whereas the Edge TPU  is a purpose-built ASIC designed to run 
AI on individual devices.

• 
Intel: They provide Nervana neural network processors  for deep learning (both training 
and inferencing) and Movidius Myriad vision processing units for inferencing in computer 
vision and neural network applications.   
• 
Mobileye: They produce the EyeQ family of SoC devices  to support complex and 
computationally intense vision processing. These have low power consumption for use in 
vehicles. 
• 
Apple: They produce the Bionic chip for on-device AI in iPhones . 
• 
Huawei: Their Kirin 970 chip for smartphones has built-in neural network processing for AI 
. 
1.7 
AI as a Service (AIaaS) 
AI components, such as ML models, can be created within an organization, downloaded from a third-
party, or used as a service on the web (AIaaS). A hybrid approach is also possible in which some of 
the AI functionality is provided from within the system and some is provided as a service. 
When ML is used as a service, access is provided to an ML model over the web and support can also 
be provided for data preparation and storage, model training, evaluation, tuning, testing and 
deployment. 
Third-party providers (e.g., AWS, Microsoft) offer specific AI services, such as facial and speech 
recognition. This allows individuals and organizations to implement AI using cloud-based services 
even when they have insufficient resources and expertise to build their own AI services. In addition, 
ML models provided as part of a third-party service are likely to have been trained on a larger, more 
diverse training dataset than is readily available to many stakeholders, such as those who have 
recently moved into the AI market. 
1.7.1 Contracts for AI as a Service 
These AI services are typically provided with similar contracts as for non-AI cloud-based Software as 
a Service (SaaS).  A contract for AIaaS typically includes a service-level agreement (SLA) that 
defines availability and security commitments.  Such SLAs typically cover an uptime for the service 
(e.g., 99.99% uptime) and a response time to fix defects, but rarely define ML functional performance 
metrics, (such as accuracy), in a similar manner .  AIaaS is often paid for on a 
subscription basis, and if the contracted availability and/or response times are not met, then the 
service provider typically provides credits for future services.  Other than these credits, most AIaaS 
contracts provide limited liability (other than in terms of fees paid), meaning that AI-based systems 
that depend on AIaaS are typically limited to relatively low-risk applications, where loss of service 
would not be too damaging.   
Services often come with an initial free trial period in lieu of an acceptance period. During this period 
the consumer of the AIaaS is expected to test whether the provided service meets their needs in 
terms of required functionality and performance (e.g., accuracy).  This is generally necessary to cover 
any lack of transparency on the provided service . 
1.7.2 AIaaS Examples 
The following are examples of AIaaS (as of April 2021): 
• 
IBM Watson Assistant: This is an AI chatbot which is priced according to the number of 
monthly active users.

• 
Google Cloud AI and ML Products: These provide document-based AI that includes a form 
parser and document OCR. Prices are based on the number of pages sent for processing. 
• 
Amazon CodeGuru: This provides a review of ML Java code that supplies developers with 
recommendations for improving their code quality. Prices are based on the number of lines of 
source code analyzed. 
• 
Microsoft Azure Cognitive Search: The provides AI cloud search. Prices are based on search 
units (defined in terms of the storage and throughput used). 
1.8 
Pre-Trained Models 
1.8.1 Introduction to Pre-Trained Models 
It can be expensive to train ML models . First, the data has to be prepared and then 
the model must be trained.  The first activity can consume large amounts of human resources, while 
the latter activity can consume a lot of computing resources. Many organizations do not have access 
to these resources. 
A cheaper and often more effective alternative is to use a pre-trained model.  This provides similar 
functionality to the required model and is used as the basis for creating a new model that extends 
and/or focuses the functionality of the pre-trained model. Such models are only available for a limited 
number of technologies, such as neural networks and random forests. 
If an image classifier is needed, it could be trained using the publicly available ImageNet dataset, 
which contains over 14 million images classified into over 1000 categories. This reduces the risk of 
consuming significant resources with no guarantee of success.  Alternatively, an existing model could 
be reused that has already been trained on this dataset. By using such a pre-trained model, training 
costs are saved and the risk of it not working largely eliminated.   
When a pre-trained model is used without modification, it can simply be embedded in the AI-based 
system, or it can be used as a service . 
1.8.2 Transfer Learning 
It is also possible to take a pre-trained model and modify it to perform a second, different requirement.  
This is known as transfer learning and is used on deep neural networks in which the early layers  of the neural network typically perform quite basic tasks (e.g., identifying the difference 
between straight and curved lines in an image classifier), whereas the later layers perform more 
specialized tasks (e.g., differentiating between building architectural types). In this example, all but the 
later layers of an image classifier can be reused, eliminating the need to train the early layers. The 
later layers are then retrained to handle the unique requirements for a new classifier.  In practice, the 
pre-trained model may be fine-tuned with additional training on new problem-specific data. 
The effectiveness of this approach largely depends on the similarity between the function performed 
by the original model and the function required by the new model. For example, modifying an image 
classifier that identifies cat species to then identify dog breeds would be far more effective than 
modifying it to identify people’s accents.   
There are many pre-trained models available, especially from academic researchers.  Some 
examples of such pre-trained models are ImageNet models  such as Inception, VGG, AlexNet, 
and MobileNet for image classification and pre-trained NLP models like Google’s BERT .

1.8.3 Risks of using Pre-Trained Models and Transfer Learning 
Using pre-trained models and transfer learning are both common approaches to building AI-based 
systems, but there are some risks associated.  These  include: 
• 
A pre-trained model may lack transparency compared to an internally generated model. 
• 
The level of similarity between the function performed by the pre-trained model and the 
required functionality may be insufficient. Also, this difference may not be understood by the 
data scientist. 
• 
Differences in the data preparation steps  used for the pre-trained model 
when originally developed and the data preparation steps used when this model is then used 
in a new system may impact the resulting functional performance.  
• 
The shortcomings of a pre-trained model are likely to be inherited by those who reuse it and 
may not be documented.  For example, inherited biases  may not be 
apparent if there is a lack of documentation about the data used to train the model.  Also, if 
the pre-trained model is not widely used, there are likely to be more unknown (or 
undocumented) defects and more rigorous testing may be needed to mitigate this risk. 
• 
Models created through transfer learning are highly likely to be sensitive to the same 
vulnerabilities as the pre-trained model on which it is based (e.g., adversarial attacks, as 
explained in 9.1.1). In addition, if an AI-based system is known to contain a specific pre-
trained model (or is based on a specific pre-trained model), then vulnerabilities associated 
with it may already be known by potential attackers. 
Note that several of the above risks can be more easily mitigated by having thorough documentation 
available for the pre-trained model . 
1.9 
Standards, Regulations and AI 
The Joint Technical Committee of IEC and ISO on information technology (ISO/IEC JTC1) prepares 
international standards which contribute towards AI.  For example, a subcommittee on AI (ISO/IEC 
JTC 1/SC42), was set up in 2017.  In addition, ISO/IEC JTC1/SC7, which covers software and system 
engineering, has published a technical report on the “Testing of AI-based systems” . 
Standards on AI are also published at the regional level (e.g., European standards) and the national 
level. 
The EU-wide General Data Protection Regulation (GDPR) came into effect in May 2018 and sets 
obligations for data controllers with regards to personal data and automated decision-making .  It 
includes requirements to assess and improve AI system functional performance, including the 
mitigation of potential discrimination, and for ensuring individuals’ rights to not be subjected to 
automated decision-making.  The most important aspect of the GDPR from a testing perspective is 
that personal data (including predictions) should be accurate.  This does not mean that every single 
prediction made by the system must be accurate, but that the system should be accurate enough for 
the purposes for which it is used. 
The German national standards body (DIN) has also developed the AI Quality Metamodel (, 
). 
Standards on AI are also published by industry bodies.  For example, the Institute of Electrical and 
Electronics Engineers (IEEE) is working on a range of standards on ethics and AI (The IEEE Global 
Initiative for Ethical Considerations in Artificial Intelligence and Autonomous Systems). Many of these 
standards are still in development at the time of writing.

Where AI is used in safety-related systems, the relevant regulatory standards are applicable, such as 
ISO 26262  and ISO/PAS 21448 (SOTIF)  for automotive systems.  Such regulatory 
standards are typically mandated by government bodies, and it would be illegal to sell a car in some 
countries if the included software did not comply with ISO 26262.  Standards in isolation are voluntary 
documents, and their use is normally only made mandatory by legislation or contract.  However, many 
users of standards do so to benefit from the expertise of the authors and to create products that are of 
higher quality.

2.1 
Flexibility and Adaptability 
Flexibility and adaptability are closely related quality characteristics.  In this syllabus, flexibility is 
considered to be the ability of the system to be used in situations that were not part of the original 
system requirements, while adaptability is considered to be the ease with which the system can be 
modified for new situations, such as different hardware and changing operational environments. 
Both flexibility and adaptability are useful if: 
• 
the operational environment is not fully known when the system is deployed. 
• 
the system is expected to cope with new operational environments. 
• 
the system is expected to adapt to new situations. 
• 
the system must determine when it should change its behavior. 
Self-learning AI-based systems are expected to demonstrate all of the above characteristics.  As a 
consequence, they must be adaptable and have the potential to be flexible. 
The flexibility and adaptability requirements of an AI-based system should include details of any 
environment changes to which the system is expected to adapt.  These requirements should also 
specify constraints on the time and resources that the system can use to adapt itself (e.g., how long 
can it take to adapt to recognizing a new type of object). 
2.2 
Autonomy 
When defining autonomy, it is important to first recognize that a fully autonomous system would be 
completely independent of human oversight and control. In practice, full autonomy is not often 
desired.  For example, fully self-driving cars, which are popularly referred to as “autonomous”, are 
officially classified as having “full driving automation” . 
Many consider autonomous systems to be “smart” or “intelligent”, which suggests they would include 
AI-based components to perform certain functions.  For example, autonomous vehicles that need to 
be situationally aware typically use several sensors and image processing to gather information about 
the vehicle’s immediate environment.  Machine learning, and especially deep learning (see Section 
6.1), has been found to be the most effective approach to performing this function.  Autonomous 
systems may also include decision-making and control functions. Both of these can be effectively 
performed using AI-based components.   
Even though some AI-based systems are considered to be autonomous, this does not apply to all AI-
based systems. In this syllabus, autonomy is considered to be the ability of the system to work 
independently of human oversight and control for prolonged periods of time. This can help with 
identifying the characteristics of an autonomous system that need to be specified and tested.  For 
example, the length of time an autonomous system is expected to perform satisfactorily without 
human intervention needs to be known.  In addition, it is important to identify the events for which the 
autonomous system must give control back to its human controllers. 
2.3 
Evolution 
In this syllabus, evolution is considered to be the ability of the system to improve itself in response to 
changing external constraints.  Some AI systems can be described as self-learning and successful 
self-learning AI-based systems need to incorporate this form of evolution.

AI-based systems often operate in an evolving environment. As with other forms of IT systems, an AI-
based system needs to be flexible and adaptable enough to cope with changes in its operational 
environment. 
Self-learning AI-based systems typically need to manage two forms of change:   
• 
One form of change is where the system learns from its own decisions and its interactions 
with its environment.  
• 
The other form of change is where the system learns from changes made to the system’s 
operational environment.  
In both cases the system will ideally evolve to improve its effectiveness and efficiency. However, this 
evolution must be constrained to prevent the system from developing any unwanted characteristics. 
Any evolution must continue to meet the original system requirements and constraints. Where these 
are lacking, the system must be managed to ensure that any evolution remains within limits and that it 
always stays aligned with human values.  Section 2.6 provides examples relating to the impact of side 
effects and reward hacking on self-learning AI-based systems. 
2.4 
Bias 
In the context of AI-based systems, bias is a statistical measure of the distance between the outputs 
provided by the system and what are considered to be “fair outputs” which show no favoritism to a 
particular group.  Inappropriate biases can be linked to attributes such as gender, race, ethnicity, 
sexual orientation, income level, and age.  Cases of inappropriate bias in AI-based systems have 
been reported, for example, in systems used for making recommendations for bank lending, in 
recruitment systems, and in judicial monitoring systems. 
Bias can be introduced into many types of AI-based systems. For example, it is difficult to prevent the 
bias of experts being built-in to the rules applied by an expert system. However, the prevalence of ML 
systems means that much of the discussion relating to bias takes place in the context of these 
systems. 
ML systems are used to make decisions and predictions, using algorithms which make use of 
collected data, and these two components can introduce bias in the results: 
• 
Algorithmic bias can occur when the learning algorithm is incorrectly configured, for example, 
when it overvalues some data compared to others. This source of bias can be caused and 
managed by the hyperparameter tuning of the ML algorithms . 
• 
Sample bias can occur when the training data is not fully representative of the data space to 
which ML is applied. 
Inappropriate bias is often caused by sample bias, but occasionally it can also be caused by 
algorithmic bias. 
2.5 
Ethics 
Ethics is defined in the Cambridge Dictionary as: 
a system of accepted beliefs that control behavior, especially such a system based on morals   
AI-based systems with enhanced capabilities are having a largely positive effect on people’s lives. As 
these systems have become more widespread, concerns have been raised as to whether they are 
used in an ethical manner.

What is considered ethical can change over time and can also change among localities and cultures.  
Care must be taken that the deployment of an AI-based system from one location to another 
considers differences in stakeholder values. 
National and international policies on the ethics of AI can be found in many countries and regions.  
The Organisation for Economic Co-operation and Development issued its principles for AI, the first 
international standards agreed by governments for the responsible development of AI, in 2019 .  
These principles were adopted by forty-two countries when they were issued and are also backed by 
the European Commission.  They include practical policy recommendations as well as value-based 
principles for the “responsible stewardship of trustworthy AI”. These are summarized as: 
• 
AI should benefit people and the planet by driving inclusive growth, sustainable development 
and well-being. 
• 
AI systems should respect the rule of law, human rights, democratic values and diversity, and 
should include appropriate safeguards to ensure a fair society. 
• 
There should be transparency around AI to ensure that people understand outcomes and can 
challenge them. 
• 
AI systems must function in a robust, secure and safe way throughout their life cycles and 
risks should be continually assessed. 
• 
Organizations and individuals developing, deploying or operating AI systems should be held 
accountable. 
2.6 
Side Effects and Reward Hacking 
Side effects and reward hacking can result in AI-based systems generating unexpected, and even 
harmful, results when the system attempts to meet its goals . 
Negative side effects can result when the designer of an AI-based system specifies a goal that 
“focuses on accomplishing some specific tasks in the environment but ignores other aspects of the 
(potentially very large) environment, and thus implicitly expresses indifference over environmental 
variables that might actually be harmful to change” .  For example, a self-driving car with a goal 
of travelling to its destination in “as fuel-efficient and safe manner as possible” may achieve the goal, 
but with the side effect of the passengers becoming extremely annoyed at the excessive time taken. 
Reward hacking can result from an AI-based system achieving a specified goal by using a “clever” or 
“easy” solution that “perverts the spirit of the designer’s intent”.  Effectively, the goal can be gamed.  A 
widely used example of reward hacking is where an AI-based system is teaching itself to play an 
arcade computer game.  It is presented with the goal of achieving the “highest score” , and to do so it 
simply hacks the data record that stores the highest score, rather than playing the game to achieve it. 
2.7 
Transparency, Interpretability and Explainability 
AI-based systems are typically applied in areas where users need to trust those systems. This may be 
for safety reasons, but also where privacy is needed and where they might provide potentially life-
changing predictions and decisions. 
Most users are presented with AI-based systems as “black boxes” and have little awareness of how 
these systems arrive at their results. In some cases, this ignorance may even apply to the data 
scientists who built the systems. Occasionally, users may not even be aware they are interacting with 
an AI-based system.

The inherent complexity of AI-based systems has led to the field of “Explainable AI” (XAI).  The aim of 
XAI is for users to be able to understand how AI-based systems come up with their results, thus 
increasing users’ trust in them. 
According to The Royal Society , there are several reasons for wanting XAI, including: 
• 
giving users confidence in the system 
• 
safeguarding against bias 
• 
meeting regulatory standards or policy requirements 
• 
improving system design 
• 
assessing risk, robustness, and vulnerability 
• 
understanding and verifying the outputs from a system 
• 
autonomy, agency (making the user feel empowered), and meeting social values 
This leads to the following three basic desirable XAI characteristics for AI-based systems from the 
perspective of a stakeholder : 
• 
Transparency: This is considered to be the ease with which the algorithm and training data 
used to generate the model can be determined. 
• 
Interpretability: This is considered to be the understandability of the AI technology by various 
stakeholders, including the users. 
• 
Explainability: This is considered to be the ease with which users can determine how the AI-
based system comes up with a particular result. 
2.8 
Safety and AI 
In this syllabus, safety is considered to be the expectancy that an AI-based system will not cause 
harm to people, property or the environment.  AI-based systems may be used to make decisions that 
affect safety.  For example, AI-based systems working in the fields of medicine, manufacturing, 
defense, security, and transportation have the potential to affect safety. 
The characteristics of AI-based systems that make it more difficult to ensure they are safe (e.g., do 
not harm humans) include: 
• 
complexity 
• 
non-determinism 
• 
probabilistic nature 
• 
self-learning 
• 
lack of transparency, interpretability and explainability 
• 
lack of robustness 
The challenges of testing several of these characteristics are covered in Chapter 8.

3.1 
Forms of ML 
ML algorithms  can be categorized as: 
• 
supervised learning, 
• 
unsupervised learning, and 
• 
reinforcement learning. 
3.1.1 Supervised Learning 
In this kind of learning, the algorithm creates the ML model from labeled data during the training 
phase.  The labeled data, which typically comprises pairs of inputs (e.g., an image of a dog and the 
label “dog”) is used by the algorithm to infer the relationship between the input data (e.g., images of 
dogs) and the output labels (e.g., “dog” and “cat”) during the training.  During the ML model testing 
phase, a new set of unseen data is applied to the trained model to predict the output.  The model is 
deployed once the output accuracy level is satisfactory. 
Problems solved by supervised learning are divided into two categories: 
• 
Classification: This is when the problem requires an input to be classified into one of a few 
pre-defined classes, classification is used.  Face recognition or object detection in an image 
are examples of problems that use classification.   
• 
Regression: This is when the problem requires the ML model to predict a numeric output 
using regression.  Predicting the age of a person based on input data about their habits or 
predicting the future prices of stocks are examples of problems that use regression. 
Note that the term regression, as used in the context of a ML problem, is different to its use in other 
ISTQB® syllabi, such as , where regression is used to describe the problem of software 
modifications causing change-related defects. 
3.1.2 Unsupervised Learning 
In this kind of learning, the algorithm creates the ML model from unlabeled data during the training 
phase.  The unlabeled data is used by the algorithm to infer patterns in the input data during the 
training and assigns inputs to different classes, based on their commonalities.  During the testing 
phase, the trained model is applied to a new set of unseen data to predict which classes the input 
data should be assigned to.  The model is deployed once the output accuracy level is considered to 
be satisfactory. 
Problems solved by unsupervised learning are divided into two categories: 
• 
Clustering: This is when the problem requires the identification of similarities in input data 
points that allows them to be grouped based on common characteristics or attributes. For 
example, clustering is used to categorize different types of customers for the purpose of 
marketing. 
• 
Association: This is when the problem requires interesting relationships or dependencies to 
be identified among data attributes. For example, a product recommendation system may 
identify associations based on customers’ shopping behavior.

3.1.3 Reinforcement Learning 
Reinforcement learning is an approach where the system (an “intelligent agent”) learns by interacting 
with the environment in an iterative manner and thereby learns from experience.  Reinforcement 
learning does not use training data. The agent is rewarded when it makes a correct decision and 
penalized when it makes an incorrect decision.   
Setting up the environment, choosing the right strategy for the agent to meet the desired goal, and 
designing a reward function, are key challenges when implementing reinforcement learning.  
Robotics, autonomous vehicles, and chatbots are examples of applications that use reinforcement 
learning. 
3.2 
ML Workflow 
The activities in the machine learning workflow are: 
Understand the Objectives 
The purpose of the ML model to be deployed needs to be understood and agreed with the 
stakeholders to ensure alignment with business priorities.  Acceptance criteria (including ML 
functional performance metrics 
Select a Framework 
A suitable AI development framework should be selected based on the objectives, 
acceptance criteria, and business priorities .   
Select & Build the Algorithm 
An ML algorithm is selected based on various factors including the objectives, acceptance 
criteria, and the available data .  The algorithm may be manually coded, but 
it is often retrieved from a library of pre-written code.  The algorithm is then compiled to 
prepare for training the model, if required. 
Prepare & Test Data 
Data preparation  comprises data acquisition, data pre-processing and 
feature engineering. Exploratory data analysis (EDA) may be performed alongside these 
activities. 
The data used by the algorithm and model will be based on the objectives and is used by all 
the activities in the “model generation and test” activity shown on Figure 1.  For example, if 
the system is a real-time trading system, the data will come from the trading market.   
The data used to train, tune and test the model must be representative of the operational data 
that will be used by the model.  In some cases, it is possible to use pre-gathered datasets for 
the initial training of the model (e.g., see Kaggle datasets ).  Otherwise, raw data 
typically needs some pre-processing and feature engineering. 
Testing of the data and any automated data preparation steps needs to be performed.  
Section 7.2.1 for more details on input data testing. 
Train the Model 
The selected ML algorithm uses training data to train the model. 
Some algorithms, such as those generating a neural network, read the training dataset 
several times.  Each iteration of training on the training dataset is referred to as an epoch.

Parameters defining the model structure (e.g., the number of layers of a neural network or the 
depth of a decision tree) are passed to the algorithm.  These parameters are known as model 
hyperparameters. 
Parameters that control the training (e.g., how many epochs to use when training a neural 
network) are also passed to the algorithm.  These parameters are known as algorithm 
hyperparameters. 
Evaluate the Model 
The model is evaluated against the agreed ML functional performance metrics, using the 
validation dataset and the results then used to improve (tune) the model.  Model evaluation 
and tuning should resemble a scientific experiment that needs to be carefully conducted 
under controlled conditions with clear documentation.  In practice, several models are 
typically created and trained using different algorithms (e.g., random forests, SVM, and neural 
networks), and the best one is chosen, based on the results of the evaluation and tuning. 
Tune the Model 
The results from evaluating the model against the agreed ML functional performance metrics 
are used to adjust the model settings to fit the data and thereby improve its performance.  
The model may be tuned by hyperparameter tuning, where the training activity is modified 
(e.g., by changing the number of training steps or by changing the amount of data used for 
training), or attributes of the model are updated (e.g., the number of neurons in a neural 
network or the depth of a decision tree). 
The three activities of training, evaluation and tuning can be considered as comprising model 
generation, 
Test the Model 
Once a model has been generated, (i.e., it has been trained, evaluated and tuned), it should 
be tested against an independent test dataset set to ensure that the agreed ML functional 
performance criteria are met .  The functional performance measures from 
testing are also compared with those from evaluation, and if the performance of the model 
with independent data is significantly lower than during evaluation, it may be necessary to 
select a different model. 
In addition to functional performance tests, non-functional tests, such as for the time to train 
the model, and the time and resource usage taken to provide a prediction, also need to be 
performed. Typically, these tests are performed by the data engineer/scientist, but testers 
with sufficient knowledge of the domain and access to the relevant resources can also 
perform these tests. 
Deploy the Model 
Once model development is complete, 
to be re-engineered for deployment along with its related resources, including the relevant 
data pipeline. This is normally achieved through the framework.  Targets might include 
embedded systems and the cloud, where the model can be accessed via a web API.

ML Workflow 
 
Use the Model 
Once deployed, the model is typically part of a larger AI-based system and can be used 
operationally.  Models may perform scheduled batch predictions at set time intervals or may 
run on request in real time. 
Monitor and Tune the Model 
While the model is being used, its situation may evolve and the model may drift away from its 
intended performance .  To ensure that any drift is identified and 
managed, the operational model should be regularly evaluated against its acceptance criteria. 
It may be deemed necessary to update the model settings to address the problem of drift or it 
may be decided that re-training with new data is needed to create a more accurate or more 
robust model. In this case a new model may be created and trained with updated training

data.  The new model may then be compared against the existing model using a form of A/B 
testing . 
The ML workflow 
manner where the steps are repeated iteratively (e.g., when the model is evaluated, it is often 
necessary to return to the training step, and sometimes to data preparation).   
The steps 
the overall system.  Typically, ML models cannot be deployed in isolation and need to be integrated 
with the non-ML parts.  For example, in vision applications, there is a data pipeline that cleans and 
modifies data before submitting it to the ML model.  Where the model is part of a larger AI-based 
system, it will need to be integrated into this system prior to deployment.  In this case, integration, 
system and acceptance test levels may be performed, as described in Section 7.2. 
3.3 
Selecting a Form of ML 
When selecting an appropriate ML approach, the following guidelines apply: 
• 
There should be sufficient training and test data available for the selected ML approach. 
• 
For supervised learning, it is necessary to have properly labeled data. 
• 
If there is an output label, it may be supervised learning. 
• 
If the output is discrete and categorical, it may be classification. 
• 
If the output is numeric and continuous in nature, it may be regression. 
• 
If no output is provided in the given dataset, it may be unsupervised learning. 
• 
If the problem involves grouping similar data, it may be clustering.  
• 
If the problem involves finding co-occurring data items, it may be association. 
• 
Reinforcement learning is better suited to contexts in which there is interaction with the 
environment. 
• 
If the problem involves the notion of multiple states, and involves decisions at each state, 
then reinforcement learning may be applicable. 
3.4 
Factors Involved in ML Algorithm Selection 
There is no definitive approach to selecting the optimal ML algorithm, ML model settings and ML 
model hyperparameters. In practice, this set is chosen based on a mix of the following factors: 
• 
The required functionality (e.g., whether the functionality is classification or prediction of a 
discrete value) 
• 
The required quality characteristics; such as 
o 
accuracy (e.g., some models may be more accurate, but be slower) 
o 
constraints on available memory (e.g., for an embedded system) 
o 
the speed of training (and retraining) the model 
o 
the speed of prediction (e.g., for real-time systems) 
o 
transparency, interpretability and explainability requirements

• 
The type of data available for training the model (e.g., some models might only work with 
image data) 
• 
The amount of data available for training and testing the model (some models might, for 
example, have a tendency to overfit with a limited amount of data, to a greater degree than 
other models) 
• 
The number of features in the input data expected to be used by the model (e.g., other 
factors, such as speed and accuracy, are likely to be directly affected by the number of 
features)  
• 
The expected number of classes for clustering (e.g., some models may be unsuitable for 
problems with more than one class) 
• 
Previous experience 
• 
Trial and error 
3.5 
Overfitting and Underfitting 
3.5.1 Overfitting 
Overfitting occurs when the model fits too closely to a set of data points and fails to properly 
generalize.  Such a model works very well with the data used to train it but can struggle to provide 
accurate predictions for new data.  Overfitting can occur when the model tries to fit to every data 
point, including those data points that may be described as noise or outliers. It can also occur when 
insufficient data is provided in the training dataset. 
3.5.2 Underfitting 
Underfitting occurs when the model is not sophisticated enough to accurately fit to the patterns in the 
training data.  Underfitting models tend to be too simplistic and can struggle to provide accurate 
predictions for both new data and data very similar to the training data.  One cause of underfitting can 
be a training dataset that does not contain features that reflect important relationships between inputs 
and outputs.  It can also occur when the algorithm does not correctly fit the data (e.g., creating a 
linear model for non-linear data). 
3.5.3 Hands-On Exercise: Demonstrate Overfitting and Underfitting 
Demonstrate the concepts of overfitting and underfitting on a model.  This could be demonstrated by 
using a dataset that contains very little data (overfitting), and a dataset with poor feature correlations 
(underfitting).

4.1 
Data Preparation as Part of the ML Workflow 
Data preparation uses an average of 43% of the ML workflow effort and is probably the most 
resource-intensive activity in the ML workflow. In comparison, model selection and building uses only 
17% .  Data preparation forms part of the data pipeline, which takes in raw data and outputs 
data in a form that can be used to both train an ML model and for prediction by a trained ML model. 
Data preparation can be considered to comprise the following activities: 
Data acquisition 
• 
Identification: The types of data to be used for training and predictions are identified.  For 
example, for a self-driving car, it could include the identification of the need for radar, video 
and laser imaging, detection, and ranging (LiDAR) data. 
• 
Gathering: The source of the data is identified and the means for collecting the data are 
determined.  For example, this could include the identification of the International Monetary 
Fund (IMF) as a source for financial data and the channels that will be used to submit the 
data into the AI-based system.   
• 
Labelling: 
The acquired data can be in various forms (e.g., numerical, categorical, image, tabular, text, time-
series, sensor, geospatial, video, and audio). 
Data pre-processing 
• 
Cleaning:  Where incorrect data, duplicate data or outliers are identified, they are either 
removed or corrected.  In addition, data imputation may be used to replace missing data 
values with estimated or guessed values (e.g., using mean, median and mode values).  The 
removal or anonymization of personal information may also be performed. 
• 
Transformation: The format of the given data is changed (e.g., breaking an address held as a 
string into its constituent parts, dropping a field holding a random identifier, converting 
categorical data into numerical data, changing image formats).  Some of the transformations 
applied on numerical data include scaling to ensure that the same range is used. 
Standardization, for example, rescales data to ensure it takes a mean of zero and a standard 
deviation of one. This normalization ensures that the data has a range between zero and one. 
• 
Augmentation: This is used to increase the number of samples in a dataset. Augmentation 
can also be used to include adversarial examples in the training data, providing robustness 
against adversarial attacks .   
• 
Sampling: This involves selection of some part of the total available dataset so that patterns 
in the larger dataset can be observed. This is typically done to reduce costs and the time 
needed to create the ML model. 
Note that all pre-processing carries a risk that it may change useful valid data or add invalid data. 
Feature engineering 
• 
Feature selection: A feature is an attribute/property reflected in the data. Feature selection 
involves the selection of those features which are most likely to contribute to model training 
and prediction.  In practice, it often includes the removal features that are not expected (or 
that are not wanted) to have any effect on the resultant model.  By removing irrelevant 
information (noise), feature selection can reduce overall training times, prevent overfitting 
, increase accuracy and make models more generalizable.

• 
Feature extraction: This involves the derivation of informative and non-redundant features 
from the existing features. The resulting data set is typically smaller and can be used to 
generate an ML model of equivalent accuracy more cheaply and more quickly. 
In parallel to these data preparation activities, exploratory data analysis (EDA) is also typically carried 
out to support the overall data preparation task.  This includes performing data analysis to discover 
trends inherent in the data and using data visualization to represent data in a visual format by plotting 
trends in the data. 
Although the above data preparation activities and sub-activities have been 
different projects may re-order them or only use a subset of them.  Some of the data preparation 
steps, such as the identification of the data source, are performed just once and can be performed 
manually. Other steps may be part of the operational data pipeline and normally work on live data. 
These tasks should be automated. 
4.1.1 Challenges in Data Preparation 
Some of the challenges related to data preparation include: 
• 
The need for knowledge of: 
o 
the application domain. 
o 
the data and its properties. 
o 
the various techniques associated with data preparation. 
• 
The difficulty of getting high quality data from multiple sources. 
• 
The difficulty of automating the data pipeline, and ensuring that the production data pipeline is 
both scalable and has reasonable performance efficiency (e.g., time needed to complete the 
processing of a data item). 
• 
The costs associated with data preparation. 
• 
Not giving sufficient priority to checking for defects introduced into the data pipeline during 
data preparation. 
• 
The introduction of sample bias . 
4.1.2 Hands-On Exercise: Data Preparation for ML 
For a given set of raw data, perform the applicable data preparation steps as outlined in Section 4.1 to 
produce a dataset that will be used to create a classification model using supervised learning. 
This activity forms the first step in creating an ML model that will be used for future exercises. 
To perform this activity, students will be provided with appropriate (and language-specific) materials, 
including: 
• 
Libraries 
• 
ML frameworks 
• 
Tools 
• 
A development environment

4.2 
Training, Validation and Test Datasets in the ML Workflow 
Logically, three sets of equivalent data (i.e., randomly selected from a single initial dataset) are 
required to develop an ML model: 
• 
A training dataset, which is used to train the model.   
• 
A validation dataset, which used for evaluating and subsequently tuning the model.   
• 
A test dataset, (also known as the holdout dataset), which is used for testing the tuned model. 
If unlimited suitable data is available, the amount of data used in the ML workflow for training, 
evaluation and testing typically depends on the following factors: 
• 
The algorithm used to train the model. 
• 
The availability of resources, such as RAM, disk space, computing power, network bandwidth 
and the available time. 
In practice, due to the challenge of acquiring sufficient suitable data, the training and validation 
datasets are often derived from a single combined dataset. The test dataset is kept separate and is 
not used during training. This is to ensure the developed model is not influenced by the test data, and 
so that test results give a true reflection of the model’s quality   
There is no optimal ratio for splitting the combined dataset into the three individual datasets, but the 
typical ratios which may be used as a guideline range from 60:20:20 to 80:10:10 (training: validation: 
test).  Splitting the data into these datasets it is often done randomly, unless the dataset is small or if 
there is a risk of the resultant datasets not being representative of the expected operational data. 
If limited data is available, then splitting the available data into three datasets may result in insufficient 
data being available for effective training.  To overcome this issue, the training and validation datasets 
may be combined (keeping the test dataset separate), and then used to create multiple split 
combinations of this dataset (e.g., 80% training / 20% validation). Data is then randomly assigned to 
the training and validation datasets. Training, validation and tuning are performed using these multiple 
split combinations to create multiple tuned models, and the overall model performance may be 
calculated as the average across all runs.  There are various methods used for creating multiple split 
combinations, which include split-test, bootstrap, K-fold cross validation and leave-one-out cross 
validation . 
4.2.1 Hands-On Exercise: Identify Training and Test Data and Create an ML Model 
Split the previously prepared data  into training, validation and test datasets. 
Train and test a classification model using supervised learning with these datasets.   
Explain the difference between evaluating/tuning and testing by comparing the accuracy achieved 
with the validation and test datasets. 
4.3 
Dataset Quality Issues 
Typical quality issues relating to the data in a dataset include, but are not limited to, those 
the following table:

Quality Aspect  
Description 
Wrong data 
The captured data was incorrect (e.g., through a faulty sensor) or entered 
incorrectly (e.g., copy-paste errors).  
Incomplete data 
Data values may be missing (e.g., a field in a record may be empty, or the 
data for a particular time interval may have been omitted).  There can be 
various reasons for incomplete data, including security issues, hardware 
issues, and human error. 
Mislabeled data 
There are several possible reasons for data to be mislabeled (see Section 
4.5.2). 
Insufficient data 
Insufficient data is available for patterns to be recognized by the learning 
algorithms in use (note that the minimum required quantity of data will vary 
for different algorithms). 
Data not pre-
processed 
Data should be pre-processed to ensure it is clean, in a consistent format and 
contains no unwanted outliers . 
Obsolete data 
Data used for both learning and prediction should be current as possible (e.g., 
using financial data from several years ago may well lead to inaccurate 
results). 
Unbalanced data 
Unbalanced data may result from inappropriate bias (e.g., based on race, 
gender, or ethnicity), poor placement of sensors (e.g., facial recognition 
cameras placed at ceiling height), variability in the availability of datasets, 
and differing motivations of data suppliers. 
Unfair data 
Fairness is a subjective quality characteristic but can often be identified. For 
example, to support diversity or gender balancing, selected data may be 
positively biased towards minorities or disadvantaged groups (note that such 
data may be considered fair but may not be balanced).   
Duplicate data 
Repeated data records may unduly influence the resultant ML model. 
Irrelevant data 
Data that is not relevant to the problem being addressed may adversely 
influence the results and may lead to wasting resources. 
Privacy issues 
Any data use should respect the relevant data privacy laws (e.g., GDPR with 
relation to individuals’ personal information in the European Union). 
Security issues 
Fraudulent or misleading data that has been deliberately inserted into the 
training data may lead to inaccuracy in the trained model.

4.4 
Data Quality and its Effect on the ML Model 
The quality of the ML model is highly dependent on the quality of the dataset from which it is created.  
Poor quality data can result in both flawed models and flawed predictions.   
The following categories of defects result from data quality issues: 
• 
Reduced accuracy: These defects are caused by data which is wrong, incomplete, 
mislabeled, insufficient, obsolete, irrelevant, and data which is not pre-processed.  For 
example, if the data was used to build a model of expected house prices, but the training data 
contained little or no data on detached houses with conservatories, then the predicted prices 
for this specific house type would probably be inaccurate. 
• 
Biased model: These defects are caused by data which is incomplete, unbalanced, unfair, 
lacking diversity, or duplicated.  For example, if the data from a particular feature is missing 
(e.g., all the medical data for disease prediction is gathered from subjects of one particular 
gender), then this is likely to have an adverse effect on the resultant model (unless the model 
is only to be used to make predictions for that gender operationally). 
• 
Compromised model: These defects are due to data privacy and security restrictions . For 
example, privacy issues in the data can lead to security vulnerabilities, which would enable 
attackers to reverse engineer information from the models and might subsequently cause 
leakage of personal information. 
4.5 
Data Labelling for Supervised Learning 
Data labelling is the enrichment of unlabeled (or poorly labeled) data by adding labels, so it becomes 
suitable for use in supervised learning.  Data labelling is a resource-intensive activity that has been 
reported to use, on average, 25% of the time on ML projects . 
In its simplest form, data labelling can consist of putting images or text files in various folders, based 
on their classes.  For example, putting all text files of positive product reviews into one folder and all 
negative reviews into another folder.  Labelling objects in images by drawing rectangles around them 
is another common labelling technique, often known as annotation.  More complex annotations could 
be required for labelling 3D objects or for drawing bounding boxes around irregular objects.  Data 
labelling and annotation are typically supported by tools. 
4.5.1 Approaches to Data Labelling  
Labelling may be performed in a number of ways:  
• 
Internal: The labelling is performed by developers, testers or a team within the organization 
which is set up for the labelling.   
• 
Outsourced: The labelling is done by an external specialist organization. 
• 
Crowdsourced: The labelling is performed by a large group of individuals.  Due to the difficulty 
of managing the quality of the labelling, several annotators may be asked to label the same 
data and a decision then taken on the label to be used. 
• 
AI-Assisted: AI-based tools are used to recognize and annotate data or to cluster similar data.  
The results are then confirmed or perhaps supplemented (e.g., by modifying the bounding 
box) by a human, as part of a two-step process.

• 
Hybrid: A combination of the above labelling approaches could be used. For example, 
crowdsourced labelling is typically managed by an external organization which has access to 
specialized AI-based crowd-management tools. 
Where applicable, it may be possible to reuse a pre-labeled dataset, hence avoiding the need for data 
labelling altogether.  Many such datasets are publicly available, for example, from Kaggle . 
4.5.2 Mislabeled Data in Datasets 
Supervised learning assumes that the data is correctly labeled by the data annotators.  However, it is 
rare in practice for all items in a dataset to be labeled correctly.  Data is mislabeled for the following 
reasons:  
• 
Random errors may be made by annotators (e.g., pressing the wrong button). 
• 
Systemic errors may be made, (e.g., where the labelers are given the wrong instructions or 
poor training). 
• 
Deliberate errors may be made by malicious data annotators. 
• 
Translation errors may take correctly labeled data in one language and mislabel it in another. 
• 
Where the choice is open to interpretation, subjective judgements made by data annotators 
may lead to conflicting data labels from different annotators. 
• 
Lack of required domain knowledge may lead to incorrect labelling. 
• 
Complex classification tasks can result in more errors being made. 
• 
The tools used to support data labelling have defects that lead to incorrect labels. 
• 
ML-based approaches to labelling are probabilistic, and this can lead to some incorrect 
labels.

5.1 
Confusion Matrix 
In a classification problem, a model will rarely predict the results correctly all the time.  For any such 
problem, a confusion matrix can be created with the following possibilities: 
 
 
 
Actual 
 
 
Positive 
Negative 
Predicted 
Positive 
True Positive 
(TP) 
False Positive 
(FP) 
Negative 
False Negative 
(FN) 
True Negative 
(TN) 
 
Confusion Matrix 
 
Note that the confusion matrix 
generate values for the four possible situations of true positive (TP), true negative (TN), false positive 
(FP) and false negative (FN). 
Based on the confusion matrix, the following metrics are defined: 
• 
Accuracy 
Accuracy = (TP + TN) / (TP +TN + FP + FN) * 100% 
Accuracy measures the percentage of all correct classifications.   
• 
Precision 
Precision = TP / (TP + FP) * 100% 
Precision measures the proportion of positives that were correctly predicted. It is a measure 
of how sure one can be about positive predictions. 
• 
Recall 
Recall = TP / (TP + FN) * 100% 
Recall (also known as sensitivity) measures the proportion of actual positives that were 
predicted correctly. It is a measure of how sure one can be about not missing any positives.   
• 
F1-score 
F1-score = 2* (Precision * Recall) / (Precision + Recall) 
F1-score is computed as the harmonic mean of precision and recall.  It will have a value 
between zero and one.  A score close to one indicates that false data has little influence on 
the result.  A low F1-score suggests that the model is poor at detecting positives.

5.2 
Additional ML Functional Performance Metrics for Classification, 
Regression and Clustering 
There are numerous metrics for different types of ML problems (in addition to those related to 
classification described in Section 5.1).  Some of the most commonly used metrics are described 
below. 
Supervised Classification Metrics 
• 
The receiver operating characteristic (ROC) curve is a graphical plot that illustrates the ability 
of a binary classifier as its discrimination threshold is varied.  The method was originally 
developed for military radars, which is why it is so named.  The ROC curve is plotted with true 
positive rate (TPR) (also known as recall) against the false positive rate (FPR = FP / (TN + 
FP)), with TPR on the y axis and FPR on the x axis. 
• 
The area under curve (AUC) is the area under the ROC curve.  It represents the degree of 
separability of a classifier, showing how well the model distinguishes between classes.  With 
a higher AUC, the model’s predictions are better. 
Supervised Regression Metrics 
For supervised regression models, the metrics represent how well the regression line fits the actual 
data points. 
• 
Mean Square Error (MSE) is the average of the squared differences between the actual value 
and the predicted value.  The value of MSE is always positive, and a value closer to zero 
suggests a better regression model.  By squaring the difference, it ensures positive and 
negative errors do not cancel each other out.   
• 
R-squared (also known as the coefficient of determination) is a measure of how well the 
regression model fits the dependent variables. 
Unsupervised Clustering Metrics 
For unsupervised clustering, there are several metrics that represent the distances between the 
various clusters and the closeness of data points within a given cluster.   
• 
Intra-cluster metrics measure the similarity of data points within a cluster. 
• 
Inter-cluster metrics measure the similarity of data points in different clusters. 
• 
The silhouette coefficient (also known as silhouette score) is a measure (between -1 and +1) 
based on the average inter-cluster and intra-cluster distances.  A score of +1 means the 
clusters are well-separated, a score of zero implies random clustering, and a score of -1 
means the clusters are wrongly assigned. 
5.3 
Limitations of ML Functional Performance Metrics 
ML functional performance metrics are limited to measuring the functionality of the model, e.g., in 
terms of accuracy, precision, recall, MSE, AUC and the silhouette coefficient.  They do not measure 
other non-functional quality characteristics, such as those defined in ISO 25010  (e.g., 
performance efficiency) and those described in Chapter 2, (e.g., explainability, flexibility, and 
autonomy).  In this syllabus, the term “ML functional performance metrics” is used because of the 
widespread use of the term “performance metrics” to refer to these functional metrics. Adding “ML 
functional” highlights that these metrics are specific to machine learning and have no relationship to 
performance efficiency metrics.

ML functional performance metrics are constrained by several other factors: 
• 
For supervised learning, the ML functional performance metrics are calculated on the basis of 
labeled data, and the accuracy of the resultant metrics depends on correct labelling . 
• 
The data used for measurement may not be representative (e.g., it may be biased) and the 
generated ML functional performance metrics depend on this data .  
• 
The system may comprise several components, but the ML functional performance metrics 
only applies to the ML model.  For example, the data pipeline is not considered by the ML 
functional performance metrics to evaluate the model. 
• 
Most of the ML functional performance metrics can only be measured with support from tools. 
5.4 
Selecting ML Functional Performance Metrics 
It is not normally possible to build an ML model that achieves the highest score for all of the ML 
functional performance metrics generated from a confusion matrix.  Instead, the most appropriate ML 
functional performance metrics are selected as acceptance criteria, based on the expected use of the 
model (e.g., to minimize false positives, a high value of precision is required, whereas to minimize 
false negatives, the recall metric should be high).  The following criteria can be used when selecting 
the ML functional performance metrics described in Sections 5.1 and 5.2: 
• 
Accuracy: This metric is likely to be applicable if the datasets are symmetric (e.g., false 
positive and false negative counts and costs are similar). This metric becomes a poor choice 
if one class of data dominates over the others, in which case the F1-score should be 
considered. 
• 
Precision: This can be a suitable metric when the cost of false positives is high and 
confidence in positive outcomes needs to be high.  A spam filter, (where classifying an email 
as spam is considered positive), is an example where high precision is required, as putting 
too many emails in the spam folder that are not actually spam will not be acceptable to most 
users.  When the classifier deals with situations where a very large percentage of cases are 
positive, then using precision alone is unlikely to be a good choice. 
• 
Recall: When it is critical that positives should not be missed, then a high recall score is 
important.  For example, missing any true positive results in cancer detection and marking 
them as negative (i.e., no cancer detected) is likely to be unacceptable.   
• 
F1-score – F1-score is most useful when there is an imbalance in the expected classes and 
when the precision and recall are of similar importance. 
In addition to the above metrics, several metrics are described in Section 5.2.  These may be 
applicable for given ML problems, for example: 
• 
The AUC for the ROC curve may be used for supervised classification problems. 
• 
MSE and R-squared may be used for supervised regression problems. 
• 
Inter-cluster metrics, intra-cluster metrics and the silhouette coefficient may be used for 
unsupervised clustering problems.

5.4.1 Hands-On Exercise: Evaluate the Created ML Model 
Using the classification model trained in the previous exercise, calculate and display the values for 
accuracy, precision, recall and F1-score.  Where applicable, use the library functions provided by your 
development framework to perform the calculations. 
5.5 
Benchmark Suites for ML 
New AI technologies such as new datasets, algorithms, models, and hardware are released regularly, 
and it can be difficult to determine the relative efficacy of each new technology. 
To provide objective comparisons between these different technologies, industry-standard ML 
benchmark suites are available.  These cover a wide range of application areas, and provide tools to 
evaluate hardware platforms, software frameworks and cloud platforms for AI and ML performance. 
ML benchmark suites can provide various measures, including training times (e.g., how fast a 
framework can train an ML model using a defined training dataset to a specified target quality metric, 
such as 75% accuracy), and inference times (e.g., how fast a trained ML model can perform 
inference).   
ML benchmark suites are provided by several different organizations, such as: 
• 
MLCommons : This is a not-for-profit organization formed in 2020 and previously named 
ML Perf, which provides benchmarks for software frameworks, AI-specific processors and ML 
cloud platforms. 
• 
DAWNBench : This is an ML benchmark suite from Stanford University. 
• 
MLMark : This is an ML benchmark suite designed to measure the performance and 
accuracy of embedded inference from the Embedded Microprocessor Benchmark 
Consortium.

6.1 
Neural Networks 
Artificial neural networks were initially intended to mimic the functioning of the human brain, which can 
be thought of as many connected biological neurons.  The single-layer perceptron is one of the first 
examples of the implementation of an artificial neural network and comprises a neural network with 
just one layer (i.e., a single neuron).  It can be used for supervised learning of classifiers, which 
decide whether an input belongs to a specific class or not. 
Most current neural networks are considered to be deep neural networks because they comprise 
several layers and can be considered as multi-layer perceptrons . 
 
 
Figure 3 Structure of a deep neural network 
 
A deep neural network comprises three types of layers.  The input layer receives inputs, for example 
pixel values from a camera.  The output layer provides results to the outside world. This might be, for 
example, a value signifying the likelihood that the input image is a cat.  Between the input and output 
layers are hidden layers made up of artificial neurons, which are also known as nodes.  The neurons 
in one layer are connected to each of the neurons in the next layer and there may be different 
numbers of neurons in each successive layer.  The neurons perform computations and pass 
information across the network from the input neurons to the output neurons.

Figure 4 Computation performed by each neuron 
 
As 
generates what is known as the activation value. This value is calculated by running a formula (the 
activation function) that receives as input the activation values from all the neurons in the previous 
layer, the weights assigned to the connections between the neurons (these weights change as the 
network learns), and the individual bias of each neuron. Note that this bias is a preset constant value 
and is not related to the bias considered earlier in Section 2.4).  Running different activation functions 
can result in different activation values being calculated. These values are typically centered around 
zero and have a range between -1 (meaning that the neuron is “disinterested”) and +1 (meaning that 
the neuron is “very interested”). 
When training the neural network, each neuron is preset to a bias value and the training data is 
passed through the network, with each neuron running the activation function, to eventually generate 
an output.  The generated output is then compared with the known correct result (labeled data is used 
in this example of supervised learning).  The difference between the actual output and the known 
correct result is then fed back through the network to modify the values of the weights on the 
connections between the neurons in order to minimize this difference.  As more training data is fed 
through the network, the weights are gradually adjusted as the network learns. Ultimately, the outputs 
produced are considered good enough to end training. 
6.1.1 Hands-On Exercise: Implement a Simple Perceptron 
Students will be led through an exercise demonstrating a Perceptron learning a simple function, such 
as an AND function. 
The exercise should cover how a Perceptron learns by modifying weights across a number of epochs 
until the error is zero. Various mechanisms may be used for this activity (e.g., spreadsheet, 
simulation).

6.2 
Coverage Measures for Neural Networks 
Achieving white-box test coverage criteria (e.g., statement, branch, modified condition/decision 
coverage (MC/DC)  is mandatory for compliance with some safety-related standards  when 
using traditional imperative source code, and is recommended by many test practitioners for other 
critical applications.  Monitoring and improving coverage supports the design of new test cases, 
leading to increased confidence in the test object.  
Using such measures for measuring the coverage of neural networks provides little value as the same 
code tends to be run each time the neural network is executed.  Instead, coverage measures have 
been proposed based on the coverage of the structure of the neural network itself, and more 
specifically, the neurons within it.  Most of these measures are based on the activation values of the 
neurons. 
Coverage for neural networks is a new research area.  Academic papers have only been published 
since 2017, and as such, there is little objective evidence available (e.g., duplicated research results) 
that show the proposed measures are effective.  It should be noted, however, that despite statement 
and decision coverage having been used for over 50 years, there is also little objective evidence of 
their relative effectiveness, even though they have been mandated for measuring coverage of 
software in safety-related applications, such as medical devices and avionics systems. 
The following coverage criteria for neural networks have been proposed and applied by researchers 
to a variety of applications: 
• 
Neuron coverage: Full neuron coverage requires that each neuron in the neural network 
achieves an activation value greater than zero .  This is very easy to achieve in practice 
and research has shown that almost 100% coverage is achieved with very few test cases on 
a variety of deep neural networks.  This coverage measure may be most useful as an alarm 
signal when it is not achieved. 
• 
Threshold coverage: Full threshold coverage requires that each neuron in the neural network 
achieves an activation value greater than a specified threshold. The researchers who created 
the DeepXplore framework actually suggested that neuron coverage should be measured 
based on the activation value exceeding a threshold which would change based on the 
situation. They performed their research with a threshold of 0.75 when they reported 
efficiently finding thousands of incorrect corner case behaviors using this white-box approach.  
This type of coverage has been renamed here to distinguish it more easily from neuron 
coverage with a threshold set to zero, as some other researchers use the term “neuron 
coverage” to mean neuron coverage with a threshold of zero.   
• 
Sign-Change coverage: To achieve full sign-change coverage, test cases need to cause each 
neuron to achieve both positive and negative activation values .   
• 
Value-Change coverage: To achieve full value-change coverage, test cases need to cause 
each neuron to achieve two activation values, where the difference between the two values 
exceeds some chosen value . 
• 
Sign-Sign coverage: This coverage considers pairs of neurons in adjacent layers and the sign 
taken by their activation values.  For a pair of neurons to be considered covered, a test case 
needs to show that changing the sign of a neuron in the first layer causes the neuron in the 
second layer to change its sign, while the signs of all other neurons in the second layer 
remain unchanged . This is a similar concept to MC/DC coverage for imperative source 
code.  
Researchers have reported on further coverage measures based on layers (although simpler than 
sign-sign coverage), and a successful approach using nearest neighbor algorithms to identify

meaningful change in neighboring sets of neurons has been implemented in the TensorFuzz tool 
.

7.1 
Specification of AI-Based Systems 
System requirements and design specifications are equally important for both AI-based systems and 
conventional systems.  These specifications provide the basis for testers to check whether actual 
system behavior aligns with the specified requirements.  However, if the specifications are incomplete 
and lack testability, this introduces a test oracle problem . 
There are several reasons why the specification of AI-based systems can be particularly challenging: 
• 
In many AI-based systems projects, requirements are specified only in terms of high-level 
business goals and required predictions.  A reason for this is the exploratory nature of AI-
based system development.  Often, AI-based systems projects start with a dataset, and the 
goal is to determine which predictions can be obtained from that data. This is in contrast with 
specifying the required logic from the start of a conventional project. 
• 
The accuracy of the AI-based system is often unknown until the test results from independent 
testing are available.  Along with the exploratory development approach, this often leads to 
inadequate specifications as implementation is already in progress by the time the desired 
acceptance criteria are determined. 
• 
The probabilistic nature of many AI-based systems can make it necessary to specify 
tolerances for some of the expected quality requirements, such as the accuracy of 
predictions. 
• 
Where the system goals call for replicating human behavior, rather than providing specific 
functionality, this often leads to poorly specified behavior requirements based on the system 
being as good as, or better than the human activities it aims to replace.  This can make it 
difficult to define a test oracle, especially when the humans it is replacing vary widely in their 
capabilities. 
• 
Where AI is used to implement user interfaces, such as by natural language recognition, 
computer vision, or physical interaction with humans, the systems need to demonstrate 
increased flexibility.  However, such flexibility can also create challenges in identifying and 
documenting all the different ways in which such interactions might happen. 
• 
Quality characteristics specific to AI-based systems, such as adaptability, flexibility, evolution, 
and autonomy, need to be considered and defined as part of requirements specification .  The novelty of these characteristics can make them difficult to define and test. 
7.2 
Test Levels for AI-Based Systems 
AI-based systems typically comprise both AI and non-AI components.  Non-AI components can be 
tested using conventional approaches , while AI components and systems containing AI 
components may need to be tested differently in some respects, as described below.  For all test 
levels that include the testing of AI components, it is important for the testing to be closely supported 
by data engineers/scientists and domain experts. 
A major difference from the test levels used for conventional software is the inclusion of two new 
specialized test levels to explicitly handle the testing of the input data and the models used in AI-
based systems .  Most of this section is applicable to all AI-based systems, although some parts 
are specifically focused on ML.

7.2.1 Input Data Testing 
The objective of input data testing is to ensure that the data used by the system for training and 
prediction is of the highest quality .  It includes the following: 
• 
Reviews 
• 
Statistical techniques (e.g., testing data for bias) 
• 
EDA of the training data 
• 
Static and dynamic testing of the data pipeline 
The data pipeline typically comprises several components performing data preparation (see Section 
4.1), and the testing of these components includes both component testing and integration testing.  
The data pipeline for training may be quite different from the data pipeline used to support operational 
prediction. For training, the data pipeline can be considered a prototype, compared to the fully 
engineered, automated version used operationally.  For this reason, the testing of these two versions 
of the data pipeline may be quite distinct. However, testing the functional equivalence of the two 
versions should also be considered. 
7.2.2 ML Model Testing 
The objective of ML model testing is to ensure that the selected model meets any performance criteria 
that may have been specified. This includes: 
• 
ML functional performance criteria  
• 
ML non-functional acceptance criteria that are appropriate for the model in isolation, such as 
speed of training, speed of prediction, computing resources used, adaptability, and 
transparency.   
ML model testing also aims to determine that the choice of ML framework, algorithm, model, model 
settings and hyperparameters is as close to optimal as possible.  Where appropriate, ML model 
testing may also include testing to achieve white-box coverage criteria . The selected 
model is later integrated with other components, AI and non-AI. 
7.2.3 Component Testing 
Component testing is a conventional test level which is applicable to any non-model components, 
such as user interfaces and communication components. 
7.2.4 Component Integration Testing 
Component integration testing is a conventional test level which is conducted to ensure that the 
system components (both AI and non-AI) interact correctly.  It tests that the inputs from the data 
pipeline are received as expected by the model, and that any predictions produced by the model are 
exchanged with the relevant system components (e.g., the user interface) and used correctly by them.  
Where AI is provided as a service , it is normal to perform API testing of the provided 
service as part of component integration testing. 
7.2.5 System Testing 
System testing is a conventional test level which is conducted to ensure that the complete system of 
integrated components (both AI and non-AI) performs as expected, from both functional and non-

functional viewpoints, in a test environment that is closely representative of the operational 
environment.  Depending on the system, this testing may take the form of field trials in the expected 
operational environment or testing within a simulator (e.g., if test scenarios are hazardous or difficult 
to replicate in an operational environment). 
During system testing, the ML functional performance criteria are re-tested to ensure that the test 
results from the initial ML model testing are not adversely affected when the model is embedded 
within a complete system.  This testing is especially important where the AI component has been 
deliberately changed (e.g., compressing a DNN to reduce its size).   
System testing  is also the test level in which many of the non-functional requirements for the system 
are tested.  For example, adversarial testing may be performed to test for robustness, and the system 
may be tested for explainability.  Where appropriate, interfaces with hardware components (e.g., 
sensors) may be tested as part of system testing. 
7.2.6 Acceptance Testing 
Acceptance testing is a conventional test level and is used to determine whether the complete system 
is acceptable to the customer.  For AI-based systems, the definition of acceptance criteria can be 
challenging .  Where AI is provided as a service , acceptance 
testing may be needed to determine the suitability of the service for the intended system and whether, 
for example, ML functional performance criteria have been sufficiently achieved. 
7.3 
Test Data for Testing AI-based Systems 
Depending on the situation and the system under test (SUT), the acquisition of test data might 
present a challenge.  There are several potential challenges in dealing with test data for AI-based 
systems, including: 
• 
Big data (high-volume, high-velocity and high-variety data) can be difficult to create and 
manage.  For example, it may be difficult to create representative test data for a system that 
consumes large volumes of images and audio at a high speed. 
• 
Input data may need to change over time, particularly if it represents events in the real world.  
For example, recorded photographs to test a facial recognition system may need to be “aged” 
to represent the ageing of people over several years in real life. 
• 
Personal or otherwise confidential data may need special techniques for sanitization, 
encryption, or redaction. Legal approval for use may also be required. 
• 
When testers use the same implementation as the data scientists for data acquisition and 
data pre-processing, then defects in these steps may be masked. 
7.4 
Testing for Automation Bias in AI-Based Systems 
One category of AI-based systems helps humans in decision-making. However, there is occasionally 
a tendency for humans to be too trusting of these systems.  This misplaced trust may be called either 
automation bias or complacency bias, and takes two forms.   
• 
The first form of automation/complacency bias is when the human accepts recommendations 
provided by the system and fails to consider inputs from other sources (including 
themselves).  For example, a procedure where a human keys data into a form might be 
improved by using machine learning to pre-populate the form, and the human then validates 
this data.  It has been shown that this form of automation bias typically reduces the quality of

decisions made by 5%, but this could be much greater depending on the system context 
.  Similarly, the automatic correction of typed text (e.g., in mobile phone messages) is 
often faulty and could change the meaning. Users often do not notice this, and do not 
override the mistake. 
• 
The second form of automation/complacency bias is where the human misses a system 
failure because they do not adequately monitor the system.  For example, semi-autonomous 
vehicles are becoming increasingly self-driving, but still rely on a human to take over in the 
event of an imminent accident.  Typically, the human vehicle occupant gradually becomes too 
trusting of the system’s abilities to control the vehicle and they begin to pay less attention. 
This may lead to a situation where they are unable to react appropriately when needed. 
In both scenarios, testers should understand how human decision-making may be compromised, and 
test for both the quality of the system’s recommendations and the quality of the corresponding human 
input provided by representative users.  
7.5 
Documenting an AI Component 
The typical content for the documentation of an AI component includes: 
• 
General: Identifiers, description, developer details, hardware requirements, license details, 
version, date and point of contact. 
• 
Design: Assumptions and technical decisions. 
• 
Usage: Primary and secondary use cases, typical users, approach to self-learning, known 
bias, ethical issues, safety issues, transparency, decision thresholds, platform and concept 
drift. 
• 
Datasets: Features, collection, availability, pre-processing requirements, use, content, 
labelling, size, privacy, security, bias/fairness and restrictions/constraints. 
• 
Testing: Test dataset (description and availability), independence of testing, test results, 
testing approach for robustness, explainability, concept drift and portability. 
• 
Training and ML Functional Performance: ML algorithm, weights, validation dataset, selection 
of ML functional performance metrics, thresholds for ML functional performance metrics, and 
actual ML functional performance metrics. 
 
Clear documentation helps improve the testing by providing transparency on the implementation of 
the AI-based system.  The key areas of documentation that are important to testing are: 
• 
The purpose of the system, and the specification of functional and non-functional 
requirements.  These types of documentation typically form part of the test basis. 
• 
Architectural and design information, outlining how the different AI and non-AI components 
interact. This supports the identification of integration testing objectives, and may provide a 
basis for white-box testing of the system structure. 
• 
The specification of the operating environment. This is required when testing the autonomy, 
flexibility and adaptability of the system.  
• 
The source of any input data, including associated metadata. This needs to be clearly 
understood when testing the following aspects: 
o 
Functional correctness of untrustworthy inputs

o 
Explicit or implicit sample bias  
o 
Flexibility, including the mis-learning from poor data inputs for self-learning systems 
• 
The way in which the system is expected to adapt to changes in its operational environment. 
This is needed as a test basis when testing for adaptability. 
• 
Details of expected system users. This is needed to ensure that testing can be made 
representative. 
7.6 
Testing for Concept Drift 
The operational environment can change over time without the trained model changing 
correspondingly.  This phenomenon is known as concept drift and typically causes the outputs of the 
model to become increasingly less accurate and less useful.  For example, the impact of marketing 
campaigns may result in a change in the behavior of potential customers over a period of time.  Such 
changes could be seasonal or abrupt changes due to cultural, moral or societal changes which are 
external to the system.  An example of such an abrupt change is the impact of the COVID-19 
pandemic and its effect on the accuracy of the models used for sales projections and stock markets. 
Systems that may be prone to concept drift should be regularly tested against their agreed ML 
functional performance criteria, to ensure that any occurrences of concept drift are detected soon 
enough for the problem to be mitigated.  Typical mitigations may include retiring the system or re-
training the system. In the case of re-training, this would be performed with up-to-date training data 
and followed by confirmation testing, regression testing, and possibly a form of A/B testing , where the updated B-system must outperform the original A-system. 
7.7 
Selecting a Test Approach for an ML System 
An AI-based system will typically include both AI and non-AI components.  The test approach is 
based on a risk analysis for such a system and will include both conventional testing as well as more 
specialized testing to address those factors specific to AI components and AI-based systems. 
The following list provides some typical risks and corresponding mitigations, specific to ML systems. 
Note that this list only provides a limited set of examples and that there are many more risks specific 
to ML systems that require mitigation through testing. 
 
Risk Aspect  
Description and possible Mitigations 
Data quality may be lower 
than expected. 
This risk may become an issue in several ways, each of which may 
be prevented in different manners . Common 
mitigations include the use of reviews, EDA and dynamic testing. 
The operational data pipeline 
may be faulty. 
This risk can be partly mitigated by the dynamic testing of the 
individual pipeline components and the integration testing of the 
complete pipeline 
The ML workflow used to 
develop the model may be 
sub-optimal. 
This risk could be due to the following: 
• 
A lack of up-front agreement on the ML workflow to be 
followed

Risk Aspect  
Description and possible Mitigations 
(
• 
A poor choice of workflow 
• 
Data engineers failing to follow the workflow 
Reviews with experts may mitigate the chance of choosing the wrong 
workflow, while more hands-on management or audits could address 
the problems of agreement on and implementation of the workflow. 
The choice of ML framework, 
algorithm, model, model 
settings and/or 
hyperparameters may be 
sub-optimal. 
This risk could be due to a lack of expertise of the decision-makers, 
or to the poor implementation of the evaluation and tuning steps (or 
test step) of the ML workflow.  
Reviews with experts may mitigate the chance of taking wrong 
decisions, and better management may ensure that the evaluation 
and tuning (and test) steps of the workflow are followed. 
The desired ML functional 
performance criteria may not 
be delivered operationally, 
despite the ML component 
meeting those criteria in 
isolation.   
This risk could be due to the datasets used for training and testing 
the model in isolation not being representative of the data 
encountered operationally.   
Reviews of the selected datasets by experts (or users) may mitigate 
the chance that the selected data is not representative. 
The desired ML functional 
performance criteria are met, 
but the users may be 
unhappy with the delivered 
results. 
This risk could be due to the selection of the wrong performance 
criteria (e.g., high recall was selected when high precision was 
needed).   
Reviews with experts may mitigate the chance of choosing the wrong 
ML functional performance metrics, or experience-based testing 
could also identify inappropriate criteria.  The risk could also be due 
to concept drift, in which case more frequent testing of the 
operational system could mitigate the risk. 
The desired ML functional 
performance criteria are met, 
but the users may be 
unhappy with the delivered 
service.   
This risk could be due to a lack of focus on the system’s non-
functional requirements).  Note that the range of quality 
characteristics for AI-based systems extends beyond those listed in 
ISO/IEC 25010 . 
Using a risk-based approach to prioritize the quality characteristics 
and performing the relevant non-functional testing could mitigate this 
risk. 
Alternatively, the problem could be due to a combination of factors 
that could be identified through experience-based testing, as part of 
system testing. Chapter 8 provides guidance on how to test these 
characteristics.

Risk Aspect  
Description and possible Mitigations 
The self-learning system may 
not be providing the service 
expected by users.   
This risk could be due to various reasons, for example:   
• 
The data used by the system for self-learning may be 
inappropriate.  In this case, reviews by experts could identify 
the problematic data.   
• 
The system may be failing due to new self-learnt functionality 
being unacceptable.  This could be mitigated by automated 
regression testing including performance comparison with the 
previous functionality.   
• 
The system may be learning in a way that is not expected by 
the users, which could be detected by experience-based 
testing. 
Users may be frustrated by 
not understanding how the 
system determines its 
decisions 
This risk could be due to a lack of explainability, interpretability 
and/or transparency.  
these characteristics. 
Users may find that the 
model provides excellent 
predictions when the data is 
similar to the training data but 
provides poor results 
otherwise. 
This risk may be due to overfitting , which may be 
detected by testing the model with a dataset that is completely 
independent from the training dataset or performing experience-
based testing.

8.1 
Challenges Testing Self-Learning Systems 
There are several potential challenges to overcome when testing self-learning systems (see Chapter 
2 for more details on these systems), including: 
• 
Unexpected change: The original requirements and constraints within which the system 
should work are generally known, but there may be little or no information available on the 
changes made by the system itself.  It is normally possible to test against the original 
requirements and design (and any specified constraints), but if the system has devised an 
innovative implementation or gamed a solution (the implementation of which cannot be seen), 
it may be difficult to design tests which are appropriate for this new implementation.  In 
addition, when systems change themselves (and their outputs), the results of previously 
passing tests can change.  This is a test design challenge. It may be addressed by designing 
appropriate tests that remain relevant as the system changes its behavior, so preventing a 
potential regression testing problem. However, it may also require new tests to be designed 
based on observed new system behaviors. 
• 
Complex acceptance criteria: It may be necessary to define expectations for improvement by 
the system when it self-learns.  For example, it may be assumed that if the system changes 
itself, its overall functional performance should improve.  Additionally, specifying anything 
other than simple “improvement” can quickly become complex.  For example, a minimum 
improvement might be expected (rather than simply any improvement), or the required 
improvement may be linked to environmental factors (e.g., a minimum 10% improvement in 
functionality X is required if environmental factor F changes by more than Y).  These 
problems may be addressed through the specification and testing against the more complex 
acceptance criteria, and by maintaining a continuous record of the current system baseline 
functional performance. 
• 
Insufficient testing time: It may be necessary to know how quickly the system is expected to 
learn and adapt, given different scenarios.  These acceptance criteria may be difficult to 
specify and acquire.  If a system adapts quickly, there might be insufficient time to manually 
execute new tests after each change, so it may be necessary to write tests that can be run 
automatically when the system changes itself.  These challenges can be addressed through 
the specification of appropriate acceptance criteria  and automated 
continuous testing. 
• 
Resource requirements: The system requirements might include acceptance criteria for the 
resources which the system is permitted to use when performing self-learning or adaptation. 
This may include, for example, the amount of processing time and memory allowed to be 
used to improve.  Additionally, consideration needs to be given on whether this resource 
usage should be linked to a measurable improvement in functionality or accuracy.  This 
challenge affects the specification of acceptance criteria. 
• 
Insufficient specifications of operational environment: A self-learning system may change if 
the environmental inputs that it receives are outside expected ranges, or if they are not 
reflected in the training data.  These inputs may be attacks in the form of data poisoning .  It can be difficult to predict the full range of operational environments and 
environmental changes, and to therefore identify the full set of representative test cases and 
environmental requirements.  Ideally, the full scope of possible changes in the operational 
environment to which the system is expected to respond will be defined as acceptance 
criteria. 
• 
Complex test environment: Managing the test environment to ensure it can mimic all the 
potential high-risk operational environment changes is a challenge and may involve the use of

test tools (e.g., a fault injection tool).  Depending on the nature of the operating environment, 
this may be tested by manipulating inputs and sensors, or by obtaining access to different 
physical environments in which the system can be tested. 
• 
Undesirable behavior modifications: A self-learning system modifies its behavior based on its 
inputs and it may not be possible for testers to prevent this from occurring. This may arise, for 
example, if a third-party system is being used, or if the production system is being tested.  By 
repeating the same tests, a self-learning system may become more effective at responding to 
those tests, which may then influence the long-term behavior of the system.  It is therefore 
important to prevent a situation where the testing causes a self-learning system to adversely 
change its behavior.  This is a challenge for test case design and test management. 
8.2 
Testing Autonomous AI-Based Systems 
Autonomous systems must be able to determine when they require human intervention and when 
they do not.  Therefore, testing the autonomy of AI-based systems requires that conditions are 
created for the system to exercise this decision-making.   
Testing for autonomy may require: 
• 
Testing whether the system requests human intervention for a specific scenario when the 
system should be relinquishing control.  Such scenarios could include a change to the 
operational environment, or the system exceeding the limits of its autonomy. 
• 
Testing whether the system requests human intervention when the system should be 
relinquishing control after a specified period of time. 
• 
Testing whether the system unnecessarily requests human intervention when it should still be 
working autonomously. 
It may be helpful to use boundary value analysis applied to the operating environment to generate the 
necessary conditions for this testing.  It can be challenging to define how the parameters that 
determine autonomy manifest themselves in the operating environment, and to create the test 
scenarios which depend on the nature of the autonomy. 
8.3 
Testing for Algorithmic, Sample and Inappropriate Bias 
An ML system should be evaluated against the different biases and actions taken to remove 
inappropriate bias. This may involve positive bias being deliberately introduced to counter the 
inappropriate bias.  
Testing with an independent dataset can often detect bias.  However, it can be difficult to identify all 
the data that causes bias because the ML algorithm can use combinations of seemingly unrelated 
features to create unwanted bias.  
AI-based systems should be tested for algorithmic bias, sample bias and inappropriate bias .  This may involve: 
• 
Analysis during the model’s training, evaluation and tuning activities to identify whether 
algorithmic bias is present.  
• 
Reviewing the source of the training data and the processes used to acquire it, such that the 
presence of sample bias can be identified. 
• 
Reviewing the pre-processing of data as part of the ML workflow to identify whether the data 
has been affected in a way that could cause sample bias.

• 
Measuring how changes in system inputs affect system outputs over a large number of 
interactions, and examining the results based on the groups of people or objects that the 
system may be inappropriately biased towards, or against.  This is similar to the LIME (Local 
Interpretable Model-Agnostic Explanations) method discussed in 8.6, and may be carried out 
in a production environment as well as part of testing prior to release. 
• 
Obtaining additional information concerning the attributes of the input data potentially related 
to bias and correlating it to the results.  This could relate to demographic data, for example, 
which might be appropriate when testing for inappropriate bias that affects groups of people, 
where membership of a group is relevant to assessing bias but is not an input to the model.  
This is because the bias can be based on “hidden” variables which are not explicitly present 
in the input data, but are inferred by the algorithm. 
8.4 
Challenges Testing Probabilistic and Non-Deterministic AI-Based 
Systems 
Most probabilistic systems are also non-deterministic, and so the following list of testing challenges 
typically applies to AI-based systems with any of these attributes: 
• 
There may be multiple, valid outcomes from a test with the same set of preconditions and 
inputs. This makes the definition of expected results more challenging and can cause 
difficulties: 
o 
when tests are reused for confirmation testing. 
o 
when tests are reused for regression testing. 
o 
where reproducibility of testing is important. 
o 
when the tests are automated. 
• 
The tester typically requires a deeper knowledge of the required system behavior so that they 
can come up with reasonable checks for whether the test has passed rather than simply 
stating an exact value for the expected test result.  For example, testers may need to define 
more sophisticated expected results compared with conventional systems. These expected 
test results may include tolerances (e.g., “is the actual result within 2% of the optimal 
solution?”). 
• 
Where a single definitive output from a test is not possible due to the probabilistic nature of 
the system, it is often necessary for the tester to run a test several times in order to generate 
a statistically valid test result. 
8.5 
Challenges Testing Complex AI-Based Systems 
AI-based systems are often used to implement tasks that are too complex for humans to perform.  
This can lead to a test oracle problem because testers are unable to determine the expected results 
as they would normally do .  For example, AI-based systems are often used to 
identify patterns in large volumes of data.  Such systems are used because they can find patterns that 
humans, even after much analysis, simply cannot find manually. Understanding the required behavior 
of such systems in sufficient depth to be able to generate expected results can be challenging. 
A similar problem arises when the internal structure of an AI-based system is generated by software, 
making it too complex for humans to understand.  This leads to the situation where the AI-based

system can only be tested as a black box.  Even when the internal structure is visible, this provides no 
additional useful information to help with the testing. 
The complexity of AI-based systems increases when they provide probabilistic results and are non-
deterministic in nature . 
The problems with non-deterministic systems are exacerbated when an AI-based system consists of 
several interacting components, each providing probabilistic results.  For example, a facial recognition 
system is likely to use one model to identify a face within an image, and a second model to recognize 
which face has been identified.  The interactions between AI components can be complex and difficult 
to comprehend, making it difficult to identify all the risks, and design tests that verify the system 
adequately. 
8.6 
Testing the Transparency, Interpretability and Explainability of AI-
Based Systems 
Information on how the system has been implemented may be provided by the system developers.  
This may include the sources of training data, how labelling was conducted, and how the system 
components have been designed.  When this information is not available, it can make the design of 
tests challenging.  For example, if training data information is not available, then identifying potential 
gaps in such data and testing the impact of those gaps, becomes difficult.  This situation can be 
compared to black-box and white-box testing, and has similar advantages and disadvantages.  
Transparency can be tested by comparing the information documented on the data and algorithm to 
the actual implementation and determining how closely they match. 
With ML, it can often be more difficult to explain the link between a specific input and a specific 
output, than with conventional systems.  This low level of explainability is primarily because the model 
generating the output is itself generated by code (the algorithm) and does not reflect the way humans 
think about a problem.  Different ML models provide different levels of explainability and should be 
selected based on the requirements for the system, which may include explainability and testability. 
One method to understand explainability is through the dynamic testing of the ML model when 
applying perturbations to the test data.  Methods exist for quantifying explainability in this manner and 
for providing visual explanations of it.  Some of these methods are model-agnostic, while others are 
specific to a particular type of model and require access to it.  Exploratory testing can also be used to 
better understand the relationship between the inputs and outputs of a model. 
The LIME method is model-agnostic and uses dynamically injected input perturbations and the 
analysis of outputs to provide testers with a view of the relationship between inputs and outputs.  This 
can be an effective method for providing model explainability.  However it is limited to providing 
possible reasons for the outputs, rather than a definitive reason, and is not applicable for all types of 
algorithms. 
The interpretability of an AI-based system is heavily dependent on who this applies to.  Different 
stakeholders may have different requirements in terms of how well they need to grasp the underlying 
technology. 
Measuring and testing the level of understanding for both interpretability and explainability can be 
challenging as stakeholders will vary in their levels of ability and may not agree.  In addition, 
identifying the profile of typical stakeholders may be difficult for many types of systems. Where 
performed, this testing typically takes the form of user surveys and/or questionnaires.

8.6.1 Hands-On Exercise: Model Explainability 
Use an appropriate tool to provide explainability based on the previously created model.  For 
example, for an image classification model or a text classification model, a model-agnostic method, 
such as LIME, may be appropriate.   
Students should use the tool to generate explanations of model decisions; in particular, how the 
features in the inputs influence the outputs.  
8.7 
Test Oracles for AI-Based Systems 
A major problem with the testing of AI-based systems can be the specification of expected results.  A 
test oracle is the source used to determine the expected result of a test . A challenge in 
determining expected results is known as the test oracle problem. 
With complex, non-deterministic or probabilistic systems, it can be difficult to establish a test oracle 
without knowing the “ground truth” (i.e., the actual result in the real world that the AI-based system is 
trying to predict).  This “ground truth” is distinct from a test oracle, in that a test oracle may not 
necessarily provide an expected value, but only a mechanism with which to determine whether the 
system is operating correctly or not.   
AI-based systems can evolve , and the testing of self-learning systems (see Section 
8.1) can also suffer from test oracle problems as they modify themselves and can thereby make it 
necessary to frequently update the functional expectations of the system. 
A further cause of difficulty in obtaining an effective test oracle is that in many cases, the correctness 
of the software behavior is subjective.  Virtual assistants (e.g., Siri and Alexa) are an example of this 
problem in that different user often have quite different expectations and may experience different 
results depending on their choice of words and clarity of speech. 
In some situations, it may be possible to define the expected result with limits or tolerances.  For 
example, the stopping point for an autonomous car could be defined as within a maximum distance of 
a specific point. In the context of expert systems, the determination of the expected results may be 
achieved by consulting an expert (noting that the expert’s opinion may still be wrong).  There are 
several important factors to consider in such circumstances:  
• 
Human experts vary in their levels of competence. Experts involved need to be at least as 
competent as the experts the system is intended to replace. 
• 
Experts may not agree with each other, even when presented with the same information. 
• 
Human experts may not approve of the automation of their judgement. In such cases their 
rating of potential outputs should be double-blind (i.e., neither the experts nor the evaluators 
of the outputs should know which ratings were automated). 
• 
Humans are more likely to caveat responses (e.g., with phrases like “I’m not sure, but...”).  If 
this kind of caveat is not available to the AI-based system, this should be considered when 
comparing the responses. 
Test techniques exist which can alleviate the test oracle problem, such as A/B testing (see Section 
9.4), back-to-back testing  and metamorphic testing . 
8.8 
Test Objectives and Acceptance Criteria 
Test objectives and acceptance criteria for a system need to be based on the perceived product risks.  
These risks can often be identified from an analysis of the required quality characteristics.  The quality

characteristics for an AI-based system include those traditionally considered in ISO/IEC 25010  
(i.e., functional suitability, performance efficiency, compatibility, usability, reliability, security, 
maintainability, and portability) but should also include a consideration of the following aspects: 
 
Aspect  
Acceptance Criteria 
Adaptability 
• 
Check the system still functions correctly and meets non-functional 
requirements when it adapts to a change in its environment.  This may 
be implemented as a form of automated regression testing. 
• 
Check the time the system takes to adapt to a change in its 
environment. 
• 
Check the resources used when the system adapts to a change in its 
environment. 
Flexibility 
• 
Consider how the system copes in contexts outside the initial 
specification. This may be implemented as a form of automated 
regression testing executed in the changed operational environment. 
• 
Check the time the system takes and/or the resources used to change 
itself to manage a new context. 
Evolution 
• 
Check how well the system learns from its own experience. 
• 
Check how well the system copes when the profile of data changes 
(i.e., concept drift). 
Autonomy 
• 
Check how the system responds when it is forced outside of the 
operational envelope in which it is expected to be fully autonomous. 
• 
Check whether the system can be “persuaded” to request human 
intervention when it should be fully autonomous. 
Transparency, 
interpretability and 
explainability 
• 
Check transparency by reviewing the ease of accessing the algorithm 
and dataset. 
• 
Check interpretability and explainability by questioning system users, or, 
if the actual system users are not available, people with a similar 
background. 
Freedom from 
inappropriate bias 
• 
Where systems are likely to be affected by bias, then this can be tested 
by using an independent bias-free test suite, or by using expert 
reviewers. 
• 
Compare the test results using external data such as census data in 
order to check for unwanted bias on inferred variables (external validity 
testing). 
Ethics 
• 
Check the system against a suitable checklist, such as the EC 
Assessment List for Trustworthy Artificial Intelligence , which 
supports the key requirements outlined by the Ethics Guidelines for 
Trustworthy Artificial Intelligence (AI) .

Aspect  
Acceptance Criteria 
Probabilistic 
systems and non-
deterministic 
systems 
• 
This cannot be evaluated with precise acceptance criteria. When 
working correctly, the system may return slightly different results for the 
same tests. 
Side-effects 
• 
Identify potentially harmful side-effects and attempt to generate tests 
that cause the system to exhibit these side-effects. 
Reward Hacking 
• 
Independent tests can identify reward hacking when these tests use a 
different means of measuring success compared to the intelligent agent 
being tested. 
Safety 
• 
This needs to carefully evaluated, perhaps in a virtual test environment 
. This could include attempts to force a system to 
cause itself harm. 
 
For ML systems, the required ML functional performance metrics for the ML model should be 
specified .

9.1 
Adversarial Attacks and Data Poisoning 
9.1.1 Adversarial Attacks 
An adversarial attack is where an attacker subtly perturbs valid inputs that are passed to the trained 
model to cause it to provide incorrect predictions.  These perturbed inputs, known as adversarial 
examples, were first noticed with spam filters, which could be tricked by slightly modifying a spam 
email without losing readability.  Recently, they have become more associated with image classifiers.  
By simply changing a few pixels which are invisible to the human eye, it is possible to persuade a 
neural network to change its image classification to a very different object and with a high degree of 
confidence.   
Adversarial examples are generally transferable , which means that an adversarial example 
which causes one ML system to fail will often cause another ML system to fail that is trained to 
perform the same task.  Even when the second ML system has been trained with different data and is 
based on different architectures, it is often still prone to failure with the same adversarial examples. 
White-box adversarial attacks are where the attacker knows which algorithm was used to train the 
model and also which model settings and parameters were used (there is a reasonable level of 
transparency).  The attacker uses this knowledge to generate adversarial examples by, for example, 
making small perturbations in inputs and monitoring which ones cause large changes to the model 
outputs.   
Black-box adversarial attacks involve the attacker exploring the model to determine its functionality 
and then building a duplicate model that provides similar functionality.  The attacker then uses a 
white-box approach to identify adversarial examples for this duplicate model.  As adversarial 
examples are generally transferable, the same adversarial examples will normally also work on the 
original model. 
If it is not possible to create a duplicate model, it may be possible to use high-volume automated 
testing to discover different adversarial examples and observe the results. 
Adversarial testing simply involves performing adversarial attacks with the purpose of identifying 
vulnerabilities so that preventative measures can be taken to protect against future failures.  Identified 
adversarial examples are added to the training data so that the model is trained to correctly recognize 
them. 
9.1.2  Data Poisoning 
Data poisoning attacks are where an attacker manipulates the training data to achieve one of two 
results.  The attacker may insert backdoors or neural network trojans to facilitate future intrusions, or 
more often, they will use corrupted training data (e.g., mislabeled data) to induce the trained model to 
provide incorrect predictions. 
Poisoning attacks may be targeted with the aim of causing the ML system to misclassify in specific 
situations.  They may also be indiscriminate, such as with a denial-of-service attack.  A well-known 
example of a poisoning attack was the corruption of the Microsoft Tay chatbot, whereby a relatively 
small number of harmful Twitter conversations trained the system through feedback to provide tainted 
conversations in the future.  A commonly used form of data poisoning attack uses the false reporting 
of millions of spam emails as not being spam in an attempt to skew spam filtering software.  An area 
of concern with data poisoning is the potential for public, widely used AI datasets to become 
poisoned.

Testing to detect data poisoning is possible using EDA, as poisoned data may show up as outliers. In 
addition, data acquisition policies can be reviewed to ensure the provenance of training data.  Where 
an operational ML system may be attacked by feeding it poisoned data, A/B testing  
could be used to check that the updated version of the system is still closely aligned with the previous 
version.  Alternatively, regression testing of an updated system using a trusted test suite may also 
determine if a system has been poisoned. 
9.2 
Pairwise Testing 
The number of parameters of interest for an AI-based system can be extremely high, especially when 
the system uses big data or interacts with the outside world, such as a self-driving car.  Exhaustive 
testing would require all possible combinations of these parameters set to all possible values to be 
tested.  However, since this would result in a practically infinite number of tests, test techniques are 
used to select a subset that can be run in the limited time available.   
Where it is possible to combine numerous parameters, each of which may have many discrete 
values, combinatorial testing can be applied to significantly reduction the number of test cases 
required, ideally without compromising the defect detection capability of the test suite.  There are 
several combinatorial testing techniques . However, in practice, pairwise testing 
is the most widely used technique because it is easy to understand, has ample tool support. In 
addition, research has shown that most defects are caused by interactions involving few parameters 
.  
In practice, even the use of pairwise testing can result in extensive test suites for some systems, and 
the use of automation and virtual test environments  often becomes necessary to 
allow the necessary number of tests to be run.  For example, when considering self-driving cars, high-
level test scenarios for system testing need to exercise both the different environments in which the 
cars are expected to operate and the various vehicle functions.  Thus, the parameters would need to 
include the range of environment constraints (e.g., road types and surfaces, weather and traffic 
conditions, and visibility) and the various self-driving functions (e.g., adaptive cruise control, lane 
keeping assistance, and lane change assistance).  In addition to these parameters, inputs from 
sensors could be considered at varying levels of effectiveness (e.g., inputs from a video camera will 
degrade as a journey progress and it gets dirtier).   
Research is currently unclear on the necessary level of rigor that would be required for the use of 
combinatorial testing with safety-critical AI-based systems such as self-driving cars. Even though 
pairwise testing may not be sufficient), it is known that the approach is effective at finding defects. 
9.2.1 Hands-On Exercise: Pairwise Testing 
For an implemented AI-based system with a minimum of five parameters and at least five hundred 
possible combinations, use a pairwise testing tool to identify a reduced set of pairwise combinations 
and execute tests for these combinations.  Compare the number of pairwise combinations tested with 
the number required if all theoretically possible combinations were to be tested. 
9.3 
Back-to-Back Testing 
One of the potential solutions to the test oracle problem  when testing AI-based 
systems is to use back-to-back testing.  This is also known as differential testing.  With back-to-back 
testing, an alternative version of the system is used as a pseudo-oracle and its outputs compared with 
the test results produced by the SUT.  The pseudo-oracle could be an existing system, or it could be 
developed by a different team, possibly on a different platform, with a different architecture and with a

different programming language. When testing the functional suitability (as opposed to non-functional 
requirements), the system used as a pseudo-oracle is not constrained to achieve the same non-
functional acceptance criteria as the SUT.  For example, it may not have to execute as quickly, in 
which case it can be far less expensive to build. 
In the context of ML, it is possible to use different frameworks, algorithms and model settings to 
create an ML pseudo-oracle. In some situations, it may also be possible to create a pseudo-oracle 
using conventional, non-AI, software. 
For pseudo-oracles to be effective in detecting defects, there should be no common software in both 
the pseudo-oracle and the SUT. Otherwise, it would be possible for the same defect in both to cause 
the two test results to match when both are defective.  With so much immature, reusable, open-
source AI software being used to develop AI-based systems, re-use of code between the pseudo-
oracle and the SUT can compromise the pseudo-oracle. Poor documentation of reusable AI solutions 
may also make it difficult for the testers to recognize that this problem is occurring. 
9.4 
A/B Testing 
A/B testing is a method where the response of two variants of the program (A and B) to the same 
inputs are compared with the purpose of determining which of the two variants is better.  It is a 
statistical testing approach which typically requires the comparison of test results from several test 
runs to determine the difference between the programs. 
A simple example of this method is where two promotional offers are emailed to a marketing list 
divided into two sets.  Half of the list gets offer A, half gets offer B, and the success of each offer 
helps decide which to use in the future.  Many e-commerce and web-based companies use A/B 
testing in production, diverting different consumers to different functionality, to help identify 
consumers’ preferences. 
A/B testing is one approach to solving the test oracle problem, where the existing system is used as a 
partial oracle.  A/B testing does not generate test cases and provides no guidance on how the tests 
should be designed, although operational inputs are often used in tests. 
A/B testing can be used to test updates to an AI-based system where there are agreed acceptance 
criteria, such as ML functional performance metrics, as described in Chapter 5.  Whenever the system 
is updated, A/B testing is used to check that the updated variant performs as well as, or better than 
the previous variant.  Such an approach can be used for a simple classifier, but can also be used for 
testing far more complex systems.  For example, an update to improve the effectiveness of a smart 
city transport routing system can also be tested using A/B testing (e.g., comparing average commute 
times for two variants of the system on consecutive weeks). 
A/B testing can also be used to test self-learning systems.  When the system makes a change, 
automated tests are run, and the resultant system characteristics are compared with those before the 
change was made.  If the system is improved, then the change is accepted, otherwise the system 
reverts to its previous state. 
One major difference between A/B testing and back-to-back testing relates to the use of A/B testing to 
compare two variants of the same system and the use of back-to-back testing to detect defects.  
9.5 
Metamorphic Testing (MT) 
Metamorphic testing  is a technique aimed at generating test cases which are based on a source 
test case that has passed.  One or more follow-up test cases are generated by changing 
(metamorphizing) the source test case based on a metamorphic relation (MR).  The MR is based on a

property of a required function of the test object, such that it describes how a change in a test case’s 
test inputs are reflected in the same test case’s expected results. 
For example, consider a program that determines the average of a set of numbers.  A source test 
case is generated comprising a set of numbers and an expected average, and the test case is run to 
confirm that it passes.  It is now possible to generate follow-up test cases based on what is known 
about the program’s average function.  Initially, the order of the numbers being averaged may simply 
be changed.  Given the average function, the expected result can be predicted to stay the same.  
Thus, a follow-up test case with the numbers in a different order can be generated without having to 
calculate the expected result.  With a large set of numbers, this could lead to generating a large 
number of different sets of numbers in which the same numbers are used in different sequences and 
each of them could be used to create a separate follow-up test case. All of these test cases would be 
based on the same source test case and have the same expected result.  
It is common to have MRs and follow-up test cases where the expected result is different from the 
original expected result of the source test case.  For example, using the same average function, an 
MR can be derived in which each element of the input set is multiplied by two.  The expected result 
for such a set is simply the original expected result multiplied by two.  Similarly, any other value could 
be used as a multiplier to potentially generate an infinite number of follow-up test cases based on this 
MR. 
MT can be used for most test objects and can be applied to both functional and non-functional testing 
(e.g., installability testing covers different target configurations where the installation parameters can 
be selected in different sequences).  It is particularly useful where the generation of expected results 
is problematic, due to the lack of an inexpensive test oracle.  This is the case with some AI-based 
systems that are based on the analysis of big data, or those where the testers are unclear on how the 
ML algorithm derives its predictions.  In the area of AI, MT has been used for testing image 
recognition, search engines, route optimization and voice recognition, among others.  
As explained above, MT can be based on a passed source test case, but it is also useful if it is not 
possible to verify that any source test case is correct.  This may be the case, for example, where the 
program implements a function which is too complex for a human tester to replicate and use as a test 
oracle, such as with some AI-based systems. In this situation, MT can be used to generate one or 
more test cases which, when run, will create a set of outputs where the relationships between the 
outputs can then be checked for validity.  With this form of MT, the individual tests are not known to 
be correct, but the relationships between them must hold true, so providing improved confidence in 
the program.  An example could be an AI-based actuarial program that predicts an age at death 
based on a large set of data, where it is known, for example, that if the number of cigarettes smoked 
is increased, the predicted age at death should decrease (or, at least, stay the same). 
MT is a relatively new test technique, first proposed in 1998. It differs from traditional test techniques 
in that the expected results of the follow-up test cases are not described in terms of absolute values, 
but are relative to the expected results in the source test case.  It is based on an easily understood 
concept, can be applied by testers with little experience of applying the technique but who understand 
the application domain, and has similar costs compared to traditional techniques.  It is also effective at 
revealing defects, with research showing that only three to six diverse MRs can reveal over 90% of 
the defects that could be detected using techniques based on a traditional test oracle .  It is 
possible to automatically generate follow-up test cases from well-specified MRs and a source test 
case. However, commercial tools are not currently available, although Google is already applying 
automated MT to test Android graphics drivers using the GraphicsFuzz tool, which has been open 
sourced .

9.5.1 Hands-On Exercise: Metamorphic Testing 
In this exercise, students will gain practical experience of the following: 
• 
Deriving several metamorphic relations (MRs) for a given AI-based application or program. 
These MRs should include some where the expected results of the source and follow-up test 
cases are the same and some where they are different. 
• 
Generating source test cases for the AI-based application or program. These do not have to 
be guaranteed to pass, but students should be reminded of the limitations of MT where there 
no such “gold standard” is available. 
• 
Using the derived MRs and generated source test cases to derive follow-up test cases. 
• 
Running the follow-up test cases. 
9.6 
Experience-Based Testing of AI-Based Systems 
Experience-based testing includes error guessing, exploratory testing, and checklist-based testing 
, all of which can be applied to the testing of AI-based systems. 
Error guessing is typically based on testers knowledge, typical developer errors, and failures in similar 
systems (or previous versions).  An example of error guessing applied to AI-based systems could be 
the use of knowledge about how ML systems have in the past failed due to the use of systemically 
biased training data. 
In exploratory testing, tests are designed, generated, and executed in an iterative manner, with the 
opportunity for later tests to be derived, based on the test results of earlier tests.  Exploratory testing 
is especially useful when there are poor specifications or test oracle problems, which is often the case 
for AI-based systems. As a result, exploratory testing is often used in this context and should be used 
to supplement the more systematic testing based on techniques, such as metamorphic testing .   
A tour is a metaphor used for a set of strategies and goals for testers to refer to when they perform 
exploratory testing organized around a special focus .  Typical tours for the exploratory testing of 
AI-based systems might focus on the concepts of bias, underfitting and overfitting in ML systems.  For 
example, a data tour might be applied to test the model. In this tour the tester could identify different 
types of data used for training, their distribution, their variations, their format and ranges, etc., and 
then use the data types to test the model. 
ML systems are highly dependent on the quality of training data, and the existing field of EDA is 
closely related to the exploratory testing approach.  EDA is where data are examined for patterns, 
relationships, trends and outliers. It involves the interactive, hypothesis-driven exploration of data and 
is described in  as “We explore data with expectations.  We revise our expectations based on 
what we see in the data.  And we iterate this process.” EDA typically requires tool support in two 
areas; for interaction with the data, to allow analysts to better understand complex data, and for data 
visualization, to allow them to easily display analysis results.  The use of exploratory techniques, 
primarily driven by data visualization, can help validate the ML algorithm being used, identify changes 
that result in efficient models, and leverage domain expertise . 
Google has a set of twenty-eight ML tests written as assertions, in the areas of data, model 
development, infrastructure and monitoring, which is used as a testing checklist within Google for ML 
systems .  The Google “ML test checklist” is presented here as published by Google: 
ML Data: 
1. 
Feature expectations are captured in a schema.

2. 
All features are beneficial. 
3. 
No feature’s cost is too much. 
4. 
Features adhere to metalevel requirements. 
5. 
The data pipeline has appropriate privacy controls. 
6. 
New features can be added quickly. 
7. 
All input feature code is tested. 
Model Development: 
1. 
Model specs are reviewed and submitted. 
2. 
Offline and online metrics correlate. 
3. 
All hyperparameters have been tuned. 
4. 
The impact of model staleness is known. 
5. 
A simpler model is not better. 
6. 
Model quality is sufficient on important data slices. 
7. 
The model is tested for considerations of inclusion. 
ML Infrastructure: 
1. 
Training is reproducible. 
2. 
Model specs are unit tested. 
3. 
The ML pipeline is integration tested. 
4. 
Model quality is validated before serving. 
5. 
The model is debuggable. 
6. 
Models are canaried before serving. 
7. 
Serving models can be rolled back 
Monitoring Tests: 
1. 
Dependency changes result in notification. 
2. 
Data invariants hold for inputs. 
3. 
Training and serving are not skewed. 
4. 
Models are not too stale. 
5. 
Models are numerically stable. 
6. 
Computing performance has not regressed. 
7. 
Prediction quality has not regressed. 
9.6.1 Hands-On Exercise: Exploratory Testing and Exploratory Data Analysis (EDA) 
For a selected model and dataset, students will perform a data tour, considering various types of data 
and their distribution for various parameters.   
Students will perform EDA on the data to identify missing data and/or potential bias in the data.

9.7 
Selecting Test Techniques for AI-Based Systems 
An AI-based system will typically include both AI and non-AI components.  The selection of test 
techniques for testing the non-AI components is generally the same as for any conventional testing.  
For the AI-based components, the choice may be more constrained.  For example, where a test 
oracle problem is perceived (i.e., generating expected results is difficult), then, based on the 
perceived risks, it is possible to mitigate this problem by the use of the following: 
• 
Back-to-back testing: This requires test cases to be available or generated and an equivalent 
system to act as a pseudo-oracle, which for regression testing can be a previous version of 
the system.  For effective detection of defects, an independently developed system may be 
required. 
• 
A/B testing: This often uses operational inputs as test cases and is normally used to compare 
two variants of the same system using statistical analysis.  A/B testing can be used to check 
for the data poisoning of a new variant, or for automated regression testing of a self-learning 
system. 
• 
Metamorphic testing: This can be used by inexperienced testers to cost-effectively find 
defects although they need to understand the application domain. MT is not suitable for 
providing definitive results as the expected results are not absolute, but, instead, relative to 
the source test cases.  Commercial tool support is not currently available, but many tests can 
be generated manually. 
Adversarial testing is typically appropriate for ML models where the mishandling of adversarial 
examples could have a significant impact, or where the system may be attacked.  Similarly, testing for 
data poisoning may be appropriate for ML systems where the system may be attacked. 
Where the AI-based systems are complex and have multiple parameters, pairwise testing is often 
appropriate. 
Experience-based testing is often suitable for testing AI-based systems, especially for consideration 
of the data used for training and operational data.  EDA can be used to validate the ML algorithm 
being used, identify efficiency improvements, and leverage domain expertise. Google have found that 
their ML test checklist is an effective approach for ML systems. 
In the specific area of neural networks, coverage of the network is often suitable for mission-critical 
systems, with some coverage criteria requiring more rigorous coverage than others.

10.1 Test Environments for AI-Based Systems 
AI-based systems can be used in a wide variety of operational environments, which means that the 
test environments are similarly diverse. Characteristics of AI-based systems that can cause the test 
environments to differ from those for conventional systems include:  
• 
Self-learning:  Self-learning systems, and some autonomous systems, are expected to adapt 
to changing operational environments that may not have been fully defined when the system 
was initially deployed .  As a result, defining test environments that can 
mimic these undefined environmental changes is inherently difficult and may require both 
imagination on the part of the testers and a level of randomness built into the test 
environment.   
• 
Autonomy: Autonomous systems are expected to respond to changes in their environment 
without human intervention, and also recognize situations where autonomy should be ceded 
back to human operators .  For some systems, identifying and then 
mimicking the circumstances for ceding autonomy may require the test environments to push 
the systems to extremes.  For some autonomous systems, their purpose is to work in 
environments that are hazardous, and setting up representative, hazardous test environments 
can be challenging. 
• 
Multi-agency: Where multi-agent AI-based systems are expected to work in concert with other 
AI-based systems, the test environment may need to incorporate a level of non-determinism 
so that it can mimic the non-determinism of the AI-based systems with which the SUT 
interacts.  
• 
Explainability: The nature of some AI-based systems can make it difficult to determine how 
the system made its decisions .  Where this is important to understand prior 
to deployment, the test environment may need to incorporate tools as a means of explaining 
how decisions are made. 
• 
Hardware: Some of the hardware used to host AI-based systems is specifically designed for 
this purpose, such as AI-specific processors .  The need to include such 
hardware in the test environment should be considered as part of the relevant test planning. 
• 
Big data: Where an AI-based system is expected to consume big data (e.g., high-volume, 
high-velocity and/or high-variety data), then setting this up as part of a test environment 
needs careful planning and implementation . 
10.2 Virtual Test Environments for Testing AI-Based Systems 
The use of a virtual test environment when testing an AI-based system brings the following benefits: 
• 
Dangerous scenarios: These can be tested without endangering the SUT, other interacting 
systems, including humans, or the operational environment (e.g., trees, buildings). 
• 
Unusual scenarios: These can be tested when it would otherwise be very time consuming or 
expensive to set up these scenarios for real operations (e.g., waiting for a rare event, such as 
a full solar eclipse or four buses entering the same road intersection simultaneously).  
Similarly, edge cases, which are difficult to create in the real world, can be created more 
easily, repeatedly and reproducibly in a virtual test environment. 
• 
Extreme scenarios: These can be tested when it would be expensive or impossible to set 
these up in reality (e.g., for a nuclear disaster or deep space exploration).

• 
Time-intensive scenarios: These can be tested in reduced timescales (e.g., several times per 
second) in a virtual environment. In contrast, these might take hours or days to set up and run 
in real time.  A further advantage is that multiple virtual test environments can be run in 
parallel. This typically takes place in the cloud and allows many scenarios to be run 
concurrently, which may not be possible using actual system hardware. 
• 
Observability and controllability: Virtual test environments provide far greater controllability of 
the test environment. For example, they can ensure that an unusual set of financial trading 
conditions is replicated. In addition, they give far better observability, as all digitally provided 
parts of the environment can be continuously monitored and recorded. 
• 
Availability: The simulation of hardware by virtual test environments allows systems to be 
tested with (simulated) hardware components that may otherwise not be available, perhaps 
because they have not been developed yet or are too expensive. 
Virtual test environments may be built specifically for a given system, may be generic, or may be 
developed to support specific application domains.  Both commercially and open-source virtual test 
environments are available to support the testing of AI-based systems.  Examples include: 
• 
Morse: The Modular Open Robots Simulation Engine, is a simulator for generic mobile robot 
simulation of single or multi robots, based on the Blender game engine .  
• 
AI Habitat: This is a simulation platform created by Facebook AI, designed to train embodied 
agents (such as virtual robots) in photo-realistic 3D environments .  
• 
DRIVE Constellation: This is an open and scalable platform for self-driving cars from NVIDIA. 
It is based on a cloud-based platform and is capable of generating billions of miles of 
autonomous vehicle testing . 
• 
MATLAB and Simulink: These provide the ability to prepare training data, produce ML models 
and simulate the execution of AI-based systems including the models using synthetic data 
.

11.1 AI Technologies for Testing 
Several AI technologies are listed in Section1.4, all of which can be used to support some specific 
aspect of software testing.  According to Harman , the software engineering community uses 
three broad areas of AI technologies: 
• 
Fuzzy logic and probabilistic methods: These involve the use of AI techniques to handle real 
world problems which are themselves probabilistic.  For example, AI can be used to analyze 
and predict possible system failures using Bayesian techniques. These may estimate the 
likelihood of components or functions failing, or reflect the potentially random nature of human 
interactions with the system. 
• 
Classification, learning and prediction: This can be used for various use cases such as 
predicting costs as part of project planning or of predicting defects. As embodied by ML, this 
area is used for many software testing tasks, including defect management (see Section 
11.2), defect prediction , and user interface testing . 
• 
Computational search and optimization techniques: These can be used to solve optimization 
problems using a computational search of potentially large and complex search spaces (e.g., 
using search algorithms).  Examples include generating test cases , 
identifying the smallest number of test cases that achieves a given coverage criterion, and 
optimizing regression test cases . 
The above categorization is necessarily broad, as there is considerable overlap between the testing 
tasks that can be implemented by AI and the different AI technologies.  It is also just one 
categorization, and others could be created that may be equally valid. 
11.1.1 Hands-On Exercise: The Use of AI in Testing 
As part of a discussion, students will identify testing activities and tasks that are currently impractical 
for implementation as AI.  These could include: 
• 
specifying test oracles. 
• 
communicating with stakeholders to clarify ambiguities and retrieve missing information. 
• 
suggesting improvements to the user experience. 
• 
challenging stakeholder assumptions and asking uncomfortable questions. 
• 
understanding user needs. 
A distinction should be drawn between weak AI, which could be used for some limited tasks, and 
general AI, which is currently not available . 
 
11.2 Using AI to Analyze Reported Defects 
Reported defects are usually categorized, prioritized, and any duplicates identified.  This activity is 
often referred to as defect triage or analysis and is intended to optimize the elapsed time spent in 
defect resolution.  AI can be used to support this activity in various ways, such as: 
• 
Categorization: NLP  can be used to analyze text within defect reports and extract 
topics, such as the area of affected functionality, that can then be provided alongside other 
meta data to clustering algorithms, such as k-nearest neighbors or support vector machines.

These algorithms can identify suitable defect categories and highlight similar or duplicate 
defects.  AI-based categorization is particularly useful for automated defect reporting systems 
(e.g., for Microsoft Windows and for Firefox) and on large projects with many software 
engineers. 
• 
Criticality: ML models trained on the features of the most critical defects can be used to 
identify those defects most likely to cause those system failures that account for a large 
percentage of reported defects . 
• 
Assignment: ML models can suggest which developers are best suited to fix particular 
defects, based on the defect content and previous developer assignments. 
11.3 Using AI for Test Case Generation 
The use of AI to generate tests can be a very effective technique for quickly create testing assets and 
maximizing coverage (e.g., code or requirements coverage).  The basis for generating these tests 
includes the source code, the user interface, and a machine-readable test model.  Some tools also 
base tests on the observation of the low-level behavior of the system through instrumentation or 
through log files . 
However, unless a test model that defines required behaviors is used as the basis of the tests, this 
form of test generation generally suffers from a test oracle problem because the AI-based tool does 
not know what the expected results should be for a given set of test data.  One solution is to use 
back-to-back testing if a suitable system is available to use as a pseudo-oracle .  
Alternatively, tests could be run with the expected result that neither an “application not responding” 
nor a system crash occurred, or other similar simple failure indicators. 
Research comparing AI-based test generation tools with similar non-AI fuzz testing tools shows that 
the AI-based tools can achieve equivalent levels of coverage and find more defects while reducing the 
average sequence of steps needed to cause a failure from an average of around 15,000 steps to 
around 100 steps. This makes debugging far easier . 
11.4 Using AI for the Optimization of Regression Test Suites 
As changes are made to a system, new tests are created, executed and become candidates for a 
regression test suite.  To prevent regression test suites from growing too large, they should be 
frequently optimized to select, prioritize and even augment test cases to create a more effective and 
efficient regression test suite.   
An AI-based tool can perform optimization of the regression test suite by analyzing, for example, the 
information from previous test results, associated defects, and the latest changes that have been 
made, such as features which are broken more frequently and which tests exercise code impacted by 
recent changes. 
Research shows that reductions of 50% in the size of a regression test suite can be achieved while 
still detecting most defects , and reductions of 40% in the test execution duration can be reached 
without significant reduction in fault detection for continuous integration testing . 
11.5 Using AI for Defect Prediction 
Defect prediction can be used to predict whether a defect is present, how many defects are present, 
or whether defects can be found. This capability depends on the sophistication of the tool used.

Results are normally used to prioritize the testing (e.g., more tests for those components where more 
defects are predicted). 
Defect prediction is typically based on source code metrics, process metrics and/or people and 
organizational metrics.  Due to there being so many potential factors to consider, determining the 
relationship between these factors and the likelihood of defects is beyond human capabilities. As a 
result, using an AI-based approach which typically uses ML is a necessity.  Defect prediction is most 
effective when based on prior experiences in a similar situation (e.g., with the same code base and/or 
the same developers). 
Defect prediction using ML has been successfully used in several different situations (e.g.,  and 
).  The best predictors have been found to be people and organizational measures rather than 
the more widely used source code metrics, such as lines of code and cyclomatic complexity . 
11.5.1 Hands-On Exercise: Build a Defect Prediction System 
Students will use a suitable dataset (e.g., including source code measures and corresponding defect 
data) to build a simple defect prediction model and use it to predict the likelihood of defects using 
source code measures from similar code. 
The model should use at least four features from the dataset, and the class should explore the results 
using several different features to highlight how the results change based on the selected features. 
11.6 Using AI for Testing User Interfaces 
11.6.1 Using AI to Test Through the Graphical User Interface (GUI) 
Testing through the GUI is the typical approach for manual testing (other than for component testing) 
and is often the starting point for test automation initiatives. The resultant tests emulate human 
interaction with the test object.  This scripted test automation can be implemented by applying a 
capture/playback approach, using either the actual coordinates of the user interface elements, or the 
software-defined objects/widgets of the interface.  However, this approach suffers several drawbacks 
with object identification, including sensitivity to interface changes, code changes, and platform 
changes. 
AI can be used to reduce the brittleness of this approach, by employing AI-based tools to identify the 
correct objects using various criteria (e.g., XPath, label, id, class, X/Y coordinates), and to choose the 
historically most stable identification criteria.  For example, the ID of a button in a particular area of 
the application may change with each release, and so the AI-based tool may assign a lower 
importance to this ID over time and place more reliance on other criteria. This approach classifies the 
objects in the user interface as matching the test, or not matching the test. 
Alternatively, visual testing uses image recognition to interact with GUI objects through the same 
interface as an actual user, and therefore does not need to access the underlying code and interface 
definitions.  This makes it completely non-intrusive and independent of the underlying technology. The 
scripts need only work through the visible user interface.  This approach allows the tester to create 
scripts that interact directly with the images, buttons and text fields on the screen in the same way as 
a human user, without being affected by the overall screen layout.  The use of image recognition in 
test automation can become restricted by the computing resources needed.  However, the availability 
of affordable AI that supports sophisticated image recognition now makes this approach possible for 
mainstream use.

11.6.2 Using AI to Test the GUI 
ML models can be used to determine the acceptability of user interface screens (e.g., by using 
heuristics and supervised learning).  Tools based on these models can identify incorrectly rendered 
elements, determine whether some objects are inaccessible or hard to detect, and detect various 
other issues with the visual appearance of the GUI. 
While image recognition is one form of computer vision algorithm, other forms of AI-based computer 
vision can be used to compare images (e.g., screenshots) to identify unintended changes to the 
layout, the size, position, color, font or other visible attributes of objects.  The results of these 
comparisons can be used to support regression testing to check that changes to the test object have 
not adversely affected the user interface. 
The technology for checking the acceptability of screens can be combined with comparison tools to 
create more sophisticated AI-based regression testing tools that are capable of advising whether 
detected user interface changes are likely to be acceptable to users, or whether these changes 
should be flagged for checking by a human.  Such AI-based tools can also be used to support testing 
for compatibility on different browsers, devices or platforms aimed at checking that the user interface 
for the same application works correctly on various browsers/devices/platforms.



Microsoft Azure Fundamentals

Chapter 10
Describe features and tools for managing and deploying Azure resources

Introduction

This module introduces you to features and tools for managing and deploying Azure resources. You learn about the Azure portal (a graphic interface for managing Azure resources), the command line, and scripting tools that help deploy or configure resources. You also learn about Azure services that help you manage your on-premises and multicloud environments from within Azure.

Learning objectives
After completing this module, you’ll be able to:

Describe the Azure portal.
Describe Azure Cloud Shell, including Azure CLI and Azure PowerShell.
Describe the purpose of Azure Arc.
Describe Azure Resource Manager (ARM), ARM templates, and Bicep.

Next unit: Describe tools for interacting with Azure

Describe tools for interacting with Azure

To get the most out of Azure, you need a way to interact with the Azure environment, the management groups, subscriptions, resource groups, resources, and so on. Azure provides multiple tools for managing your environment, including the:

Azure portal
Azure PowerShell
Azure Command Line Interface (CLI)
What is the Azure portal?
The Azure portal is a web-based, unified console that provides an alternative to command-line tools. With the Azure portal, you can manage your Azure subscription by using a graphical user interface. You can:

Build, manage, and monitor everything from simple web apps to complex cloud deployments
Create custom dashboards for an organized view of resources
Configure accessibility options for an optimal experience
The following video introduces you to the Azure portal:


The Azure portal is designed for resiliency and continuous availability. It maintains a presence in every Azure datacenter. This configuration makes the Azure portal resilient to individual datacenter failures and avoids network slowdowns by being close to users. The Azure portal updates continuously and requires no downtime for maintenance activities.

Azure Cloud Shell
Azure Cloud Shell is a browser-based shell tool that allows you to create, configure, and manage Azure resources using a shell. Azure Cloud Shell support both Azure PowerShell and the Azure Command Line Interface (CLI), which is a Bash shell.

You can access Azure Cloud Shell via the Azure portal by selecting the Cloud Shell icon:

Screenshot of the Azure portal with the Cloud Shell icon emphasized.

Azure Cloud Shell has several features that make it a unique offering to support you in managing Azure. Some of those features are:

It is a browser-based shell experience, with no local installation or configuration required.
It is authenticated to your Azure credentials, so when you log in it inherently knows who you are and what permissions you have.
You choose the shell you’re most familiar with; Azure Cloud Shell supports both Azure PowerShell and the Azure CLI (which uses Bash).
What is Azure PowerShell?
Azure PowerShell is a shell with which developers, DevOps, and IT professionals can run commands called command-lets (cmdlets). These commands call the Azure REST API to perform management tasks in Azure. Cmdlets can be run independently to handle one-off changes, or they may be combined to help orchestrate complex actions such as:

The routine setup, teardown, and maintenance of a single resource or multiple connected resources.
The deployment of an entire infrastructure, which might contain dozens or hundreds of resources, from imperative code.
Capturing the commands in a script makes the process repeatable and automatable.

In addition to be available via Azure Cloud Shell, you can install and configure Azure PowerShell on Windows, Linux, and Mac platforms.

What is the Azure CLI?
The Azure CLI is functionally equivalent to Azure PowerShell, with the primary difference being the syntax of commands. While Azure PowerShell uses PowerShell commands, the Azure CLI uses Bash commands.

The Azure CLI provides the same benefits of handling discrete tasks or orchestrating complex operations through code. It’s also installable on Windows, Linux, and Mac platforms, as well as through Azure Cloud Shell.

Due to the similarities in capabilities and access between Azure PowerShell and the Bash based Azure CLI, it mainly comes down to which language you’re most familiar with.

Next unit: Describe the purpose of Azure Arc

Describe the purpose of Azure Arc

Managing hybrid and multi-cloud environments can rapidly get complicated. Azure provides a host of tools to provision, configure, and monitor Azure resources. What about the on-premises resources in a hybrid configuration or the cloud resources in a multi-cloud configuration?

In utilizing Azure Resource Manager (ARM), Arc lets you extend your Azure compliance and monitoring to your hybrid and multi-cloud configurations. Azure Arc simplifies governance and management by delivering a consistent multi-cloud and on-premises management platform.

Azure Arc provides a centralized, unified way to:

Manage your entire environment together by projecting your existing non-Azure resources into ARM.
Manage multi-cloud and hybrid virtual machines, Kubernetes clusters, and databases as if they are running in Azure.
Use familiar Azure services and management capabilities, regardless of where they live.
Continue using traditional ITOps while introducing DevOps practices to support new cloud and native patterns in your environment.
Configure custom locations as an abstraction layer on top of Azure Arc-enabled Kubernetes clusters and cluster extensions.
What can Azure Arc do outside of Azure?
Currently, Azure Arc allows you to manage the following resource types hosted outside of Azure:

Servers
Kubernetes clusters
Azure data services
SQL Server
Virtual machines (preview)

Next unit: Describe Azure Resource Manager and Azure ARM templates

Describe Azure Resource Manager and Azure ARM templates

Azure Resource Manager (ARM) is the deployment and management service for Azure. It provides a management layer that enables you to create, update, and delete resources in your Azure account. Anytime you do anything with your Azure resources, ARM is involved.

When a user sends a request from any of the Azure tools, APIs, or SDKs, ARM receives the request. ARM authenticates and authorizes the request. Then, ARM sends the request to the Azure service, which takes the requested action. You see consistent results and capabilities in all the different tools because all requests are handled through the same API.

Azure Resource Manager benefits
With Azure Resource Manager, you can:

Manage your infrastructure through declarative templates rather than scripts. A Resource Manager template is a JSON file that defines what you want to deploy to Azure.
Deploy, manage, and monitor all the resources for your solution as a group, rather than handling these resources individually.
Re-deploy your solution throughout the development life-cycle and have confidence your resources are deployed in a consistent state.
Define the dependencies between resources, so they're deployed in the correct order.
Apply access control to all services because RBAC is natively integrated into the management platform.
Apply tags to resources to logically organize all the resources in your subscription.
Clarify your organization's billing by viewing costs for a group of resources that share the same tag.
The following video provides an overview of Azure Resource Manager.


Infrastructure as code
Infrastructure as code is a concept where you manage your infrastructure as lines of code. At an introductory level, it's things like using Azure Cloud Shell, Azure PowerShell, or the Azure CLI to manage and configure your resources. As you get more comfortable in the cloud, you can use the infrastructure as code concept to manage entire deployments using repeatable templates and configurations. ARM templates and Bicep are two examples of using infrastructure as code with the Azure Resource Manager to maintain your environment.

ARM templates
By using ARM templates, you can describe the resources you want to use in a declarative JSON format. With an ARM template, the deployment code is verified before any code is run. This ensures that the resources will be created and connected correctly. The template then orchestrates the creation of those resources in parallel. That is, if you need 50 instances of the same resource, all 50 instances are created at the same time.

Ultimately, the developer, DevOps professional, or IT professional needs only to define the desired state and configuration of each resource in the ARM template, and the template does the rest. Templates can even execute PowerShell and Bash scripts before or after the resource has been set up.

Benefits of using ARM templates
ARM templates provide many benefits when planning for deploying Azure resources. Some of those benefits include:

Declarative syntax: ARM templates allow you to create and deploy an entire Azure infrastructure declaratively. Declarative syntax means you declare what you want to deploy but don’t need to write the actual programming commands and sequence to deploy the resources.
Repeatable results: Repeatedly deploy your infrastructure throughout the development lifecycle and have confidence your resources are deployed in a consistent manner. You can use the same ARM template to deploy multiple dev/test environments, knowing that all the environments are the same.
Orchestration: You don't have to worry about the complexities of ordering operations. Azure Resource Manager orchestrates the deployment of interdependent resources, so they're created in the correct order. When possible, Azure Resource Manager deploys resources in parallel, so your deployments finish faster than serial deployments. You deploy the template through one command, rather than through multiple imperative commands.
Modular files: You can break your templates into smaller, reusable components and link them together at deployment time. You can also nest one template inside another template. For example, you could create a template for a VM stack, and then nest that template inside of templates that deploy entire environments, and that VM stack will consistently be deployed in each of the environment templates.
Extensibility: With deployment scripts, you can add PowerShell or Bash scripts to your templates. The deployment scripts extend your ability to set up resources during deployment. A script can be included in the template or stored in an external source and referenced in the template. Deployment scripts give you the ability to complete your end-to-end environment setup in a single ARM template.
Bicep
Bicep is a language that uses declarative syntax to deploy Azure resources. A Bicep file defines the infrastructure and configuration. Then, ARM deploys that environment based on your Bicep file. While similar to an ARM template, which is written in JSON, Bicep files tend to use a simpler, more concise style.

Some benefits of Bicep are:

Support for all resource types and API versions: Bicep immediately supports all preview and GA versions for Azure services. As soon as a resource provider introduces new resource types and API versions, you can use them in your Bicep file. You don't have to wait for tools to be updated before using the new services.
Simple syntax: When compared to the equivalent JSON template, Bicep files are more concise and easier to read. Bicep requires no previous knowledge of programming languages. Bicep syntax is declarative and specifies which resources and resource properties you want to deploy.
Repeatable results: Repeatedly deploy your infrastructure throughout the development lifecycle and have confidence your resources are deployed in a consistent manner. Bicep files are idempotent, which means you can deploy the same file many times and get the same resource types in the same state. You can develop one file that represents the desired state, rather than developing lots of separate files to represent updates.
Orchestration: You don't have to worry about the complexities of ordering operations. Resource Manager orchestrates the deployment of interdependent resources so they're created in the correct order. When possible, Resource Manager deploys resources in parallel so your deployments finish faster than serial deployments. You deploy the file through one command, rather than through multiple imperative commands.
Modularity: You can break your Bicep code into manageable parts by using modules. The module deploys a set of related resources. Modules enable you to reuse code and simplify development. Add the module to a Bicep file anytime you need to deploy those resources.

Next unit: Knowledge check

Knowledge check

Choose the best response for each question. Then select Check your answers.

Check your knowledge

1. What service helps you manage your Azure, on-premises, and multicloud environments? 

A- Azure Arc

B- Azure Policy

C- Azure Cloud Manager

2. What two components could you use to implement a “infrastructure as code” deployment? 

A- Bicep and ARM Templates

B- Azure Policy and Azure Arc

C- Azure Monitor and Azure Arc

Microsoft Azure Fundamentals

Chapter 11
Describe monitoring tools in Azure

Introduction

In this module, you’ll be introduced to tools that help you monitor your environment and applications, both in Azure and in on-premises or multicloud environments.

Learning objectives
After completing this module, you’ll be able to:

Describe the purpose of Azure Advisor.
Describe Azure Service Health.
Describe Azure Monitor, including Azure Log Analytics, Azure Monitor Alerts, and Application Insights.

Next unit: Describe the purpose of Azure Advisor

Describe the purpose of Azure Advisor

Azure Advisor evaluates your Azure resources and makes recommendations to help improve reliability, security, and performance, achieve operational excellence, and reduce costs. Azure Advisor is designed to help you save time on cloud optimization. The recommendation service includes suggested actions you can take right away, postpone, or dismiss.

The recommendations are available via the Azure portal and the API, and you can set up notifications to alert you to new recommendations.

When you're in the Azure portal, the Advisor dashboard displays personalized recommendations for all your subscriptions. You can use filters to select recommendations for specific subscriptions, resource groups, or services. The recommendations are divided into five categories:

Reliability is used to ensure and improve the continuity of your business-critical applications.
Security is used to detect threats and vulnerabilities that might lead to security breaches.
Performance is used to improve the speed of your applications.
Operational Excellence is used to help you achieve process and workflow efficiency, resource manageability, and deployment best practices.
Cost is used to optimize and reduce your overall Azure spending.
The following image shows the Azure Advisor dashboard.

Screenshot of the Azure Advisor dashboard with boxes for the main areas of recommendations.

Next unit: Describe Azure Service Health

Describe Azure Service Health

Microsoft Azure provides a global cloud solution to help you manage your infrastructure needs, reach your customers, innovate, and adapt rapidly. Knowing the status of the global Azure infrastructure and your individual resources could seem like a daunting task. Azure Service Health helps you keep track of Azure resource, both your specifically deployed resources and the overall status of Azure. Azure service health does this by combining three different Azure services:

Azure Status is a broad picture of the status of Azure globally. Azure status informs you of service outages in Azure on the Azure Status page. The page is a global view of the health of all Azure services across all Azure regions. It’s a good reference for incidents with widespread impact.
Service Health provides a narrower view of Azure services and regions. It focuses on the Azure services and regions you're using. This is the best place to look for service impacting communications about outages, planned maintenance activities, and other health advisories because the authenticated Service Health experience knows which services and resources you currently use. You can even set up Service Health alerts to notify you when service issues, planned maintenance, or other changes may affect the Azure services and regions you use.
Resource Health is a tailored view of your actual Azure resources. It provides information about the health of your individual cloud resources, such as a specific virtual machine instance. Using Azure Monitor, you can also configure alerts to notify you of availability changes to your cloud resources.
By using Azure status, Service health, and Resource health, Azure Service Health gives you a complete view of your Azure environment-all the way from the global status of Azure services and regions down to specific resources. Additionally, historical alerts are stored and accessible for later review. Something you initially thought was a simple anomaly that turned into a trend, can readily be reviewed and investigated thanks to the historical alerts.

Finally, in the event that a workload you’re running is impacted by an event, Azure Service Health provides links to support.

Next unit: Describe Azure Monitor

Describe Azure Monitor

Azure Monitor is a platform for collecting data on your resources, analyzing that data, visualizing the information, and even acting on the results. Azure Monitor can monitor Azure resources, your on-premises resources, and even multi-cloud resources like virtual machines hosted with a different cloud provider.

The following diagram illustrates just how comprehensive Azure Monitor is:

An illustration showing the flow of information that Azure Monitor uses to provide monitoring and data visualization.

On the left is a list of the sources of logging and metric data that can be collected at every layer in your application architecture, from application to operating system and network.

In the center, the logging and metric data are stored in central repositories.

On the right, the data is used in several ways. You can view real-time and historical performance across each layer of your architecture or aggregated and detailed information. The data is displayed at different levels for different audiences. You can view high-level reports on the Azure Monitor Dashboard or create custom views by using Power BI and Kusto queries.

Additionally, you can use the data to help you react to critical events in real time, through alerts delivered to teams via SMS, email, and so on. Or you can use thresholds to trigger autoscaling functionality to scale to meet the demand.

Azure Log Analytics
Azure Log Analytics is the tool in the Azure portal where you’ll write and run log queries on the data gathered by Azure Monitor. Log Analytics is a robust tool that supports both simple, complex queries, and data analysis. You can write a simple query that returns a set of records and then use features of Log Analytics to sort, filter, and analyze the records. You can write an advanced query to perform statistical analysis and visualize the results in a chart to identify a particular trend. Whether you work with the results of your queries interactively or use them with other Azure Monitor features such as log query alerts or workbooks, Log Analytics is the tool that you're going to use to write and test those queries.

Azure Monitor Alerts
Azure Monitor Alerts are an automated way to stay informed when Azure Monitor detects a threshold being crossed. You set the alert conditions, the notification actions, and then Azure Monitor Alerts notifies when an alert is triggered. Depending on your configuration, Azure Monitor Alerts can also attempt corrective action.

Screenshot of Azure Monitor Alerts showing total alerts, and then the alerts grouped by severity.

Alerts can be set up to monitor the logs and trigger on certain log events, or they can be set to monitor metrics and trigger when certain metrics are crossed. For example, you could set a metric-based alert up to notify you when the CPU usage on a virtual machine exceeded 80%. Alert rules based on metrics provide near real time alerts based on numeric values. Rules based on logs allow for complex logic across data from multiple sources.

Azure Monitor Alerts use action groups to configure who to notify and what action to take. An action group is simply a collection of notification and action preferences that you associate with one or multiple alerts. Azure Monitor, Service Health, and Azure Advisor all use actions groups to notify you when an alert has been triggered.

Application Insights
Application Insights, an Azure Monitor feature, monitors your web applications. Application Insights is capable of monitoring applications that are running in Azure, on-premises, or in a different cloud environment.

There are two ways to configure Application Insights to help monitor your application. You can either install an SDK in your application, or you can use the Application Insights agent. The Application Insights agent is supported in C#.NET, VB.NET, Java, JavaScript, Node.js, and Python.

Once Application Insights is up and running, you can use it to monitor a broad array of information, such as:

Request rates, response times, and failure rates
Dependency rates, response times, and failure rates, to show whether external services are slowing down performance
Page views and load performance reported by users' browsers
AJAX calls from web pages, including rates, response times, and failure rates
User and session counts
Performance counters from Windows or Linux server machines, such as CPU, memory, and network usage
Not only does Application Insights help you monitor the performance of your application, but you can also configure it to periodically send synthetic requests to your application, allowing you to check the status and monitor your application even during periods of low activity.

Next unit: Knowledge check

Knowledge check

Choose the best response for each question. Then select Check your answers.

Check your knowledge

1. Which is not one of the recommendation categories for Azure Advisor? 

A- Reliability

B- Capacity

C- Cost

2. You receive an email notification that virtual machines (VMs) in an Azure region where you have VMs deployed is experiencing an outage. Which component of Azure Service Health will let you know if your application is impacted? 

A- Azure status

B- Service health

C- Resource health
Microsoft Azure Fundamentals

Chapter 3
Describe cloud service types

Introduction

In this module, you’ll be introduced to cloud service types. You’ll learn how each cloud service type determines the flexibility you’ll have with managing and configuring resources. You'll understand how the shared responsibility model applies to each cloud service type, and about various use cases for each cloud service type.

Learning objectives
After completing this module, you’ll be able to:

Describe infrastructure as a service (IaaS).
Describe platform as a service (PaaS).
Describe software as a service (SaaS).
Identify appropriate use cases for each cloud service (IaaS, PaaS, SaaS).

Next unit: Describe Infrastructure as a Service

Describe Infrastructure as a Service

Infrastructure as a service (IaaS) is the most flexible category of cloud services, as it provides you the maximum amount of control for your cloud resources. In an IaaS model, the cloud provider is responsible for maintaining the hardware, network connectivity (to the internet), and physical security. You’re responsible for everything else: operating system installation, configuration, and maintenance; network configuration; database and storage configuration; and so on. With IaaS, you’re essentially renting the hardware in a cloud datacenter, but what you do with that hardware is up to you.

Shared responsibility model
The shared responsibility model applies to all the cloud service types. IaaS places the largest share of responsibility with you. The cloud provider is responsible for maintaining the physical infrastructure and its access to the internet. You’re responsible for installation and configuration, patching and updates, and security.

Diagram showing the responsibilities of the shared responsibility model.

Scenarios
Some common scenarios where IaaS might make sense include:

Lift-and-shift migration: You’re setting up cloud resources similar to your on-prem datacenter, and then simply moving the things running on-prem to running on the IaaS infrastructure.
Testing and development: You have established configurations for development and test environments that you need to rapidly replicate. You can start up or shut down the different environments rapidly with an IaaS structure, while maintaining complete control.

Next unit: Describe Platform as a Service

Describe Platform as a Service

Platform as a service (PaaS) is a middle ground between renting space in a datacenter (infrastructure as a service) and paying for a complete and deployed solution (software as a service). In a PaaS environment, the cloud provider maintains the physical infrastructure, physical security, and connection to the internet. They also maintain the operating systems, middleware, development tools, and business intelligence services that make up a cloud solution. In a PaaS scenario, you don't have to worry about the licensing or patching for operating systems and databases.

PaaS is well suited to provide a complete development environment without the headache of maintaining all the development infrastructure.

Shared responsibility model
The shared responsibility model applies to all the cloud service types. PaaS splits the responsibility between you and the cloud provider. The cloud provider is responsible for maintaining the physical infrastructure and its access to the internet, just like in IaaS. In the PaaS model, the cloud provider will also maintain the operating systems, databases, and development tools. Think of PaaS like using a domain joined machine: IT maintains the device with regular updates, patches, and refreshes.

Depending on the configuration, you or the cloud provider may be responsible for networking settings and connectivity within your cloud environment, network and application security, and the directory infrastructure.

Diagram showing the responsibilities of the shared responsibility model.

Scenarios
Some common scenarios where PaaS might make sense include:

Development framework: PaaS provides a framework that developers can build upon to develop or customize cloud-based applications. Similar to the way you create an Excel macro, PaaS lets developers create applications using built-in software components. Cloud features such as scalability, high-availability, and multi-tenant capability are included, reducing the amount of coding that developers must do.
Analytics or business intelligence: Tools provided as a service with PaaS allow organizations to analyze and mine their data, finding insights and patterns and predicting outcomes to improve forecasting, product design decisions, investment returns, and other business decisions.

Next unit: Describe Software as a Service

Describe Software as a Service

Software as a service (SaaS) is the most complete cloud service model from a product perspective. With SaaS, you’re essentially renting or using a fully developed application. Email, financial software, messaging applications, and connectivity software are all common examples of a SaaS implementation.

While the SaaS model may be the least flexible, it’s also the easiest to get up and running. It requires the least amount of technical knowledge or expertise to fully employ.

Shared responsibility model
The shared responsibility model applies to all the cloud service types. SaaS is the model that places the most responsibility with the cloud provider and the least responsibility with the user. In a SaaS environment you’re responsible for the data that you put into the system, the devices that you allow to connect to the system, and the users that have access. Nearly everything else falls to the cloud provider. The cloud provider is responsible for physical security of the datacenters, power, network connectivity, and application development and patching.

Diagram showing the responsibilities of the shared responsibility model.

Scenarios
Some common scenarios for SaaS are:

Email and messaging.
Business productivity applications.
Finance and expense tracking.

Next unit: Knowledge check

Choose the best response for each question. Then select Check your answers.

Check your knowledge

1. Which cloud service type is most suited to a lift and shift migration from an on-premises datacenter to a cloud deployment? 

A- Infrastructure as a Service (IaaS)

B- Platform as a Service (PaaS)

C- Software as a Service (SaaS)

2. What type of cloud service type would a Finance and Expense tracking solution typically be in? 

A- Infrastructure as a Service (IaaS)

B- Platform as a Service (PaaS)

C- Software as a Service (SaaS)
Microsoft Azure Fundamentals

Chapter 4
Describe the core architectural components of Azure

Introduction

In this module, you’ll be introduced to the core architectural components of Azure. You’ll learn about the physical organization of Azure: datacenters, availability zones, and regions; and you’ll learn about the organizational structure of Azure: resources and resource groups, subscriptions, and management groups.

Learning objectives
After completing this module, you’ll be able to:

Describe Azure regions, region pairs, and sovereign regions.
Describe Availability Zones.
Describe Azure datacenters.
Describe Azure resources and Resource Groups.
Describe subscriptions.
Describe management groups.
Describe the hierarchy of resource groups, subscriptions, and management groups.

Next unit: What is Microsoft Azure

What is Microsoft Azure
Azure is a continually expanding set of cloud services that help you meet current and future business challenges. Azure gives you the freedom to build, manage, and deploy applications on a massive global network using your favorite tools and frameworks.

What does Azure offer?
Limitless innovation. Build intelligent apps and solutions with advanced technology, tools, and services to take your business to the next level. Seamlessly unify your technology to simplify platform management and to deliver innovations efficiently and securely on a trusted cloud.

Bring ideas to life: Build on a trusted platform to advance your organization with industry-leading AI and cloud services.
Seamlessly unify: Efficiently manage all your infrastructure, data, analytics, and AI solutions across an integrated platform.
Innovate on trust: Rely on trusted technology from a partner who's dedicated to security and responsibility.
What can I do with Azure?
Azure provides more than 100 services that enable you to do everything from running your existing applications on virtual machines to exploring new software paradigms, such as intelligent bots and mixed reality.

Many teams start exploring the cloud by moving their existing applications to virtual machines (VMs) that run in Azure. Migrating your existing apps to VMs is a good start, but the cloud is much more than a different place to run your VMs.

For example, Azure provides artificial intelligence (AI) and machine-learning (ML) services that can naturally communicate with your users through vision, hearing, and speech. It also provides storage solutions that dynamically grow to accommodate massive amounts of data. Azure services enable solutions that aren't feasible without the power of the cloud.

Next unit: Get started with Azure accounts

Get started with Azure accounts

To create and use Azure services, you need an Azure subscription. When you're completing Learn modules, most of the time a temporary subscription is created for you, which runs in an environment called the Learn sandbox. When you're working with your own applications and business needs, you need to create an Azure account, and a subscription will be created for you. After you've created an Azure account, you're free to create additional subscriptions. For example, your company might use a single Azure account for your business and separate subscriptions for development, marketing, and sales departments. After you've created an Azure subscription, you can start creating Azure resources within each subscription.

Diagram showing the different levels of account scope.

If you're new to Azure, you can sign up for a free account on the Azure website to start exploring at no cost to you. When you're ready, you can choose to upgrade your free account. You can also create a new subscription that enables you to start paying for Azure services you need beyond the limits of a free account.

Create an Azure account
You can purchase Azure access directly from Microsoft by signing up on the Azure website or through a Microsoft representative. You can also purchase Azure access through a Microsoft partner. Cloud Solution Provider partners offer a range of complete managed-cloud solutions for Azure.


What is the Azure free account?
The Azure free account includes:

Free access to popular Azure products for 12 months.
A credit to use for the first 30 days.
Access to more than 25 products that are always free.
The Azure free account is an excellent way for new users to get started and explore. To sign up, you need a phone number, a credit card, and a Microsoft or GitHub account. The credit card information is used for identity verification only. You won't be charged for any services until you upgrade to a paid subscription.

What is the Azure free student account?
The Azure free student account offer includes:

Free access to certain Azure services for 12 months.
A credit to use in the first 12 months.
Free access to certain software developer tools.
The Azure free student account is an offer for students that gives $100 credit and free developer tools. Also, you can sign up without a credit card.

What is the Microsoft Learn sandbox?
Many of the Learn exercises use a technology called the sandbox, which creates a temporary subscription that's added to your Azure account. This temporary subscription allows you to create Azure resources during a Learn module. Learn automatically cleans up the temporary resources for you after you've completed the module.

When you're completing a Learn module, you're welcome to use your personal subscription to complete the exercises in a module. However, the sandbox is the preferred method to use because it allows you to create and test Azure resources at no cost to you.

Next unit: Exercise - Explore the Learn sandbox

This is pratical and should be done on Microsoft Azure portal

Next unit: Describe Azure physical infrastructure

Describe Azure physical infrastructure

Throughout your journey with Microsoft Azure, you’ll hear and use terms like Regions, Availability Zones, Resources, Subscriptions, and more. This module focuses on the core architectural components of Azure. The core architectural components of Azure may be broken down into two main groupings: the physical infrastructure, and the management infrastructure.

Physical infrastructure
The physical infrastructure for Azure starts with datacenters. Conceptually, the datacenters are the same as large corporate datacenters. They’re facilities with resources arranged in racks, with dedicated power, cooling, and networking infrastructure.

As a global cloud provider, Azure has datacenters around the world. However, these individual datacenters aren’t directly accessible. Datacenters are grouped into Azure Regions or Azure Availability Zones that are designed to help you achieve resiliency and reliability for your business-critical workloads.

The Global infrastructure site gives you a chance to interactively explore the underlying Azure infrastructure.

Regions
A region is a geographical area on the planet that contains at least one, but potentially multiple datacenters that are nearby and networked together with a low-latency network. Azure intelligently assigns and controls the resources within each region to ensure workloads are appropriately balanced.

When you deploy a resource in Azure, you'll often need to choose the region where you want your resource deployed.

 Note

Some services or virtual machine (VM) features are only available in certain regions, such as specific VM sizes or storage types. There are also some global Azure services that don't require you to select a particular region, such as Microsoft Entra ID, Azure Traffic Manager, and Azure DNS.

Availability Zones
Availability zones are physically separate datacenters within an Azure region. Each availability zone is made up of one or more datacenters equipped with independent power, cooling, and networking. An availability zone is set up to be an isolation boundary. If one zone goes down, the other continues working. Availability zones are connected through high-speed, private fiber-optic networks.

Diagram showing three datacenters connected in a single Azure region representing an availability zone.

 Important

To ensure resiliency, a minimum of three separate availability zones are present in all availability zone-enabled regions. However, not all Azure Regions currently support availability zones.

Use availability zones in your apps
You want to ensure your services and data are redundant so you can protect your information in case of failure. When you host your infrastructure, setting up your own redundancy requires that you create duplicate hardware environments. Azure can help make your app highly available through availability zones.

You can use availability zones to run mission-critical applications and build high-availability into your application architecture by co-locating your compute, storage, networking, and data resources within an availability zone and replicating in other availability zones. Keep in mind that there could be a cost to duplicating your services and transferring data between availability zones.

Availability zones are primarily for VMs, managed disks, load balancers, and SQL databases. Azure services that support availability zones fall into three categories:

Zonal services: You pin the resource to a specific zone (for example, VMs, managed disks, IP addresses).
Zone-redundant services: The platform replicates automatically across zones (for example, zone-redundant storage, SQL Database).
Non-regional services: Services are always available from Azure geographies and are resilient to zone-wide outages as well as region-wide outages.
Even with the additional resiliency that availability zones provide, it’s possible that an event could be so large that it impacts multiple availability zones in a single region. To provide even further resilience, Azure has Region Pairs.

Region pairs
Most Azure regions are paired with another region within the same geography (such as US, Europe, or Asia) at least 300 miles away. This approach allows for the replication of resources across a geography that helps reduce the likelihood of interruptions because of events such as natural disasters, civil unrest, power outages, or physical network outages that affect an entire region. For example, if a region in a pair was affected by a natural disaster, services would automatically fail over to the other region in its region pair.

 Important

Not all Azure services automatically replicate data or automatically fall back from a failed region to cross-replicate to another enabled region. In these scenarios, recovery and replication must be configured by the customer.

Examples of region pairs in Azure are West US paired with East US and South-East Asia paired with East Asia. Because the pair of regions are directly connected and far enough apart to be isolated from regional disasters, you can use them to provide reliable services and data redundancy.

Diagram showing the relationship between geography, region pair, region, and availability zone.

Additional advantages of region pairs:
If an extensive Azure outage occurs, one region out of every pair is prioritized to make sure at least one is restored as quickly as possible for applications hosted in that region pair.
Planned Azure updates are rolled out to paired regions one region at a time to minimize downtime and risk of application outage.
Data continues to reside within the same geography as its pair (except for Brazil South) for tax- and law-enforcement jurisdiction purposes.
 Important

Most regions are paired in two directions, meaning they are the backup for the region that provides a backup for them (West US and East US back each other up). However, some regions, such as West India and Brazil South, are paired in only one direction. In a one-direction pairing, the Primary region does not provide backup for its secondary region. So, even though West India’s secondary region is South India, South India does not rely on West India. West India's secondary region is South India, but South India's secondary region is Central India. Brazil South is unique because it's paired with a region outside of its geography. Brazil South's secondary region is South Central US. The secondary region of South Central US isn't Brazil South.

Sovereign Regions
In addition to regular regions, Azure also has sovereign regions. Sovereign regions are instances of Azure that are isolated from the main instance of Azure. You may need to use a sovereign region for compliance or legal purposes.

Azure sovereign regions include:

US DoD Central, US Gov Virginia, US Gov Iowa and more: These regions are physical and logical network-isolated instances of Azure for U.S. government agencies and partners. These datacenters are operated by screened U.S. personnel and include additional compliance certifications.
China East, China North, and more: These regions are available through a unique partnership between Microsoft and 21Vianet, whereby Microsoft doesn't directly maintain the datacenters.

Next unit: Describe Azure management infrastructure

Describe Azure management infrastructure

The management infrastructure includes Azure resources and resource groups, subscriptions, and accounts. Understanding the hierarchical organization will help you plan your projects and products within Azure.

Azure resources and resource groups
A resource is the basic building block of Azure. Anything you create, provision, deploy, etc. is a resource. Virtual Machines (VMs), virtual networks, databases, cognitive services, etc. are all considered resources within Azure.

Diagram showing a resource group box with a function, VM, database, and app included.

Resource groups are simply groupings of resources. When you create a resource, you’re required to place it into a resource group. While a resource group can contain many resources, a single resource can only be in one resource group at a time. Some resources may be moved between resource groups, but when you move a resource to a new group, it will no longer be associated with the former group. Additionally, resource groups can't be nested, meaning you can’t put resource group B inside of resource group A.

Resource groups provide a convenient way to group resources together. When you apply an action to a resource group, that action will apply to all the resources within the resource group. If you delete a resource group, all the resources will be deleted. If you grant or deny access to a resource group, you’ve granted or denied access to all the resources within the resource group.

When you’re provisioning resources, it’s good to think about the resource group structure that best suits your needs.

For example, if you’re setting up a temporary dev environment, grouping all the resources together means you can deprovision all of the associated resources at once by deleting the resource group. If you’re provisioning compute resources that will need three different access schemas, it may be best to group resources based on the access schema, and then assign access at the resource group level.

There aren’t hard rules about how you use resource groups, so consider how to set up your resource groups to maximize their usefulness for you.

Azure subscriptions
In Azure, subscriptions are a unit of management, billing, and scale. Similar to how resource groups are a way to logically organize resources, subscriptions allow you to logically organize your resource groups and facilitate billing.

Diagram showing Azure subscriptions using authentication and authorization to access Azure accounts.

Using Azure requires an Azure subscription. A subscription provides you with authenticated and authorized access to Azure products and services. It also allows you to provision resources. An Azure subscription links to an Azure account, which is an identity in Microsoft Entra ID or in a directory that Microsoft Entra ID trusts.

An account can have multiple subscriptions, but it’s only required to have one. In a multi-subscription account, you can use the subscriptions to configure different billing models and apply different access-management policies. You can use Azure subscriptions to define boundaries around Azure products, services, and resources. There are two types of subscription boundaries that you can use:

Billing boundary: This subscription type determines how an Azure account is billed for using Azure. You can create multiple subscriptions for different types of billing requirements. Azure generates separate billing reports and invoices for each subscription so that you can organize and manage costs.
Access control boundary: Azure applies access-management policies at the subscription level, and you can create separate subscriptions to reflect different organizational structures. An example is that within a business, you have different departments to which you apply distinct Azure subscription policies. This billing model allows you to manage and control access to the resources that users provision with specific subscriptions.
Create additional Azure subscriptions
Similar to using resource groups to separate resources by function or access, you might want to create additional subscriptions for resource or billing management purposes. For example, you might choose to create additional subscriptions to separate:

Environments: You can choose to create subscriptions to set up separate environments for development and testing, security, or to isolate data for compliance reasons. This design is particularly useful because resource access control occurs at the subscription level.
Organizational structures: You can create subscriptions to reflect different organizational structures. For example, you could limit one team to lower-cost resources, while allowing the IT department a full range. This design allows you to manage and control access to the resources that users provision within each subscription.
Billing: You can create additional subscriptions for billing purposes. Because costs are first aggregated at the subscription level, you might want to create subscriptions to manage and track costs based on your needs. For instance, you might want to create one subscription for your production workloads and another subscription for your development and testing workloads.
Azure management groups
The final piece is the management group. Resources are gathered into resource groups, and resource groups are gathered into subscriptions. If you’re just starting in Azure that might seem like enough hierarchy to keep things organized. But imagine if you’re dealing with multiple applications, multiple development teams, in multiple geographies.

If you have many subscriptions, you might need a way to efficiently manage access, policies, and compliance for those subscriptions. Azure management groups provide a level of scope above subscriptions. You organize subscriptions into containers called management groups and apply governance conditions to the management groups. All subscriptions within a management group automatically inherit the conditions applied to the management group, the same way that resource groups inherit settings from subscriptions and resources inherit from resource groups. Management groups give you enterprise-grade management at a large scale, no matter what type of subscriptions you might have. Management groups can be nested.

Management group, subscriptions, and resource group hierarchy
You can build a flexible structure of management groups and subscriptions to organize your resources into a hierarchy for unified policy and access management. The following diagram shows an example of creating a hierarchy for governance by using management groups.

Diagram showing an example of a management group hierarchy tree.

Some examples of how you could use management groups might be:

Create a hierarchy that applies a policy. You could limit VM locations to the US West Region in a group called Production. This policy will inherit onto all the subscriptions that are descendants of that management group and will apply to all VMs under those subscriptions. This security policy can't be altered by the resource or subscription owner, which allows for improved governance.
Provide user access to multiple subscriptions. By moving multiple subscriptions under a management group, you can create one Azure role-based access control (Azure RBAC) assignment on the management group. Assigning Azure RBAC at the management group level means that all sub-management groups, subscriptions, resource groups, and resources underneath that management group would also inherit those permissions. One assignment on the management group can enable users to have access to everything they need instead of scripting Azure RBAC over different subscriptions.
Important facts about management groups:

10,000 management groups can be supported in a single directory.
A management group tree can support up to six levels of depth. This limit doesn't include the root level or the subscription level.
Each management group and subscription can support only one parent.

Next unit: Exercise - Create an Azure resource

This is pratical and should be done on Microsoft Azure portal

Next unit: Knowledge check

Knowledge check

Choose the best response for each question. Then select Check your answers.

Check your knowledge

1. How many resource groups can a resource be in at the same time? 

A- One

B- Two

C- Three

2. What happens to the resources within a resource group when an action or setting at the Resource Group level is applied? 

A- Current resources inherit the setting, but future resources don't.

B- Future resources inherit the setting, but current ones don't.

C- The setting is applied to current and future resources.

3. What Azure feature replicates resources across regions that are at least 300 miles away from each other? 

A- Region pairs

B- Availability Zones

C- Sovereign regions




Microsoft Azure Fundamentals

Chapter 5
Describe Azure compute and networking services

Introduction

In this module, you’ll be introduced to the compute and networking services of Azure. You’ll learn about three of the compute options (virtual machines, containers, and Azure functions). You’ll also learn about some of the networking features, such as Azure virtual networks, Azure DNS, and Azure ExpressRoute.

Learning objectives
After completing this module, you’ll be able to:

Compare compute types, including container instances, virtual machines, and functions.
Describe virtual machine options, including virtual machines (VMs), virtual machine scale sets, virtual machine availability sets, and Azure Virtual Desktop.
Describe resources required for virtual machines.
Describe application hosting options, including Azure Web Apps, containers, and virtual machines.
Describe virtual networking, including the purpose of Azure Virtual Networks, Azure virtual subnets, peering, Azure DNS, VPN Gateway, and ExpressRoute.
Define public and private endpoints.

Next unit: Describe Azure virtual machines

Describe Azure virtual machines

With Azure Virtual Machines (VMs), you can create and use VMs in the cloud. VMs provide infrastructure as a service (IaaS) in the form of a virtualized server and can be used in many ways. Just like a physical computer, you can customize all of the software running on your VM. VMs are an ideal choice when you need:

Total control over the operating system (OS).
The ability to run custom software.
To use custom hosting configurations.
An Azure VM gives you the flexibility of virtualization without having to buy and maintain the physical hardware that runs the VM. However, as an IaaS offering, you still need to configure, update, and maintain the software that runs on the VM.

You can even create or use an already created image to rapidly provision VMs. You can create and provision a VM in minutes when you select a preconfigured VM image. An image is a template used to create a VM and may already include an OS and other software, like development tools or web hosting environments.

Scale VMs in Azure
You can run single VMs for testing, development, or minor tasks. Or you can group VMs together to provide high availability, scalability, and redundancy. Azure can also manage the grouping of VMs for you with features such as scale sets and availability sets.

Virtual machine scale sets
Virtual machine scale sets let you create and manage a group of identical, load-balanced VMs. If you simply created multiple VMs with the same purpose, you’d need to ensure they were all configured identically and then set up network routing parameters to ensure efficiency. You’d also have to monitor the utilization to determine if you need to increase or decrease the number of VMs.

Instead, with virtual machine scale sets, Azure automates most of that work. Scale sets allow you to centrally manage, configure, and update a large number of VMs in minutes. The number of VM instances can automatically increase or decrease in response to demand, or you can set it to scale based on a defined schedule. Virtual machine scale sets also automatically deploy a load balancer to make sure that your resources are being used efficiently. With virtual machine scale sets, you can build large-scale services for areas such as compute, big data, and container workloads.

Virtual machine availability sets
Virtual machine availability sets are another tool to help you build a more resilient, highly available environment. Availability sets are designed to ensure that VMs stagger updates and have varied power and network connectivity, preventing you from losing all your VMs with a single network or power failure.

Availability sets do this by grouping VMs in two ways: update domain and fault domain.

Update domain: The update domain groups VMs that can be rebooted at the same time. This allows you to apply updates while knowing that only one update domain grouping will be offline at a time. All of the machines in one update domain will be updated. An update group going through the update process is given a 30-minute time to recover before maintenance on the next update domain starts.
Fault domain: The fault domain groups your VMs by common power source and network switch. By default, an availability set will split your VMs across up to three fault domains. This helps protect against a physical power or networking failure by having VMs in different fault domains (thus being connected to different power and networking resources).
Best of all, there’s no additional cost for configuring an availability set. You only pay for the VM instances you create.

Examples of when to use VMs
Some common examples or use cases for virtual machines include:

During testing and development. VMs provide a quick and easy way to create different OS and application configurations. Test and development personnel can then easily delete the VMs when they no longer need them.
When running applications in the cloud. The ability to run certain applications in the public cloud as opposed to creating a traditional infrastructure to run them can provide substantial economic benefits. For example, an application might need to handle fluctuations in demand. Shutting down VMs when you don't need them or quickly starting them up to meet a sudden increase in demand means you pay only for the resources you use.
When extending your datacenter to the cloud: An organization can extend the capabilities of its own on-premises network by creating a virtual network in Azure and adding VMs to that virtual network. Applications like SharePoint can then run on an Azure VM instead of running locally. This arrangement makes it easier or less expensive to deploy than in an on-premises environment.
During disaster recovery: As with running certain types of applications in the cloud and extending an on-premises network to the cloud, you can get significant cost savings by using an IaaS-based approach to disaster recovery. If a primary datacenter fails, you can create VMs running on Azure to run your critical applications and then shut them down when the primary datacenter becomes operational again.
Move to the cloud with VMs
VMs are also an excellent choice when you move from a physical server to the cloud (also known as lift and shift). You can create an image of the physical server and host it within a VM with little or no changes. Just like a physical on-premises server, you must maintain the VM: you’re responsible for maintaining the installed OS and software.

VM Resources
When you provision a VM, you’ll also have the chance to pick the resources that are associated with that VM, including:

Size (purpose, number of processor cores, and amount of RAM)
Storage disks (hard disk drives, solid state drives, etc.)
Networking (virtual network, public IP address, and port configuration)


Next unit: Exercise - Create an Azure virtual machine

Exercise - Create an Azure virtual machine

In this exercise, you create an Azure virtual machine (VM) and install Nginx, a popular web server.

You could use the Azure portal, the Azure CLI, Azure PowerShell, or an Azure Resource Manager (ARM) template.

In this instance, you're going to use the Azure CLI.

Task 1: Create a Linux virtual machine and install Nginx
Use the following Azure CLI commands to create a Linux VM and install Nginx. After your VM is created, you'll use the Custom Script Extension to install Nginx. The Custom Script Extension is an easy way to download and run scripts on your Azure VMs. It's just one of the many ways you can configure the system after your VM is up and running.

From Cloud Shell, run the following az vm create command to create a Linux VM:

Azure CLI

Copy
az vm create \
  --resource-group "[sandbox resource group name]" \
  --name my-vm \
  --public-ip-sku Standard \
  --image Ubuntu2204 \
  --admin-username azureuser \
  --generate-ssh-keys    
Your VM will take a few moments to come up. You named the VM my-vm. You use this name to refer to the VM in later steps.

Run the following az vm extension set command to configure Nginx on your VM:

Azure CLI

Copy
az vm extension set \
  --resource-group "[sandbox resource group name]" \
  --vm-name my-vm \
  --name customScript \
  --publisher Microsoft.Azure.Extensions \
  --version 2.1 \
  --settings '{"fileUris":["https://raw.githubusercontent.com/MicrosoftDocs/mslearn-welcome-to-azure/master/configure-nginx.sh"]}' \
  --protected-settings '{"commandToExecute": "./configure-nginx.sh"}'    
This command uses the Custom Script Extension to run a Bash script on your VM. The script is stored on GitHub. While the command runs, you can choose to examine the Bash script from a separate browser tab. To summarize, the script:

Runs apt-get update to download the latest package information from the internet. This step helps ensure that the next command can locate the latest version of the Nginx package.
Installs Nginx.
Sets the home page, /var/www/html/index.html, to print a welcome message that includes your VM's host name.
Continue
That's all for this exercise. The sandbox will keep running, and you'll come back to this point in a few units to update the network configuration so you can get to the website.

Next unit: Describe Azure virtual desktop

Describe Azure virtual desktop

Another type of virtual machine is the Azure Virtual Desktop. Azure Virtual Desktop is a desktop and application virtualization service that runs on the cloud. It enables you to use a cloud-hosted version of Windows from any location. Azure Virtual Desktop works across devices and operating systems, and works with apps that you can use to access remote desktops or most modern browsers.

The following video gives you an overview of Azure Virtual Desktop:


Enhance security
Azure Virtual Desktop provides centralized security management for users' desktops with Microsoft Entra ID. You can enable multifactor authentication to secure user sign-ins. You can also secure access to data by assigning granular role-based access controls (RBACs) to users.

With Azure Virtual Desktop, the data and apps are separated from the local hardware. The actual desktop and apps are running in the cloud, meaning the risk of confidential data being left on a personal device is reduced. Additionally, user sessions are isolated in both single and multi-session environments.

Multi-session Windows 10 or Windows 11 deployment
Azure Virtual Desktop lets you use Windows 10 or Windows 11 Enterprise multi-session, the only Windows client-based operating system that enables multiple concurrent users on a single VM. Azure Virtual Desktop also provides a more consistent experience with broader application support compared to Windows Server-based operating systems.

Next unit: Describe Azure containers

Describe Azure containers

While virtual machines are an excellent way to reduce costs versus the investments that are necessary for physical hardware, they're still limited to a single operating system per virtual machine. If you want to run multiple instances of an application on a single host machine, containers are an excellent choice.

What are containers?
Containers are a virtualization environment. Much like running multiple virtual machines on a single physical host, you can run multiple containers on a single physical or virtual host. Unlike virtual machines, you don't manage the operating system for a container. Virtual machines appear to be an instance of an operating system that you can connect to and manage. Containers are lightweight and designed to be created, scaled out, and stopped dynamically. It's possible to create and deploy virtual machines as application demand increases, but containers are a lighter weight, more agile method. Containers are designed to allow you to respond to changes on demand. With containers, you can quickly restart if there's a crash or hardware interruption. One of the most popular container engines is Docker, and Azure supports Docker.

Compare virtual machines to containers
The following video highlights several of the important differences between virtual machines and containers:


Azure Container Instances
Azure Container Instances offer the fastest and simplest way to run a container in Azure; without having to manage any virtual machines or adopt any additional services. Azure Container Instances are a platform as a service (PaaS) offering. Azure Container Instances allow you to upload your containers and then the service will run the containers for you.

Azure Container Apps
Azure Container Apps are similar in many ways to a container instance. They allow you to get up and running right away, they remove the container management piece, and they're a PaaS offering. Container Apps have extra benefits such as the ability to incorporate load balancing and scaling. These other functions allow you to be more elastic in your design.

Azure Kubernetes Service
Azure Kubernetes Service (AKS) is a container orchestration service. An orchestration service manages the lifecycle of containers. When you're deploying a fleet of containers, AKS can make fleet management simpler and more efficient.

Use containers in your solutions
Containers are often used to create solutions by using a microservice architecture. This architecture is where you break solutions into smaller, independent pieces. For example, you might split a website into a container hosting your front end, another hosting your back end, and a third for storage. This split allows you to separate portions of your app into logical sections that can be maintained, scaled, or updated independently.

Imagine your website back-end has reached capacity but the front end and storage aren't being stressed. With containers, you could scale the back end separately to improve performance. If something necessitated such a change, you could also choose to change the storage service or modify the front end without impacting any of the other components.

Next unit: Describe Azure functions

Describe Azure functions

Azure Functions is an event-driven, serverless compute option that doesn’t require maintaining virtual machines or containers. If you build an app using VMs or containers, those resources have to be “running” in order for your app to function. With Azure Functions, an event wakes the function, alleviating the need to keep resources provisioned when there are no events.

Serverless computing in Azure

Benefits of Azure Functions
Using Azure Functions is ideal when you're only concerned about the code running your service and not about the underlying platform or infrastructure. Functions are commonly used when you need to perform work in response to an event (often via a REST request), timer, or message from another Azure service, and when that work can be completed quickly, within seconds or less.

Functions scale automatically based on demand, so they may be a good choice when demand is variable.

Azure Functions runs your code when it's triggered and automatically deallocates resources when the function is finished. In this model, you're only charged for the CPU time used while your function runs.

Functions can be either stateless or stateful. When they're stateless (the default), they behave as if they're restarted every time they respond to an event. When they're stateful (called Durable Functions), a context is passed through the function to track prior activity.

Functions are a key component of serverless computing. They're also a general compute platform for running any type of code. If the needs of the developer's app change, you can deploy the project in an environment that isn't serverless. This flexibility allows you to manage scaling, run on virtual networks, and even completely isolate the functions.

Next unit: Describe application hosting options

Describe application hosting options

If you need to host your application on Azure, you might initially turn to a virtual machine (VM) or containers. Both VMs and containers provide excellent hosting solutions. VMs give you maximum control of the hosting environment and allow you to configure it exactly how you want. VMs also may be the most familiar hosting method if you’re new to the cloud. Containers, with the ability to isolate and individually manage different aspects of the hosting solution, can also be a robust and compelling option.

There are other hosting options that you can use with Azure, including Azure App Service.

Azure App Service
App Service enables you to build and host web apps, background jobs, mobile back-ends, and RESTful APIs in the programming language of your choice without managing infrastructure. It offers automatic scaling and high availability. App Service supports Windows and Linux. It enables automated deployments from GitHub, Azure DevOps, or any Git repo to support a continuous deployment model.

Azure App Service is a robust hosting option that you can use to host your apps in Azure. Azure App Service lets you focus on building and maintaining your app, and Azure focuses on keeping the environment up and running.

Azure App Service is an HTTP-based service for hosting web applications, REST APIs, and mobile back ends. It supports multiple languages, including .NET, .NET Core, Java, Ruby, Node.js, PHP, or Python. It also supports both Windows and Linux environments.

Types of app services
With App Service, you can host most common app service styles like:

Web apps
API apps
WebJobs
Mobile apps
App Service handles most of the infrastructure decisions you deal with in hosting web-accessible apps:

Deployment and management are integrated into the platform.
Endpoints can be secured.
Sites can be scaled quickly to handle high traffic loads.
The built-in load balancing and traffic manager provide high availability.
All of these app styles are hosted in the same infrastructure and share these benefits. This flexibility makes App Service the ideal choice to host web-oriented applications.

Web apps
App Service includes full support for hosting web apps by using ASP.NET, ASP.NET Core, Java, Ruby, Node.js, PHP, or Python. You can choose either Windows or Linux as the host operating system.

API apps
Much like hosting a website, you can build REST-based web APIs by using your choice of language and framework. You get full Swagger support and the ability to package and publish your API in Azure Marketplace. The produced apps can be consumed from any HTTP- or HTTPS-based client.

WebJobs
You can use the WebJobs feature to run a program (.exe, Java, PHP, Python, or Node.js) or script (.cmd, .bat, PowerShell, or Bash) in the same context as a web app, API app, or mobile app. They can be scheduled or run by a trigger. WebJobs are often used to run background tasks as part of your application logic.

Mobile apps
Use the Mobile Apps feature of App Service to quickly build a back end for iOS and Android apps. With just a few actions in the Azure portal, you can:

Store mobile app data in a cloud-based SQL database.
Authenticate customers against common social providers, such as MSA, Google, Twitter, and Facebook.
Send push notifications.
Execute custom back-end logic in C# or Node.js.
On the mobile app side, there's SDK support for native iOS and Android, Xamarin, and React native apps.

Next unit: Describe Azure virtual networking

Describe Azure virtual networking

Azure virtual networks and virtual subnets enable Azure resources, such as VMs, web apps, and databases, to communicate with each other, with users on the internet, and with your on-premises client computers. You can think of an Azure network as an extension of your on-premises network with resources that link other Azure resources.

Azure virtual networks provide the following key networking capabilities:

Isolation and segmentation
Internet communications
Communicate between Azure resources
Communicate with on-premises resources
Route network traffic
Filter network traffic
Connect virtual networks
Azure virtual networking supports both public and private endpoints to enable communication between external or internal resources with other internal resources.

Public endpoints have a public IP address and can be accessed from anywhere in the world.
Private endpoints exist within a virtual network and have a private IP address from within the address space of that virtual network.
Isolation and segmentation
Azure virtual network allows you to create multiple isolated virtual networks. When you set up a virtual network, you define a private IP address space by using either public or private IP address ranges. The IP range only exists within the virtual network and isn't internet routable. You can divide that IP address space into subnets and allocate part of the defined address space to each named subnet.

For name resolution, you can use the name resolution service that's built into Azure. You also can configure the virtual network to use either an internal or an external DNS server.

Internet communications
You can enable incoming connections from the internet by assigning a public IP address to an Azure resource, or putting the resource behind a public load balancer.

Communicate between Azure resources
You'll want to enable Azure resources to communicate securely with each other. You can do that in one of two ways:

Virtual networks can connect not only VMs but other Azure resources, such as the App Service Environment for Power Apps, Azure Kubernetes Service, and Azure virtual machine scale sets.
Service endpoints can connect to other Azure resource types, such as Azure SQL databases and storage accounts. This approach enables you to link multiple Azure resources to virtual networks to improve security and provide optimal routing between resources.
Communicate with on-premises resources
Azure virtual networks enable you to link resources together in your on-premises environment and within your Azure subscription. In effect, you can create a network that spans both your local and cloud environments. There are three mechanisms for you to achieve this connectivity:

Point-to-site virtual private network connections are from a computer outside your organization back into your corporate network. In this case, the client computer initiates an encrypted VPN connection to connect to the Azure virtual network.
Site-to-site virtual private networks link your on-premises VPN device or gateway to the Azure VPN gateway in a virtual network. In effect, the devices in Azure can appear as being on the local network. The connection is encrypted and works over the internet.
Azure ExpressRoute provides a dedicated private connectivity to Azure that doesn't travel over the internet. ExpressRoute is useful for environments where you need greater bandwidth and even higher levels of security.
Route network traffic
By default, Azure routes traffic between subnets on any connected virtual networks, on-premises networks, and the internet. You also can control routing and override those settings, as follows:

Route tables allow you to define rules about how traffic should be directed. You can create custom route tables that control how packets are routed between subnets.
Border Gateway Protocol (BGP) works with Azure VPN gateways, Azure Route Server, or Azure ExpressRoute to propagate on-premises BGP routes to Azure virtual networks.
Filter network traffic
Azure virtual networks enable you to filter traffic between subnets by using the following approaches:

Network security groups are Azure resources that can contain multiple inbound and outbound security rules. You can define these rules to allow or block traffic, based on factors such as source and destination IP address, port, and protocol.
Network virtual appliances are specialized VMs that can be compared to a hardened network appliance. A network virtual appliance carries out a particular network function, such as running a firewall or performing wide area network (WAN) optimization.
Connect virtual networks
You can link virtual networks together by using virtual network peering. Peering allows two virtual networks to connect directly to each other. Network traffic between peered networks is private, and travels on the Microsoft backbone network, never entering the public internet. Peering enables resources in each virtual network to communicate with each other. These virtual networks can be in separate regions, which allows you to create a global interconnected network through Azure.

User-defined routes (UDR) allow you to control the routing tables between subnets within a virtual network or between virtual networks. This allows for greater control over network traffic flow.

Next unit: Exercise - Configure network access

Exercise - Configure network access

In this exercise, you'll configure the access to the virtual machine (VM) you created earlier in this module.

To verify the VM you created previously is still running, use the following command:

az vm list


If you receive an empty response [], you need to complete the first exercise in this module again. If the result lists your current VM and its settings, you may continue.

Right now, the VM you created and installed Nginx on isn't accessible from the internet. You'll create a network security group that changes that by allowing inbound HTTP access on port 80.

Task 1: Access your web server
In this procedure, you get the IP address for your VM and attempt to access your web server's home page.

Run the following az vm list-ip-addresses command to get your VM's IP address and store the result as a Bash variable:

IPADDRESS="$(az vm list-ip-addresses \
  --resource-group "[sandbox resource group name]" \
  --name my-vm \
  --query "[].virtualMachine.network.publicIpAddresses[*].ipAddress" \
  --output tsv)"    
Run the following curl command to download the home page:

Bash

curl --connect-timeout 5 http://$IPADDRESS
The --connect-timeout argument specifies to allow up to five seconds for the connection to occur. After five seconds, you see an error message that states that the connection timed out:

Output

curl: (28) Connection timed out after 5001 milliseconds
This message means that the VM was not accessible within the timeout period.

As an optional step, try to access the web server from a browser:

Run the following to print your VM's IP address to the console:

Bash


echo $IPADDRESS       
You see an IP address, for example, 23.102.42.235.

Copy the IP address that you see to the clipboard.

Open a new browser tab and go to your web server. After a few moments, you see that the connection isn't happening. If you wait for the browser to time out, you'll see something like this:

Screenshot of a web browser showing an error message that says the connection timed out.

Keep this browser tab open for later.

Task 2: List the current network security group rules
Your web server wasn't accessible. To find out why, let's examine your current NSG rules.

Run the following az network nsg list command to list the network security groups that are associated with your VM:

Azure CLI

az network nsg list \
  --resource-group "[sandbox resource group name]" \
  --query '[].name' \
  --output tsv    
You see this:

Output

my-vmNSG

Every VM on Azure is associated with at least one network security group. In this case, Azure created an NSG for you called my-vmNSG.

Run the following az network nsg rule list command to list the rules associated with the NSG named my-vmNSG:

Azure CLI

az network nsg rule list \
  --resource-group "[sandbox resource group name]" \
  --nsg-name my-vmNSG    
You see a large block of text in JSON format in the output. In the next step, you'll run a similar command that makes this output easier to read.

Run the az network nsg rule list command a second time. This time, use the --query argument to retrieve only the name, priority, affected ports, and access (Allow or Deny) for each rule. The --output argument formats the output as a table so that it's easy to read.

Azure CLI

az network nsg rule list \
  --resource-group "[sandbox resource group name]" \
  --nsg-name my-vmNSG \
  --query '[].{Name:name, Priority:priority, Port:destinationPortRange, Access:access}' \
  --output table    
You see this:

Output

Copy
Name              Priority    Port    Access
-----------------  ----------  ------  --------
default-allow-ssh  1000        22      Allow

You see the default rule, default-allow-ssh. This rule allows inbound connections over port 22 (SSH). SSH (Secure Shell) is a protocol that's used on Linux to allow administrators to access the system remotely. The priority of this rule is 1000. Rules are processed in priority order, with lower numbers processed before higher numbers.

By default, a Linux VM's NSG allows network access only on port 22. This enables administrators to access the system. You need to also allow inbound connections on port 80, which allows access over HTTP.

Task 3: Create the network security rule
Here, you create a network security rule that allows inbound access on port 80 (HTTP).

Run the following az network nsg rule create command to create a rule called allow-http that allows inbound access on port 80:

Azure CLI

az network nsg rule create \
  --resource-group "[sandbox resource group name]" \
  --nsg-name my-vmNSG \
  --name allow-http \
  --protocol tcp \
  --priority 100 \
  --destination-port-range 80 \
  --access Allow    
For learning purposes, here you set the priority to 100. In this case, the priority doesn't matter. You would need to consider the priority if you had overlapping port ranges.

To verify the configuration, run az network nsg rule list to see the updated list of rules:

Azure CLI

az network nsg rule list \
  --resource-group "[sandbox resource group name]" \
  --nsg-name my-vmNSG \
  --query '[].{Name:name, Priority:priority, Port:destinationPortRange, Access:access}' \
  --output table    
You see this both the default-allow-ssh rule and your new rule, allow-http:

Output

Name              Priority    Port    Access
-----------------  ----------  ------  --------
default-allow-ssh  1000        22      Allow
allow-http          100        80      Allow    
Task 4: Access your web server again
Now that you've configured network access to port 80, let's try to access the web server a second time.

 Note

After you update the NSG, it may take a few moments before the updated rules propagate. Retry the next step, with pauses between attempts, until you get the desired results.

Run the same curl command that you ran earlier:

Bash

curl --connect-timeout 5 http://$IPADDRESS
You see this:

HTML

<html><body><h2>Welcome to Azure! My name is my-vm.</h2></body></html>
As an optional step, refresh your browser tab that points to your web server. You see this:

A screenshot of a web browser showing the home page from the web server. The home page displays a welcome message.

Nice work. In practice, you can create a standalone network security group that includes the inbound and outbound network access rules you need. If you have multiple VMs that serve the same purpose, you can assign that NSG to each VM at the time you create it. This technique enables you to control network access to multiple VMs under a single, central set of rules.

Clean up
The sandbox automatically cleans up your resources when you're finished with this module.

When you're working in your own subscription, it's a good idea at the end of a project to identify whether you still need the resources you created. Resources that you leave running can cost you money. You can delete resources individually or delete the resource group to delete the entire set of resources.

Next unit: Describe Azure virtual private networks

Describe Azure virtual private networks
A virtual private network (VPN) uses an encrypted tunnel within another network. VPNs are typically deployed to connect two or more trusted private networks to one another over an untrusted network (typically the public internet). Traffic is encrypted while traveling over the untrusted network to prevent eavesdropping or other attacks. VPNs can enable networks to safely and securely share sensitive information.

VPN gateways
A VPN gateway is a type of virtual network gateway. Azure VPN Gateway instances are deployed in a dedicated subnet of the virtual network and enable the following connectivity:

Connect on-premises datacenters to virtual networks through a site-to-site connection.
Connect individual devices to virtual networks through a point-to-site connection.
Connect virtual networks to other virtual networks through a network-to-network connection.
All data transfer is encrypted inside a private tunnel as it crosses the internet. You can deploy only one VPN gateway in each virtual network. However, you can use one gateway to connect to multiple locations, which includes other virtual networks or on-premises datacenters.

When setting up a VPN gateway, you must specify the type of VPN - either policy-based or route-based. The primary distinction between these two types is how they determine which traffic needs encryption. In Azure, regardless of the VPN type, the method of authentication employed is a pre-shared key.

Policy-based VPN gateways specify statically the IP address of packets that should be encrypted through each tunnel. This type of device evaluates every data packet against those sets of IP addresses to choose the tunnel where that packet is going to be sent through.
In Route-based gateways, IPSec tunnels are modeled as a network interface or virtual tunnel interface. IP routing (either static routes or dynamic routing protocols) decides which one of these tunnel interfaces to use when sending each packet. Route-based VPNs are the preferred connection method for on-premises devices. They're more resilient to topology changes such as the creation of new subnets.
Use a route-based VPN gateway if you need any of the following types of connectivity:

Connections between virtual networks
Point-to-site connections
Multisite connections
Coexistence with an Azure ExpressRoute gateway
High-availability scenarios
If you’re configuring a VPN to keep your information safe, you also want to be sure that it’s a highly available and fault tolerant VPN configuration. There are a few ways to maximize the resiliency of your VPN gateway.

Active/standby
By default, VPN gateways are deployed as two instances in an active/standby configuration, even if you only see one VPN gateway resource in Azure. When planned maintenance or unplanned disruption affects the active instance, the standby instance automatically assumes responsibility for connections without any user intervention. Connections are interrupted during this failover, but they're typically restored within a few seconds for planned maintenance and within 90 seconds for unplanned disruptions.

Active/active
With the introduction of support for the BGP routing protocol, you can also deploy VPN gateways in an active/active configuration. In this configuration, you assign a unique public IP address to each instance. You then create separate tunnels from the on-premises device to each IP address. You can extend the high availability by deploying an additional VPN device on-premises.

ExpressRoute failover
Another high-availability option is to configure a VPN gateway as a secure failover path for ExpressRoute connections. ExpressRoute circuits have resiliency built in. However, they aren't immune to physical problems that affect the cables delivering connectivity or outages that affect the complete ExpressRoute location. In high-availability scenarios, where there's risk associated with an outage of an ExpressRoute circuit, you can also provision a VPN gateway that uses the internet as an alternative method of connectivity. In this way, you can ensure there's always a connection to the virtual networks.

Zone-redundant gateways
In regions that support availability zones, VPN gateways and ExpressRoute gateways can be deployed in a zone-redundant configuration. This configuration brings resiliency, scalability, and higher availability to virtual network gateways. Deploying gateways in Azure availability zones physically and logically separates gateways within a region while protecting your on-premises network connectivity to Azure from zone-level failures. These gateways require different gateway stock keeping units (SKUs) and use Standard public IP addresses instead of Basic public IP addresses.

Next unit: Describe Azure ExpressRoute

Describe Azure ExpressRoute

Azure ExpressRoute lets you extend your on-premises networks into the Microsoft cloud over a private connection, with the help of a connectivity provider. This connection is called an ExpressRoute Circuit. With ExpressRoute, you can establish connections to Microsoft cloud services, such as Microsoft Azure and Microsoft 365. This allows you to connect offices, datacenters, or other facilities to the Microsoft cloud. Each location would have its own ExpressRoute circuit.

Connectivity can be from an any-to-any (IP VPN) network, a point-to-point Ethernet network, or a virtual cross-connection through a connectivity provider at a colocation facility. ExpressRoute connections don't go over the public Internet. This allows ExpressRoute connections to offer more reliability, faster speeds, consistent latencies, and higher security than typical connections over the Internet.

Features and benefits of ExpressRoute
There are several benefits to using ExpressRoute as the connection service between Azure and on-premises networks.

Connectivity to Microsoft cloud services across all regions in the geopolitical region.
Global connectivity to Microsoft services across all regions with the ExpressRoute Global Reach.
Dynamic routing between your network and Microsoft via Border Gateway Protocol (BGP).
Built-in redundancy in every peering location for higher reliability.
Connectivity to Microsoft cloud services
ExpressRoute enables direct access to the following services in all regions:

Microsoft Office 365
Microsoft Dynamics 365
Azure compute services, such as Azure Virtual Machines
Azure cloud services, such as Azure Cosmos DB and Azure Storage
Global connectivity
You can enable ExpressRoute Global Reach to exchange data across your on-premises sites by connecting your ExpressRoute circuits. For example, say you had an office in Asia and a datacenter in Europe, both with ExpressRoute circuits connecting them to the Microsoft network. You could use ExpressRoute Global Reach to connect those two facilities, allowing them to communicate without transferring data over the public internet.

Dynamic routing
ExpressRoute uses the BGP. BGP is used to exchange routes between on-premises networks and resources running in Azure. This protocol enables dynamic routing between your on-premises network and services running in the Microsoft cloud.

Built-in redundancy
Each connectivity provider uses redundant devices to ensure that connections established with Microsoft are highly available. You can configure multiple circuits to complement this feature.

ExpressRoute connectivity models
ExpressRoute supports four models that you can use to connect your on-premises network to the Microsoft cloud:

CloudExchange colocation
Point-to-point Ethernet connection
Any-to-any connection
Directly from ExpressRoute sites
Co-location at a cloud exchange
Co-location refers to your datacenter, office, or other facility being physically co-located at a cloud exchange, such as an ISP. If your facility is co-located at a cloud exchange, you can request a virtual cross-connect to the Microsoft cloud.

Point-to-point Ethernet connection
Point-to-point ethernet connection refers to using a point-to-point connection to connect your facility to the Microsoft cloud.

Any-to-any networks
With any-to-any connectivity, you can integrate your wide area network (WAN) with Azure by providing connections to your offices and datacenters. Azure integrates with your WAN connection to provide a connection like you would have between your datacenter and any branch offices.

Directly from ExpressRoute sites
You can connect directly into the Microsoft's global network at a peering location strategically distributed across the world. ExpressRoute Direct provides dual 100 Gbps or 10-Gbps connectivity, which supports Active/Active connectivity at scale.

Security considerations
With ExpressRoute, your data doesn't travel over the public internet, so it's not exposed to the potential risks associated with internet communications. ExpressRoute is a private connection from your on-premises infrastructure to your Azure infrastructure. Even if you have an ExpressRoute connection, DNS queries, certificate revocation list checking, and Azure Content Delivery Network requests are still sent over the public internet.

Next unit: Describe Azure DNS

Describe Azure DNS

Azure DNS is a hosting service for DNS domains that provides name resolution by using Microsoft Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records using the same credentials, APIs, tools, and billing as your other Azure services.

Benefits of Azure DNS
Azure DNS leverages the scope and scale of Microsoft Azure to provide numerous benefits, including:

Reliability and performance
Security
Ease of Use
Customizable virtual networks
Alias records
Reliability and performance
DNS domains in Azure DNS are hosted on Azure's global network of DNS name servers, providing resiliency and high availability. Azure DNS uses anycast networking, so each DNS query is answered by the closest available DNS server to provide fast performance and high availability for your domain.

Security
Azure DNS is based on Azure Resource Manager, which provides features such as:

Azure role-based access control (Azure RBAC) to control who has access to specific actions for your organization.
Activity logs to monitor how a user in your organization modified a resource or to find an error when troubleshooting.
Resource locking to lock a subscription, resource group, or resource. Locking prevents other users in your organization from accidentally deleting or modifying critical resources.
Ease of use
Azure DNS can manage DNS records for your Azure services and provide DNS for your external resources as well. Azure DNS is integrated in the Azure portal and uses the same credentials, support contract, and billing as your other Azure services.

Because Azure DNS is running on Azure, it means you can manage your domains and records with the Azure portal, Azure PowerShell cmdlets, and the cross-platform Azure CLI. Applications that require automated DNS management can integrate with the service by using the REST API and SDKs.

Customizable virtual networks with private domains
Azure DNS also supports private DNS domains. This feature allows you to use your own custom domain names in your private virtual networks, rather than being stuck with the Azure-provided names.

Alias records
Azure DNS also supports alias record sets. You can use an alias record set to refer to an Azure resource, such as an Azure public IP address, an Azure Traffic Manager profile, or an Azure Content Delivery Network (CDN) endpoint. If the IP address of the underlying resource changes, the alias record set seamlessly updates itself during DNS resolution. The alias record set points to the service instance, and the service instance is associated with an IP address.

 Important

You can't use Azure DNS to buy a domain name. For an annual fee, you can buy a domain name by using App Service domains or a third-party domain name registrar. Once purchased, your domains can be hosted in Azure DNS for record management.

Next unit: Knowledge check

Knowledge check

Choose the best response for each question. Then select Check your answers.

Check your knowledge

1. Which Azure Virtual Machine feature staggers updates across VMs based on their update domain and fault domain? 

A- Availability sets

B- Scale sets

C- Update sets

2. Which Azure service allows users to use a cloud hosted version of Windows from any location and connect from most modern browsers? 

A- Azure Virtual Desktop

B- Azure Virtual Machines

C- Azure Container Instances


Microsoft Azure Fundamentals

Chapter 6
Describe Azure storage services

Introduction

In this module, you’ll be introduced to the Azure storage services. You’ll learn about the Azure Storage Account and how that relates to the different storage services that are available. You’ll also learn about blob storage tiers, data redundancy options, and ways to move data or even entire infrastructures to Azure.

Learning objectives
After completing this module, you’ll be able to:

Compare Azure storage services.
Describe storage tiers.
Describe redundancy options.
Describe storage account options and storage types.
Identify options for moving files, including AzCopy, Azure Storage Explorer, and Azure File Sync.
Describe migration options, including Azure Migrate and Azure Data Box.

Next unit: Describe Azure storage accounts

Describe Azure storage accounts

The following video introduces the different services that should be available with Azure Storage.


A storage account provides a unique namespace for your Azure Storage data that's accessible from anywhere in the world over HTTP or HTTPS. Data in this account is secure, highly available, durable, and massively scalable.

When you create your storage account, you’ll start by picking the storage account type. The type of account determines the storage services and redundancy options and has an impact on the use cases. Below is a list of redundancy options that will be covered later in this module:

Locally redundant storage (LRS)
Geo-redundant storage (GRS)
Read-access geo-redundant storage (RA-GRS)
Zone-redundant storage (ZRS)
Geo-zone-redundant storage (GZRS)
Read-access geo-zone-redundant storage (RA-GZRS)
Type	Supported services	Redundancy Options	Usage
Standard general-purpose v2	Blob Storage (including Data Lake Storage), Queue Storage, Table Storage, and Azure Files	LRS, GRS, RA-GRS, ZRS, GZRS, RA-GZRS	Standard storage account type for blobs, file shares, queues, and tables. Recommended for most scenarios using Azure Storage. If you want support for network file system (NFS) in Azure Files, use the premium file shares account type.
Premium block blobs	Blob Storage (including Data Lake Storage)	LRS, ZRS	Premium storage account type for block blobs and append blobs. Recommended for scenarios with high transaction rates or that use smaller objects or require consistently low storage latency.
Premium file shares	Azure Files	LRS, ZRS	Premium storage account type for file shares only. Recommended for enterprise or high-performance scale applications. Use this account type if you want a storage account that supports both Server Message Block (SMB) and NFS file shares.
Premium page blobs	Page blobs only	LRS	Premium storage account type for page blobs only.
Storage account endpoints
One of the benefits of using an Azure Storage Account is having a unique namespace in Azure for your data. In order to do this, every storage account in Azure must have a unique-in-Azure account name. The combination of the account name and the Azure Storage service endpoint forms the endpoints for your storage account.

When naming your storage account, keep these rules in mind:

Storage account names must be between 3 and 24 characters in length and may contain numbers and lowercase letters only.
Your storage account name must be unique within Azure. No two storage accounts can have the same name. This supports the ability to have a unique, accessible namespace in Azure.
The following table shows the endpoint format for Azure Storage services.

Storage service	Endpoint
Blob Storage	https://<storage-account-name>.blob.core.windows.net
Data Lake Storage Gen2	https://<storage-account-name>.dfs.core.windows.net
Azure Files	https://<storage-account-name>.file.core.windows.net
Queue Storage	https://<storage-account-name>.queue.core.windows.net
Table Storage	https://<storage-account-name>.table.core.windows.net

Next unit: Describe Azure storage redundancy

Describe Azure storage redundancy

Azure Storage always stores multiple copies of your data so that it's protected from planned and unplanned events such as transient hardware failures, network or power outages, and natural disasters. Redundancy ensures that your storage account meets its availability and durability targets even in the face of failures.

When deciding which redundancy option is best for your scenario, consider the tradeoffs between lower costs and higher availability. The factors that help determine which redundancy option you should choose include:

How your data is replicated in the primary region.
Whether your data is replicated to a second region that is geographically distant to the primary region, to protect against regional disasters.
Whether your application requires read access to the replicated data in the secondary region if the primary region becomes unavailable.
Redundancy in the primary region
Data in an Azure Storage account is always replicated three times in the primary region. Azure Storage offers two options for how your data is replicated in the primary region, locally redundant storage (LRS) and zone-redundant storage (ZRS).

Locally redundant storage
Locally redundant storage (LRS) replicates your data three times within a single data center in the primary region. LRS provides at least 11 nines of durability (99.999999999%) of objects over a given year.

Diagram showing the structure used for locally redundant storage.

LRS is the lowest-cost redundancy option and offers the least durability compared to other options. LRS protects your data against server rack and drive failures. However, if a disaster such as fire or flooding occurs within the data center, all replicas of a storage account using LRS may be lost or unrecoverable. To mitigate this risk, Microsoft recommends using zone-redundant storage (ZRS), geo-redundant storage (GRS), or geo-zone-redundant storage (GZRS).

Zone-redundant storage
For Availability Zone-enabled Regions, zone-redundant storage (ZRS) replicates your Azure Storage data synchronously across three Azure availability zones in the primary region. ZRS offers durability for Azure Storage data objects of at least 12 nines (99.9999999999%) over a given year.

Diagram showing ZRS, with a copy of data stored in each of three availability zones.

With ZRS, your data is still accessible for both read and write operations even if a zone becomes unavailable. No remounting of Azure file shares from the connected clients is required. If a zone becomes unavailable, Azure undertakes networking updates, such as DNS repointing. These updates may affect your application if you access data before the updates have completed.

Microsoft recommends using ZRS in the primary region for scenarios that require high availability. ZRS is also recommended for restricting replication of data within a country or region to meet data governance requirements.

Redundancy in a secondary region
For applications requiring high durability, you can choose to additionally copy the data in your storage account to a secondary region that is hundreds of miles away from the primary region. If the data in your storage account is copied to a secondary region, then your data is durable even in the event of a catastrophic failure that prevents the data in the primary region from being recovered.

When you create a storage account, you select the primary region for the account. The paired secondary region is based on Azure Region Pairs, and can't be changed.

Azure Storage offers two options for copying your data to a secondary region: geo-redundant storage (GRS) and geo-zone-redundant storage (GZRS). GRS is similar to running LRS in two regions, and GZRS is similar to running ZRS in the primary region and LRS in the secondary region.

By default, data in the secondary region isn't available for read or write access unless there's a failover to the secondary region. If the primary region becomes unavailable, you can choose to fail over to the secondary region. After the failover has completed, the secondary region becomes the primary region, and you can again read and write data.

 Important

Because data is replicated to the secondary region asynchronously, a failure that affects the primary region may result in data loss if the primary region can't be recovered. The interval between the most recent writes to the primary region and the last write to the secondary region is known as the recovery point objective (RPO). The RPO indicates the point in time to which data can be recovered. Azure Storage typically has an RPO of less than 15 minutes, although there's currently no SLA on how long it takes to replicate data to the secondary region.

Geo-redundant storage
GRS copies your data synchronously three times within a single physical location in the primary region using LRS. It then copies your data asynchronously to a single physical location in the secondary region (the region pair) using LRS. GRS offers durability for Azure Storage data objects of at least 16 nines (99.99999999999999%) over a given year.

Diagram showing GRS, with primary region LRS replicating data to LRS in a second region.

Geo-zone-redundant storage
GZRS combines the high availability provided by redundancy across availability zones with protection from regional outages provided by geo-replication. Data in a GZRS storage account is copied across three Azure availability zones in the primary region (similar to ZRS) and is also replicated to a secondary geographic region, using LRS, for protection from regional disasters. Microsoft recommends using GZRS for applications requiring maximum consistency, durability, and availability, excellent performance, and resilience for disaster recovery.

Diagram showing GZRS, with primary region ZRS replicating data to LRS in a second region.

GZRS is designed to provide at least 16 nines (99.99999999999999%) of durability of objects over a given year.

Read access to data in the secondary region
Geo-redundant storage (with GRS or GZRS) replicates your data to another physical location in the secondary region to protect against regional outages. However, that data is available to be read only if the customer or Microsoft initiates a failover from the primary to secondary region. However, if you enable read access to the secondary region, your data is always available, even when the primary region is running optimally. For read access to the secondary region, enable read-access geo-redundant storage (RA-GRS) or read-access geo-zone-redundant storage (RA-GZRS).

 Important

Remember that the data in your secondary region may not be up-to-date due to RPO.

Next unit: Describe Azure storage services

Describe Azure storage services

The Azure Storage platform includes the following data services:

Azure Blobs: A massively scalable object store for text and binary data. Also includes support for big data analytics through Data Lake Storage Gen2.
Azure Files: Managed file shares for cloud or on-premises deployments.
Azure Queues: A messaging store for reliable messaging between application components.
Azure Disks: Block-level storage volumes for Azure VMs.
Azure Tables: NoSQL table option for structured, non-relational data.
Benefits of Azure Storage
Azure Storage services offer the following benefits for application developers and IT professionals:

Durable and highly available. Redundancy ensures that your data is safe if transient hardware failures occur. You can also opt to replicate data across data centers or geographical regions for additional protection from local catastrophes or natural disasters. Data replicated in this way remains highly available if an unexpected outage occurs.
Secure. All data written to an Azure storage account is encrypted by the service. Azure Storage provides you with fine-grained control over who has access to your data.
Scalable. Azure Storage is designed to be massively scalable to meet the data storage and performance needs of today's applications.
Managed. Azure handles hardware maintenance, updates, and critical issues for you.
Accessible. Data in Azure Storage is accessible from anywhere in the world over HTTP or HTTPS. Microsoft provides client libraries for Azure Storage in a variety of languages, including .NET, Java, Node.js, Python, PHP, Ruby, Go, and others, as well as a mature REST API. Azure Storage supports scripting in Azure PowerShell or Azure CLI. And the Azure portal and Azure Storage Explorer offer easy visual solutions for working with your data.
Azure Blobs
Azure Blob storage is an object storage solution for the cloud. It can store massive amounts of data, such as text or binary data. Azure Blob storage is unstructured, meaning that there are no restrictions on the kinds of data it can hold. Blob storage can manage thousands of simultaneous uploads, massive amounts of video data, constantly growing log files, and can be reached from anywhere with an internet connection.

Blobs aren't limited to common file formats. A blob could contain gigabytes of binary data streamed from a scientific instrument, an encrypted message for another application, or data in a custom format for an app you're developing. One advantage of blob storage over disk storage is that it doesn't require developers to think about or manage disks. Data is uploaded as blobs, and Azure takes care of the physical storage needs.

Blob storage is ideal for:

Serving images or documents directly to a browser.
Storing files for distributed access.
Streaming video and audio.
Storing data for backup and restore, disaster recovery, and archiving.
Storing data for analysis by an on-premises or Azure-hosted service.
Accessing blob storage
Objects in blob storage can be accessed from anywhere in the world via HTTP or HTTPS. Users or client applications can access blobs via URLs, the Azure Storage REST API, Azure PowerShell, Azure CLI, or an Azure Storage client library. The storage client libraries are available for multiple languages, including .NET, Java, Node.js, Python, PHP, and Ruby.

Blob storage tiers
Data stored in the cloud can grow at an exponential pace. To manage costs for your expanding storage needs, it's helpful to organize your data based on attributes like frequency of access and planned retention period. Data stored in the cloud can be handled differently based on how it's generated, processed, and accessed over its lifetime. Some data is actively accessed and modified throughout its lifetime. Some data is accessed frequently early in its lifetime, with access dropping drastically as the data ages. Some data remains idle in the cloud and is rarely, if ever, accessed after it's stored. To accommodate these different access needs, Azure provides several access tiers, which you can use to balance your storage costs with your access needs.

Azure Storage offers different access tiers for your blob storage, helping you store object data in the most cost-effective manner. The available access tiers include:

Hot access tier: Optimized for storing data that is accessed frequently (for example, images for your website).
Cool access tier: Optimized for data that is infrequently accessed and stored for at least 30 days (for example, invoices for your customers).
Cold access tier: Optimized for storing data that is infrequently accessed and stored for at least 90 days.
Archive access tier: Appropriate for data that is rarely accessed and stored for at least 180 days, with flexible latency requirements (for example, long-term backups).
The following considerations apply to the different access tiers:

Hot and cool access tiers can be set at the account level. The cold and archive access tiers aren't available at the account level.
Hot, cool, cold, and archive tiers can be set at the blob level, during or after upload.
Data in the cool and cold access tiers can tolerate slightly lower availability, but still requires high durability, retrieval latency, and throughput characteristics similar to hot data. For cool and cold data, a lower availability service-level agreement (SLA) and higher access costs compared to hot data are acceptable trade-offs for lower storage costs.
Archive storage stores data offline and offers the lowest storage costs, but also the highest costs to rehydrate and access data.
Azure Files
Azure File storage offers fully managed file shares in the cloud that are accessible via the industry standard Server Message Block (SMB) or Network File System (NFS) protocols. Azure Files file shares can be mounted concurrently by cloud or on-premises deployments. SMB Azure file shares are accessible from Windows, Linux, and macOS clients. NFS Azure Files shares are accessible from Linux or macOS clients. Additionally, SMB Azure file shares can be cached on Windows Servers with Azure File Sync for fast access near where the data is being used.

Azure Files key benefits:
Shared access: Azure file shares support the industry standard SMB and NFS protocols, meaning you can seamlessly replace your on-premises file shares with Azure file shares without worrying about application compatibility.
Fully managed: Azure file shares can be created without the need to manage hardware or an OS. This means you don't have to deal with patching the server OS with critical security upgrades or replacing faulty hard disks.
Scripting and tooling: PowerShell cmdlets and Azure CLI can be used to create, mount, and manage Azure file shares as part of the administration of Azure applications. You can create and manage Azure file shares using Azure portal and Azure Storage Explorer.
Resiliency: Azure Files has been built from the ground up to always be available. Replacing on-premises file shares with Azure Files means you don't have to wake up in the middle of the night to deal with local power outages or network issues.
Familiar programmability: Applications running in Azure can access data in the share via file system I/O APIs. Developers can therefore use their existing code and skills to migrate existing applications. In addition to System IO APIs, you can use Azure Storage Client Libraries or the Azure Storage REST API.
Azure Queues
Azure Queue storage is a service for storing large numbers of messages. Once stored, you can access the messages from anywhere in the world via authenticated calls using HTTP or HTTPS. A queue can contain as many messages as your storage account has room for (potentially millions). Each individual message can be up to 64 KB in size. Queues are commonly used to create a backlog of work to process asynchronously.

Queue storage can be combined with compute functions like Azure Functions to take an action when a message is received. For example, you want to perform an action after a customer uploads a form to your website. You could have the submit button on the website trigger a message to the Queue storage. Then, you could use Azure Functions to trigger an action once the message was received.

Azure Disks
Azure Disk storage, or Azure managed disks, are block-level storage volumes managed by Azure for use with Azure VMs. Conceptually, they’re the same as a physical disk, but they’re virtualized – offering greater resiliency and availability than a physical disk. With managed disks, all you have to do is provision the disk, and Azure will take care of the rest.

Azure Tables
Azure Table storage stores large amounts of structured data. Azure tables are a NoSQL datastore that accepts authenticated calls from inside and outside the Azure cloud. This enables you to use Azure tables to build your hybrid or multi-cloud solution and have your data always available. Azure tables are ideal for storing structured, non-relational data.

Next unit: Exercise - Create a storage blob

Create a storage account
In this task, you'll create a new storage account.

Sign in to the Azure portal at https://portal.azure.com

Select Create a resource.

Under Categories, select Storage.

Under Storage account, select Create.

On the Basics tab of the Create a storage account blade, fill in the following information. Leave the defaults for everything else.

Setting	Value
Subscription	Concierge Subscription
Resource group	Select the resource group that starts with learn
Storage account name	Create a unique storage account name
Region	Leave default
Performance	Standard
Redundancy	Locally redundant storage (LRS)
On the Advanced tab of the Create a storage account blade, fill in the following information. Leave the defaults for everything else.

Setting	Value
Allow enabling anonymous access on individual containers	Checked
Screenshot showing how to enable anonymous-access containers on a storage account.

Select Review to review your storage account settings and allow Azure to validate the configuration.

Once validated, select Create. Wait for the notification that the account was successfully created.

Select Go to resource.

Work with blob storage
In this section, you'll create a Blob container and upload a picture.

Under Data storage, select Containers.

Screenshot of the Container add section of a storage account.

Select + Container and complete the information.

Setting	Value
Name	Enter a name for the container
Anonymous access level	Private (no anonymous access)
Select Create.

 Note

Step 4 will need an image. If you want to upload an image you already have on your computer, continue to Step 4. Otherwise, open a new browser window and search Bing for an image of a flower. Save the image to your computer.

Back in the Azure portal, select the container you created, then select Upload.

Browse for the image file you want to upload. Select it and then select upload.

 Note

You can upload as many blobs as you like in this way. New blobs will be listed within the container.

Select the Blob (file) you just uploaded. You should be on the properties tab.

Copy the URL from the URL field and paste it into a new tab.

You should receive an error message similar to the following.


Copy
<Error>
  <Code>ResourceNotFound</Code>
  <Message>The specified resource does not exist. RequestId:4a4bd3d9-101e-005a-1a3e-84bd42000000</Message>
</Error>

Change the access level of your blob
Go back to the Azure portal.

Select Change access level.

Set the Anonymous access level to Blob (anonymous read access for blobs only).

Screenshot with Change access level highlighted.

Select OK.

Refresh the tab where you attempted to access the file earlier.

Congratulations - you've completed this exercise. You created a storage account, added a container to the storage account, and then uploaded blobs (files) to your container. Then you changed the access level so you could access your file from the internet.

Clean up
The sandbox automatically cleans up your resources when you're finished with this module.

When you're working in your own subscription, it's a good idea at the end of a project to identify whether you still need the resources you created. Resources that you leave running can cost you money. You can delete resources individually or delete the resource group to delete the entire set of resources.

Next unit: Identify Azure data migration options

Identify Azure data migration options

Now that you understand the different storage options within Azure, it’s important to also understand how to get your data and information into Azure. Azure supports both real-time migration of infrastructure, applications, and data using Azure Migrate as well as asynchronous migration of data using Azure Data Box.

Azure Migrate
Azure Migrate is a service that helps you migrate from an on-premises environment to the cloud. Azure Migrate functions as a hub to help you manage the assessment and migration of your on-premises datacenter to Azure. It provides the following:

Unified migration platform: A single portal to start, run, and track your migration to Azure.
Range of tools: A range of tools for assessment and migration. Azure Migrate tools include Azure Migrate: Discovery and assessment and Azure Migrate: Server Migration. Azure Migrate also integrates with other Azure services and tools, and with independent software vendor (ISV) offerings.
Assessment and migration: In the Azure Migrate hub, you can assess and migrate your on-premises infrastructure to Azure.
Integrated tools
In addition to working with tools from ISVs, the Azure Migrate hub also includes the following tools to help with migration:

Azure Migrate: Discovery and assessment. Discover and assess on-premises servers running on VMware, Hyper-V, and physical servers in preparation for migration to Azure.
Azure Migrate: Server Migration. Migrate VMware VMs, Hyper-V VMs, physical servers, other virtualized servers, and public cloud VMs to Azure.
Data Migration Assistant. Data Migration Assistant is a stand-alone tool to assess SQL Servers. It helps pinpoint potential problems blocking migration. It identifies unsupported features, new features that can benefit you after migration, and the right path for database migration.
Azure Database Migration Service. Migrate on-premises databases to Azure VMs running SQL Server, Azure SQL Database, or SQL Managed Instances.
Azure App Service migration assistant. Azure App Service migration assistant is a standalone tool to assess on-premises websites for migration to Azure App Service. Use Migration Assistant to migrate .NET and PHP web apps to Azure.
Azure Data Box. Use Azure Data Box products to move large amounts of offline data to Azure.
Azure Data Box
Azure Data Box is a physical migration service that helps transfer large amounts of data in a quick, inexpensive, and reliable way. The secure data transfer is accelerated by shipping you a proprietary Data Box storage device that has a maximum usable storage capacity of 80 terabytes. The Data Box is transported to and from your datacenter via a regional carrier. A rugged case protects and secures the Data Box from damage during transit.

You can order the Data Box device via the Azure portal to import or export data from Azure. Once the device is received, you can quickly set it up using the local web UI and connect it to your network. Once you’re finished transferring the data (either into or out of Azure), simply return the Data Box. If you’re transferring data into Azure, the data is automatically uploaded once Microsoft receives the Data Box back. The entire process is tracked end-to-end by the Data Box service in the Azure portal.

Use cases
Data Box is ideally suited to transfer data sizes larger than 40 TBs in scenarios with no to limited network connectivity. The data movement can be one-time, periodic, or an initial bulk data transfer followed by periodic transfers.

Here are the various scenarios where Data Box can be used to import data to Azure.

Onetime migration - when a large amount of on-premises data is moved to Azure.
Moving a media library from offline tapes into Azure to create an online media library.
Migrating your VM farm, SQL server, and applications to Azure.
Moving historical data to Azure for in-depth analysis and reporting using HDInsight.
Initial bulk transfer - when an initial bulk transfer is done using Data Box (seed) followed by incremental transfers over the network.
Periodic uploads - when large amount of data is generated periodically and needs to be moved to Azure.
Here are the various scenarios where Data Box can be used to export data from Azure.

Disaster recovery - when a copy of the data from Azure is restored to an on-premises network. In a typical disaster recovery scenario, a large amount of Azure data is exported to a Data Box. Microsoft then ships this Data Box, and the data is restored on your premises in a short time.
Security requirements - when you need to be able to export data out of Azure due to government or security requirements.
Migrate back to on-premises or to another cloud service provider - when you want to move all the data back to on-premises, or to another cloud service provider, export data via Data Box to migrate the workloads.
Once the data from your import order is uploaded to Azure, the disks on the device are wiped clean in accordance with NIST 800-88r1 standards. For an export order, the disks are erased once the device reaches the Azure datacenter.

Next unit: Identify Azure file movement options

Identify Azure file movement options

In addition to large scale migration using services like Azure Migrate and Azure Data Box, Azure also has tools designed to help you move or interact with individual files or small file groups. Among those tools are AzCopy, Azure Storage Explorer, and Azure File Sync.

AzCopy
AzCopy is a command-line utility that you can use to copy blobs or files to or from your storage account. With AzCopy, you can upload files, download files, copy files between storage accounts, and even synchronize files. AzCopy can even be configured to work with other cloud providers to help move files back and forth between clouds.

 Important

Synchronizing blobs or files with AzCopy is one-direction synchronization. When you synchronize, you designated the source and destination, and AzCopy will copy files or blobs in that direction. It doesn't synchronize bi-directionally based on timestamps or other metadata.

Azure Storage Explorer
Azure Storage Explorer is a standalone app that provides a graphical interface to manage files and blobs in your Azure Storage Account. It works on Windows, macOS, and Linux operating systems and uses AzCopy on the backend to perform all of the file and blob management tasks. With Storage Explorer, you can upload to Azure, download from Azure, or move between storage accounts.

Azure File Sync
Azure File Sync is a tool that lets you centralize your file shares in Azure Files and keep the flexibility, performance, and compatibility of a Windows file server. It’s almost like turning your Windows file server into a miniature content delivery network. Once you install Azure File Sync on your local Windows server, it will automatically stay bi-directionally synced with your files in Azure.

With Azure File Sync, you can:

Use any protocol that's available on Windows Server to access your data locally, including SMB, NFS, and FTPS.
Have as many caches as you need across the world.
Replace a failed local server by installing Azure File Sync on a new server in the same datacenter.
Configure cloud tiering so the most frequently accessed files are replicated locally, while infrequently accessed files are kept in the cloud until requested.

Next unit: Knowledge check

Knowledge check
200 XP
4 minutes
Choose the best response for each question. Then select Check your answers.

Check your knowledge

1. Which tool automatically keeps files between an on-premises Windows server and an Azure cloud environment updated? 

A- Azure File Sync

B- Azure Storage Explorer

C- AzCopy
2. Which storage redundancy option provides the highest degree of durability, with 16 nines of durability? 

A- Locally redundant storage

B- Zone-redundant storage

C- Geo-zone-redundant-storage

3. Which Azure Storage service supports big data analytics, as well as handling text and binary data types? 

A- Azure Blobs

B- Azure Files

C- Azure Disks
Microsoft Azure Fundamentals

Chapter 7
Describe Azure identity, access, and security

Introduction

In this module, you’ll be introduced to the Azure identity, access, and security services and tools. You’ll learn about directory services in Azure, authentication methods, and access control. You’ll also cover things like Zero Trust and defense in depth, and how they keep your cloud safer. You’ll wrap up with an introduction to Microsoft Defender for Cloud.

Learning objectives
After completing this module, you’ll be able to:

Describe directory services in Azure, including Microsoft Entra ID and Microsoft Entra Domain Services.
Describe authentication methods in Azure, including single sign-on (SSO), multifactor authentication (MFA), and passwordless.
Describe external identities and guest access in Azure.
Describe Microsoft Entra Conditional Access.
Describe Azure Role Based Access Control (RBAC).
Describe the concept of Zero Trust.
Describe the purpose of the defense in depth model.
Describe the purpose of Microsoft Defender for Cloud.

Next unit: Describe Azure directory services

Describe Azure directory services

Microsoft Entra ID is a directory service that enables you to sign in and access both Microsoft cloud applications and cloud applications that you develop. Microsoft Entra ID can also help you maintain your on-premises Active Directory deployment.

For on-premises environments, Active Directory running on Windows Server provides an identity and access management service that's managed by your organization. Microsoft Entra ID is Microsoft's cloud-based identity and access management service. With Microsoft Entra ID, you control the identity accounts, but Microsoft ensures that the service is available globally. If you've worked with Active Directory, Microsoft Entra ID will be familiar to you.

When you secure identities on-premises with Active Directory, Microsoft doesn't monitor sign-in attempts. When you connect Active Directory with Microsoft Entra ID, Microsoft can help protect you by detecting suspicious sign-in attempts at no extra cost. For example, Microsoft Entra ID can detect sign-in attempts from unexpected locations or unknown devices.


Who uses Microsoft Entra ID?
Microsoft Entra ID is for:

IT administrators. Administrators can use Microsoft Entra ID to control access to applications and resources based on their business requirements.
App developers. Developers can use Microsoft Entra ID to provide a standards-based approach for adding functionality to applications that they build, such as adding SSO functionality to an app or enabling an app to work with a user's existing credentials.
Users. Users can manage their identities and take maintenance actions like self-service password reset.
Online service subscribers. Microsoft 365, Microsoft Office 365, Azure, and Microsoft Dynamics CRM Online subscribers are already using Microsoft Entra ID to authenticate into their account.

What does Microsoft Entra ID do?
Microsoft Entra ID provides services such as:

Authentication: This includes verifying identity to access applications and resources. It also includes providing functionality such as self-service password reset, multifactor authentication, a custom list of banned passwords, and smart lockout services.
Single sign-on: Single sign-on (SSO) enables you to remember only one username and one password to access multiple applications. A single identity is tied to a user, which simplifies the security model. As users change roles or leave an organization, access modifications are tied to that identity, which greatly reduces the effort needed to change or disable accounts.
Application management: You can manage your cloud and on-premises apps by using Microsoft Entra ID. Features like Application Proxy, SaaS apps, the My Apps portal, and single sign-on provide a better user experience.
Device management: Along with accounts for individual people, Microsoft Entra ID supports the registration of devices. Registration enables devices to be managed through tools like Microsoft Intune. It also allows for device-based Conditional Access policies to restrict access attempts to only those coming from known devices, regardless of the requesting user account.

Can I connect my on-premises AD with Microsoft Entra ID?
If you had an on-premises environment running Active Directory and a cloud deployment using Microsoft Entra ID, you would need to maintain two identity sets. However, you can connect Active Directory with Microsoft Entra ID, enabling a consistent identity experience between cloud and on-premises.

One method of connecting Microsoft Entra ID with your on-premises AD is using Microsoft Entra Connect. Microsoft Entra Connect synchronizes user identities between on-premises Active Directory and Microsoft Entra ID. Microsoft Entra Connect synchronizes changes between both identity systems, so you can use features like SSO, multifactor authentication, and self-service password reset under both systems.


What is Microsoft Entra Domain Services?
Microsoft Entra Domain Services is a service that provides managed domain services such as domain join, group policy, lightweight directory access protocol (LDAP), and Kerberos/NTLM authentication. Just like Microsoft Entra ID lets you use directory services without having to maintain the infrastructure supporting it, with Microsoft Entra Domain Services, you get the benefit of domain services without the need to deploy, manage, and patch domain controllers (DCs) in the cloud.

A Microsoft Entra Domain Services managed domain lets you run legacy applications in the cloud that can't use modern authentication methods, or where you don't want directory lookups to always go back to an on-premises AD DS environment. You can lift and shift those legacy applications from your on-premises environment into a managed domain, without needing to manage the AD DS environment in the cloud.

Microsoft Entra Domain Services integrates with your existing Microsoft Entra tenant. This integration lets users sign into services and applications connected to the managed domain using their existing credentials. You can also use existing groups and user accounts to secure access to resources. These features provide a smoother lift-and-shift of on-premises resources to Azure.


How does Microsoft Entra Domain Services work?
When you create a Microsoft Entra Domain Services managed domain, you define a unique namespace. This namespace is the domain name. Two Windows Server domain controllers are then deployed into your selected Azure region. This deployment of DCs is known as a replica set.

You don't need to manage, configure, or update these DCs. The Azure platform handles the DCs as part of the managed domain, including backups and encryption at rest using Azure Disk Encryption.

Is information synchronized?
A managed domain is configured to perform a one-way synchronization from Microsoft Entra ID to Microsoft Entra Domain Services. You can create resources directly in the managed domain, but they aren't synchronized back to Microsoft Entra ID. In a hybrid environment with an on-premises AD DS environment, Microsoft Entra Connect synchronizes identity information with Microsoft Entra ID, which is then synchronized to the managed domain.

Diagram of Microsoft Entra Connect Sync synchronizing information back to the Microsoft Entra tenant from on-premises AD.

Applications, services, and VMs in Azure that connect to the managed domain can then use common Microsoft Entra Domain Services features such as domain join, group policy, LDAP, and Kerberos/NTLM authentication.

Next unit: Describe Azure authentication methods

Describe Azure authentication methods

Authentication is the process of establishing the identity of a person, service, or device. It requires the person, service, or device to provide some type of credential to prove who they are. Authentication is like presenting ID when you’re traveling. It doesn’t confirm that you’re ticketed, it just proves that you're who you say you are. Azure supports multiple authentication methods, including standard passwords, single sign-on (SSO), multifactor authentication (MFA), and passwordless.

For the longest time, security and convenience seemed to be at odds with each other. Thankfully, new authentication solutions provide both security and convenience.

The following diagram shows the security level compared to the convenience. Notice Passwordless authentication is high security and high convenience while passwords on their own are low security but high convenience.

Four quadrant diagram showing security vs convenience, with Passwords + 2 Factor authentication being high security but low convenience.

What's single sign-on?
Single sign-on (SSO) enables a user to sign in one time and use that credential to access multiple resources and applications from different providers. For SSO to work, the different applications and providers must trust the initial authenticator.

More identities mean more passwords to remember and change. Password policies can vary among applications. As complexity requirements increase, it becomes increasingly difficult for users to remember them. The more passwords a user has to manage, the greater the risk of a credential-related security incident.

Consider the process of managing all those identities. More strain is placed on help desks as they deal with account lockouts and password reset requests. If a user leaves an organization, tracking down all those identities and ensuring they're disabled can be challenging. If an identity is overlooked, this might allow access when it should have been eliminated.

With SSO, you need to remember only one ID and one password. Access across applications is granted to a single identity that's tied to the user, which simplifies the security model. As users change roles or leave an organization, access is tied to a single identity. This change greatly reduces the effort needed to change or disable accounts. Using SSO for accounts makes it easier for users to manage their identities and for IT to manage users.

 Important

Single sign-on is only as secure as the initial authenticator because the subsequent connections are all based on the security of the initial authenticator.

What’s multifactor authentication?
Multifactor authentication is the process of prompting a user for an extra form (or factor) of identification during the sign-in process. MFA helps protect against a password compromise in situations where the password was compromised but the second factor wasn't.

Think about how you sign into websites, email, or online services. After entering your username and password, have you ever needed to enter a code that was sent to your phone? If so, you've used multifactor authentication to sign in.

Multifactor authentication provides additional security for your identities by requiring two or more elements to fully authenticate. These elements fall into three categories:

Something the user knows – this might be a challenge question.
Something the user has – this might be a code that's sent to the user's mobile phone.
Something the user is – this is typically some sort of biometric property, such as a fingerprint or face scan.
Multifactor authentication increases identity security by limiting the impact of credential exposure (for example, stolen usernames and passwords). With multifactor authentication enabled, an attacker who has a user's password would also need to have possession of their phone or their fingerprint to fully authenticate.

Compare multifactor authentication with single-factor authentication. Under single-factor authentication, an attacker would need only a username and password to authenticate. Multifactor authentication should be enabled wherever possible because it adds enormous benefits to security.


What's Microsoft Entra multifactor authentication?
Microsoft Entra multifactor authentication is a Microsoft service that provides multifactor authentication capabilities. Microsoft Entra multifactor authentication enables users to choose an additional form of authentication during sign-in, such as a phone call or mobile app notification.

What’s passwordless authentication?
Features like MFA are a great way to secure your organization, but users often get frustrated with the additional security layer on top of having to remember their passwords. People are more likely to comply when it's easy and convenient to do so. Passwordless authentication methods are more convenient because the password is removed and replaced with something you have, plus something you are, or something you know.

Passwordless authentication needs to be set up on a device before it can work. For example, your computer is something you have. Once it’s been registered or enrolled, Azure now knows that it’s associated with you. Now that the computer is known, once you provide something you know or are (such as a PIN or fingerprint), you can be authenticated without using a password.

Each organization has different needs when it comes to authentication. Microsoft global Azure and Azure Government offer the following three passwordless authentication options that integrate with Microsoft Entra ID:

Windows Hello for Business
Microsoft Authenticator app
FIDO2 security keys
Windows Hello for Business
Windows Hello for Business is ideal for information workers that have their own designated Windows PC. The biometric and PIN credentials are directly tied to the user's PC, which prevents access from anyone other than the owner. With public key infrastructure (PKI) integration and built-in support for single sign-on (SSO), Windows Hello for Business provides a convenient method for seamlessly accessing corporate resources on-premises and in the cloud.

Microsoft Authenticator App
You can also allow your employee's phone to become a passwordless authentication method. You may already be using the Microsoft Authenticator App as a convenient multifactor authentication option in addition to a password. You can also use the Authenticator App as a passwordless option.

The Authenticator App turns any iOS or Android phone into a strong, passwordless credential. Users can sign-in to any platform or browser by getting a notification to their phone, matching a number displayed on the screen to the one on their phone, and then using their biometric (touch or face) or PIN to confirm. Refer to Download and install the Microsoft Authenticator app for installation details.

FIDO2 security keys
The FIDO (Fast IDentity Online) Alliance helps to promote open authentication standards and reduce the use of passwords as a form of authentication. FIDO2 is the latest standard that incorporates the web authentication (WebAuthn) standard.

FIDO2 security keys are an unphishable standards-based passwordless authentication method that can come in any form factor. Fast Identity Online (FIDO) is an open standard for passwordless authentication. FIDO allows users and organizations to leverage the standard to sign-in to their resources without a username or password by using an external security key or a platform key built into a device.

Users can register and then select a FIDO2 security key at the sign-in interface as their main means of authentication. These FIDO2 security keys are typically USB devices, but could also use Bluetooth or NFC. With a hardware device that handles the authentication, the security of an account is increased as there's no password that could be exposed or guessed.

Next unit: Describe Azure external identities

Describe Azure external identities

An external identity is a person, device, service, etc. that is outside your organization. Microsoft Entra External ID refers to all the ways you can securely interact with users outside of your organization. If you want to collaborate with partners, distributors, suppliers, or vendors, you can share your resources and define how your internal users can access external organizations. If you're a developer creating consumer-facing apps, you can manage your customers' identity experiences.

External identities may sound similar to single sign-on. With External Identities, external users can "bring their own identities." Whether they have a corporate or government-issued digital identity, or an unmanaged social identity like Google or Facebook, they can use their own credentials to sign in. The external user’s identity provider manages their identity, and you manage access to your apps with Microsoft Entra ID or Azure AD B2C to keep your resources protected.

Diagram showing B2B collaborators accessing your tenant and B2C collaborators accessing the AD B2C tennant.

The following capabilities make up External Identities:

Business to business (B2B) collaboration - Collaborate with external users by letting them use their preferred identity to sign-in to your Microsoft applications or other enterprise applications (SaaS apps, custom-developed apps, etc.). B2B collaboration users are represented in your directory, typically as guest users.
B2B direct connect - Establish a mutual, two-way trust with another Microsoft Entra organization for seamless collaboration. B2B direct connect currently supports Teams shared channels, enabling external users to access your resources from within their home instances of Teams. B2B direct connect users aren't represented in your directory, but they're visible from within the Teams shared channel and can be monitored in Teams admin center reports.
Microsoft Azure Active Directory business to customer (B2C) - Publish modern SaaS apps or custom-developed apps (excluding Microsoft apps) to consumers and customers, while using Azure AD B2C for identity and access management.
Depending on how you want to interact with external organizations and the types of resources you need to share, you can use a combination of these capabilities.

With Microsoft Entra ID, you can easily enable collaboration across organizational boundaries by using the Microsoft Entra B2B feature. Guest users from other tenants can be invited by administrators or by other users. This capability also applies to social identities such as Microsoft accounts.

You also can easily ensure that guest users have appropriate access. You can ask the guests themselves or a decision maker to participate in an access review and recertify (or attest) to the guests' access. The reviewers can give their input on each user's need for continued access, based on suggestions from Microsoft Entra ID. When an access review is finished, you can then make changes and remove access for guests who no longer need it.

Next unit: Describe Azure conditional access

Describe Azure conditional access

Conditional Access is a tool that Microsoft Entra ID uses to allow (or deny) access to resources based on identity signals. These signals include who the user is, where the user is, and what device the user is requesting access from.

Conditional Access helps IT administrators:

Empower users to be productive wherever and whenever.
Protect the organization's assets.
Conditional Access also provides a more granular multifactor authentication experience for users. For example, a user might not be challenged for second authentication factor if they're at a known location. However, they might be challenged for a second authentication factor if their sign-in signals are unusual or they're at an unexpected location.

During sign-in, Conditional Access collects signals from the user, makes decisions based on those signals, and then enforces that decision by allowing or denying the access request or challenging for a multifactor authentication response.

The following diagram illustrates this flow:

Diagram showing the conditional access flow of a signal leading to a decision, leading to enforcement.

Here, the signal might be the user's location, the user's device, or the application that the user is trying to access.

Based on these signals, the decision might be to allow full access if the user is signing in from their usual location. If the user is signing in from an unusual location or a location that's marked as high risk, then access might be blocked entirely or possibly granted after the user provides a second form of authentication.

Enforcement is the action that carries out the decision. For example, the action is to allow access or require the user to provide a second form of authentication.

When can I use Conditional Access?
Conditional Access is useful when you need to:

Require multifactor authentication (MFA) to access an application depending on the requester’s role, location, or network. For example, you could require MFA for administrators but not regular users or for people connecting from outside your corporate network.
Require access to services only through approved client applications. For example, you could limit which email applications are able to connect to your email service.
Require users to access your application only from managed devices. A managed device is a device that meets your standards for security and compliance.
Block access from untrusted sources, such as access from unknown or unexpected locations.

Next unit: Describe Azure role-based access control

Describe Azure role-based access control

When you have multiple IT and engineering teams, how can you control what access they have to the resources in your cloud environment? The principle of least privilege says you should only grant access up to the level needed to complete a task. If you only need read access to a storage blob, then you should only be granted read access to that storage blob. Write access to that blob shouldn’t be granted, nor should read access to other storage blobs. It’s a good security practice to follow.

However, managing that level of permissions for an entire team would become tedious. Instead of defining the detailed access requirements for each individual, and then updating access requirements when new resources are created or new people join the team, Azure enables you to control access through Azure role-based access control (Azure RBAC).

Azure provides built-in roles that describe common access rules for cloud resources. You can also define your own roles. Each role has an associated set of access permissions that relate to that role. When you assign individuals or groups to one or more roles, they receive all the associated access permissions.

So, if you hire a new engineer and add them to the Azure RBAC group for engineers, they automatically get the same access as the other engineers in the same Azure RBAC group. Similarly, if you add additional resources and point Azure RBAC at them, everyone in that Azure RBAC group will now have those permissions on the new resources as well as the existing resources.

How is role-based access control applied to resources?
Role-based access control is applied to a scope, which is a resource or set of resources that this access applies to.

The following diagram shows the relationship between roles and scopes. A management group, subscription, or resource group might be given the role of owner, so they have increased control and authority. An observer, who isn't expected to make any updates, might be given a role of Reader for the same scope, enabling them to review or observe the management group, subscription, or resource group.

A diagram showing scopes and roles. Role and scope combinations map to a specific kind of user or account, such as an observer or an admin.

Scopes include:

A management group (a collection of multiple subscriptions).
A single subscription.
A resource group.
A single resource.
Observers, users managing resources, admins, and automated processes illustrate the kinds of users or accounts that would typically be assigned each of the various roles.

Azure RBAC is hierarchical, in that when you grant access at a parent scope, those permissions are inherited by all child scopes. For example:

When you assign the Owner role to a user at the management group scope, that user can manage everything in all subscriptions within the management group.
When you assign the Reader role to a group at the subscription scope, the members of that group can view every resource group and resource within the subscription.
How is Azure RBAC enforced?
Azure RBAC is enforced on any action that's initiated against an Azure resource that passes through Azure Resource Manager. Resource Manager is a management service that provides a way to organize and secure your cloud resources.

You typically access Resource Manager from the Azure portal, Azure Cloud Shell, Azure PowerShell, and the Azure CLI. Azure RBAC doesn't enforce access permissions at the application or data level. Application security must be handled by your application.

Azure RBAC uses an allow model. When you're assigned a role, Azure RBAC allows you to perform actions within the scope of that role. If one role assignment grants you read permissions to a resource group and a different role assignment grants you write permissions to the same resource group, you have both read and write permissions on that resource group.

Next unit: Describe zero trust model

Describe zero trust model

Zero Trust is a security model that assumes the worst case scenario and protects resources with that expectation. Zero Trust assumes breach at the outset, and then verifies each request as though it originated from an uncontrolled network.

Today, organizations need a new security model that effectively adapts to the complexity of the modern environment; embraces the mobile workforce: and protects people, devices, applications, and data wherever they're located.

To address this new world of computing, Microsoft highly recommends the Zero Trust security model, which is based on these guiding principles:

Verify explicitly - Always authenticate and authorize based on all available data points.
Use least privilege access - Limit user access with Just-In-Time and Just-Enough-Access (JIT/JEA), risk-based adaptive policies, and data protection.
Assume breach - Minimize blast radius and segment access. Verify end-to-end encryption. Use analytics to get visibility, drive threat detection, and improve defenses.
Adjusting to Zero Trust
Traditionally, corporate networks were restricted, protected, and generally assumed safe. Only managed computers could join the network, VPN access was tightly controlled, and personal devices were frequently restricted or blocked.

The Zero Trust model flips that scenario. Instead of assuming that a device is safe because it’s within the corporate network, it requires everyone to authenticate. Then grants access based on authentication rather than location.

Diagram comparing zero trust authenticating everyone compared to classic relying on network location.

Next unit: Describe defense-in-depth

Describe defense-in-depth

The objective of defense-in-depth is to protect information and prevent it from being stolen by those who aren't authorized to access it.

A defense-in-depth strategy uses a series of mechanisms to slow the advance of an attack that aims at acquiring unauthorized access to data.

Layers of defense-in-depth
You can visualize defense-in-depth as a set of layers, with the data to be secured at the center and all the other layers functioning to protect that central data layer.

A diagram the defense in depth layers. From the center: data, application, compute, network, perimeter, identity & access, physical security.

Each layer provides protection so that if one layer is breached, a subsequent layer is already in place to prevent further exposure. This approach removes reliance on any single layer of protection. It slows down an attack and provides alert information that security teams can act upon, either automatically or manually.

Here's a brief overview of the role of each layer:

The physical security layer is the first line of defense to protect computing hardware in the datacenter.
The identity and access layer controls access to infrastructure and change control.
The perimeter layer uses distributed denial of service (DDoS) protection to filter large-scale attacks before they can cause a denial of service for users.
The network layer limits communication between resources through segmentation and access controls.
The compute layer secures access to virtual machines.
The application layer helps ensure that applications are secure and free of security vulnerabilities.
The data layer controls access to business and customer data that you need to protect.
These layers provide a guideline for you to help make security configuration decisions in all of the layers of your applications.

Azure provides security tools and features at every level of the defense-in-depth concept. Let's take a closer look at each layer:

Physical security
Physically securing access to buildings and controlling access to computing hardware within the datacenter are the first line of defense.

With physical security, the intent is to provide physical safeguards against access to assets. These safeguards ensure that other layers can't be bypassed, and loss or theft is handled appropriately. Microsoft uses various physical security mechanisms in its cloud datacenters.

Identity and access
The identity and access layer is all about ensuring that identities are secure, that access is granted only to what's needed, and that sign-in events and changes are logged.

At this layer, it's important to:

Control access to infrastructure and change control.
Use single sign-on (SSO) and multifactor authentication.
Audit events and changes.
Perimeter
The network perimeter protects from network-based attacks against your resources. Identifying these attacks, eliminating their impact, and alerting you when they happen are important ways to keep your network secure.

At this layer, it's important to:

Use DDoS protection to filter large-scale attacks before they can affect the availability of a system for users.
Use perimeter firewalls to identify and alert on malicious attacks against your network.
Network
At this layer, the focus is on limiting the network connectivity across all your resources to allow only what's required. By limiting this communication, you reduce the risk of an attack spreading to other systems in your network.

At this layer, it's important to:

Limit communication between resources.
Deny by default.
Restrict inbound internet access and limit outbound access where appropriate.
Implement secure connectivity to on-premises networks.
Compute
Malware, unpatched systems, and improperly secured systems open your environment to attacks. The focus in this layer is on making sure that your compute resources are secure and that you have the proper controls in place to minimize security issues.

At this layer, it's important to:

Secure access to virtual machines.
Implement endpoint protection on devices and keep systems patched and current.
Application
Integrating security into the application development lifecycle helps reduce the number of vulnerabilities introduced in code. Every development team should ensure that its applications are secure by default.

At this layer, it's important to:

Ensure that applications are secure and free of vulnerabilities.
Store sensitive application secrets in a secure storage medium.
Make security a design requirement for all application development.
Data
Those who store and control access to data are responsible for ensuring that it's properly secured. Often, regulatory requirements dictate the controls and processes that must be in place to ensure the confidentiality, integrity, and availability of the data.

In almost all cases, attackers are after data:

Stored in a database.
Stored on disk inside virtual machines.
Stored in software as a service (SaaS) applications, such as Office 365.
Managed through cloud storage.

Next unit: Describe Microsoft Defender for Cloud

Describe Microsoft Defender for Cloud

Defender for Cloud is a monitoring tool for security posture management and threat protection. It monitors your cloud, on-premises, hybrid, and multi-cloud environments to provide guidance and notifications aimed at strengthening your security posture.

Defender for Cloud provides the tools needed to harden your resources, track your security posture, protect against cyber attacks, and streamline security management. Deployment of Defender for Cloud is easy, it’s already natively integrated to Azure.

Protection everywhere you’re deployed
Because Defender for Cloud is an Azure-native service, many Azure services are monitored and protected without needing any deployment. However, if you also have an on-premises datacenter or are also operating in another cloud environment, monitoring of Azure services may not give you a complete picture of your security situation.

When necessary, Defender for Cloud can automatically deploy a Log Analytics agent to gather security-related data. For Azure machines, deployment is handled directly. For hybrid and multi-cloud environments, Microsoft Defender plans are extended to non Azure machines with the help of Azure Arc. Cloud security posture management (CSPM) features are extended to multi-cloud machines without the need for any agents.

Azure-native protections
Defender for Cloud helps you detect threats across:

Azure PaaS services – Detect threats targeting Azure services including Azure App Service, Azure SQL, Azure Storage Account, and more data services. You can also perform anomaly detection on your Azure activity logs using the native integration with Microsoft Defender for Cloud Apps (formerly known as Microsoft Cloud App Security).
Azure data services – Defender for Cloud includes capabilities that help you automatically classify your data in Azure SQL. You can also get assessments for potential vulnerabilities across Azure SQL and Storage services, and recommendations for how to mitigate them.
Networks – Defender for Cloud helps you limit exposure to brute force attacks. By reducing access to virtual machine ports, using the just-in-time VM access, you can harden your network by preventing unnecessary access. You can set secure access policies on selected ports, for only authorized users, allowed source IP address ranges or IP addresses, and for a limited amount of time.
Defend your hybrid resources
In addition to defending your Azure environment, you can add Defender for Cloud capabilities to your hybrid cloud environment to protect your non-Azure servers. To help you focus on what matters the most, you'll get customized threat intelligence and prioritized alerts according to your specific environment.

To extend protection to on-premises machines, deploy Azure Arc and enable Defender for Cloud's enhanced security features.

Defend resources running on other clouds
Defender for Cloud can also protect resources in other clouds (such as AWS and GCP).

For example, if you've connected an Amazon Web Services (AWS) account to an Azure subscription, you can enable any of these protections:

Defender for Cloud's CSPM features extend to your AWS resources. This agentless plan assesses your AWS resources according to AWS-specific security recommendations, and includes the results in the secure score. The resources will also be assessed for compliance with built-in standards specific to AWS (AWS CIS, AWS PCI DSS, and AWS Foundational Security Best Practices). Defender for Cloud's asset inventory page is a multi-cloud enabled feature helping you manage your AWS resources alongside your Azure resources.
Microsoft Defender for Containers extends its container threat detection and advanced defenses to your Amazon EKS Linux clusters.
Microsoft Defender for Servers brings threat detection and advanced defenses to your Windows and Linux EC2 instances.
Assess, Secure, and Defend
Defender for Cloud fills three vital needs as you manage the security of your resources and workloads in the cloud and on-premises:

Continuously assess – Know your security posture. Identify and track vulnerabilities.
Secure – Harden resources and services with Azure Security Benchmark.
Defend – Detect and resolve threats to resources, workloads, and services.
Diagram reinforcing assess, secure, and defend.

Continuously assess
Defender for cloud helps you continuously assess your environment. Defender for Cloud includes vulnerability assessment solutions for your virtual machines, container registries, and SQL servers.

Microsoft Defender for servers includes automatic, native integration with Microsoft Defender for Endpoint. With this integration enabled, you'll have access to the vulnerability findings from Microsoft threat and vulnerability management.

Between these assessment tools you’ll have regular, detailed vulnerability scans that cover your compute, data, and infrastructure. You can review and respond to the results of these scans all from within Defender for Cloud.

Secure
From authentication methods to access control to the concept of Zero Trust, security in the cloud is an essential basic that must be done right. In order to be secure in the cloud, you have to ensure your workloads are secure. To secure your workloads, you need security policies in place that are tailored to your environment and situation. Because policies in Defender for Cloud are built on top of Azure Policy controls, you're getting the full range and flexibility of a world-class policy solution. In Defender for Cloud, you can set your policies to run on management groups, across subscriptions, and even for a whole tenant.

One of the benefits of moving to the cloud is the ability to grow and scale as you need, adding new services and resources as necessary. Defender for Cloud is constantly monitoring for new resources being deployed across your workloads. Defender for Cloud assesses if new resources are configured according to security best practices. If not, they're flagged and you get a prioritized list of recommendations for what you need to fix. Recommendations help you reduce the attack surface across each of your resources.

The list of recommendations is enabled and supported by the Azure Security Benchmark. This Microsoft-authored, Azure-specific, benchmark provides a set of guidelines for security and compliance best practices based on common compliance frameworks.

In this way, Defender for Cloud enables you not just to set security policies, but to apply secure configuration standards across your resources.

To help you understand how important each recommendation is to your overall security posture, Defender for Cloud groups the recommendations into security controls and adds a secure score value to each control. The secure score gives you an at-a-glance indicator of the health of your security posture, while the controls give you a working list of things to consider to improve your security score and your overall security posture.

Screenshot showing the Microsoft Defender for Cloud secure score.

Defend
The first two areas were focused on assessing, monitoring, and maintaining your environment. Defender for Cloud also helps you defend your environment by providing security alerts and advanced threat protection features.

Security alerts
When Defender for Cloud detects a threat in any area of your environment, it generates a security alert. Security alerts:

Describe details of the affected resources
Suggest remediation steps
Provide, in some cases, an option to trigger a logic app in response
Whether an alert is generated by Defender for Cloud or received by Defender for Cloud from an integrated security product, you can export it. Defender for Cloud's threat protection includes fusion kill-chain analysis, which automatically correlates alerts in your environment based on cyber kill-chain analysis, to help you better understand the full story of an attack campaign, where it started, and what kind of impact it had on your resources.

Advanced threat protection
Defender for cloud provides advanced threat protection features for many of your deployed resources, including virtual machines, SQL databases, containers, web applications, and your network. Protections include securing the management ports of your VMs with just-in-time access, and adaptive application controls to create allowlists for what apps should and shouldn't run on your machines.

Next unit: Knowledge check

Knowledge check

Choose the best response for each question. Then select Check your answers.

Check your knowledge

1. Which Microsoft Entra tool can vary the credentials needed to log in based on signals, such as where the user is located? 

A- Conditional Access

B- Guest access

C- Passwordless

2. Which security model assumes the worst-case security scenario, and protects resources accordingly? 

A- Zero trust

B- Defense-in-depth

C- Role-based access control

3. A user is simultaneously assigned multiple roles that use role-based access control. What are their actual permissions? The role permissions are: Role 1 - read || Role 2 - write || Role 3 - read and write. 

A- Read only

B- Write only

C- Read and write

Microsoft Azure Fundamentals

Chapter 8
Describe cost management in Azure

Introduction

In this module, you’ll be introduced to factors that impact costs in Azure and tools to help you both predict potential costs and monitor and control costs.

Learning objectives
After completing this module, you’ll be able to:

Describe factors that can affect costs in Azure.
Compare the Pricing calculator and Total Cost of Ownership (TCO) calculator.
Describe the Microsoft Cost Management Tool.
Describe the purpose of tags.

Next unit: Describe factors that can affect costs in Azure

Describe factors that can affect costs in Azure

The following video provides an introduction to things that can impact your costs in Azure.


Azure shifts development costs from the capital expense (CapEx) of building out and maintaining infrastructure and facilities to an operational expense (OpEx) of renting infrastructure as you need it, whether it’s compute, storage, networking, and so on.

That OpEx cost can be impacted by many factors. Some of the impacting factors are:

Resource type
Consumption
Maintenance
Geography
Subscription type
Azure Marketplace
Resource type
A number of factors influence the cost of Azure resources. The type of resources, the settings for the resource, and the Azure region will all have an impact on how much a resource costs. When you provision an Azure resource, Azure creates metered instances for that resource. The meters track the resources' usage and generate a usage record that is used to calculate your bill.

Examples
With a storage account, you specify a type such as blob, a performance tier, an access tier, redundancy settings, and a region. Creating the same storage account in different regions may show different costs and changing any of the settings may also impact the price.

Screenshot of storage blob settings showing hot and cool access tiers.

With a virtual machine (VM), you may have to consider licensing for the operating system or other software, the processor and number of cores for the VM, the attached storage, and the network interface. Just like with storage, provisioning the same virtual machine in different regions may result in different costs.

Screenshot of Azure virtual machine settings showing the virtual machine size options.

Consumption
Pay-as-you-go has been a consistent theme throughout, and that’s the cloud payment model where you pay for the resources that you use during a billing cycle. If you use more compute this cycle, you pay more. If you use less in the current cycle, you pay less. It’s a straight forward pricing mechanism that allows for maximum flexibility.

However, Azure also offers the ability to commit to using a set amount of cloud resources in advance and receiving discounts on those “reserved” resources. Many services, including databases, compute, and storage all provide the option to commit to a level of use and receive a discount, in some cases up to 72 percent.

When you reserve capacity, you’re committing to using and paying for a certain amount of Azure resources during a given period (typically one or three years). With the back-up of pay-as-you-go, if you see a sudden surge in demand that eclipses what you’ve pre-reserved, you just pay for the additional resources in excess of your reservation. This model allows you to recognize significant savings on reliable, consistent workloads while also having the flexibility to rapidly increase your cloud footprint as the need arises.

Maintenance
The flexibility of the cloud makes it possible to rapidly adjust resources based on demand. Using resource groups can help keep all of your resources organized. In order to control costs, it’s important to maintain your cloud environment. For example, every time you provision a VM, additional resources such as storage and networking are also provisioned. If you deprovision the VM, those additional resources may not deprovision at the same time, either intentionally or unintentionally. By keeping an eye on your resources and making sure you’re not keeping around resources that are no longer needed, you can help control cloud costs.

Geography
When you provision most resources in Azure, you need to define a region where the resource deploys. Azure infrastructure is distributed globally, which enables you to deploy your services centrally or closest to your customers, or something in between. With this global deployment comes global pricing differences. The cost of power, labor, taxes, and fees vary depending on the location. Due to these variations, Azure resources can differ in costs to deploy depending on the region.

Network traffic is also impacted based on geography. For example, it’s less expensive to move information within Europe than to move information from Europe to Asia or South America.

Network Traffic
Billing zones are a factor in determining the cost of some Azure services.

Bandwidth refers to data moving in and out of Azure datacenters. Some inbound data transfers (data going into Azure datacenters) are free. For outbound data transfers (data leaving Azure datacenters), data transfer pricing is based on zones.

A zone is a geographical grouping of Azure regions for billing purposes. The bandwidth pricing page has additional information on pricing for data ingress, egress, and transfer.

Subscription type
Some Azure subscription types also include usage allowances, which affect costs.

For example, an Azure free trial subscription provides access to a number of Azure products that are free for 12 months. It also includes credit to spend within your first 30 days of sign-up. You'll get access to more than 25 products that are always free (based on resource and region availability).

Azure Marketplace
Azure Marketplace lets you purchase Azure-based solutions and services from third-party vendors. This could be a server with software preinstalled and configured, or managed network firewall appliances, or connectors to third-party backup services. When you purchase products through Azure Marketplace, you may pay for not only the Azure services that you’re using, but also the services or expertise of the third-party vendor. Billing structures are set by the vendor.

All solutions available in Azure Marketplace are certified and compliant with Azure policies and standards. The certification policies may vary based on the service or solution type and Azure service involved. Commercial marketplace certification policies has additional information on Azure Marketplace certifications.

Next unit: Compare the Pricing and Total Cost of Ownership calculators

Compare the Pricing and Total Cost of Ownership calculators

The pricing calculator and the total cost of ownership (TCO) calculator are two calculators that help you understand potential Azure expenses. Both calculators are accessible from the internet, and both calculators allow you to build out a configuration. However, the two calculators have very different purposes.

Pricing calculator
The pricing calculator is designed to give you an estimated cost for provisioning resources in Azure. You can get an estimate for individual resources, build out a solution, or use an example scenario to see an estimate of the Azure spend. The pricing calculator’s focus is on the cost of provisioned resources in Azure.

 Note

The Pricing calculator is for information purposes only. The prices are only an estimate. Nothing is provisioned when you add resources to the pricing calculator, and you won't be charged for any services you select.

With the pricing calculator, you can estimate the cost of any provisioned resources, including compute, storage, and associated network costs. You can even account for different storage options like storage type, access tier, and redundancy.

Screenshot of the pricing calculator for reference.

TCO calculator
The TCO calculator is designed to help you compare the costs for running an on-premises infrastructure compared to an Azure Cloud infrastructure. With the TCO calculator, you enter your current infrastructure configuration, including servers, databases, storage, and outbound network traffic. The TCO calculator then compares the anticipated costs for your current environment with an Azure environment supporting the same infrastructure requirements.

With the TCO calculator, you enter your configuration, add in assumptions like power and IT labor costs, and are presented with an estimation of the cost difference to run the same environment in your current datacenter or in Azure.

Screenshot of the Total Cost of Ownership calculator.

Next unit: Exercise - Estimate workload costs by using the Pricing calculator

Exercise - Estimate workload costs by using the Pricing calculator

In this exercise, you use the Pricing calculator to estimate the cost of running a basic web application on Azure.

Start by defining which Azure services you need.

 Note

The Pricing calculator is for information purposes only. The prices are only an estimate, and you won't be charged for any services you select.

Define your requirements
Before you run the Pricing calculator, you need a sense of what Azure services you need.

For a basic web application hosted in your datacenter, you might run a configuration similar to the following.

An ASP.NET web application that runs on Windows. The web application provides information about product inventory and pricing. There are two virtual machines that are connected through a central load balancer. The web application connects to a SQL Server database that holds inventory and pricing information.

To migrate to Azure, you might:

Use Azure Virtual Machines instances, similar to the virtual machines used in your datacenter.
Use Azure Application Gateway for load balancing.
Use Azure SQL Database to hold inventory and pricing information.
Here's a diagram that shows the basic configuration:

A diagram showing a potential Azure solution for hosting an application.

In practice, you would define your requirements in greater detail. But here are some basic facts and requirements to get you started:

The application is used internally. It's not accessible to customers.
This application doesn't require a massive amount of computing power.
The virtual machines and the database run all the time (730 hours per month).
The network processes about 1 TB of data per month.
The database doesn't need to be configured for high-performance workloads and requires no more than 32 GB of storage.
Explore the Pricing calculator
Let's start with a quick tour of the Pricing calculator.

Go to the Pricing calculator.

Notice the following tabs:

A screenshot of the Pricing calculator menu bar with the Products tab selected.

Products This is where you choose the Azure services that you want to include in your estimate. You'll likely spend most of your time here.
Example scenarios Here you'll find several reference architectures, or common cloud-based solutions that you can use as a starting point.
Saved estimates Here you'll find your previously saved estimates.
FAQs Here you'll discover answers to frequently asked questions about the Pricing calculator.
Estimate your solution
Here you add each Azure service that you need to the calculator. Then you configure each service to fit your needs.

 Tip

Make sure you have a clean calculator with nothing listed in the estimate. You can reset the estimate by selecting the trash can icon next to each item.

Add services to the estimate
On the Products tab, select the service from each of these categories:

Category	Service
Compute	Virtual Machines
Databases	Azure SQL Database
Networking	Application Gateway
Scroll to the bottom of the page. Each service is listed with its default configuration.

Configure services to match your requirements
Under Virtual Machines, set these values:

Setting	Value
Region	West US
Operating system	Windows
Type	(OS Only)
Tier	Standard
Instance	D2 v3
Virtual machines	2 x 730 Hours
Leave the remaining settings at their current values.

Under Azure SQL Database, set these values:

Setting	Value
Region	West US
Type	Single Database
Backup storage tier	RA-GRS
Purchase model	vCore
Service tier	General Purpose
Compute tier	Provisioned
Generation	Gen 5
Instance	8 vCore
Leave the remaining settings at their current values.

Under Application Gateway, set these values:

Setting	Value
Region	West US
Tier	Web Application Firewall
Size	Medium
Gateway hours	2 x 730 Hours
Data processed	1 TB
Outbound data transfer	5 GB
Leave the remaining settings at their current values.

Review, share, and save your estimate
At the bottom of the page, you see the total estimated cost of running the solution. You can change the currency type if you want.

At this point, you have a few options:

Select Export to save your estimate as an Excel document.
Select Save or Save as to save your estimate to the Saved Estimates tab for later.
Select Share to generate a URL so you can share the estimate with your team.
You now have a cost estimate that you can share with your team. You can make adjustments as you discover any changes to your requirements.

Experiment with some of the options you worked with here, or create a purchase plan for a workload you want to run on Azure.

Next unit: Exercise - Compare workload costs using the TCO calculator

Exercise - Compare workload costs using the TCO calculator

In this exercise, you use the Total Cost of Ownership (TCO) Calculator to compare the cost of running a sample workload in your datacenter versus on Azure.

Assume you're considering moving some of your on-premises workloads to the cloud. But first, you need to understand more about moving from a relatively fixed cost structure to an ongoing monthly cost structure.

You'll need to investigate whether there are any potential cost savings in moving your datacenter to the cloud over the next three years. You need to take into account all of the potentially hidden costs involved with operating on-premises and in the cloud.

Instead of manually collecting everything you think might be included, you use the TCO Calculator as a starting point. You adjust the provided cost assumptions to match your on-premises environment.

 Note

Remember, you don't need an Azure subscription to work with the TCO Calculator.

Let's say that:

You run two sets, or banks, of 50 virtual machines (VMs) in each bank.
The first bank of VMs runs Windows Server under Hyper-V virtualization.
The second bank of VMs runs Linux under VMware virtualization.
There's also a storage area network (SAN) with 60 TB of disk storage.
You consume an estimated 15 TB of outbound network bandwidth each month.
There are also a number of databases involved, but for now, you'll omit those details.
Recall that the TCO Calculator involves three steps:

Illustration of the three steps: define your workloads, adjust assumptions, and view the report.

Define your workloads
Enter the specifications of your on-premises infrastructure into the TCO Calculator.

Go to the TCO Calculator.

Under Define your workloads, select Add server workload to create a row for your bank of Windows Server VMs.

Under Servers, set the value for each of these settings:

Setting	Value
Name	Servers: Windows VMs
Workload	Windows/Linux Server
Environment	Virtual Machines
Operating system	Windows
Operating System License	Datacenter
VMs	50
Virtualization	Hyper-V
Core(s)	8
RAM (GB)	16
Optimize by	CPU
Windows Server 2008/2008 R2	Off
Select Add server workload to create a second row for your bank of Linux VMs. Then specify these settings:

Setting	Value
Name	Servers: Linux VMs
Workload	Windows/Linux Server
Environment	Virtual Machines
Operating system	Linux
VMs	50
Virtualization	VMware
Core(s)	8
RAM (GB)	16
Optimize by	CPU
Under Storage, select Add storage. Then specify these settings:

Setting	Value
Name	Server Storage
Storage type	Local Disk/SAN
Disk type	HDD
Capacity	60 TB
Backup	120 TB
Archive	0 TB
Under Networking, set Outbound bandwidth to 15 TB.

Select Next.

Adjust assumptions
Here, you specify your currency. For brevity, you leave the remaining fields at their default values.

In practice, you would adjust any cost assumptions and make any adjustments to match your current on-premises environment.

At the top of the page, select your currency. This example uses US Dollar ($).
Select Next.
View the report
Take a moment to review the generated report.

Remember, you've been tasked to investigate cost savings for your European datacenter over the next three years.

To make these adjustments:

Set Timeframe to 3 Years.
Set Region to North Europe.
Scroll to the summary at the bottom. You see a comparison of running your workloads in the datacenter versus on Azure.

Select Download to download or print a copy of the report in PDF format.

Great work. You now have the information that you can share with your Chief Financial Officer. If you need to make adjustments, you can revisit the TCO Calculator to generate a fresh report.

Next unit: Describe the Microsoft Cost Management tool

Describe the Microsoft Cost Management tool

Microsoft Azure is a global cloud provider, meaning you can provision resources anywhere in the world. You can provision resources rapidly to meet a sudden demand, or to test out a new feature, or on accident. If you accidentally provision new resources, you may not be aware of them until it’s time for your invoice. Cost Management is a service that helps avoid those situations.

What is Cost Management?
Cost Management provides the ability to quickly check Azure resource costs, create alerts based on resource spend, and create budgets that can be used to automate management of resources.

Cost analysis is a subset of Cost Management that provides a quick visual for your Azure costs. Using cost analysis, you can quickly view the total cost in a variety of different ways, including by billing cycle, region, resource, and so on.

Screenshot of initial view of cost analysis in the Azure portal.

You use cost analysis to explore and analyze your organizational costs. You can view aggregated costs by organization to understand where costs are accrued and to identify spending trends. And you can see accumulated costs over time to estimate monthly, quarterly, or even yearly cost trends against a budget.

Cost alerts
Cost alerts provide a single location to quickly check on all of the different alert types that may show up in the Cost Management service. The three types of alerts that may show up are:

Budget alerts
Credit alerts
Department spending quota alerts.
Budget alerts
Budget alerts notify you when spending, based on usage or cost, reaches or exceeds the amount defined in the alert condition of the budget. Cost Management budgets are created using the Azure portal or the Azure Consumption API.

In the Azure portal, budgets are defined by cost. Budgets are defined by cost or by consumption usage when using the Azure Consumption API. Budget alerts support both cost-based and usage-based budgets. Budget alerts are generated automatically whenever the budget alert conditions are met. You can view all cost alerts in the Azure portal. Whenever an alert is generated, it appears in cost alerts. An alert email is also sent to the people in the alert recipients list of the budget.

Credit alerts
Credit alerts notify you when your Azure credit monetary commitments are consumed. Monetary commitments are for organizations with Enterprise Agreements (EAs). Credit alerts are generated automatically at 90% and at 100% of your Azure credit balance. Whenever an alert is generated, it's reflected in cost alerts, and in the email sent to the account owners.

Department spending quota alerts
Department spending quota alerts notify you when department spending reaches a fixed threshold of the quota. Spending quotas are configured in the EA portal. Whenever a threshold is met, it generates an email to department owners, and appears in cost alerts. For example, 50 percent or 75 percent of the quota.

Budgets
A budget is where you set a spending limit for Azure. You can set budgets based on a subscription, resource group, service type, or other criteria. When you set a budget, you will also set a budget alert. When the budget hits the budget alert level, it will trigger a budget alert that shows up in the cost alerts area. If configured, budget alerts will also send an email notification that a budget alert threshold has been triggered.

A more advanced use of budgets enables budget conditions to trigger automation that suspends or otherwise modifies resources once the trigger condition has occurred.

Next unit: Describe the purpose of tags

Describe the purpose of tags

As your cloud usage grows, it's increasingly important to stay organized. A good organization strategy helps you understand your cloud usage and can help you manage costs.

One way to organize related resources is to place them in their own subscriptions. You can also use resource groups to manage related resources. Resource tags are another way to organize resources. Tags provide extra information, or metadata, about your resources. This metadata is useful for:

Resource management Tags enable you to locate and act on resources that are associated with specific workloads, environments, business units, and owners.
Cost management and optimization Tags enable you to group resources so that you can report on costs, allocate internal cost centers, track budgets, and forecast estimated cost.
Operations management Tags enable you to group resources according to how critical their availability is to your business. This grouping helps you formulate service-level agreements (SLAs). An SLA is an uptime or performance guarantee between you and your users.
Security Tags enable you to classify data by its security level, such as public or confidential.
Governance and regulatory compliance Tags enable you to identify resources that align with governance or regulatory compliance requirements, such as ISO 27001. Tags can also be part of your standards enforcement efforts. For example, you might require that all resources be tagged with an owner or department name.
Workload optimization and automation Tags can help you visualize all of the resources that participate in complex deployments. For example, you might tag a resource with its associated workload or application name and use software such as Azure DevOps to perform automated tasks on those resources.
How do I manage resource tags?
You can add, modify, or delete resource tags through Windows PowerShell, the Azure CLI, Azure Resource Manager templates, the REST API, or the Azure portal.

You can use Azure Policy to enforce tagging rules and conventions. For example, you can require that certain tags be added to new resources as they're provisioned. You can also define rules that reapply tags that have been removed. Resources don't inherit tags from subscriptions and resource groups, meaning that you can apply tags at one level and not have those tags automatically show up at a different level, allowing you to create custom tagging schemas that change depending on the level (resource, resource group, subscription, and so on).

An example tagging structure
A resource tag consists of a name and a value. You can assign one or more tags to each Azure resource.

Name	Value
AppName	The name of the application that the resource is part of.
CostCenter	The internal cost center code.
Owner	The name of the business owner who's responsible for the resource.
Environment	An environment name, such as "Prod," "Dev," or "Test."
Impact	How important the resource is to business operations, such as "Mission-critical," "High-impact," or "Low-impact."
Keep in mind that you don't need to enforce that a specific tag is present on all of your resources. For example, you might decide that only mission-critical resources have the Impact tag. All non-tagged resources would then not be considered as mission-critical.

Next unit: Knowledge check

Knowledge check

Choose the best response for each question. Then select Check your answers.

Check your knowledge

1. What Azure feature can help stay organized and track usage based on metadata associated with resources? 

A- Tags

B- Tracers

C- Values

2. What’s the best method to estimate the cost of migrating to the cloud while incurring minimal costs? 

A- Migrate a small sample to the cloud and track costs over time.

B- Use the Total Cost of Ownership calculator to estimate expected costs.

C- Migrate to the cloud, but track usage closely using tags to rapidly understand costs.

Microsoft Azure Fundamentals

Chapter 9
Describe features and tools in Azure for governance and compliance

Introduction

In this module, you’ll be introduced to some of the features and tools you can use to help with governance of your Azure environment. You’ll also learn about tools you can use to help keep resources in compliance with corporate or regulatory requirements.

Learning objectives
After completing this module, you’ll be able to:

Describe the purpose of Microsoft Purview.
Describe the purpose of Azure Policy.
Describe the purpose of resource locks.
Describe the purpose of the Service Trust portal.

Next unit: Describe the purpose of Microsoft Purview

Describe the purpose of Microsoft Purview

Microsoft Purview is a family of data governance, risk, and compliance solutions that helps you get a single, unified view into your data. Microsoft Purview brings insights about your on-premises, multicloud, and software-as-a-service data together.

With Microsoft Purview, you can stay up-to-date on your data landscape thanks to:

Automated data discovery
Sensitive data classification
End-to-end data lineage
Two main solution areas comprise Microsoft Purview: risk and compliance and unified data governance.

Illustration showing the main areas for Microsoft Purview.

Microsoft Purview risk and compliance solutions
Microsoft 365 features as a core component of the Microsoft Purview risk and compliance solutions. Microsoft Teams, OneDrive, and Exchange are just some of the Microsoft 365 services that Microsoft Purview uses to help manage and monitor your data. Microsoft Purview, by managing and monitoring your data, is able to help your organization:

Protect sensitive data across clouds, apps, and devices.
Identify data risks and manage regulatory compliance requirements.
Get started with regulatory compliance.
Unified data governance
Microsoft Purview has robust, unified data governance solutions that help manage your on-premises, multicloud, and software as a service data. Microsoft Purview’s robust data governance capabilities enable you to manage your data stored in Azure, SQL and Hive databases, locally, and even in other clouds like Amazon S3.

Microsoft Purview’s unified data governance helps your organization:

Create an up-to-date map of your entire data estate that includes data classification and end-to-end lineage.
Identify where sensitive data is stored in your estate.
Create a secure environment for data consumers to find valuable data.
Generate insights about how your data is stored and used.
Manage access to the data in your estate securely and at scale.

Next unit: Describe the purpose of Azure Policy

Describe the purpose of Azure Policy

How do you ensure that your resources stay compliant? Can you be alerted if a resource's configuration has changed?

Azure Policy is a service in Azure that enables you to create, assign, and manage policies that control or audit your resources. These policies enforce different rules across your resource configurations so that those configurations stay compliant with corporate standards.

How does Azure Policy define policies?
Azure Policy enables you to define both individual policies and groups of related policies, known as initiatives. Azure Policy evaluates your resources and highlights resources that aren't compliant with the policies you've created. Azure Policy can also prevent noncompliant resources from being created.

Azure Policies can be set at each level, enabling you to set policies on a specific resource, resource group, subscription, and so on. Additionally, Azure Policies are inherited, so if you set a policy at a high level, it will automatically be applied to all of the groupings that fall within the parent. For example, if you set an Azure Policy on a resource group, all resources created within that resource group will automatically receive the same policy.

Azure Policy comes with built-in policy and initiative definitions for Storage, Networking, Compute, Security Center, and Monitoring. For example, if you define a policy that allows only a certain size for the virtual machines (VMs) to be used in your environment, that policy is invoked when you create a new VM and whenever you resize existing VMs. Azure Policy also evaluates and monitors all current VMs in your environment, including VMs that were created before the policy was created.

In some cases, Azure Policy can automatically remediate noncompliant resources and configurations to ensure the integrity of the state of the resources. For example, if all resources in a certain resource group should be tagged with AppName tag and a value of "SpecialOrders," Azure Policy will automatically apply that tag if it is missing. However, you still retain full control of your environment. If you have a specific resource that you don’t want Azure Policy to automatically fix, you can flag that resource as an exception – and the policy won’t automatically fix that resource.

Azure Policy also integrates with Azure DevOps by applying any continuous integration and delivery pipeline policies that pertain to the pre-deployment and post-deployment phases of your applications.

What are Azure Policy initiatives?
An Azure Policy initiative is a way of grouping related policies together. The initiative definition contains all of the policy definitions to help track your compliance state for a larger goal.

For example, Azure Policy includes an initiative named Enable Monitoring in Azure Security Center. Its goal is to monitor all available security recommendations for all Azure resource types in Azure Security Center.

Under this initiative, the following policy definitions are included:

Monitor unencrypted SQL Database in Security Center This policy monitors for unencrypted SQL databases and servers.
Monitor OS vulnerabilities in Security Center This policy monitors servers that don't satisfy the configured OS vulnerability baseline.
Monitor missing Endpoint Protection in Security Center This policy monitors for servers that don't have an installed endpoint protection agent.
In fact, the Enable Monitoring in Azure Security Center initiative contains over 100 separate policy definitions.

Next unit: Describe the purpose of resource locks

Describe the purpose of resource locks

A resource lock prevents resources from being accidentally deleted or changed.

Even with Azure role-based access control (Azure RBAC) policies in place, there's still a risk that people with the right level of access could delete critical cloud resources. Resource locks prevent resources from being deleted or updated, depending on the type of lock. Resource locks can be applied to individual resources, resource groups, or even an entire subscription. Resource locks are inherited, meaning that if you place a resource lock on a resource group, all of the resources within the resource group will also have the resource lock applied.

Types of Resource Locks
There are two types of resource locks, one that prevents users from deleting and one that prevents users from changing or deleting a resource.

Delete means authorized users can still read and modify a resource, but they can't delete the resource.
ReadOnly means authorized users can read a resource, but they can't delete or update the resource. Applying this lock is similar to restricting all authorized users to the permissions granted by the Reader role.
How do I manage resource locks?
You can manage resource locks from the Azure portal, PowerShell, the Azure CLI, or from an Azure Resource Manager template.

To view, add, or delete locks in the Azure portal, go to the Settings section of any resource's Settings pane in the Azure portal.

A screenshot showing the resource lock control, under settings, for a storage account.

How do I delete or change a locked resource?
Although locking helps prevent accidental changes, you can still make changes by following a two-step process.

To modify a locked resource, you must first remove the lock. After you remove the lock, you can apply any action you have permissions to perform. Resource locks apply regardless of RBAC permissions. Even if you're an owner of the resource, you must still remove the lock before you can perform the blocked activity.

Next unit: Exercise - Configure a resource lock

Exercise - Configure a resource lock

In this exercise, you’ll create a resource and configure a resource lock. Storage accounts are one of the easiest types of resource locks to quickly see the impact, so you’ll use a storage account for this exercise.

This exercise is a Bring your own subscription exercise, meaning you’ll need to provide your own Azure subscription to complete the exercise. Don’t worry though, the entire exercise can be completed for free with the 12 month free services when you sign up for an Azure account.

For help with signing up for an Azure account, see the Create an Azure account learning module.

Once you’ve created your free account, follow the steps below. If you don’t have an Azure account, you can review the steps to see the process for adding a simple resource lock to a resource.

Task 1: Create a resource
In order to apply a resource lock, you have to have a resource created in Azure. The first task focuses on creating a resource that you can then lock in subsequent tasks.

Sign in to the Azure portal at https://portal.azure.com

Select Create a resource.

Under Categories, select Storage.

Under Storage Account, select Create.

On the Basics tab of the Create storage account blade, fill in the following information. Leave the defaults for everything else.

Setting	Value
Resource group	Create new
Storage account name	enter a unique storage account name
Location	default
Performance	Standard
Redundancy	Locally redundant storage (LRS)
Select Review + Create to review your storage account settings and allow Azure to validate the configuration.

Once validated, select Create. Wait for the notification that the account was successfully created.

Select Go to resource.

Task 2: Apply a read-only resource lock
In this task you apply a read-only resource lock to the storage account. What impact do you think that will have on the storage account?

Scroll down until you find the Settings section of the blade on the left of the screen.

Select Locks.

Select + Add.

Screenshot of the Add lock feature on a storage account set for a read-only lock.

Enter a Lock name.

Verify the Lock type is set to Read-only.

Select OK.

Task 3: Add a container to the storage account
In this task, you add a container to the storage account, this container is where you can store your blobs.

Scroll up until you find the Data storage section of the blade on the left of the screen.

Select Containers.

Select + Container.

Screenshot of the add container process outlined in this task.

Enter a container name and select Create.

You should receive an error message: Failed to create storage container.

Screenshot of the Failed to create storage container error message.

 Note

The error message lets you know that you couldn't create a storage container because a lock is in place. The read-only lock prevents any create or update operations on the storage account, so you're unable to create a storage container.

Task 4: Modify the resource lock and create a storage container
Scroll down until you find the Settings section of the blade on the left of the screen.

Select Locks.

Select the read-only resource lock you created.

Change the Lock type to Delete and select OK.

Screenshot midway through task process of changing the lock type on a resource lock.

Scroll up until you find the Data storage section of the blade on the left of the screen.

Select Containers.

Select + Container.

Enter a container name and select Create.

Your storage container should appear in your list of containers.

You can now understand how the read-only lock prevented you from adding a container to your storage account. Once the lock type was changed (you could have removed it instead), you were able to add a container.

Task 5: Delete the storage account
You'll actually do this last task twice. Remember that there is a delete lock on the storage account, so you won't actually be able to delete the storage account yet.

Scroll up until you find Overview at the top of the blade on the left of the screen.

Select Overview.

Select Delete.

Screenshot of the deletion process for deleting a storage account.

You should get a notification letting you know you can't delete the resource because it has a delete lock. In order to delete the storage account, you'll need to remove the delete lock.

Screenshot of the Delete storage account error, explaining that a resource lock prevents deletion.

Task 6: Remove the delete lock and delete the storage account
In the final task, you remove the resource lock and delete the storage account from your Azure account. This step is important. You want to make sure you don't have any idle resource just sitting in your account.

Select your storage account name in the breadcrumb at the top of the screen.

Scroll down until you find the Settings section of the blade on the left of the screen.

Select Locks.

Select Delete.

Select Home in the breadcrumb at the top of the screen.

Select Storage accounts

Select the storage account you used for this exercise.

Select Delete.

To prevent accidental deletion, Azure prompts you to enter the name of the storage account you want to delete. Enter the name of the storage account and select Delete.

Screenshot of the deletion confirmation message before deleting a storage account.

You should receive a message that the storage account was deleted. If you go Home > Storage accounts, you should see that the storage account you created for this exercise is gone.

Congratulations! You've completed configuring, updating, and removing a resource lock on an Azure resource.

 Important

Make sure you complete Task 6, the removal of the storage account. You are solely responsible for the resources in your Azure account. Make sure you clean up your account after completing this exercise.

Next unit: Describe the purpose of the Service Trust portal

Describe the purpose of the Service Trust portal

The Microsoft Service Trust Portal is a portal that provides access to various content, tools, and other resources about Microsoft security, privacy, and compliance practices.

The Service Trust Portal contains details about Microsoft's implementation of controls and processes that protect our cloud services and the customer data therein. To access some of the resources on the Service Trust Portal, you must sign in as an authenticated user with your Microsoft cloud services account (Microsoft Entra organization account). You'll need to review and accept the Microsoft non-disclosure agreement for compliance materials.

Accessing the Service Trust Portal
You can access the Service Trust Portal at https://servicetrust.microsoft.com/.

Screenshot of the service trust portal with the main menu items visible.

The Service Trust Portal features and content are accessible from the main menu. The categories on the main menu are:

Service Trust Portal provides a quick access hyperlink to return to the Service Trust Portal home page.
My Library lets you save (or pin) documents to quickly access them on your My Library page. You can also set up to receive notifications when documents in your My Library are updated.
All Documents is a single landing place for documents on the service trust portal. From All Documents, you can pin documents to have them show up in your My Library.
 Note

Service Trust Portal reports and documents are available to download for at least 12 months after publishing or until a new version of document becomes available.

Next unit: Knowledge Check

Knowledge Check

Choose the best response for each question. Then select Check your answers.

Check your knowledge

1. How can you prevent creation of non-compliant resources, without having to manually evaluate each resource? 

A- Azure Policy

B- Azure Purview

C- Azure Resource Monitor

2. What's the best way to prevent inadvertent deletion of a resource? 

A- Azure Policy

B- Microsoft Purview

C- Azure resource locks
Azure Administrator Associate
Chapter 1: Prerequisites for Azure administrators

Modules in this learning path

Use Azure Resource Manager
You'll learn how to use resource groups to organize your Azure resources.


Introduction to Azure Cloud Shell
Describe Microsoft Azure Cloud Shell, learn how it works, and explore basic steps for its usage.

Introduction to Bash

Use Bash to manage IT infrastructure.

Introduction to PowerShell
Learn about the basics of PowerShell, a cross-platform command-line shell and scripting language that's built for task automation and configuration management. Learn what PowerShell is, what it's used for, and how to use it.

Configure resources with Azure Resource Manager templates
You'll learn how to use Azure Resource Manager templates to consistently deploy assets.



Point 1: Use Azure Resource Manager
You'll learn how to use resource groups to organize your Azure resources.

Learning objectives
After completing this module, you'll be able to:

Identify the features and usage cases for Azure Resource Manager.
Describe each Azure Resource Manager component and its usage.
Organize your Azure resources with resource groups.
Apply Azure Resource Manager locks.
Move Azure resources between groups, subscriptions, and regions.
Remove resources and resource groups.
Apply and track resource limits.

Introduction
Scenario
Your company is beginning to create resources in Azure. There is no organizational plan for standardizing the effort. There have been several instances where critical resources were inadvertently deleted. It is difficult to determine who owns which resource.

You need to use resource groups to organize the company's Azure resources.

Skills measured
Managing resources is part of Exam AZ-104: Microsoft Azure Administrator.

Manage Azure identities and governance (15–20%)

Manage subscriptions and governance

Configure resource locks.
Manage resource groups.
Deploy and manage Azure compute resources (20–25%)

Configure VMs

Move VMs from one resource group to another.
Learning objectives
In this module, you'll learn how to:

Identify the features and usage cases for Azure Resource Manager.
Describe each Azure Resource Manager component and its usage.
Organize your Azure resources with resource groups.
Apply Azure Resource Manager locks.
Move Azure resources between groups, subscriptions, and regions.
Remove resources and resource groups.
Apply and track resource limits.
Prerequisites
None

Next unit: Review Azure Resource Manager benefits

1- Review Azure Resource Manager benefits

The infrastructure for your application is typically made up of many components – maybe a virtual machine, storage account, and virtual network, or a web app, database, database server, and third-party services. These components are not separate entities, instead they are related and interdependent parts of a single entity. You want to deploy, manage, and monitor them as a group.

Azure Resource Manager enables you to work with the resources in your solution as a group. You can deploy, update, or delete all the resources for your solution in a single, coordinated operation. You use a template for deployment and that template can work for different environments such as testing, staging, and production. Azure Resource Manager provides security, auditing, and tagging features to help you manage your resources after deployment.

Consistent management layer
Azure Resource Manager provides a consistent management layer to perform tasks through Azure PowerShell, Azure CLI, Azure portal, REST API, and client SDKs. Choose the tools and APIs that work best for you.

The following image shows how all the tools interact with the same Azure Resource Manager API. The API passes requests to the Azure Resource Manager service, which authenticates and authorizes the requests. Azure Resource Manager then routes the requests to the appropriate resource providers.

Diagram of the Resource Manager request model.

Benefits
Azure Resource Manager provides several benefits:

You can deploy, manage, and monitor all the resources for your solution as a group, rather than handling these resources individually.
You can repeatedly deploy your solution throughout the development lifecycle and have confidence your resources are deployed in a consistent state.
You can manage your infrastructure through declarative templates rather than scripts.
You can define the dependencies between resources so they're deployed in the correct order.
You can apply access control to all services in your resource group because Role-Based Access Control (RBAC) is natively integrated into the management platform.
You can apply tags to resources to logically organize all the resources in your subscription.
You can clarify your organization's billing by viewing costs for a group of resources sharing the same tag.
Guidance
The following suggestions help you take full advantage of Azure Resource Manager when working with your solutions.

Define and deploy your infrastructure through the declarative syntax in Azure Resource Manager templates, rather than through imperative commands.
Define all deployment and configuration steps in the template. You should have no manual steps for setting up your solution.
Run imperative commands to manage your resources, such as to start or stop an app or machine.
Arrange resources with the same lifecycle in a resource group. Use tags for all other organizing of resources.

Next unit: Review Azure resource terminology

2- Review Azure resource terminology

If you're new to Azure Resource Manager, there are some terms you might not be familiar with.

resource - A manageable item that is available through Azure. Some common resources are a virtual machine, storage account, web app, database, and virtual network, but there are many more.
resource group - A container that holds related resources for an Azure solution. The resource group can include all the resources for the solution, or only those resources that you want to manage as a group. You decide how you want to allocate resources to resource groups based on what makes the most sense for your organization.
resource provider - A service that supplies the resources you can deploy and manage through Resource Manager. Each resource provider offers operations for working with the resources that are deployed. Some common resource providers are Microsoft.Compute, which supplies the virtual machine resource, Microsoft.Storage, which supplies the storage account resource, and Microsoft.Web, which supplies resources related to web apps.
template - A JavaScript Object Notation (JSON) file that defines one or more resources to deploy to a resource group. It also defines the dependencies between the deployed resources. The template can be used to deploy the resources consistently and repeatedly.
declarative syntax - Syntax that lets you state "Here is what I intend to create" without having to write the sequence of programming commands to create it. The Resource Manager template is an example of declarative syntax. In the file, you define the properties for the infrastructure to deploy to Azure.
Resource providers
Each resource provider offers a set of resources and operations for working with an Azure service. For example, if you want to store keys and secrets, you work with the Microsoft.KeyVault resource provider. This resource provider offers a resource type called vaults for creating the key vault.

The name of a resource type is in the format: {resource-provider}/{resource-type}. For example, the key vault type is Microsoft.KeyVault/vaults.

 Note

Before deploying your resources, you should gain an understanding of the available resource providers. Knowing the names of resource providers and resources helps you define resources you want to deploy to Azure. Also, you need to know the valid locations and API versions for each resource type.

Next unit: Create resource groups

3- Create resource groups

Resources can be deployed to any new or existing resource group. Deployment of resources to a resource group becomes a job where you can track the template execution. If deployment fails, the output of the job can describe why the deployment failed. Whether the deployment is a single resource to a group or a template to a group, you can use the information to fix any errors and redeploy. Deployments are incremental; if a resource group contains two web apps and you decide to deploy a third, the existing web apps will not be removed.

Considerations
Resource Groups are at their simplest a logical collection of resources. There are a few rules for resource groups.

Resources can only exist in one resource group.
Resource Groups cannot be renamed.
Resource Groups can have resources of many different types (services).
Resource Groups can have resources from many different regions.
Creating resource groups
There are some important factors to consider when defining your resource group:

All the resources in your group should share the same lifecycle. You deploy, update, and delete them together. If one resource, such as a database server, needs to exist on a different deployment cycle it should be in another resource group.
Each resource can only exist in one resource group.
You can add or remove a resource to a resource group at any time.
You can move a resource from one resource group to another group. Limitations do apply to moving resources.
A resource group can contain resources that reside in different regions.
A resource group can be used to scope access control for administrative actions.
A resource can interact with resources in other resource groups. This interaction is common when the two resources are related but don't share the same lifecycle (for example, web apps connecting to a database).
When creating a resource group, you need to provide a location for that resource group. You may be wondering, "Why does a resource group need a location? And, if the resources can have different locations than the resource group, why does the resource group location matter at all?" The resource group stores metadata about the resources. Therefore, when you specify a location for the resource group, you're specifying where that metadata is stored. For compliance reasons, you may need to ensure that your data is stored in a particular region.

 Note

By scoping permissions to a resource group, you can add/remove and modify resources easily without having to recreate assignments and scopes.

Next unit: Create Azure Resource Manager locks

4- Create Azure Resource Manager locks

A common concern with resources provisioned in Azure is the ease with which they can be deleted. An over-zealous or careless administrator can accidentally erase months of work with a few steps. Resource Manager locks allow organizations to put a structure in place that prevents the accidental deletion of resources in Azure.

You can associate the lock with a subscription, resource group, or resource.
Locks are inherited by child resources.
Screenshot of the Management locks page. In the Settings options, Locks are highlighted and in the Add Lock page, the Lock type, Ready-only, and Delete option are displayed and highlighted.

Lock types
There are two types of resource locks.

Read-Only locks, which prevent any changes to the resource.
Delete locks, which prevent deletion.
 Note

Only the Owner and User Access Administrator roles can create or delete management locks.

Next unit: Reorganize Azure resources

5- Reorganize Azure resources

Sometimes you may need to move resources to either a new subscription or a new resource group in the same subscription.

Diagram showing two subscriptions.

When moving resources, both the source group and the target group are locked during the operation. Write and delete operations are blocked on the resource groups until the move completes. This lock means you can't add, update, or delete resources in the resource groups. Locks don't mean the resources aren't available. For example, if you move a virtual machine to a new resource group, an application can still access the virtual machine.

Limitations
Before beginning this process be sure to read the Move operation support for resources page. This page details what resources can be moved between resources group, subscriptions, and regions.

Implementation
To move resources, select the resource group containing those resources, and then select the Move button. Select the resources to move and the destination resource group. Acknowledge that you need to update scripts.

Screenshot of the Move a Resource page.

 Note

Just because a service can be moved doesn’t mean there aren’t restrictions. For example, you can move a virtual network, but you must also move its dependent resources, like gateways.

Next unit: Remove resources and resource groups

6- Remove resources and resource groups

Use caution when deleting a resource group. Deleting a resource group deletes all the resources contained within it. That resource group might contain resources that resources in other resource groups depend on.

Screenshot showing the Delete resource group button (highlighted) in the portal.

Using PowerShell to delete resource groups
To remove a resource group use, Remove-AzResourceGroup. In this example, we are removing the ContosoRG01 resource group from the subscription. The cmdlet prompts you for confirmation and returns no output.


Copy
Remove-AzResourceGroup -Name "ContosoRG01"

Removing resources
You can also delete individual resources within a resource group. For example, here we are deleting a virtual network. Instead, of deleting you can move the resource to another resource group.

Screenshot from the portal of the route table page, with the Delete button highlighted to show you can delete an individual resource within a resource group.

Next unit: Determine resource limits

7- Determine resource limits

Azure lets you view resource usage against limits. This is helpful to track current usage, and plan for future use.

Screenshot of the Subscription usage and quotas page. It shows quotas for Network Watchers, Public IP Addresses, Route Tables, and Virtual Networks by their location with the usage numbers by percent used and number of resources.

The limits shown are the limits for your subscription.
When you need to increase a default limit, there is a Request Increase link.
All resources have a maximum limit listed in Azure limits.
If you are at the maximum limit, the limit can't be increased.

Next unit: Knowledge check

Knowledge check

Choose the best response for each question. Then select Check your answers.


1. A new project has several resources that need to be administered together. Which of the following strategies would provide a good solution? 

Azure templates

Azure resource groups

Azure subscriptions

2. Which of the following situations would be good example of when to use a resource lock? 

A ExpressRoute circuit with connectivity back to the on-premises network.

A non-production virtual machine used to test occasional application builds.

A storage account used to temporarily store images processed in a development environment.

3. Which of the following is true about resource groups? 

Resources can be in only one resource group.

Role-based access control can't be applied to a resource group

Resource groups can be nested.

Summary and resources

Azure Resource Manager is the deployment and management service for Azure. It provides a management layer that enables you to create, update, and delete resources in your Azure account. You use management features, like access control, locks, and tags, to secure and organize your resources after deployment.

You should now be able to:

Identify the features and usage cases for Azure Resource Manager.
Describe each Azure Resource Manager component and its usage.
Organize your Azure resources with resource groups.
Apply Azure Resource Manager locks.
Move Azure resources between groups, subscriptions, and regions.
Remove resources and resource groups.
Apply and track resource limits.
Learn more
You can learn more by reviewing the following.

Azure Resource Manager documentation
Learn - Control and organize Azure resources with Azure Resource Manager



Point 2: Introduction to Azure Cloud Shell

Describe Microsoft Azure Cloud Shell, learn how it works, and explore basic steps for its usage.

Learning objectives

By the end of this module, you're able to:

Describe Azure Cloud Shell and the functionality it provides.
Determine whether Azure Cloud Shell meets the needs of your organization.
Recognize how to use Azure Cloud Shell and persist files for multiple sessions.

1- Introduction

Azure Cloud Shell is a browser-accessible command-line experience for managing Azure resources. It provides the flexibility of choosing the shell experience that best suits the way you work, either Bash or PowerShell. Traditionally, to interact with Azure resources via command-line, you need to install the necessary components into your local computer (PC, Mac, Linux). With Cloud Shell, you have an authenticated, interactive shell that isn't part of a local machine.

You're an IT admin for Contoso Corporation, responsible for the cloud infrastructure on which the company hosts its applications. These applications use different cloud resources, such as Azure VMs, Azure blob storage, Azure networking, and others. On many occasions, you're asked to perform operations on these cloud resources at moments when you’re not using your work laptop. In some cases, you have a friend’s laptop, a smartphone, or another personal PC.

This module explains what Azure Cloud Shell does, how it works, and when you should choose to use Cloud Shell as a solution to meet your organization's needs.

Learning objectives
In this module, you'll:

Describe Azure Cloud Shell and the functionality it provides.
Determine whether Cloud Shell meets the needs of your organization.
Recognize how to use Cloud Shell and persist files for multiple sessions.

Next unit: What is Azure Cloud Shell?

2- What is Azure Cloud Shell?

Azure Cloud Shell is a command-line environment you can access through your web browser. You can use this environment to manage Azure resources, including VMs, storage, and networking. Just like you do when using the Azure CLI or Azure PowerShell.

Because Microsoft manages Cloud Shell, you always have access to the most recent versions of the Azure CLI and PowerShell modules right from any browser. You don't have to worry about keeping modules up to date. With Cloud Shell, you just open your browser and sign in. Just like that, you have access to a command-line environment fully connected with your account's permissions and the resources to which you have access. All that works in an infrastructure that's compliant with double encryption at rest by default. You don't need to take any further action!

Azure Cloud Shell also provides cloud storage to persist files such as SSH keys, scripts, and more. This functionality lets you access important files in between sessions and with different machines. Finally, you can use the Cloud Shell editor to make changes to files, such as scripts, that are saved into this cloud storage directly from the Cloud Shell interface.

Next unit: How does Azure Cloud Shell work?

3- How does Azure Cloud Shell work?

As an IT admin for Contoso Corporation, you're frequently on-call to perform administrative tasks and resolve workload disruptions to resources in your organization's Azure subscriptions. When visiting a family member during a weekend that you're on call, the development team notifies you of a problem with an Azure virtual machine (VM). The VM became nonresponsive during scheduled maintenance for the upgrade of an application that runs on it. Because the developers weren't granted access to the underlying Azure virtual machine hosting infrastructure, they're only able to remotely access the VM when it's operating normally. So, you're being called to diagnose and remediate the problem.

Since you're visiting family, you don’t have access to your administrative workstation and diagnostic scripts. You do have access to a laptop with an internet browser. Using the laptop, you browse to the Azure portal, authenticate against your organization’s Azure subscription, open Azure Cloud Shell, mount an Azure File Share, access your diagnostic scripts, and diagnose and remediate the problems with the VM, returning it to operation.

Access Cloud Shell
You have a few different options for accessing Azure Cloud Shell:

From a direct link: https://shell.azure.com

A screenshot of Cloud Shell accessed directly from a link.

From the Azure portal

A screenshot of Cloud Shell accessed from Azure portal.

From code snippets when accessing Microsoft Learn:

A screenshot of Cloud Shell accessed from code snippets.

When you open a Cloud Shell session, a temporary host is allocated to your session. This VM is preconfigured with the latest versions of PowerShell and Bash. You can then select the command-line experience you want to use:

A screenshot of how to choose a command-line experience in a Cloud Shell session.

After you select the shell experience you want to use, you can start managing your Azure resources:

A screenshot of how to use Cloud Shell to manage Azure resources.

Cloud Shell sessions terminate after 20 minutes of inactivity. When a session terminates, files on your CloudDrive are persisted, but you need to start a new session to access the Cloud Shell environment.

Access your own scripts and files
When using Cloud Shell, you might also need to run scripts or use files for different actions. You can persist files on Cloud Shell by using the Azure CloudDrive:

A screenshot of how to access CloudDrive in a Cloud Shell session.

After uploading files, you can interact with them as you would in a regular PowerShell or Bash session:

A screenshot of how to manage files in CloudDrive.

Now that your file resides on CloudDrive, you can close the session and open another session on a different device and still access the same file. Cloud Shell also lets you map an Azure Storage File Share, which is tied to a specific region. Access to an Azure File Share lets you work with the contents of that share through Cloud Shell.

If you need to edit scripts hosted on the CloudDrive or File Share, you can use the Cloud Shell editor. Select the curly brackets {} icon on the browser and open the file you want to edit, or use the command code and specify the filename; for example:

Bash

Copy
code temp.txt
A screenshot of how to access the Cloud Shell editor mode.

Cloud Shell tools
If you need to manage resources (such as Docker containers or Kubernetes Clusters) or want to use non-Microsoft tools (such as Ansible and Terraform) in Cloud Shell, the Cloud Shell session comes with these add-ons already preconfigured.

Here’s a list of all add-ons available to you within a Cloud Shell session:

Category	Name
Linux tools	bash
zsh
sh
tmux
dig
Azure tools	Azure CLI and Azure classic CLI
AzCopy
Azure Functions CLI
Service Fabric CLI
Batch Shipyard
blobxfer
Text editors	code (Cloud Shell editor)
vim
nano
emacs
Source control	git
Build tools	make
maven
npm
pip
Containers	Docker Machine
Kubectl
Helm
DC/OS CLI
Databases	MySQL client
PostgreSql client
sqlcmd Utility
mssql-scripter
Other	iPython Client
Cloud Foundry CLI
Terraform
Ansible
Chef InSpec
Puppet Bolt
HashiCorp Packer
Office 365 CLI

Next unit: When should you use Azure Cloud Shell?

4- When should you use Azure Cloud Shell?

As an IT Admin for Contoso Corporation, you need alternatives to interact with Azure resources from the command line even when not using your default administrative device.

You can use Azure Cloud Shell to:

Open a secure command-line session from any browser-based device.
Interact with Azure resources without the need to install plug-ins or add-ons to your device.
Persist files between sessions for later use.
Use either Bash or PowerShell, whichever you prefer, to manage Azure resources.
Edit files (such as scripts) via the Cloud Shell editor.
You shouldn't use Azure Cloud Shell if:

You intend to leave a session open for more than 20 minutes for long running scripts or activities. In these cases, your session is disconnected without warning, and the current state is lost.
You need admin permissions, such as sudo access, from within the Azure CLI or PowerShell environment.
You need to install tools that aren't supported in the limited Cloud Shell environment, but instead require an environment such as a custom virtual machine or container.
You need storage from different regions. You might need to back up and synchronize this content since only one region can have the storage allocated to Azure Cloud Shell.
You need to open multiple sessions at the same time. Azure Cloud Shell allows only one instance at time and isn't suitable for concurrent work across multiple subscriptions or tenants.

Next unit: Knowledge check

Check your knowledge

1. Which of these methods can you use to access Azure Cloud Shell from a computer running Windows 11? 

Install Windows Subsystem for Linux and make an SSH connection to Azure.

Make a Remote Desktop Protocol connection to Azure.

Use the Microsoft Edge Browser to log into an Azure Subscription.

2. You want to store a script that you constantly use for operations on Azure resources. This script needs to be rapidly available when you open a new Azure Cloud Shell session. Which of these procedures should you use? 

Upload the script to your CloudDrive on an Azure Cloud Shell session.

Create a new script and store it on an Azure Storage Blob.

Use Cloud Shell editor to create and edit the script you want to use.

3. You have a script stored on the Cloud Shell storage. You constantly use this script for resource management, but you need to perform small changes to it. Which of these solutions is the best way to handle the situation? 

Download the script from the Azure Cloud Shell session, edit the script, and upload it back before using it.

Push the script to GitHub and run it from the Azure Cloud Shell session.

Use the Cloud Shell editor to make the necessary changes and save it directly on the CloudDrive.


Summary

Azure Cloud Shell is a browser-accessible command-line experience for managing Azure resources. Rather than having to configure an Azure CLI or PowerShell session from your workstation, you can access Cloud Shell on any standard, compliant browser.

Cloud Shell provides the flexibility of choosing the shell experience that best suits the way you work, allowing you to work either in Bash or PowerShell, right from the browser. Cloud Shell also provides you with the mechanisms to persist files between sessions, and provides access to a minimalist version of the Visual Studio Code editor for more complex operations.


Point 3: Introduction to Bash

Use Bash to manage IT infrastructure.

Learning objectives
In this module, you will:

Learn what shells are and what Bash is.
Learn about the syntax of Bash commands.
Learn about important Bash commands, such as ls, cat, and ps.
Learn how to use I/O operators to redirect input and output.
Learn how to update a server's operating system.
Learn how to find and terminate rogue processes.
Learn how to use Bash to filter Azure CLI output.

1- Introduction

Imagine that you just started a new job as a system administrator (sysadmin) at Northwind, a high-frequency trading (HFT) firm that runs Windows on its client computers and Linux on its server computers. Computers are the lifeblood of the company, and you need to learn more about how to manage Linux boxes. It's time to skill up!

Bash is the standard shell scripting language for Linux. Let's learn the basics of Bash, starting with the syntax and commonly used commands, like ls and cat.

Learning objectives
In this module, you will:

Learn what shells are and what Bash is
Learn about the syntax of Bash commands
Learn about important Bash commands, such as ls, cat, and ps
Learn how to use I/O operators to redirect input and output
Learn how to update a server's operating system
Learn how to find and terminate rogue processes
Learn how to use Bash to filter Azure CLI output

Next unit: What is Bash?

2- What is Bash?

Bash is a vital tool for managing Linux machines. The name is short for "Bourne Again Shell."

A shell is a program that commands the operating system to perform actions. You can enter commands in a console on your computer and run the commands directly, or you can use scripts to run batches of commands. Shells like PowerShell and Bash give system administrators the power and precision they need for fine-tuned control of the computers they're responsible for.

There are other Linux shells, including csh and zsh, but Bash has become the de facto Linux standard. That's because Bash is compatible with Unix's first serious shell, the Bourne shell, also known as sh. Bash incorporates the best features of its predecessors. But Bash also has some fine features of its own, including built-in commands and the ability to invoke external programs.

One reason for Bash's success is its simplicity. Bash, like the rest of Linux, is based on the Unix design philosophy. As Peter Salus summarized in his book A Quarter Century of Unix, three of the "big ideas" embodied in Unix are:

Programs do one thing and do it well
Programs work together
Programs use text streams as the universal interface
The last part is key to understanding how Bash works. In Unix and Linux, everything is a file. That means you can use the same commands without worrying about whether the I/O stream — the input and output — comes from a keyboard, a disk file, a socket, a pipe, or another I/O abstraction.

Let's learn the basics of Bash, starting with the syntax and commonly used commands, like ls and cat.

Next unit: Bash fundamentals

3- Bash fundamentals

An understanding of Bash starts with an understanding of Bash syntax. After you know the syntax, you can apply it to every Bash command you run.

The full syntax for a Bash command is:

Bash

Copy
command [options] [arguments]
Bash treats the first string it encounters as a command. The following command uses Bash's ls (for "list") command to display the contents of the current working directory:

Bash

Copy
ls
Bash commands are often accompanied by arguments. For example, you can include a path name in an ls command to list the contents of another directory:

Bash

Copy
ls /etc
Most Bash commands have options for modifying how they work. Options, also called flags, give a command more specific instructions. As an example, files and directories whose names begin with a period are hidden from the user and are not displayed by ls. However, you can include the -a (for "all") flag in an ls command and see everything in the target directory:

Bash

Copy
ls -a /etc
You can even combine flags for brevity. For example, rather than enter ls -a -l /etc to show all files and directories in Linux's /etc directory in long form, you can enter this instead:

Bash

Copy
ls -al /etc
Bash is concise. It's sometimes remarkable (and a point of pride among Bash aficionados) how much you can accomplish with a single command.

Get help
Which options and arguments can be used, or must be used, varies from command to command. Fortunately, Bash documentation is built into the operating system. Help is never more than a command away. To learn about the options for a command, use the man (for "manual") command. For instance, to see all the options for the mkdir ("make directory") command, do this:

Bash

Copy
man mkdir
man will be your best friend as you learn Bash. man is how you find the information you need to understand how any command works.

Most Bash and Linux commands support the --help option. This shows a description of the command's syntax and options. To demonstrate, enter mkdir --help. The output will look something like this:

Output

Copy
Usage: mkdir [OPTION]... DIRECTORY...
Create the DIRECTORY(ies), if they do not already exist.
    
Mandatory arguments to long options are mandatory for short options too.
  -m, --mode=MODE   set file mode (as in chmod), not a=rwx - umask
  -p, --parents     no error if existing, make parent directories as needed
  -v, --verbose     print a message for each created directory
  -Z                   set SELinux security context of each created directory
                         to the default type
      --context[=CTX]  like -Z, or if CTX is specified then set the SELinux
                         or SMACK security context to CTX
      --help     display this help and exit
      --version  output version information and exit
    
GNU coreutils online help: <http://www.gnu.org/software/coreutils/>
Report mkdir translation bugs to <http://translationproject.org/team/>
Full documentation at: <http://www.gnu.org/software/coreutils/mkdir>
or available locally via: info '(coreutils) mkdir invocation'
Help obtained this way is typically more concise than help obtained with man.

Use wildcards
Wildcards are symbols that represent one or more characters in Bash commands. The most frequently used wildcard is the asterisk. It represents zero characters or a sequence of characters. Suppose your current directory contains hundreds of image files, but you only want to see the PNG files; the ones whose file names end with .png. Here's the command to list only those files:

Bash

Copy
ls *.png
 Note

Linux has no formal concept of a file-name extension as other operating systems do. This doesn't mean that PNG files won't have a .png extension. It simply means Linux attaches no special significance to the fact that the file names end with .png.

Now let's say the current directory also contains JPEG files. Some end in .jpg, while others end in .jpeg. Here's one way to list all the JPEG files:

Bash

Copy
ls *.jpg *.jpeg
And here is another:

Bash

Copy
ls *.jp*g
The * wildcard matches on zero or more characters, but the ? wildcard represents a single character. If the current directory contains files named 0001.jpg, 0002.jpg, and so on through 0009.jpg, the following command lists them all:

Bash

Copy
ls 000?.jpg
Yet another way to use wildcards to filter output is to use square brackets, which denote groups of characters. The following command lists all the files in the current directory whose names contain a period immediately followed a lowercase J or P. It lists all the .jpg, .jpeg, and .png files, but not .gif files:

Bash

Copy
ls *.[jp]*
In Linux, file names and the commands that operate upon them are case-sensitive. So to list all the files in the current directory whose names contain periods followed by an uppercase or lowercase J or P, you could enter this:

Bash

Copy
ls *.[jpJP]*
Expressions in square brackets can represent ranges of characters. For example, the following command lists all the files in the current directory whose names begin with a lowercase letter:

Bash

Copy
ls [a-z]*
This command, by contrast, lists all the files in the current directory whose names begin with an uppercase letter:

Bash

Copy
ls [A-Z]*
And this one lists all the files in the current directory whose names begin with a lowercase or uppercase letter:

Bash

Copy
ls [a-zA-Z]*
Based on all this, can you guess what the following commands will do?

Bash

Copy
ls [0-9]*
ls *[0-9]*
ls *[0-9]
If you need to use one of the wildcard characters as an ordinary character, you make it literal or "escape it" by prefacing it with a backslash. So, if for some reason you had an asterisk as part of a file name — something you should never do intentionally — you could search for it by using a command such as:

Bash

Copy
$ ls *\**

Next unit: Bash commands and operators

4- Bash commands and operators

Every shell language has its most-used commands. Let's start building your Bash repertoire by examining the most commonly used commands.

Bash commands
Let's look at common Bash commands and how to use them.

ls command
ls lists the contents of your current directory or the directory specified in an argument to the command. By itself, it lists the files and directories in the current directory:

Bash

Copy
ls
Files and directories whose names begin with a period are hidden by default. To include these items in a directory listing, use an -a flag:

Bash

Copy
ls -a
To get even more information about the files and directories in the current directory, use an -l flag:

Bash

Copy
ls -l
Here's some sample output from a directory that contains a handful of JPEGs and PNGs and a subdirectory named gifs:

Output

Copy
-rw-rw-r-- 1 azureuser azureuser  473774 Jun 13 15:38 0001.png
-rw-rw-r-- 1 azureuser azureuser 1557965 Jun 13 14:43 0002.jpg
-rw-rw-r-- 1 azureuser azureuser  473774 Mar 26 09:21 0003.png
-rw-rw-r-- 1 azureuser azureuser 4193680 Jun 13 09:40 0004.jpg
-rw-rw-r-- 1 azureuser azureuser  423325 Jun 10 12:53 0005.jpg
-rw-rw-r-- 1 azureuser azureuser 2278001 Jun 12 04:21 0006.jpg
-rw-rw-r-- 1 azureuser azureuser 1220517 Jun 13 14:44 0007.jpg
drwxrwxr-x 2 azureuser azureuser    4096 Jun 13 20:16 gifs
Each line provides detailed information about the corresponding file or directory. That information includes the permissions assigned to it, its owner, its size in bytes, the last time it was modified, and the file or directory name.

cat command
Suppose you want to see what's inside a file. You can use the cat command for that. The output won't make much sense unless the file is a text file. The following command shows the contents of the os-release file stored in the /etc directory:

Bash

Copy
cat /etc/os-release
This is a useful command because it tells you which Linux distribution you're running:

Output

Copy
NAME="Ubuntu"
VERSION="18.04.2 LTS (Bionic Beaver)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 18.04.2 LTS"
VERSION_ID="18.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
The /etc directory is a special one in Linux. It contains system-configuration files. You don't want to delete any files from this directory unless you know what you're doing.

sudo command
Some Bash commands can only be run by the root user; a system administrator or superuser. If you try one of these commands without sufficient privileges, it fails. For example, only users logged in as a superuser can use cat to display the contents of /etc/at.deny:

Bash

Copy
cat /etc/at.deny
at.deny is a special file that determines who can use other Bash commands to submit jobs for later execution.

You don't want to run as root most of the time; it's too dangerous. To run commands that require admin privilege without logging in as a superuser, you'll preface the commands with sudo:

Bash

Copy
sudo cat /etc/at.deny
sudo stands for "superuser do." When you use it, you're telling the shell that for this one command, you're acting with the root-user level of permission.

cd, mkdir, and rmdir commands
cd stands for "change directory," and it does exactly what the name suggests: it changes the current directory to another directory. It enables you to move from one directory to another just like its counterpart in Windows. The following command changes to a subdirectory of the current directory named orders:

Bash

Copy
cd orders
You can move up a directory by specifying .. as the directory name:

Bash

Copy
cd ..
This command changes to your home directory; the one you land in when you first log in:

Bash

Copy
cd ~
You can create directories by using the mkdir command. The following command creates a subdirectory named orders in the current working directory:

Bash

Copy
mkdir orders
If you want to create a subdirectory and another subdirectory under it with one command, use the --parents flag:

Bash

Copy
mkdir --parents orders/2019
The rmdir command deletes (removes) a directory, but only if it's empty. If it's not empty, you'll get a warning instead. Fortunately, you can use the rm command to delete directories that aren't empty in combination with the -r (recursive) flag. The command would then look like so, rm -r.

rm command
The rm command is short for "remove." As you'd expect, rm deletes files. So this command puts an end to 0001.jpg:

Bash

Copy
rm 0001.jpg
And this command deletes all the files in the current directory:

Bash

Copy
rm *
Be wary of rm. It's a dangerous command.

Running rm with a -i flag lets you think before you delete:

Bash

Copy
rm -i *
Make it a habit to include -i in every rm command, and you might avoid falling victim to one of Linux's biggest blunders. The dreaded rm -rf / command deletes every file on an entire drive. It works by recursively deleting all the subdirectories of root and their subdirectories. The -f (for "force") flag compounds the problem by suppressing prompts. Don't do this.

If you want to delete a subdirectory named orders that isn't empty, you can use the rm command this way:

Bash

Copy
rm -r orders
This deletes the orders subdirectory and everything in it, including other subdirectories.

cp command
The cp command copies not just files, but entire directories (and subdirectories) if you want. To make a copy of 0001.jpg named 0002.jpg, use this command:

Bash

Copy
cp 0001.jpg 0002.jpg
If 0002.jpg already exists, Bash silently replaces it. That's great if it's what you intended, but not so wonderful if you didn't realize you were about to overwrite the old version.

Fortunately, if you use the -i (for "interactive") flag, Bash warns you before deleting existing files. This is much safer:

Bash

Copy
cp -i 0001.jpg 0002.jpg
Of course, you can use wildcards to copy several files at once. To copy all the files in the current directory to a subdirectory named photos, do this:

Bash

Copy
cp * photos
To copy all the files in a subdirectory named photos into a subdirectory named images, do this:

Bash

Copy
cp photos/* images
This assumes that the images directory already exists. If it doesn't, you can create it and copy the contents of the photos directory by using this command:

Bash

Copy
cp -r photos images
The -r stands for "recursive." An added benefit of the -r flag is that if photos contains subdirectories of its own, they too are copied to the images directory.

ps command
The ps command gives you a snapshot of all the currently running processes. By itself, with no arguments, it shows all your shell processes; in other words, not much. But it's a different story when you include a -e flag:

Bash

Copy
ps -e
-e lists all running processes, and there are typically many of them.

For a more comprehensive look at what processes are running in the system, use the -ef flag:

Bash

Copy
ps -ef 
This flag shows the names of all the running processes, their process identification numbers (PIDs), the PIDs of their parents (PPIDs), and when they began (STIME). It also shows what terminal, if any, they're attached to (TTY), how much CPU time they've racked up (TIME), and their full path names. Here is an abbreviated example:


Copy
UID         PID   PPID  C STIME TTY          TIME CMD
root          1      0  0 13:35 ?        00:00:03 /sbin/init
root          2      0  0 13:35 ?        00:00:00 [kthreadd]
root          3      2  0 13:35 ?        00:00:00 [rcu_gp]
root          4      2  0 13:35 ?        00:00:00 [rcu_par_gp]
root          5      2  0 13:35 ?        00:00:00 [kworker/0:0-cgr]
root          6      2  0 13:35 ?        00:00:00 [kworker/0:0H-kb]
root          8      2  0 13:35 ?        00:00:00 [mm_percpu_wq]
root          9      2  0 13:35 ?        00:00:01 [ksoftirqd/0]
root         10      2  0 13:35 ?        00:00:02 [rcu_sched]
As an aside, you might find documentation that shows ps being used this way:

Bash

Copy
ps aux
ps aux and ps -ef are the same. This duality traces back to historical differences between POSIX Unix systems (of which Linux is one) and BSD Unix systems (the most common of which is macOS). In the beginning, POSIX used -ef while the BSD required aux. Today, both operating-system families accept either format.

This serves as an excellent reminder of why you should look closely at the manual for all Linux commands. Learning Bash is like learning English as a second language. There are many exceptions to the rules.

w command
Users come, users go, and sometimes you get users you don't want at all. When an employee leaves to pursue other opportunities, the sysadmin is called upon to ensure that the worker can no longer log in to the company's computer systems. Sysadmins are also expected to know who's logged in, and who shouldn't be.

To find out who's on your servers, Linux provides the w (for "who") command. It displays information about the users currently on the computer system and those users' activities. w shows user names, their IP addresses, when they logged in, what processes they're currently running, and how much time those processes are consuming. It's a valuable tool for sysadmins.

Bash I/O operators
You can do a lot in Linux just by exercising Bash commands and their many options. But you can really get work done when you combine commands by using I/O operators:

< for redirecting input to a source other than the keyboard
> for redirecting output to destination other than the screen
>> for doing the same, but appending rather than overwriting
| for piping output from one command to the input of another
Suppose you want to list everything in the current directory but capture the output in a file named listing.txt. The following command does just that:

Bash

Copy
ls > listing.txt
If listing.txt already exists, it gets overwritten. If you use the >> operator instead, the output from ls is appended to what's already in listing.txt:

Bash

Copy
ls >> listing.txt
The piping operator is extremely powerful (and often used). It redirects the output of the first command to the input of the second command. Let's say you use cat to display the contents of a large file, but the content scrolls by too quickly for you to read. You can make the output more manageable by piping the results to another command such as more. The following command lists all the currently running processes. But once the screen is full, the output pauses until you select Enter to show the next line:

Bash

Copy
ps -ef | more
You can also pipe output to head to see just the first several lines:

Bash

Copy
ps -ef | head
Or suppose you want to filter the output to include only the lines that contain the word "daemon." One way to do that is by piping the output from ps to Linux's useful grep tool:

Bash

Copy
ps -ef | grep daemon
The output might look like this:

Output

Copy
azureus+  52463  50702  0 23:28 pts/0    00:00:00 grep --color=auto deamon
azureuser@bash-vm:~$ ps -ef | grep daemon
root        449      1  0 13:35 ?        00:00:17 /usr/lib/linux-tools/4.18.0-1018-azure/hv_kvp_daemon -n
root        988      1  0 13:35 ?        00:00:00 /usr/lib/accountsservice/accounts-daemon
message+   1002      1  0 13:35 ?        00:00:00 /usr/bin/dbus-daemon --system --address=systemd: --nofork --nopidfile --systemd-activation --syslog-only
daemon     1035      1  0 13:35 ?        00:00:00 /usr/sbin/atd -f
root       1037      1  0 13:35 ?        00:00:00 /usr/bin/python3 -u /usr/sbin/waagent -daemon
root       1039      1  0 13:35 ?        00:00:00 /usr/lib/linux-tools/4.18.0-1018-azure/hv_vss_daemon -n
azureus+  52477  50702  0 23:28 pts/0    00:00:00 grep --color=auto daemon
You can also use files as input. By default, standard input comes from the keyboard, but it too can be redirected. To get input from a file instead of the keyboard, use the < operator. One common sysadmin task is to sort the contents of a file. As the name suggests, sort sorts text in alphabetical order:

Bash

Copy
sort < file.txt
To save the sorted results to a new file, you can redirect input and output:

Bash

Copy
sort < file.txt > sorted_file.txt
You can use I/O operators to chain Linux commands as needed. Consider the following command:

Bash

Copy
cat file.txt | fmt | pr | lpr
The output from cat goes to fmt, the output from fmt goes to pr, and so on. fmt formats the results into a tidy paragraph. pr paginates the results. And lpr sends the paginated output to the printer. All in a single line!

Next unit: Exercise - Try Bash

5- Exercise - Try Bash

On your own Linux computer, you can run Bash commands locally. If you have access to Linux servers, you can remote in to them and run Bash commands there. But nobody wants to experiment on a live production system, particularly on their first day at Northwind.

In this unit, you'll use Azure Cloud Shell on the right as your Linux terminal. Azure Cloud Shell is a shell you can access through the Azure portal or at https://shell.azure.com. You don't have to install anything on your PC or laptop to use it.

Familiarize yourself with Cloud Shell
First, let's explore what's in Cloud Shell by using the Bash commands we've learned.

Use the ls command to list all files and subdirectories in the current directory:

Bash

Copy
ls
You should see output that looks similar to this:

Output

Copy
yourname@Azure:~$ ls
clouddrive
clouddrive is a subdirectory of your current directory. It's a mounted file share that persists if you're using Cloud Shell on your own account. Right now, you're using it on the Microsoft Learn sandbox.

But wait, what is the current directory? Let's use the pwd command to find out. pwd stands for "print working directory." It prints out the long-form path to what directory you're in now.

Bash

Copy
pwd
You should see an output like this:

Output

Copy
yourname@Azure:~$ pwd
/home/yourname
This output means that you're in a directory called yourname within a directory called home, at the root of the Linux file system.

There doesn't appear to be much in our current directory. Let's use a Bash flag to print all hidden files and directories to double check that's correct.

Bash

Copy
ls -a
Whoa! That output showed us a lot more stuff in this directory than we initially thought.

Output

Copy
yourname@Azure:~$ ls -a
.  ..  .azure  .bash_history  .bash_logout  .bashrc  clouddrive  .profile  .tmux.conf  .viminfo
What were all of those files and subdirectories? Some are behind-the-scenes files to make Cloud Shell work. Let's discuss a few of the others.

. refers to your current directory, and .. refers to your parent directory. Wherever you are, if you print all hidden files and directories, you'll see . and .. printed.
.bash_history is a special Bash file where all commands that you enter into the shell are stored. Bash remembers your command history, which, as we'll see later, is useful.
.bash_logout is another special Bash file that is read and run every time a login shell exists. Linux superusers can modify it to customize your environment.
.bashrc is an important Bash configuration file that runs whenever you start a new shell. If you decide to open this file to look at it, be careful about making changes, because they can have unintended consequences.
Recall your history and autocomplete commands
When you're entering complicated commands like this one, it's easy to make a mistake:

Bash

Copy
ls -a .azure/commands/202?*.log
Fortunately, Bash offers a couple pieces of functionality to help you.

Recalling previous commands
Try entering this command that has a typo (203? instead of 202?):

Bash

Copy
ls -a .azure/commands/203?*.log
You should see this output letting you know that there weren't any files that matched that pattern:

Output

Copy
ls: cannot access '.azure/commands/203?*.log': No such file or directory
Rather than entering the whole thing again to correct your mistake, you can recall previously entered commands by using the Up arrow and Down arrow keys. Try using the Up arrow key to bring back your incorrect command. Then use the Left arrow key to fix it by replacing the final 3 with a 2. Select Enter again to submit the corrected command.

Using the Up arrow key multiple times in a row will move you back multiple commands. Use the Down arrow key to move to later commands.

Now you should see something like the following output. It lets you know that your command worked correctly to list files that matched the given pattern.

Output

Copy
.azure/commands/2020-01-29.21-56-35.login.103.log
.azure/commands/2020-01-29.21-56-38.account_set.112.log
Autocompletion
Let's say you want to read the contents of one of the files that you just found. You can use the cat (short for "catenate") command to print the contents of a file to the screen.

To use this command, you could use the full file name, such as:

Bash

Copy
cat .azure/commands/2020-01-29.21-56-35.login.103.log
But that's a lot to type, and very error prone. Instead, you can use Bash's rudimentary autocompletion to do most of the work for you. Try typing:

Bash

Copy
cat .a
Then select the Tab key. What happens?

You should see the rest of the word "azure/" appear in your command:

Bash

Copy
cat .azure/
Keep typing the beginnings of words and using Tab to autocomplete. Keep in mind that if there's an ambiguity, Bash will not fill in anything. You can select Tab twice to have Bash print out all the files and directories in a given path that match the letters you've typed already.

Play around until you've gotten to a real .log file in the command directory. Then select Enter to use the cat command to print its contents to screen. It might look something like this:

Output

Copy
CMD-LOG-LINE-BEGIN 103 | 2020-01-29 21:56:35,426 | INFO | az_command_data_logger | command args: login --identity
CMD-LOG-LINE-BEGIN 103 | 2020-01-29 21:56:37,604 | INFO | az_command_data_logger | exit code: 0
Keep in mind that if you've typed an incorrect letter already, Bash will not be able to correctly guess what letter you meant to type.

Use man
We just used the cat command, but you don't know much about it yet. Practice man to bring up more information about the cat command.

Enter the following command to understand more about what cat is and how to use it:

Bash

Copy
man cat
Yes, you entered "man cat" into your shell. Bash commands can be both cryptic and amusing!

You should see an output like this:

Output

Copy
CAT(1)                                       User Commands                                       CAT(1)

NAME
       cat - concatenate files and print on the standard output

SYNOPSIS
       cat [OPTION]... [FILE]...

DESCRIPTION
       Concatenate FILE(s) to standard output.

       With no FILE, or when FILE is -, read standard input.

       -A, --show-all
              equivalent to -vET

       -b, --number-nonblank
              number nonempty output lines, overrides -n

       -e     equivalent to -vE

...
Use up and down arrows to scroll through the manual page, and enter q to exit.

Change directories
Let's practice one more basic Bash command: cd.

While using the shell, you're always sitting inside a directory—just like a folder on your PC or Mac. To change folders, you use the cd (change directory) command.

It's simple, but let's get some practice.

First, enter this command to make sure you're in the right place:

Bash

Copy
cd ~
This command moved you back to your special home directory in the shell, if you weren't already there.

Double check by using the pwd command one more time:

Bash

Copy
pwd
You should see an output like this:

Output

Copy
/home/yourname
~ is another special character in Bash that refers to this home directory. You can use ~ to refer to the location /home/yourname no matter where you are in the shell.

Let's change to the directory that holds log files (where we were earlier):

Bash

Copy
cd .azure/commands/
You can either enter the full command yourself, or use Tab to autocomplete.

Now you should see that the line where you enter commands looks different, showing you where you are in the shell:

Output

Copy
yourname@Azure:~/.azure/commands$
Try using the special .. syntax to move up one directory:

Bash

Copy
cd ..
Now you should be one level up in the directory structure, and your command line should look like this:

Output

Copy
yourname@Azure:~/.azure$
Great work! You've taken your first steps to being a Bash expert. Let's keep learning.

Check your knowledge

1. What directory would you switch to if you entered cd . as a Bash command? 

My special "home" directory

The parent directory

The first alphabetical subdirectory

I wouldn't switch directories.

6- Exercise - Terminate a misbehaving process

Computers are imperfect. Sooner or later, something will go wrong. That's why you have a job as a sysadmin; it's up to you to troubleshoot and fix system problems.

Imagine that a Python application is causing problems. Perhaps it's taking up too much CPU time, or maybe it has stopped responding. In either case, you want to stop the application. To identify a process or application, you can use ps and grep. Then, to stop it, you can use the kill command. Let's practice this in your Linux virtual machine.

Start a misbehaving process
If you're going to kill a process, you need a process to kill. Let's create one.

Get back to your home base by typing the following command:

Bash

Copy
cd ~
In Azure Cloud Shell, enter the following command to start Linux's vi editor:

Bash

Copy
vi bad.py
vi is a widely used text editor that Linux inherited from Unix. Love it or hate it, a Bash user needs to know the basics of vi.

Select the i key to put vi in insert mode. Then type in the following Python program:

Python

Copy
i = 0
while i == 0:
    pass
This program, when executed, runs in an infinite loop—clearly not something you want running on your server.

Select the Esc key to exit insert mode. Then type the following command followed by the Enter key to save the program and exit vi:

vim

Copy
:wq
Be sure to include the colon at the beginning of the command. As for the remainder of the command, w stands for "write" and q stands for "quit."

Now use the following command to start the program and leave it running in the background:

Bash

Copy
python3 bad.py &
Be sure to include the ampersand (&) at the end of the command. Otherwise, you won't return to the Bash prompt. In Bash, the ampersand runs a command and returns to the command line, even if the command hasn't finished running.

It's not obvious, but bad.py is now running in the background and stealing CPU cycles from other processes. Let's take a close look at what's happening.

Kill the process
To kill a process, you need the process name or process ID. This is a job for ps.

To refresh your memory, a ps -ef command lists all running processes and displays a great deal of information about each. Use the following command to list all running processes and filter the results to lines that contain "python":

Bash

Copy
ps -ef | grep python
The results should look something like this:

Output

Copy
yourname+    342    254 99 23:34 pts/1    00:00:31 python3 bad.py
yourname+    344    254  0 23:35 pts/1    00:00:00 grep --color=auto python
From the listing, it appears that bad.py is consuming 99 percent of the server's CPU time. The program is living up to its name.

The kill command kills a running process based on its process ID. (A related command named killall kills a process based on the process name.) When you call kill, you have to decide what kind of "signal" to use to kill the process. Use the following command to display a list of signal types:

Bash

Copy
kill -l
If you were killing a daemon process—one that runs in the background and provides vital services to the operating system—you might want to kill it and immediately restart it. To do that, you could use a SIGHUP signal.

In this example, you want to kill the process without restarting it. Therefore, you want to use the SIGKILL signal, which corresponds to the number 9. To that end, grab bad.py's process ID from the ps -ef output (it's in the second column) and use the following command to terminate the process. Replace PROCESS_ID with the process ID.

Bash

Copy
kill -9 PROCESS_ID
The same command can also be entered as kill -s SIGKILL PROCESS_ID. Whether you use a signal's name or number is up to you.

Finish by running ps again to confirm that bad.py is no longer running.

Another common use for ps and kill is to identify and terminate "zombie processes," which are child processes left behind by poorly written programs.

Next unit: Exercise - Use Bash and grep to filter CLI output

7- Exercise - Use Bash and grep to filter CLI output

Until now, you've been running Bash commands on their own. Bash is extremely powerful when combined with other tools, so let's get some practice by using Bash to filter output from the Azure CLI.

Let's say you want to see an up-to-date list of the VM sizes available in the westus region of Azure. You can do that with this command:

Bash

Copy
az vm list-sizes --location westus --output table
You should see a long list of VM types as an output. To narrow down this list to the VM sizes you're interested in, you can use grep, Linux's universal pattern-matching program. To find the "DS" sizes, popular for use in data science, use the following command:

Bash

Copy
az vm list-sizes --location westus --output table | grep DS
This pipes output from the az command to grep, which filters out lines that lack the "DS" string.

That's still a lot of VMs. You know that DS V2 VMs are a more recent series. Let's adjust the grep command to use a more intricate regular expression:

Bash

Copy
az vm list-sizes --location westus --output table | grep DS.*_v2
This filters out lines that don't match the regular expression DS.*_v2. You might recognize some of the characters in that expression from our discussion of "wildcards" in an earlier unit. Regular expressions make great use of wildcards.

Regular expressions are a topic for another module, but come in handy for Bash scripting.

Using Bash with other CLI commands makes the latter easier to work with. And because a sysadmin's work never ends, any tool that reduces the workload is welcome.

Next unit: Knowledge check

Check your knowledge

1. Which of the following commands writes a list of processes associated with a user named scottgu to a file? 

cat | grep scottgu > processes.txt

cat > grep scottgu | processes.txt

ps -ef | grep scottgu > processes.txt

2. Which of the following commands, called with the -r option, would you use to delete a subdirectory that isn't empty? 

rm

rmdir

destroy

3. Which of the following commands combines the contents of foo.txt and bar.txt into a new file named foobar.txt? 

concat foo.txt bar.txt > foobar.txt

cat foo.txt bar.txt | foobar.txt

cat foo.txt bar.txt > foobar.txt

4. The purpose of the sudo command is to: 

Run a command with elevated privilege

Run a program and leave it running in the background

Prevent system files from being deleted by non-administrative users

5. Which of the following statements is true about the command python3 app.py &? 

It runs app.py after creating a restore point in the system

It runs app.py and returns immediately to the command prompt

It runs app.py, but only if it's located in the /etc directory

Summary

In this module, you learned the basics of using Bash. Among other things, you:

Learned what a shell is and what Bash is
Learned how Bash commands are structured
Learned key Bash commands, such as ls, cat, and ps
Learned how to use I/O operators in Bash commands to redirect input and output
Learned how to find and terminate rogue processes
Learned how to use Bash to filter output from another CLI tool
There is much more you can do with Bash. We've gotten comfortable using Bash as a way to interact with our shell, but you can use the commands you've learned (and many more) to use Bash for full-fledged programming. Check out these resources for taking your Bash knowledge to the next level:



Point 4: Introduction to PowerShell

Learn about the basics of PowerShell, a cross-platform command-line shell and scripting language that's built for task automation and configuration management. Learn what PowerShell is, what it's used for, and how to use it.

Learn about the basics of PowerShell, a cross-platform command-line shell and scripting language that's built for task automation and configuration management. Learn what PowerShell is, what it's used for, and how to use it.

Learning objectives
After completing this module, you'll be able to:

Understand what PowerShell is and what you can use it for.
Use commands to automate tasks.

1- Introduction

PowerShell is a command-line shell and a scripting language all in one. It was designed as a task engine that uses cmdlets to wrap tasks that people need to do. In PowerShell, you can run commands on local or remote machines. You can do tasks like managing users and automating workflows.

Whether you're part of an operations team or a development team that's adopting DevOps principles, PowerShell can help. You can use it to address various tasks, such as managing cloud resources and continuous integration and continuous delivery (CI/CD). PowerShell offers many helpful commands, but you can expand its capabilities at any time by installing modules.

When you install PowerShell, you can evaluate its features to see if it's a good fit for your team.

Learning objectives
After completing this module, you'll be able to:

Understand what PowerShell is and what you can use it for.
Use PowerShell commands to automate tasks.
Prerequisites
Basic familiarity with using a command-line shell like Command Prompt or Git Bash.
Visual Studio Code installed.
Ability to install Visual Studio Code extensions.
Ability to install software on your computer, if you're not using a Windows operating system.

Next unit: What is PowerShell?

2- What is PowerShell?

PowerShell consists of two parts: a command-line shell and a scripting language. It started out as a framework to automate administrative tasks in Windows. PowerShell has grown into a cross-platform tool that's used for many kinds of tasks.

A command-line shell lacks a graphical interface, where you use a mouse to interact with graphical elements. Instead, you type text commands into a computer console. Here are some of the benefits of using a console:

Interacting with a console is often faster than using a graphical interface.
In a console, you can run batches of commands, so it's ideal for task automation for continuous-integration pipelines.
You can use a console to interact with cloud resources and other resources.
You can store commands and scripts in a text file and use a source-control system. This capability is probably one of the biggest benefits, because your commands are repeatable and auditable. In many systems, especially government systems, everything must be traced and evaluated, or audited. Audits cover everything from database changes to changes done by a script.
Features
PowerShell shares some features with traditional shells:

Built-in help system: Most shells have some kind of help system, in which you can learn more about a command. For example, you can learn what the command does and what parameters it supports. The help system in PowerShell provides information about commands and also integrates with online help articles.
Pipeline: Traditional shells use a pipeline to run many commands sequentially. The output of one command is the input for the next command. PowerShell implements this concept like traditional shells, but it differs because it operates on objects over text. You learn more about this feature later in this module.
Aliases: Aliases are alternate names that can be used to run commands. PowerShell supports the use of common aliases such as cls (clear the screen) and ls (list the files). Therefore, new users can use their knowledge of other frameworks and don't necessarily have to remember the PowerShell name for familiar commands.
PowerShell differs from a traditional command-line shell in a few ways:

It operates on objects over text. In a command-line shell, you have to run scripts whose output and input might differ, so you end up spending time formatting the output and extracting the data you need. By contrast, in PowerShell you use objects as input and output. That means you spend less time formatting and extracting.
It has cmdlets. Commands in PowerShell are called cmdlets (pronounced commandlets). In PowerShell, cmdlets are built on a common runtime rather than separate executables as they are in many other shell environments. This characteristic provides a consistent experience in parameter parsing and pipeline behavior. Cmdlets typically take object input and return objects. The core cmdlets in PowerShell are built in .NET Core, and are open source. You can extend PowerShell by using more cmdlets, scripts, and functions from the community and other sources, or you can build your own cmdlets in .NET Core or PowerShell.
It has many types of commands. Commands in PowerShell can be native executables, cmdlets, functions, scripts, or aliases. Every command you run belongs to one of these types. The words command and cmdlet are often used interchangeably, because a cmdlet is a type of command.
Installation
In this module, you practice using PowerShell on your computer. PowerShell is available across platforms. However, if you use a computer that runs Linux, macOS, or an older version of Windows, you need to install it.

Instructions for installing PowerShell are different for each OS. Before you continue, take a few minutes to install PowerShell or to verify your PowerShell installation. The next unit in this module shows you how to verify your installation.

Windows
If you're running Windows 8 or later, a version of PowerShell called Windows PowerShell should already be installed. This version differs slightly from the most up-to-date PowerShell release, but it works fine for learning purposes.

You can open Windows PowerShell from the Start menu.

Other operating systems
If your computer runs something other than Windows 8 or later, you need to install PowerShell. To find the installation instructions for your OS, see Install various versions of PowerShell.

PowerShell extension for Visual Studio Code
We recommend that you use the PowerShell extension for Visual Studio Code to author your PowerShell scripts and to run the commands in this module. This extension lets you run commands, and also helps you with snippets, code completion, and syntax highlighting.

Next unit: Exercise - Run your first PowerShell commands

3- Exercise - Run your first PowerShell commands

In this unit, you use Azure Cloud Shell as a Linux terminal. You also can access Cloud Shell through the Azure portal or at Cloud Shell sign-in. You don't need to install anything on your PC or laptop to use Cloud Shell.

 Note

In this module, you're using the Azure Cloud Shell on the right-hand side of the screen, but in real-world situations, you can also use the integrated Terminal in Visual Studio Code by selecting Terminal > New Terminal, then selecting Powershell in the drop-down in the top-left of the Terminal window.

Before beginning this exercise, be sure to activate the sandbox.

Run the following command in Cloud Shell, and then press Enter to verify that your system is set up to use PowerShell. The $PSVersionTable verifies your installation.

PowerShell

Copy
$PSVersionTable
Your output resembles the following table:

Output

Copy
 Name                           Value
 ----                           -----
 PSVersion                      7.3.6
 PSEdition                      Core
 GitCommitId                    7.3.6
 OS                             Linux 5.4.0-1058-azure #60~18.04.1-Ubuntu SMP Tue Aug 31 20:34:4…
 Platform                       Unix
 PSCompatibleVersions           {1.0, 2.0, 3.0, 4.0…}
 PSRemotingProtocolVersion      2.3
 SerializationVersion           1.1.0.1
 WSManStackVersion              3.0
The output provides information about your PowerShell version, and your platform and edition.

For information limited to your version of PowerShell, you can run a modified version of $PSVersionTable.

Run the following command in Cloud Shell, and then press Enter.

PowerShell

Copy
$PSVersionTable.PSVersion
Your output now resembles the following table:

Output

Copy
Major  Minor  Patch  PreReleaseLabel BuildLabel
-----  -----  -----  --------------- ----------
7      3      6  
This output gives you more details about your version of PowerShell.

Running $PSVersionTable results in output that looks like a table, but is actually an object. For this reason, you can use a period (.) to access a specific property, such as PSVersion.

Next unit: Locate commands

4- Locate commands

A cmdlet (pronounced "command-let") is a compiled command. A cmdlet can be developed in .NET or .NET Core and invoked as a command within PowerShell. Thousands of cmdlets are available in your PowerShell installation. The challenge lies in discovering what the cmdlets are and what they can do for you.

Cmdlets are named according to a verb-noun naming standard. This pattern can help you to understand what they do and how to search for them. It also helps cmdlet developers create consistent names. You can see the list of approved verbs by using the Get-Verb cmdlet. Verbs are organized according to activity type and function.

Here's a part of the output from running Get-Verb:

Output

Copy
Verb        AliasPrefix Group          Description
----        ----------- -----          -----------
Add         a           Common         Adds a resource to a container, or atta…
Clear       cl          Common         Removes all the resources from a contai…
This listing shows the verb and its description. Cmdlet developers should use an approved verb, and also ensure that the verb description fits their cmdlet's function.

Three core cmdlets allow you to delve into what cmdlets exist and what they do:

Get-Command: The Get-Command cmdlet lists all of the available cmdlets on your system. Filter the list to quickly find the command you need.
Get-Help: Run the Get-Help core cmdlet to invoke a built-in help system. You can also run an alias help command to invoke Get-Help but improve the reading experience by paginating the response.
Get-Member: When you call a command, the response is an object that contains many properties. Run the Get-Member core cmdlet to drill down into that response and learn more about it.
Locate commands by using Get-Command
When you run the Get-Command cmdlet in Cloud Shell, you get a list of every command that's installed in PowerShell. Because thousands of commands are installed, you need a way to filter the response so you can quickly locate the command that you need.

To filter the list, keep in mind the verb-noun naming standard for cmdlets. For example, in the Get-Random command, Get is the verb and Random is the noun. Use flags to target either the verb or the noun in the command you want. The flag you specify expects a value that's a string. You can add pattern-matching characters to that string to ensure you express that, for example, a flag's value should start or end with a certain string.

These examples show how to use flags to filter a command list:

-Noun: The -Noun flag targets the part of the command name that's related to the noun. Here's a typical search for a command name using alias as the noun for which we're searching:

PowerShell

Copy
Get-Command -Noun alias*
This command searches for all cmdlets whose noun part starts with alias.

-Verb: The -Verb flag targets the part of the command name that's related to the verb. You can combine the -Noun flag and the -Verb flag to create an even more detailed search query and type. Here's an example:

PowerShell

Copy
Get-Command -Verb Get -Noun alias*
Now you've narrowed the search to specify that the verb part needs to match Get, and the noun part needs to match alias.

Next unit: Exercise - Locate commands

5- Exercise - Locate commands

In this unit, you use the Azure Cloud Shell on the right-hand side as your Linux terminal. You can access Cloud Shell through the Azure portal or the Cloud Shell sign-in. You don't have to install anything on your PC or laptop to use it.

Here, you run commands that help you learn more about PowerShell. PowerShell isn't something you learn overnight; it's learned command by command. You can speed up your learning by effectively using the core cmdlets.

Locate a command
Locate commands by running the Get-Command cmdlet. This cmdlet helps you search all of the cmdlets installed on your system. Use flags to narrow down your search results to just the cmdlets that fit your scenario.

In this scenario, you're looking for a cmdlet that can help you work with files.

Run the command Get-Command with the flag -Noun. Specify File* to find anything related to files.

PowerShell

Copy
 Get-Command -Noun File*
The response shows something similar to the following text:

Output

Copy
CommandType     Name                                               Version    Source
-----------     ----                                               -------    ------
Cmdlet          Get-FileHash                                       7.0.0.0    Microsoft.PowerShell.Utility
Cmdlet          Out-File                                           7.0.0.0    Microsoft.PowerShell.Utility
Cmdlet          Unblock-File                                       7.0.0.0    Microsoft.PowerShell.Utility
The cmdlets Get-FileHash, Out-File, and Unblock-File all match your query. Now, you have a manageable response. To further filter the response, add the -Verb parameter to your query.

Run Get-Command. Specify the flags -Verb and -Noun.

PowerShell

Copy
Get-Command -Verb Get -Noun File*
The result is similar to the following output:

Output

Copy
CommandType     Name                                               Version    Source
-----------     ----                                               -------    ------
Cmdlet          Get-FileHash                                       7.0.0.0    Microsoft.PowerShell.Utility
This time, only one record matches your search, because you specified both the -Noun parameter and the -Verb parameter.

Because the domain you work in is file management, you specified File as the noun. If you know what you want to do within that domain, you can specify -Verb parameters. By using one or possibly two parameters, you can quickly find the cmdlet you need.

Next unit: Knowledge check

Choose the best response for each question, then select Check your answers.


1. What's a correct way to locate a command in PowerShell? 

Call Get-Command 'name of command'

Call Find 'name of command'

Call Locate 'name of command'

2. How would you search for commands that deal with files? 

Call Get-Command -Verb File*

Call Get-Command -Noun File

Call Get-Command -Noun File*

Summary

In this module, you started by learning what PowerShell is and what you can use it for. You explored its primary features and learned how to run your first commands. You then learned about compiled commands called cmdlets. You looked specifically at a command called Get-Command that helps you locate the command you need.

You should now have a good understanding of PowerShell, what it's used for, and how to use its commands efficiently.


Point 5: Configure resources with Azure Resource Manager templates

You'll learn how to use Azure Resource Manager templates to consistently deploy assets.

Learning objectives
List the advantages of Azure templates.
Identify the Azure template schema components.
Specify Azure template parameters.
Locate and use Azure Quickstart Templates.

1- Introduction

Scenario
Your company needs to ensure virtual machine deployments are consistent across the organization.

You use Azure Resource Manager templates to deploy resources including virtual machines.

Skills measured
Deploying resources using Azure Resource Manager templates is part of Exam AZ-104: Microsoft Azure Administrator.

Deploy and manage Azure compute resources (20–25%)

Automate deployment of virtual machines (VMs) by using Azure Resource Manager templates

Modify an Azure Resource Manager template.
Deploy from a template.
Save a deployment as an Azure Resource Manager template.
Learning objectives
In this module, you'll learn how to:

List the advantages of Azure templates.
Identify the Azure template schema components.
Specify Azure template parameters.
Locate and use Azure Quickstart Templates.
Prerequisites
None

Next unit: Review Azure Resource Manager template advantages

2- Review Azure Resource Manager template advantages

An Azure Resource Manager template precisely defines all the Resource Manager resources in a deployment. You can deploy a Resource Manager template into a resource group as a single operation.

Using Resource Manager templates will make your deployments faster and more repeatable. For example, you no longer have to create a VM in the portal, wait for it to finish, and then create the next VM. Resource Manager template takes care of the entire deployment for you.

Template benefits
Templates improve consistency. Resource Manager templates provide a common language for you and others to describe your deployments. Regardless of the tool or SDK that you use to deploy the template, the structure, format, and expressions inside the template remain the same.
Templates help express complex deployments. Templates enable you to deploy multiple resources in the correct order. For example, you wouldn't want to deploy a virtual machine prior to creating an operating system (OS) disk or network interface. Resource Manager maps out each resource and its dependent resources, and creates dependent resources first. Dependency mapping helps ensure that the deployment is carried out in the correct order.
Templates reduce manual, error-prone tasks. Manually creating and connecting resources can be time consuming, and it's easy to make mistakes. Resource Manager ensures that the deployment happens the same way every time.
Templates are code. Templates express your requirements through code. Think of a template as a type of Infrastructure as Code that can be shared, tested, and versioned similar to any other piece of software. Also, because templates are code, you can create a "paper trail" that you can follow. The template code documents the deployment. Most users maintain their templates under some kind of revision control, such as GIT. When you change the template, its revision history also documents how the template (and your deployment) has evolved over time.
Templates promote reuse. Your template can contain parameters that are filled in when the template runs. A parameter can define a username or password, a domain name, and so on. Template parameters enable you to create multiple versions of your infrastructure, such as staging and production, while still using the exact same template.
Templates are linkable. You can link Resource Manager templates together to make the templates themselves modular. You can write small templates that each define a piece of a solution, and then combine them to create a complete system.
Templates simplify orchestration. You only need to deploy the template to deploy all of your resources. Normally this would take multiple operations.

Next unit: Explore the Azure Resource Manager template schema

3- Explore the Azure Resource Manager template schema

Azure Resource Manager templates are written in JSON, which allows you to express data stored as an object (such as a virtual machine) in text. A JSON document is essentially a collection of key-value pairs. Each key is a string, whose value can be:

A string
A number
A Boolean expression
A list of values
An object (which is a collection of other key-value pairs)
A Resource Manager template can contain sections that are expressed using JSON notation, but aren't related to the JSON language itself:

JSON

Copy
{
    "$schema": "http://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#",
    "contentVersion": "",
    "parameters": {},
    "variables": {},
    "functions": [],
    "resources": [],
    "outputs": {}
}

Element name

Required

Description

$schema

Yes

Location of the JSON schema file that describes the version of the template language. Use the URL shown in the preceding example.

contentVersion

Yes

Version of the template (such as 1.0.0.0). You can provide any value for this element. Use this value to document significant changes in your template. This value can be used to make sure that the right template is being used.

parameters

No

Values that are provided when deployment is executed to customize resource deployment.

variables

No

Values that are used as JSON fragments in the template to simplify template language expressions.

functions

No

User-defined functions that are available within the template.

resources

Yes

Resource types that are deployed or updated in a resource group.

outputs

No

Values that are returned after deployment.

Next unit: Explore the Azure Resource Manager template parameters

4- Explore the Azure Resource Manager template parameters

In the parameters section of the template, you specify which values you can input when deploying the resources. The available properties for a parameter are:

JSON

Copy
"parameters": {
    "<parameter-name>" : {
        "type" : "<type-of-parameter-value>",
        "defaultValue": "<default-value-of-parameter>",
        "allowedValues": [ "<array-of-allowed-values>" ],
        "minValue": <minimum-value-for-int>,
        "maxValue": <maximum-value-for-int>,
        "minLength": <minimum-length-for-string-or-array>,
        "maxLength": <maximum-length-for-string-or-array-parameters>,
        "metadata": {
        "description": "<description-of-the parameter>"
        }
    }
}
Here's an example that illustrates two parameters: one for a virtual machine's username, and one for its password:

JSON

Copy
"parameters": {
  "adminUsername": {
    "type": "string",
    "metadata": {
      "description": "Username for the Virtual Machine."
    }
  },
  "adminPassword": {
    "type": "securestring",
    "metadata": {
      "description": "Password for the Virtual Machine."
    }
  }
}
 Note

You're limited to 256 parameters in a template. You can reduce the number of parameters by using objects that contain multiple properties.

Next unit: Consider Bicep templates

5- Consider Bicep templates

Azure Bicep is a domain-specific language (DSL) that uses declarative syntax to deploy Azure resources. It provides concise syntax, reliable type safety, and support for code reuse.

You can use Bicep instead of JSON to develop your Azure Resource Manager templates (ARM templates). The JSON syntax to create an ARM template can be verbose and require complicated expressions. Bicep syntax reduces that complexity and improves the development experience. Bicep is a transparent abstraction over ARM template JSON and doesn't lose any of the JSON template capabilities.

How does Bicep work?

When you deploy a resource or series of resources to Azure, the tooling that's built into Bicep converts your Bicep template into a JSON template. This process is known as transpilation. Transpilation is the process of converting source code written in one language into another language.

Bicep templates are converted to JSON templates..

Bicep provides many improvements over JSON for template authoring, including:

Simpler syntax: Bicep provides a simpler syntax for writing templates. You can reference parameters and variables directly, without using complicated functions. String interpolation is used in place of concatenation to combine values for names and other items. You can reference the properties of a resource directly by using its symbolic name instead of complex reference statements. These syntax improvements help both with authoring and reading Bicep templates.

Modules: You can break down complex template deployments into smaller module files and reference them in a main template. These modules provide easier management and greater reusability.

Automatic dependency management: In most situations, Bicep automatically detects dependencies between your resources. This process removes some of the work involved in template authoring.

Type validation and IntelliSense: The Bicep extension for Visual Studio Code features rich validation and IntelliSense for all Azure resource type API definitions. This feature helps provide an easier authoring experience.

Next unit: Review QuickStart templates

6- Review QuickStart templates

Azure Quickstart Templates are Azure Resource Manager templates provided by the Azure community.

Screenshot of the QuickStart templates page.

Some templates provide everything you need to deploy your solution, while others might serve as a starting point for your template. Either way, you can study these templates to learn how to best author and structure your own templates.

The README.md file provides an overview of what the template does.
The azuredeploy.json file defines the resources that will be deployed.
The azuredeploy.parameters.json file provides the values the template needs.
 Note

Take a few minutes to browse the available templates. Anything of interest?

Next unit: Interactive lab simulation - templates

7- Interactive lab simulation - templates

Scenario
Tailwind Traders are migrating their storage needs to Azure. You've successfully deployed a managed disk in a resource group. You've decided to create an Azure Resource Manager template to simplify the other disk deployments.

Architecture diagram
Your first disk deployment in the resource group az104-03a-rg1 is complete. You plan to customize the template and use it to deploy another disk in resource group az104-03b-rg1.

Architecture diagram as explained in the text.

Tasks
Task 1: Review an ARM template for deployment of an Azure managed disk.

Task 2: Create an Azure managed disk by using an ARM template.

Task 3: Review the ARM template-based deployment of the managed disk.

 Note

Click on the thumbnail image to start the lab simulation. When you're done, be sure to return to this page so you can continue learning.

Screenshot of the simulation page.

Next unit: Knowledge check

Choose the best response for each question. Then select Check your answers.


1. What is an Azure Resource Manager template? 

A series of Azure CLI commands to deploy infrastructure to Azure.

A JavaScript Object Notation (JSON) file that defines the infrastructure and configuration for the deployment.

A script used by the Azure Resource Manager to manage the Azure storage account.

2. Which of the following parameters is an element in the template schema? 

Includes

Scripts

Outputs

3. What happens if the same template is run a second time? 

Azure Resource Manager deploys the new resources as copies of the previously deployed resources.

Azure Resource Manager doesn't change the deployed resources.

Azure Resource Manager deletes the previously deployed resources and redeploys them.

Summary and resources

To implement infrastructure as code for your Azure solutions, use Azure Resource Manager templates. The template is a JavaScript Object Notation (JSON) file that defines the infrastructure and configuration for your project. The template uses declarative syntax, which lets you state what you intend to deploy without having to write the sequence of programming commands to create it. In the template, you specify the resources to deploy and the properties for those resources.

You should now be able to:

List the advantages of Azure templates.
Identify the Azure template schema components.
Specify Azure template parameters.
Locate and use Azure Quickstart Templates.
Learn more
You can learn more by reviewing the following. A sandbox indicates a hands-on exercise.

Azure Resource Manager template documentation
Azure Quickstart Templates
Deploy Azure infrastructure by using JSON Azure Resource Manager templates (Sandbox)
Create Azure resources using Azure Resource Manager templates
Build your first Bicep template (Sandbox)

Module incomplete
Azure Administrator Associate
Chapter 2: Manage identities and governance in Azure

Modules in this learning path

Understand Microsoft Entra ID

This module explains Microsoft Entra ID. You'll compare Microsoft Entra ID to Active Directory DS, learn about Microsoft Entra ID P1 and P2, and explore Microsoft Entra Domain Services for managing domain-joined devices and apps in the cloud.


Configure user and group accounts

Learn how to configure user and group accounts.

Configure subscriptions

Learn how to configure Azure subscriptions, including how to obtain a subscription, implement cost management, and apply Azure resource tags.

Configure Azure Policy

Learn how to configure Azure Policy to implement compliance requirements.


Configure role-based access control

Learn how to use role-based access control (RBAC) to ensure resources are protected, but users can still access the resources they need.


Create Azure users and groups in Microsoft Entra ID

Create users in Microsoft Entra ID. Understand different types of groups. Create a group and add members. Manage business-to-business guest accounts.


Secure your Azure resources with Azure role-based access control (Azure RBAC)

Learn how to use Azure RBAC to manage access to resources in Azure.

Allow users to reset their password with Microsoft Entra self-service password reset

Evaluate self-service password reset to allow users in your organization to reset their passwords or unlock their accounts. Set up, configure, and test self-service password reset.





Point 1: Understand Microsoft Entra ID

This module explains Microsoft Entra ID. You'll compare Microsoft Entra ID to Active Directory DS, learn about Microsoft Entra ID P1 and P2, and explore Microsoft Entra Domain Services for managing domain-joined devices and apps in the cloud.

Learning objectives
After this module, you should be able to:

Describe Microsoft Entra ID.
Compare Microsoft Entra ID to Active Directory Domain Services (AD DS).
Describe how Microsoft Entra ID is used as a directory for cloud apps.
Describe Microsoft Entra ID P1 and P2.
Describe Microsoft Entra Domain Services.

1- Introduction

Welcome to the Microsoft Entra ID learning module! Microsoft Entra ID is a cloud-based identity and access management service provided by Microsoft. Microsoft Entra ID is a comprehensive solution for managing identities, enforcing access policies, and securing your applications and data in the cloud and on-premises.

This module aims to equip you with a comprehensive understanding of the following:

Describe Microsoft Entra ID.
Compare Microsoft Entra ID to Active Directory Domain Services (AD DS).
Describe how Microsoft Entra ID is used as a directory for cloud apps.
Describe Microsoft Entra ID P1 and P2.
Describe Microsoft Entra Domain Services.
Whether you're a beginner or an experienced IT professional, this module provides you with the knowledge and skills necessary to understand Microsoft Entra ID effectively. So, let's explore the exciting world of Microsoft Entra ID!

Next unit: Examine Microsoft Entra ID

2- Examine Microsoft Entra ID

Students should be familiar with Active Directory Domain Services (AD DS or traditionally called just "Active Directory"). AD DS is a directory service that provides the methods for storing directory data, such as user accounts and passwords, and makes this data available to network users, administrators, and other devices and services. It runs as a service on Windows Server, referred to as a domain controller.

Microsoft Entra ID is part of the platform as a service (PaaS) offering and operates as a Microsoft-managed directory service in the cloud. It’s not a part of the core infrastructure that customers own and manage, nor is it an Infrastructure as a service offering. While this implies that you have less control over its implementation, it also means that you don’t have to dedicate resources to its deployment or maintenance.

With Microsoft Entra ID, you also have access to a set of features that aren’t natively available in AD DS, such as support for multi-factor authentication, identity protection, and self-service password reset.

You can use Microsoft Entra ID to provide more secure access to cloud-based resources for organizations and individuals by:

Configuring access to applications
Configuring single sign-on (SSO) to cloud-based SaaS applications
Managing users and groups
Provisioning users
Enabling federation between organizations
Providing an identity management solution
Identifying irregular sign-in activity
Configuring multi-factor authentication
Extending existing on-premises Active Directory implementations to Microsoft Entra ID
Configuring Application Proxy for cloud and local applications
Configuring Conditional Access for users and devices
Diagram that shows the Microsoft Entra Connect Stack.

Microsoft Entra constitutes a separate Azure service. Its most elementary form, which any new Azure subscription includes automatically, doesn't incur any extra cost and is referred to as the Free tier. If you subscribe to any Microsoft Online business services (for example, Microsoft 365 or Microsoft Intune), you automatically get Microsoft Entra ID with access to all the Free features.

 Note

By default, when you create a new Azure subscription by using a Microsoft account, the subscription automatically includes a new Microsoft Entra tenant named Default Directory.

Some of the more advanced identity management features require paid versions of Microsoft Entra ID, offered in the form of Basic and Premium tiers. Some of these features are also automatically included in Microsoft Entra instances generated as part of Microsoft 365 subscriptions. Differences between Microsoft Entra versions are discussed later in this module.

Implementing Microsoft Entra ID isn't the same as deploying virtual machines in Azure, adding AD DS, and then deploying some domain controllers for a new forest and domain. Microsoft Entra ID is a different service, much more focused on providing identity management services to web-based apps, unlike AD DS, which is more focused on on-premises apps.


Microsoft Entra tenants
Unlike AD DS, Microsoft Entra ID is multi-tenant by design and is implemented specifically to ensure isolation between its individual directory instances. It’s the world’s largest multi-tenant directory, hosting over a million directory services instances, with billions of authentication requests per week. The term tenant in this context typically represents a company or organization that signed up for a subscription to a Microsoft cloud-based service such as Microsoft 365, Intune, or Azure, each of which uses Microsoft Entra ID. However, from a technical standpoint, the term tenant represents an individual Microsoft Entra instance. Within an Azure subscription, you can create multiple Microsoft Entra tenants. Having multiple Microsoft Entra tenants might be convenient if you want to test Microsoft Entra functionality in one tenant without affecting the others.

At any given time, an Azure subscription must be associated with one, and only one, Microsoft Entra tenant. This association allows you to grant permissions to resources in the Azure subscription (via RBAC) to users, groups, and applications that exist in that particular Microsoft Entra tenant.

 Note

You can associate the same Microsoft Entra tenant with multiple Azure subscriptions. This allows you to use the same users, groups, and applications to manage resources across multiple Azure subscriptions.

Each Microsoft Entra tenant is assigned the default Domain Name System (DNS) domain name, consisting of a unique prefix. The prefix, derived from the name of the Microsoft account you use to create an Azure subscription or provided explicitly when creating a Microsoft Entra tenant, is followed by the onmicrosoft.com suffix. Adding at least one custom domain name to the same Microsoft Entra tenant is possible and common. This name utilizes the DNS domain namespace that the corresponding company or organization owns. The Microsoft Entra tenant serves as the security boundary and a container for Microsoft Entra objects such as users, groups, and applications. A single Microsoft Entra tenant can support multiple Azure subscriptions.


Microsoft Entra schema
The Microsoft Entra schema contains fewer object types than that of AD DS. Most notably, it doesn't include a definition of the computer class, although it does include the device class. The process of joining devices to Microsoft Entra differs considerably from the process of joining computers to AD DS. The Microsoft Entra schema is also easily extensible, and its extensions are fully reversible.

The lack of support for the traditional computer domain membership means that you can't use Microsoft Entra ID to manage computers or user settings by using traditional management techniques, such as Group Policy Objects (GPOs). Instead, Microsoft Entra ID and its services define a concept of modern management. Microsoft Entra ID’s primary strength lies in providing directory services; storing and publishing user, device, and application data; and handling the authentication and authorization of the users, devices, and applications. The effectiveness and efficiency of these features are apparent based on existing deployments of cloud services such as Microsoft 365, which rely on Microsoft Entra ID as their identity provider and support millions of users.

Microsoft Entra ID doesn't include the organizational unit (OU) class, which means that you can't arrange its objects into a hierarchy of custom containers, which is frequently used in on-premises AD DS deployments. However, this isn't a significant shortcoming, because OUs in AD DS are used primarily for Group Policy scoping and delegation. You can accomplish equivalent arrangements by organizing objects based on their group membership.

Objects of the Application and servicePrincipal classes represent applications in Microsoft Entra ID. An object in the Application class contains an application definition and an object in the servicePrincipal class constitutes its instance in the current Microsoft Entra tenant. Separating these two sets of characteristics allows you to define an application in one tenant and use it across multiple tenants by creating a service principal object for this application in each tenant. Microsoft Entra ID creates the service principal object when you register the corresponding application in that Microsoft Entra tenant.

Next unit: Compare Microsoft Entra ID and Active Directory Domain Services

3- Compare Microsoft Entra ID and Active Directory Domain Services

You could view Microsoft Entra ID simply as the cloud-based counterpart of AD DS; however, while Microsoft Entra ID and AD DS share some common characteristics, there are several significant differences between them.

Characteristics of AD DS
AD DS is the traditional deployment of Windows Server-based Active Directory on a physical or virtual server. Although AD DS is commonly considered being primarily a directory service, it’s only one component of the Windows Active Directory suite of technologies, which also includes Active Directory Certificate Services (AD CS), Active Directory Lightweight Directory Services (AD LDS), Active Directory Federation Services (AD FS), and Active Directory Rights Management Services (AD RMS).

When comparing AD DS with Microsoft Entra ID, it’s important to note the following characteristics of AD DS:

AD DS is a true directory service, with a hierarchical X.500-based structure.
AD DS uses Domain Name System (DNS) for locating resources such as domain controllers.
You can query and manage AD DS by using Lightweight Directory Access Protocol (LDAP) calls.
AD DS primarily uses the Kerberos protocol for authentication.
AD DS uses OUs and GPOs for management.
AD DS includes computer objects, representing computers that join an Active Directory domain.
AD DS uses trusts between domains for delegated management.
You can deploy AD DS on an Azure virtual machine to enable scalability and availability for an on-premises AD DS. However, deploying AD DS on an Azure virtual machine doesn't make any use of Microsoft Entra ID.

 Note

Deploying AD DS on an Azure virtual machine requires one or more extra Azure data disks because you shouldn't use drive C for AD DS storage. These disks are needed to store the AD DS database, logs, and the sysvol folder. The Host Cache Preference setting for these disks must be set to None.


Characteristics of Microsoft Entra ID
Although Microsoft Entra ID has many similarities to AD DS, there are also many differences. It’s important to realize that using Microsoft Entra isn’t the same as deploying an Active Directory domain controller on an Azure virtual machine and adding it to your on-premises domain.

When comparing Microsoft Entra ID with AD DS, it’s important to note the following characteristics of Microsoft Entra ID:

Microsoft Entra ID is primarily an identity solution, and it’s designed for internet-based applications by using HTTP (port 80) and HTTPS (port 443) communications.
Microsoft Entra ID is a multi-tenant directory service.
Microsoft Entra users and groups are created in a flat structure, and there are no OUs or GPOs.
You can't query Microsoft Entra ID by using LDAP; instead, Microsoft Entra ID uses the REST API over HTTP and HTTPS.
Microsoft Entra ID doesn't use Kerberos authentication; instead, it uses HTTP and HTTPS protocols such as SAML, WS-Federation, and OpenID Connect for authentication, and uses OAuth for authorization.
Microsoft Entra ID includes federation services, and many third-party services such as Facebook are federated with and trust Microsoft Entra ID.

Next unit: Examine Microsoft Entra ID as a directory service for cloud apps

4- Examine Microsoft Entra ID as a directory service for cloud apps

When you deploy cloud services such as Microsoft 365 or Intune, you also need to have directory services in the cloud to provide authentication and authorization for these services. Because of this, each cloud service that needs authentication will create its own Microsoft Entra tenant. When a single organization uses more than one cloud service, it’s much more convenient for these cloud services to use a single cloud directory instead of having separate directories for each service.

It’s now possible to have one identity service that covers all Microsoft cloud-based services, such as Microsoft 365, Azure, Microsoft Dynamics 365, and Intune. Microsoft Entra ID provides developers with centralized authentication and authorization for applications in Azure by using other identity providers or on-premises AD DS. Microsoft Entra ID can provide users with an SSO experience when using applications such as Facebook, Google services, Yahoo, or Microsoft cloud services.

The process of implementing Microsoft Entra ID support for custom applications is rather complex and beyond the scope of this course. However, the Azure portal and Microsoft Visual Studio 2013 and later make the process of configuring such support more straightforward.

In particular, you can enable Microsoft Entra authentication for the Web Apps feature of Azure App Service directly from the Authentication/Authorization blade in the Azure portal. By designating the Microsoft Entra tenant, you can ensure that only users with accounts in that directory can access the website. It’s possible to apply different authentication settings to individual deployment slots.

For more information, see Configure your App Service app to use Microsoft Entra login.

Next unit: Compare Microsoft Entra ID P1 and P2 plans

5- Compare Microsoft Entra ID P1 and P2 plans

The Microsoft Entra ID P1 or P2 tier provides extra functionality as compared to the Free and Office 365 editions. However, premium versions require additional cost per user provisioning. Microsoft Entra ID P1 or P2 comes in two versions P1 and P2. You can procure it as an extra license or as a part of the Microsoft Enterprise Mobility + Security, which also includes the license for Azure Information Protection and Intune.

Microsoft provides a free trial period that can be used to experience the full functionality of the Microsoft Entra ID P2 edition. The following features are available with the Microsoft Entra ID P1 edition:

Self-service group management. It simplifies the administration of groups where users are given the rights to create and manage the groups. End users can create requests to join other groups, and group owners can approve requests and maintain their groups’ memberships.
Advanced security reports and alerts. You can monitor and protect access to your cloud applications by viewing detailed logs that show advanced anomalies and inconsistent access pattern reports. Advanced reports are machine learning based and can help you gain new insights to improve access security and respond to potential threats.
Multi-factor authentication. Full multi-factor authentication (MFA) works with on-premises applications (using virtual private network [VPN], RADIUS, and others), Azure, Microsoft 365, Dynamics 365, and third-party Microsoft Entra gallery applications. It doesn't work with non-browser off-the-shelf apps, such as Microsoft Outlook. Full multi-factor authentication is covered in more detail in the following units in this lesson.
Microsoft Identity Manager (MIM) licensing. MIM integrates with Microsoft Entra ID P1 or P2 to provide hybrid identity solutions. MIM can bridge multiple on-premises authentication stores such as AD DS, LDAP, Oracle, and other applications with Microsoft Entra ID. This provides consistent experiences to on-premises line-of-business (LOB) applications and SaaS solutions.
Enterprise SLA of 99.9%. You're guaranteed at least 99.9% availability of the Microsoft Entra ID P1 or P2 service. The same SLA applies to Microsoft Entra Basic.
Password reset with writeback. Self-service password reset follows the Active Directory on-premises password policy.
Cloud App Discovery feature of Microsoft Entra ID. This feature discovers the most frequently used cloud-based applications.
Conditional Access based on device, group, or location. This lets you configure conditional access for critical resources, based on several criteria.
Microsoft Entra Connect Health. You can use this tool to gain operational insight into Microsoft Entra ID. It works with alerts, performance counters, usage patterns, and configuration settings, and presents the collected information in the Microsoft Entra Connect Health portal.
In addition to these features, the Microsoft Entra ID P2 license provides extra functionalities:

Microsoft Entra ID Protection. This feature provides enhanced functionalities for monitoring and protecting user accounts. You can define user risk policies and sign-in policies. In addition, you can review users’ behavior and flag users for risk.
Microsoft Entra Privileged Identity Management. This functionality lets you configure additional security levels for privileged users such as administrators. With Privileged Identity Management, you define permanent and temporary administrators. You also define a policy workflow that activates whenever someone wants to use administrative privileges to perform some task.
 Note

Plans change frequently. Check Microsoft's website for the current plans and capabilities.

Next unit: Examine Microsoft Entra Domain Services

6- Examine Microsoft Entra Domain Services

In most organizations today, line-of-business (LOB) applications are deployed on computers and devices that are domain members. These organizations use AD DS–based credentials for authentication, and Group Policy manages them. When you consider moving these apps to run in Azure, one key issue is how to provide authentication services to these apps. To satisfy this need, you can choose to implement a site-to-site virtual private network (VPN) between your local infrastructure and the Azure IaaS, or you can deploy replica domain controllers from your local AD DS as virtual machines (VMs) in Azure. These approaches can entail additional costs and administrative effort. Additionally, the difference between these two approaches is that with the first option, authentication traffic will cross the VPN, while in the second option, replication traffic will cross the VPN and authentication traffic stays in the cloud.

Microsoft provides Microsoft Entra Domain Services as an alternative to these approaches. This service, which runs as part of the Microsoft Entra ID P1 or P2 tier, provides domain services such as Group Policy management, domain joining, and Kerberos authentication to your Microsoft Entra tenant. These services are fully compatible with locally deployed AD DS, so you can use them without deploying and managing additional domain controllers in the cloud.

Diagram that shows the Microsoft Entra Domain Services Overview.

Because Microsoft Entra ID can integrate with your local AD DS, when you implement Microsoft Entra Connect, users can utilize organizational credentials in both on-premises AD DS and in Microsoft Entra Domain Services. Even if you don’t have AD DS deployed locally, you can choose to use Microsoft Entra Domain Services as a cloud-only service. This enables you to have similar functionality of locally deployed AD DS without having to deploy a single domain controller on-premises or in the cloud. For example, an organization can choose to create a Microsoft Entra tenant and enable Microsoft Entra Domain Services, and then deploy a virtual network between its on-premises resources and the Microsoft Entra tenant. You can enable Microsoft Entra Domain Services for this virtual network so that all on-premises users and services can use domain services from Microsoft Entra ID.

Microsoft Entra Domain Services provides several benefits for organizations, such as:

Administrators don't need to manage, update, and monitor domain controllers.
Administrators don't need to deploy and manage Active Directory replication.
There’s no need to have Domain Admins or Enterprise Admins groups for domains that Microsoft Entra ID manages.
If you choose to implement Microsoft Entra Domain Services, you need to be aware of the service's current limitations. These include:

Only the base computer Active Directory object is supported.
It’s not possible to extend the schema for the Microsoft Entra Domain Services domain.
The organizational unit (OU) structure is flat and nested OUs aren't currently supported.
There’s a built-in Group Policy Object (GPO), and it exists for computer and user accounts.
It’s not possible to target OUs with built-in GPOs. Additionally, you can't use Windows Management Instrumentation filters or security-group filtering.
By using Microsoft Entra Domain Services, you can freely migrate applications that use LDAP, NTLM, or the Kerberos protocols from your on-premises infrastructure to the cloud. You can also use applications such as Microsoft SQL Server or Microsoft SharePoint Server on VMs or deploy them in the Azure IaaS, without needing domain controllers in the cloud or a VPN to local infrastructure.

You can enable Microsoft Entra Domain Services by using the Azure portal. This service charges per hour based on the size of your directory.

Next unit: Knowledge check

Choose the best response for each of the questions below. Then select Check your answers.

Check your knowledge

1. What is a benefit of using Microsoft Entra ID versus Active Directory (AD)? 

Microsoft Entra ID uses Kerberos authentication for access across applications

Microsoft Entra ID is a cloud-based identity solution

Microsoft Entra ID can query using Lightweight Directory Access Protocol (LDAP)

2. In addition to the free features of Microsoft Entra ID, what is the minimum Premium version licensing needed to implement risk-based sign-ins? 

Office 365

Microsoft Entra ID P2

Microsoft Entra ID P1


Summary

Active Directory provides the core service of identity management. AD DS is the traditional on-premises solution, whereas Microsoft Entra ID is the cloud-based solution. Microsoft Entra ID is frequently adopted at first to facilitate authentication for cloud-based apps, but is capable of providing authentication services for the entire infrastructure. While they provide similar solutions, each offer different capability and are often used together to provide a best-of-breed solution. Microsoft Entra ID is offered as a free service, with paid tiers for additional capabilities, depending on an organization's needs.





Point 2: Configure user and group accounts

Learn how to configure user and group accounts.

Learning objectives
In this module, you learn how to:

Configure users accounts and user account properties.

Create new user accounts.

Import bulk user accounts with a template.

Configure group accounts and assignment types.

1- Introduction

Access to Azure resources is controlled through user accounts and identities that are defined in Microsoft Entra ID. Microsoft Entra ID supports group accounts to help you organize user accounts for easier administration.

In this module, your company wants to take advantage of the user and group account features in Microsoft Entra ID. You need to understand the concepts of user accounts and group accounts. You're looking for information about how to create, configure, and manage these accounts. Your organization needs support for bulk configuration of settings, group account organization, and managing accounts across multiple directories.

In this module, you learn about user accounts and group accounts. You learn about an administrator shortcut for bulk creation of user accounts. You learn about controlling administrator access through administrative units. You also simulate user and group creation in the Azure portal.

The goal of this module is to successfully create and manage user and group accounts.

Learning objectives
In this module, you learn how to:

Create and configure user accounts including account properties and bulk updates.

Create and configure group accounts including assigning group members.

Use administrative units to control administrator access.

Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Understand basic concepts of centralized identity solutions. This knowledge includes identities, accounts, and authentication methods.

Familiarity with managing user and group accounts, including the principle of least privilege.

Next unit: Create user accounts

2- Create user accounts

There are several ways to add cloud identity user accounts in Microsoft Entra ID. A common approach is by using the Azure portal. User accounts can also be added to Microsoft Entra ID through Microsoft 365 Admin Center, Microsoft Intune admin console, and the Azure CLI.

Things to know about cloud identity accounts
Let's review how cloud identity user accounts are defined in Microsoft Entra ID. Here's an example of the new User page in the Azure portal. The administrator can Create a user within the organization or Invite a guest user to provide access to organization resources:

Screenshot of the User page in the Azure portal.

A new user account must have a display name and an associated user account name. An example display name is Aran Sawyer-Miller and the associated user account name could be asawmill@contoso.com.

Information and settings that describe a user are stored in the user account profile.

The profile can have other settings like a user's job title, and their contact email address.

A user with Global administrator or User administrator privileges can preset profile data in user accounts, such as the main phone number for the company.

Non-admin users can set some of their own profile data, but they can't change their display name or account name.

Things to consider when managing cloud identity accounts
There are several points to consider about managing user accounts. As you review this list, consider how you can add cloud identity user accounts for your organization.

Consider user profile data. Allow users to set their profile information for their accounts, as needed. User profile data, including the user's picture, job, and contact information is optional. You can also supply certain profile settings for each user based on your organization's requirements.

Consider restore options for deleted accounts. Include restore scenarios in your account management plan. Restore operations for a deleted account are available up to 30 days after an account is removed. After 30 days, a deleted user account can't be restored.

Consider gathered account data. Collect sign-in and audit log information for user accounts. Microsoft Entra ID lets you gather this data to help you analyze and improve your infrastructure.

Next unit: Create bulk user accounts

3- Create bulk user accounts

Microsoft Entra ID supports several bulk operations, including bulk create and delete for user accounts. The most common approach for these operations is to use the Azure portal. Azure PowerShell can be used for bulk upload of user accounts.

Things to know about bulk account operations
Let's examine some characteristics of bulk operations in the Azure portal. Here's an example that shows the Bulk create user option for new user accounts in Microsoft Entra ID:

Screenshot that shows the Bulk create user option for new user accounts in Azure AD.

Only Global administrators or User administrators have privileges to create and delete user accounts in the Azure portal.

To complete bulk create or delete operations, the admin fills out a comma-separated values (CSV) template of the data for the user accounts.

Bulk operation templates can be downloaded from the Microsoft Entra admin center.

Bulk lists of user accounts can be downloaded.

Things to consider when creating user accounts
Here are some design considerations for creating and deleting user accounts. Think about what user account conventions and processes might be required by your organization.

Consider naming conventions. Establish or implement a naming convention for your user accounts. Apply conventions to user account names, display names, and user aliases for consistency across the organization. Conventions for names and aliases can simplify the bulk create process by reducing areas of uniqueness in the CSV file. A convention for user names could begin with the user's last name followed by a period, and end with the user's first name, as in Sawyer-Miller.Aran@contoso.com.

Consider using initial passwords. Implement a convention for the initial password of a newly created user. Design a system to notify new users about their passwords in a secure way. You might generate a random password and email it to the new user or their manager.

Consider strategies for minimizing errors. View and address any errors, by downloading the results file on the Bulk operation results page in the Azure portal. The results file contains the reason for each error. An error might be a user account that's already been created or an account that's duplicated. Generally, it's easier to upload and troubleshoot smaller groups of user accounts.

Next unit: Create group accounts


4- Create group accounts

Microsoft Entra ID allows your organization to define two different types of group accounts. Security groups are used to manage member and computer access to shared resources for a group of users. You can create a security group for a specific security policy and apply the same permissions to all members of a group. Microsoft 365 groups provide collaboration opportunities. Group members have access to a shared mailbox, calendar, files, SharePoint site, and more.

Things to know about creating group accounts
Review the following characteristics of group accounts in Microsoft Entra ID. The following screenshot shows a list of groups in the Azure portal:

Screenshot that shows a list of groups in the Azure portal, and their group and membership types.

Use security groups to set permissions for all group members at the same time, rather than adding permissions to each member individually.

Add Microsoft 365 groups to enable group access for guest users outside your Microsoft Entra organization.

Security groups can be implemented only by a Microsoft Entra administrator.

Normal users and Microsoft Entra admins can both use Microsoft 365 groups.

Things to consider when adding group members
When you add members to a group, there are different ways you can assign member access rights. As you read through these options, consider which groups are needed to support your organization, and what access rights should be applied to group members.

Access rights	Description
Assigned	Add specific users as members of a group, where each user can have unique permissions.
Dynamic user	Use dynamic membership rules to automatically add and remove group members. When member attributes change, Azure reviews the dynamic group rules for the directory. If the member attributes meet the rule requirements, the member is added to the group. If the member attributes no longer meet the rule requirements, the member is removed.
Dynamic device	(Security groups only) Apply dynamic group rules to automatically add and remove devices in security groups. When device attributes change, Azure reviews the dynamic group rules for the directory. If the device attributes meet the rule requirements, the device is added to the security group. If the device attributes no longer meet the rule requirements, the device is removed.

Next unit: Create administrative units

5- Create administrative units

As you design your strategy for managing identities and governance in Azure, planning for comprehensive management of your Microsoft Entra infrastructure is critical. It can be useful to restrict administrative scope by using administrative units for your organization. The division of roles and responsibilities is especially helpful for organizations that have many independent divisions.

Consider the management tasks for a large university that's composed of several different schools like Business, Engineering, and Medicine. The university has administrative offices, academic buildings, social buildings, and student dormitories. For security purposes, each business office has its own internal network for resources like servers, printers, and fax machines. Each academic building is connected to the university network, so both instructors and students can access their accounts. The network is also available to students and deans in the dormitories and social buildings. Across the university, guest users require access to the internet via the university network.

The university has a team of IT admins who work together to control resource access, manage users, and set policies for the school. Some admins have greater privileges than others depending on the scope of their responsibilities. A central authority is needed to plan, manage, and oversee the complete structure. In this scenario, you can assign administrative units to make it easier to manage the organization.

Diagram of administrative units for each university department.

Things to think about administrative units
Consider how a central admin role can use administrative units to support the Engineering department in our scenario:

Create a role that has administrative permissions for only Microsoft Entra users in the Engineering department administrative unit.

Create an administrative unit for the Engineering department.

Populate the administrative unit with only the Engineering department students, staff, and resources.

Add the Engineering department IT team to the role, along with its scope.

Things to consider when working with administrative units
Think about how you can implement administrative units in your organization. Here are some considerations:

Consider management tools. Review your options for managing AUs. You can use the Azure portal, PowerShell cmdlets and scripts, or Microsoft Graph.

Consider role requirements in the Azure portal. Plan your strategy for administrative units according to role privileges. In the Azure portal, only the Global Administrator or Privileged Role Administrator users can manage AUs.

Consider scope of administrative units. Recognize that the scope of an administrative unit applies only to management permissions. Members and admins of an administrative unit can exercise their default user permissions to browse other users, groups, or resources outside of their administrative unit.

Next unit: Knowledge check

Your company decides to implement the user and group account features of Microsoft Entra ID in their identity and governance strategy. You need to explain the types of accounts, roles, and assignments to the planning team, and guide them in their strategy.

Answer the following questions
Choose the best response for each of the questions. Then select Check your answers.


1. What type of user account allows an external organization to access your resources? 

A Contributor user account for each member of the team.

An administrator account for each member of the team.

A guest user account for each member of the external team.

2. What kind of group account can you create so you can apply the same permissions to all group members? 

Security group.

Microsoft Entra bulk group.

Microsoft 365 group.

3. Which Microsoft Entra role enables a user to manage all groups in your Teams tenants, and also assign other admin roles? 

Global administrator.

Security administrator.

User administrator.

Summary and resources

Azure Administrators must be familiar with configuring user and group accounts in Microsoft Entra ID.

In this module, you learned that every user who wants access to Azure resources needs an Azure user account. Microsoft Entra ID supports access to your organization's resources by assigning access rights to users and groups. You discovered how user and group accounts are created in Microsoft Entra ID. You explored how to configure and manage user and group accounts, including bulk configuration. You reviewed how your organization can support group account organization, and manage accounts across multiple directories.

The main takeaways for this module are:

Microsoft Entra ID supports three types of user accounts: cloud identities, directory-synchronized identities, and guest user identities.

Cloud identities have profile information such as job title and office location. This information can be customized for your organization's needs.

You can bulk create user and group accounts. The process uses a template file managed through the portal.

There are two types of group accounts: Security and Microsoft 365.

Administrative units help you control administrator access to resources.

Learn more with Azure documentation
Enterprise user management documentation. This collection of articles that covers various topics related to user authentication in Microsoft Entra. You learn how to use groups, domain names, and licenses to manage user access to apps and resources3.

Manage Microsoft Entra groups and group membership. This article explains how to create, edit, and delete groups in Microsoft Entra. You also learn how to add or remove members and owners, assign roles, and use dynamic rules for group membership2.

Learn more with self-paced training
Manage users and groups in Microsoft Entra ID. This training module covers the basic principles of user and groups.

Create Azure users and groups in Microsoft Entra ID (exercise, subscription required). This training module covers creating users and assigning them to groups.




Point 3: Configure subscriptions

Learn how to configure Azure subscriptions, including how to obtain a subscription, implement cost management, and apply Azure resource tags.

Learning objectives
In this module, you learn how to:

Determine the correct region to locate Azure services.
Review features and use cases for Azure subscriptions.
Obtain an Azure subscription.
Understand billing and features for different Azure subscriptions.
Use Microsoft Cost Management for cost analysis.
Discover when to use Azure resource tagging.
Identify ways to reduce costs.


1- Introduction

This module provides an overview of Azure subscriptions and their importance in managing costs for organizations.

You work for a multinational company that has recently decided to migrate its infrastructure to the cloud. As part of this migration, you have been tasked with managing the costs associated with the company's Azure resources. You need to understand how Azure subscriptions work and how they can help you effectively manage and optimize costs.

The goal of this module is to equip you with the knowledge and skills to successfully manage Azure subscriptions and control costs for your organization. You learn how Azure subscriptions work and use cost management tools and techniques. You optimize resource usage, prevent overspending, and make informed decisions to achieve cost savings.

Learning objectives
In this module, you learn how to:

Determine the correct region to locate Azure services.

Review features and use cases for Azure subscriptions.

Obtain an Azure subscription.

Understand billing and features for different Azure subscriptions.

Use Microsoft Cost Management and Billing for cost analysis.

Discover when to use Azure resource tagging.

Identify ways to reduce costs.

Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Familiarity with cloud computing and Azure cloud services.
Familiarity with cloud service billing models and subscription methods.
Familiarity with cost management techniques such as reports and budgets.

Next unit: Identify Azure regions

2- Identify Azure regions

Microsoft Azure is made up of datacenters located around the globe. These datacenters are organized and made available to end users by region. A region is a geographical area on the planet containing at least one, but potentially multiple datacenters. The datacenters are in close proximity and networked together with a low-latency network. A few examples of regions are West US, Canada Central, West Europe, Australia East, and Japan West.

Things to know about regions
Here are some points to consider about regions:

Azure is generally available in more than 60 regions in 140 countries.

Azure has more global regions than any other cloud provider.

Regions provide you with the flexibility and scale needed to bring applications closer to your users.

Regions preserve data residency and offer comprehensive compliance and resiliency options for customers.

Things to know about regional pairs
Most Azure regions are paired with another region within the same geography to make a regional pair (or paired regions). Regional pairs help to support always-on availability of Azure resources used by your infrastructure. The following table describes some prominent characteristics of paired regions:

Characteristic	Description
Physical isolation	Azure prefers at least 300 miles of separation between datacenters in a regional pair. This principle isn't practical or possible in all geographies. Physical datacenter separation reduces the likelihood of natural disasters, civil unrest, power outages, or physical network outages affecting both regions at once.
Platform-provided replication	Some services like Geo-Redundant Storage provide automatic replication to the paired region.
Region recovery order	During a broad outage, recovery of one region is prioritized out of every pair. Applications that are deployed across paired regions are guaranteed to have one of the regions recovered with priority.
Sequential updates	Planned Azure system updates are rolled out to paired regions sequentially (not at the same time). Rolling updates minimizes downtime, reduces bugs, and logical failures in the rare event of a bad update.
Data residency	Regions reside within the same geography as their enabled set (except for the Brazil South and Singapore regions).
Things to consider when using regions and regional pairs
You've reviewed the important considerations about regions and regional pairs. Now think about how you might implement regions in your organization.

Consider resource and region deployment. Plan the regions where you want to deploy your resources. For most Azure services, when you deploy a resource in Azure, you choose the region where you want your resource to be deployed.

Consider service support by region. Research region and service availability. Some services or Azure Virtual Machines features are available only in certain regions, such as specific Virtual Machines sizes or storage types.

Consider services that don't require regions. Identify services that don't need region support. Some global Azure services that don't require you to select a region. These services include Microsoft Entra ID, Microsoft Azure Traffic Manager, and Azure DNS.

Consider exceptions to region pairing. Check the Azure website for current region availability and exceptions. If you plan to support the Brazil South region, note this region is paired with a region outside its geography. The Singapore region also has an exception to standard regional pairing.

Consider benefits of data residency. Take advantage of the benefits of data residency offered by regional pairs. This feature can help you meet requirements for tax and law enforcement jurisdiction purposes.

Find regions for your business geography
Visit the Azure global infrastructure website to find supported regions for your business geography. You can search by country/region name or by Microsoft product. A list of supported region pairs and exceptions is also available.

Screenshot of the Azure global infrastructure and Azure geographies website.

By geography	By product	Paired regions
Search Azure regions by geography.	Search Azure products by region or geography.	Search for paired regions and exceptions.
Screenshot that shows how to search for available regions by geographic location.	Screenshot that shows how to find products available according to region or geographic location.	Screenshot that shows how to search for regional pairs.

Next unit: Implement Azure subscriptions

3- Implement Azure subscriptions

An Azure subscription is a logical unit of Azure services that's linked to an Azure account. An Azure account is an identity in Microsoft Entra ID or a directory that's trusted by Microsoft Entra ID, such as a work or school account. Subscriptions help you organize access to Azure cloud service resources, and help you control how resource usage is reported, billed, and paid.

Diagram that shows the relationship between an Azure subscription and an Azure account, which is an identity in Microsoft Entra ID.

Things to know about subscriptions
As you think about the subscriptions to implement for your company, consider the following points:

Every Azure cloud service belongs to a subscription.

Each subscription can have a different billing and payment configuration.

Multiple subscriptions can be linked to the same Azure account.

More than one Azure account can be linked to the same subscription.

Billing for Azure services is done on a per-subscription basis.

If your Azure account is the only account associated with a subscription, you're responsible for the billing requirements.

Programmatic operations for a cloud service might require a subscription ID.

Things to consider when using subscriptions
Consider how many subscriptions your organization needs to support the business scenarios. As you plan, think about how you can organize your resources into resource groups.

Consider the types of Azure accounts required. Determine the types of Azure accounts your users will link with Azure subscriptions. You can use a Microsoft Entra account or a directory that's trusted by Microsoft Entra ID like a work or school account. If you don't belong to one of these organizations, you can sign up for an Azure account by using your Microsoft Account, which is also trusted by Microsoft Entra ID.

Consider multiple subscriptions. Set up different subscriptions and payment options according to your company's departments, projects, regional offices, and so on. A user can have more than one subscription linked to their Azure account, where each subscription pertains to resources, access privileges, limits, and billing for a specific project.

Consider a dedicated shared services subscription. Plan for how users can share resources allocated in a single subscription. Use a shared services subscription to ensure all common network resources are billed together and isolated from other workloads. Examples of shared services subscriptions include Azure ExpressRoute and Virtual WAN.

Consider access to resources. Every Azure subscription can be associated with a Microsoft Entra ID. Users and services authenticate with Microsoft Entra ID before they access resources.

Next unit: Obtain an Azure subscription

4- Obtain an Azure subscription

To use Azure, you must have an Azure subscription. There are several ways to procure an Azure subscription. You can obtain an Azure subscription as part of an Enterprise agreement, or through a Microsoft reseller or Microsoft partner. Users can also open a personal free account for a trial subscription.

Things to know about obtaining an Azure subscription
Review the following ways to obtain an Azure subscription and consider which options would work for your organization.

Procurement option	Description
	Enterprise agreement

Any Enterprise Agreement customer can add Azure to their agreement by making an upfront monetary commitment to Azure. The commitment is consumed throughout the year by using any combination of the wide variety of cloud services Azure offers.
	Microsoft reseller

Buy Azure through the Open Licensing program, which provides a simple, flexible way to purchase cloud services from your Microsoft reseller. If you already purchased an Azure in Open license key, activate a new subscription or add more credits now.
	Microsoft partner

Find a Microsoft partner who can design and implement your Azure cloud solution. These partners have the business and technology expertise to recommend solutions that meet the unique needs of your business.
	Personal free account

Any user can sign up for a free trial account. You can get started using Azure right away, and you won't be charged until you choose to upgrade.

Next unit: Identify Azure subscription usage

5- Identify Azure subscription usage

We reviewed the ways you can obtain an Azure subscription. Now let's look at the types of Azure subscriptions that are available.

Azure offers free and paid subscription options to meet different needs and requirements. The most common subscriptions are Free, Pay-As-You-Go, Enterprise Agreement, and Student. For your organization, you can choose a combination of procurement options and subscription choices to meet your business scenarios.

Things to consider when choosing Azure subscriptions
As you think about which types of Azure subscriptions would work for your organization, consider these scenarios:

Consider trying Azure for free. An Azure free subscription includes a monetary credit to spend on any service for the first 30 days. You get free access to the most popular Azure products for 12 months, and access to more than 25 products that are always free. An Azure free subscription is an excellent way for new users to get started.

To set up a free subscription, you need a phone number, a credit card, and a Microsoft account.
The credit card information is used for identity verification only. You aren't charged for any services until you upgrade to a paid subscription.
Consider paying monthly for used services. A Pay-As-You-Go (PAYG) subscription charges you monthly for the services you used in that billing period. This subscription type is appropriate for a wide range of users, from individuals to small businesses, and many large organizations as well.

Consider using an Azure Enterprise Agreement. An Enterprise Agreement provides flexibility to buy cloud services and software licenses under one agreement. The agreement comes with discounts for new licenses and Software Assurance. This type of subscription targets enterprise-scale organizations.

Consider supporting Azure for students. An Azure for Students subscription includes a monetary credit that can be used within the first 12 months.

Students can select free services without providing a credit card during the sign-up process.
You must verify your student status through your organizational email address.
 Note

For a complete list of Azure subscription options, see the current Microsoft Azure offers.

Next unit: Implement Microsoft Cost Management

6- Implement Microsoft Cost Management

With Azure products and services, you pay only for what you use. As you create and use Azure resources, you're charged for the resources.

Microsoft Cost Management provides support for administrative billing tasks and helps you manage billing access to costs. You can use the product to monitor and control Azure spending, and optimize your Azure resource usage.

Screenshot of the Microsoft Cost Management dashboard showing service name and location costs, and billing forecasts.

Things to know about Microsoft Cost Management
Your organization is interested in the benefits of using Microsoft Cost Management to monitor their subscription billing and resource usage. As you plan for your implementation, review the following product characteristics and features:

Microsoft Cost Management shows organizational cost and usage patterns with advanced analytics. Costs are based on negotiated prices and factor in reservation and Azure Hybrid Benefit discounts. Predictive analytics are also available.

Reports in Microsoft Cost Management show the usage-based costs consumed by Azure services and third-party Marketplace offerings. Collectively, the reports show your internal and external costs for usage and Azure Marketplace charges. The reports help you understand your spending and resource use, and can help find spending anomalies. Charges, such as reservation purchases, support, and taxes might not be visible in reports.

The product uses Azure management groups, budgets, and recommendations to show clearly how your expenses are organized and how you might reduce costs.

You can use the Azure portal or various APIs for export automation to integrate cost data with external systems and processes. Automated billing data export and scheduled reports are also available.

Things to consider when using Microsoft Cost Management
Microsoft Cost Management can help you plan for and control your organization costs. Consider how the product features can be implemented to support your business scenarios:

Consider cost analysis. Take advantage of Microsoft Cost Management cost analysis features to explore and analyze your organizational costs. You can view aggregated costs by organization to understand where costs are accrued, and to identify spending trends. Monitor accumulated costs over time to estimate monthly, quarterly, or even yearly cost trends against a budget.

Consider budget options. Use Microsoft Cost Management features to establish and maintain budgets. The product helps you plan for and meet financial accountability in your organization. Budgets help prevent cost thresholds or limits from being surpassed. You can utilize analysis data to inform others about their spending to proactively manage costs. The budget features help you see how company spending progresses over time.

Consider recommendations. Review the Microsoft Cost Management recommendations to learn how you can optimize and improve efficiency by identifying idle and underutilized resources. Recommendations can reveal less expensive resource options. When you act on the recommendations, you change the way you use your resources to save money. Using recommendations is an easy process:

View cost optimization recommendations to see potential usage inefficiencies.
Act on a recommendation to modify your Azure resource use and implement a more cost-effective option.
Verify the new action to make sure the change has the desired effect.
Consider exporting cost management data. Microsoft Cost Management helps you work with your billing information. If you use external systems to access or review cost management data, you can easily export the data from Azure.

Set a daily scheduled export in comma-separated-value (CSV) format and store the data files in Azure storage.
Access your exported data from your external system.

Next unit: Apply resource tagging

7- Apply resource tagging

You can apply tags to your Azure resources to logically organize them by categories. Tags are useful for sorting, searching, managing, and doing analysis on your resources.

Each resource tag consists of a name and a value. You could have the tag name Server and the value Production or Development, and then apply the tag/value pair to your Engineering computer resources.

Here's an example that shows how to add tags for a resource group in the Azure portal:

Screenshot that shows how to add tags for a resource group in the Azure portal.

Things to know about resource tags
As you plan your Azure subscriptions, resources, and services, review these characteristics of Azure resource tags:

Each resource tag has a name and a value.

The tag name remains constant for all resources that have the tag applied.

The tag value can be selected from a defined set of values, or unique for a specific resource instance.

A resource or resource group can have a maximum of 50 tag name/value pairs.

Tags applied to a resource group aren't inherited by the resources in the resource group.

Things to consider when using resource tags
Here are a few things you can do with resource tags:

Consider searching on tag data. Search for resources in your subscription by querying on the tag name and value.

Consider finding related resources. Retrieve related resources from other resource groups by searching on the tag name or value.

Consider grouping billing data. Group resources like virtual machines by cost center and production environment. When you download the resource usage comma-separated values (CSV) file for your services, the tags appear in the Tags column.

Consider creating tags with PowerShell or the Azure CLI. Create many resource tags programmatically by using Azure PowerShell or the Azure CLI.

How to keep your Azure subscription clean

Next unit: Apply cost savings

8- Apply cost savings

Azure has several options that can help you gain significant cost savings for your organization. As you prepare your implementation plan for Azure subscriptions, services, and resources, consider the following cost saving advantages.

Cost saving	Description
Reservations	Save money by paying ahead. You can pay for one year or three years of virtual machine, SQL Database compute capacity, Azure Cosmos DB throughput, or other Azure resources. Pre-paying allows you to get a discount on the resources you use. Reservations can significantly reduce your virtual machine, SQL database compute, Azure Cosmos DB, or other resource costs up to 72% on pay-as-you-go prices. Reservations provide a billing discount and don't affect the runtime state of your resources.
Azure Hybrid Benefits	Access pricing benefits if you have a license that includes Software Assurance. Azure Hybrid Benefits helps maximize the value of existing on-premises Windows Server or SQL Server license investments when migrating to Azure. There's an Azure Hybrid Benefit Savings Calculator to help you determine your savings.
Azure Credits	Use the monthly credit benefit to develop, test, and experiment with new solutions on Azure. As a Visual Studio subscriber, you could use Microsoft Azure at no extra charge. With your monthly Azure credit, Azure is your personal sandbox for development and testing.
Azure regions	Compare pricing across regions. Pricing can vary from one region to another, even in the US. Double check the pricing in various regions to see if you can save by selecting a different region for your subscription.
Budgets	Apply the budgeting features in Microsoft Cost Management to help plan and drive organizational accountability. With budgets, you can account for the Azure services you consume or subscribe to during a specific period. Monitor spending over time and inform others about their spending to proactively manage costs. Use budgets to compare and track spending as you analyze costs.
Pricing Calculator	The Pricing Calculator provides estimates in all areas of Azure, including compute, networking, storage, web, and databases.
The following image shows a scenario for using the Pricing Calculator. The customer has an instance of a D1 series VM on Windows. The instance is running in the West US region at the standard tier level.

Screenshot of the Pricing Calculator with estimates for an instance of a D1 series VM on Windows. The instance is running in the West US region at the standard tier level. |

Next unit: Knowledge check

Your company is moving to the Azure cloud platform. You're researching how to implement Azure subscriptions to support the business regions and work scenarios. In your deployment plan, you're considering how to preserve data residency and apply resource tagging. The company financial controller has asked to know more about cost management and billing for Azure services.

Answer the following questions
Choose the best response for each of the questions below. Then select Check your answers.


1. The company financial controller wants to be notified whenever the company is half-way to spending the money allocated for cloud services. Which approach supports this request? 

Create an Azure reservation.

Create a budget and a spending threshold.

Create a management group.

2. The company financial controller wants to identify which billing department each Azure resource belongs to. Which approach enables this requirement? 

Track resource usage in a spreadsheet.

Place the resources in different regions.

Apply a tag to each resource that includes the associated billing department.

3. Which option preserves data residency, and offers comprehensive compliance and resiliency options? 

Microsoft Entra account

Regions

Subscriptions


Summary and resources

Azure Administrators commonly obtain and manage Azure subscriptions. Azure subscriptions help you effectively identify and manage costs for your organization, so you can provide services and resources for specific scenarios.

In this module, you learned about supported Azure regions, and how to locate Azure services. You reviewed the features and use cases for Azure subscriptions, and how to obtain subscriptions. You explored features and billing for different types of Azure subscriptions, and how to apply resource tagging. You discovered how Microsoft Cost Management can be used for cost analysis. You learned how to identify ways you can reduce your billing costs.

The main takeaways from this module are:

Azure regions provide flexibility, data residency, compliance, and resiliency options.

Azure subscriptions are essential for managing access to Azure resources and billing.

Azure offers various subscription options such as Free, Pay-As-You-Go, Enterprise Agreement, and Student.

Azure offers cost-saving options such as reservations, Azure Hybrid Benefits and Azure credits.

Resource tagging allows for organizing and analyzing resources in Azure.

Microsoft Cost Management helps monitor and control Azure spending.

The Pricing Calculator provides billing estimates for different usage cases.

Learn more with Azure documentation
Cost management. This collection of articles covers pricing, reporting, analytics, monitoring and optimizing costs.

Billing and subscriptions management. This collection of articles covers managing your account, subscriptions, bills, and payments.

Pricing calculator. This tool estimates the hourly or monthly costs for using Azure.

Learn more with self-paced training
Control Azure spending and manage bills with Microsoft Cost Management + Billing. Learn how to monitor and control your Azure spending and optimize the use of Azure resources.

Introduction to analyzing costs and creating budgets with Microsoft Cost Management (exercise, subscription required). Learn how to use cost analysis to understand how your costs accrue each month. Use this understanding to create an Azure budget to monitor and alert on your costs.

Describe cost management in Azure (exercise, subscription required). Explore methods to estimate, track, and manage costs in Azure.





Point 4: Configure Azure Policy

Learn how to configure Azure Policy to implement compliance requirements.

Learning objectives
In this module, you learn how to:

Create management groups to target policies and spending budgets.
Implement Azure Policy with policy and initiative definitions.
Scope Azure policies and determine compliance.


1- Introduction

Azure Policy is a service in Azure that enables you to create, assign, and manage policies to control or audit your resources. These policies enforce different rules over your resource configurations so the configurations stay compliant with corporate standards.

Your company is subject to many regulations and compliance rules. Your company wants to ensure each department implements and deploys resources correctly. You're responsible for investigating how to use Azure Policy and management groups to implement compliance measures.

In this module, you learn how to implement Azure policies. The content includes examples of policy definitions and initiative definitions. You learn how to scope your policies and identify noncompliant resources. You also learn the benefits and usage of management groups.

The goal to this module is to ensure Administrators can use Azure policy to ensure resource compliance.

Learning objectives
In this module, you learn how to:

Implement Azure Policy with policy and initiative definitions.

Scope Azure policies and determine compliance.

Create management groups to target policies and spending budgets.

Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Basic understanding of Azure concepts, such as subscriptions, resource groups, and resources.
Working knowledge of governance concepts, such as compliance and reporting.
Familiarity with Azure Resource Manager and how to use it to deploy and manage resources.
Knowledge of JSON syntax so you can create policy definitions and policy assignments.

Next unit: Create management groups

2- Create management groups

Organizations that use multiple subscriptions need a way to efficiently manage access, policies, and compliance. Azure management groups provide a level of scope and control above your subscriptions. You can use management groups as containers to manage access, policy, and compliance across your subscriptions.

Things to know about management groups
Consider the following characteristics of Azure management groups:

By default, all new subscriptions are placed under the top-level management group, or root group.

All subscriptions within a management group automatically inherit the conditions applied to that management group.

A management group tree can support up to six levels of depth.

Azure role-based access control authorization for management group operations isn't enabled by default.

The following diagram shows how Azure management groups can be used to organize subscriptions in a hierarchy of unified policy and access management. In this scenario, the organization has a single top-level management group. Every directory under the root group is folded into the top-level group.



Things to consider when using management groups
Review the following ways you can use management groups in Azure Policy to manage your subscriptions:

Consider custom hierarchies and groups. Align your Azure subscriptions by using custom hierarchies and grouping that meet your company's organizational structure and business scenarios. You can use management groups to target policies and spending budgets across subscriptions.

Consider policy inheritance. Control the hierarchical inheritance of access and privileges in policy definitions. All subscriptions within a management group inherit the conditions applied to the management group. You can apply policies to a management group to limit the regions available for creating virtual machines (VMs). The policy can be applied to all management groups, subscriptions, and resources under the initial management group, to ensure VMs are created only in the specified regions.

Consider compliance rules. Organize your subscriptions into management groups to help meet compliance rules for individual departments and teams.

Consider cost reporting. Use management groups to do cost reporting by department or for specific business scenarios. You can use management groups to report on budget details across subscriptions.

Create management groups
You can create a management group with Azure Policy by using the Azure portal, PowerShell, or the Azure CLI. Here's an example of what you see in the Azure portal:



A management group has a directory unique identifier (ID) and a display name. The ID is used to submit commands on the management group. The ID value can't be changed after it's created because it's used throughout the Azure system to identify the management group. The display name for the management group is optional and can be changed at any time.

Next unit: Implement Azure policies

3- Implement Azure policies

Azure Policy is a service in Azure that you can use to create, assign, and manage policies. You can use policies to enforce rules on your resources to meet corporate compliance standards and service level agreements. Azure Policy runs evaluations and scans on your resources to make sure they're compliant.

Things to know about Azure Policy
The main advantages of Azure Policy are in the areas of enforcement and compliance, scaling, and remediation. Azure Policy is also important for teams that run an environment that requires different forms of governance.

Advantage	Description
Enforce rules and compliance	Enable built-in policies, or build custom policies for all resource types. Support real-time policy evaluation and enforcement, and periodic or on-demand compliance evaluation.
Apply policies at scale	Apply policies to a management group with control across your entire organization. Apply multiple policies and aggregate policy states with policy initiative. Define an exclusion scope.
Perform remediation	Conduct real-time remediation, and remediation on your existing resources.
Exercise governance	Implement governance tasks for your environment:
- Support multiple engineering teams (deploying to and operating in the environment)
- Manage multiple subscriptions
- Standardize and enforce how cloud resources are configured
- Manage regulatory compliance, cost control, security, and design consistency
Things to consider when using Azure Policy
Review the following scenarios for using Azure Policy. Consider how you can implement the service in your organization.

Consider deployable resources. Specify the resource types that your organization can deploy by using Azure Policy. You can specify the set of virtual machine SKUs that your organization can deploy.

Consider location restrictions. Restrict the locations your users can specify when deploying resources. You can choose the geographic locations or regions that are available to your organization.

Consider rules enforcement. Enforce compliance rules and configuration options to help manage your resources and user options. You can enforce a required tag on resources and define the allowed values.

Consider inventory audits. Use Azure Policy with Azure Backup service on your VMs and run inventory audits.

Next unit: Create Azure policies

4- Create Azure policies

Azure Administrators use Azure Policy to create policies that define conventions for resources. A policy definition describes the compliance conditions for a resource, and the actions to complete when the conditions are met. One or more policy definitions are grouped into an initiative definition, to control the scope of your policies and evaluate the compliance of your resources.

Diagram that shows an initiative definition for a group of policy definitions that are applied to resources.

There are four basic steps to create and work with policy definitions in Azure Policy.

Step 1: Create policy definitions
A policy definition expresses a condition to evaluate and the actions to perform when the condition is met. You can create your own policy definitions, or choose from built-in definitions in Azure Policy. You can create a policy definition to prevent VMs in your organization from being deployed, if they're exposed to a public IP address.

Step 2: Create an initiative definition
An initiative definition is a set of policy definitions that help you track your resource compliance state to meet a larger goal. You can create your own initiative definitions, or use built-in definitions in Azure Policy. You can use an initiative definition to ensure resources are compliant with security regulations.

Step 3: Scope the initiative definition
Azure Policy lets you control how your initiative definitions are applied to resources in your organization. You can limit the scope of an initiative definition to specific management groups, subscriptions, or resource groups.

Step 4: Determine compliance
After you assign an initiative definition, you can evaluate the state of compliance for all your resources. Individual resources, resource groups, and subscriptions within a scope can be exempted from having the policy rules affect it. Exclusions are handled individually for each assignment.

Next unit: Create policy definitions

5- Create policy definitions

Azure Policy offers built-in policy definitions to help you quickly configure control conditions for your resources. In addition to the built-in policies, you can also create your own definitions, or import definitions from other sources.

Access built-in policy definitions
You can sort the list of built-in definitions by category to search for policies that meet your business needs.

Screenshot that shows a list of built-in policy definitions in Azure Policy.

Here are some examples of built-in policy definitions:

Allowed virtual machine size SKUs: Specify a set of VM size SKUs that your organization can deploy. This policy is located under the Compute category.

Allowed locations: Restrict the locations users can specify when deploying resources. Use this policy to enforce your geo-compliance requirements. This policy is located under the General category.

Configure Azure Device Update for IoT Hub accounts to disable public network access: Disable public network access for your Device Update for IoT Hub resources. This policy is located under the Internet of Things category.

Add new policy definitions
If you don't find a built-in policy to meet your business needs, you can add or create a new definition. Policy definitions can also be imported into Azure Policy from GitHub. New policy definitions are added to the samples repository almost daily.

Screenshot that shows how to add a new policy definition, and the option to import a sample policy definition from GitHub.

 Note

When you add or create a policy definition, be sure the definition uses the specific JSON format required by Azure. For more information, see Azure Policy definition structure.

Next unit: Create an initiative definition

6- Create an initiative definition

After you determine your policy definitions, the next step is to create an initiative definition for your policies. An initiative definition has one or more policy definitions. One example for using initiative definitions is to ensure your resources are compliant with security regulations.

 Tip

Even if you have only a few policy definitions in your organization, we recommend creating and applying an initiative definition.

Add a new initiative definition
When you create an initiative definition, be sure the definition uses the specific JSON format required by Azure. For more information, see Azure Policy initiative definition structure.

Here's an example of how to create a new initiative definition in the Azure portal:

Screenshot that shows how to create a new initiative definition.

Use a built-in initiative definition
You can create your own initiative definitions, or use built-in definitions in Azure Policy. You can sort the list of built-in initiatives by category to search for definitions for your organization.

Here are some examples of built-in initiative definitions:

Audit machines with insecure password security settings: Use this initiative to deploy an audit policy to specified resources in your organization. The definition evaluates the resources to check for insecure password security settings. This initiative is located in the Guest Configuration category.

Configure Windows machines to run Azure Monitor Agent and associate them to a Data Collection Rule: Use this initiative to monitor and secure your Windows VMs, Virtual Machine Scale Sets, and Arc machines. The definition deploys the Azure Monitor Agent extension and associates the resources with a specified Data Collection Rule. This initiative is located in the Monitoring category.

Configure Azure Defender to be enabled on SQL servers: Enable Azure Defender on your Azure SQL Servers to detect anomalous activities indicating unusual and potentially harmful attempts to access or exploit databases. This initiative is located in the SQL category.

Next unit: Scope the initiative definition

7- Scope the initiative definition

After you create your initiative definition, the next step is to assign the initiative to establish the scope for the policies. The scope determines what resources or grouping of resources are affected by the conditions of the policies.

Here's an example that shows how to configure the scope assignment:

Screenshot that shows how to assign an initiative definition to resources or groups or resources to establish the scope.

To establish the scope, you select the affected subscriptions. As an option, you can also choose the affected resource groups.

The following example shows how to apply the scope:

Screenshot that shows how a scope is applied to a subscription, and optionally applied to a resource group.

Next unit: Determine compliance

8- Determine compliance

You have your policies defined, your initiative definition created, and your policies assigned to affected resources. The last step is to evaluate the state of compliance for your scoped resources.

The following example shows how you can use the Compliance feature to look for non-compliant initiatives, policies, and resources.

Screenshot that shows how to use the compliance feature to look for non-compliant initiatives, policies, and resources.

Your policy conditions are evaluated against your existing scoped resources. Although the Azure portal doesn't show the evaluation logic, the compliance state results are shown. The compliance state result is either compliant or non-compliant.

Next unit: Interactive lab simulation

9- Interactive lab simulation

Lab scenario
Your organization is piloting a new infrastructure project. The CTO wants to know which Azure resources are being used on the new project. Your specific tasks are:

Create a way to tag the project resources.
Don't allow resources to be created without the resource tag.
If a resource is created without the tag, automatically add the tag.
Architecture diagram
Architecture diagram as explained in the text.

Objectives
Task 1: Create and assign tags via the Azure portal.
For testing purposes, identify the Cloud Shell resource group.
Add a tag to the resource group. Assign the value of the tag.
Verify the storage account in the resource group doesn't have the tag.
Task 2: Enforce tagging by using an Azure policy.
Locate the Require a tag and its value on resources built-in policy and review the definition.
Assign the policy to the resource group.
Configure the required tag: Role with a value of Infra.
Create a new storage account in the resource group and verify without the tag you can't create the resource.
Task 3: Automatically apply tagging by using an Azure policy.
Assign the Inherit a tag from the resource group if missing built-in policy to the resource group.
Configure remediation to automatically add the Role tag if it is missing from a new resource.
Create a new storage account and verify the tag and value are added.
 Note

Click on the thumbnail image to start the lab simulation. When you're done, be sure to return to this page so you can continue learning.

Screenshot of the simulation page.

Next unit: Knowledge check

Your company is going to implement Azure Policy to manage governance across multiple Azure subscriptions. You're exploring how to use Azure policies, initiatives, and definitions for the different departments. You're researching how management groups can support your business scenarios.

The finance team requests resources and billing to be categorized by department, such as Marketing, Research, and Human Resources. They'd like billing consolidated across multiple resource groups to ensure all users comply with the solution.

Answer the following questions
Choose the best response for each of the questions below. Then select Check your answers.


1. There are several Azure policies that need to be applied to a new branch office. What's the best approach? 

Create a management group

Create a policy initiative

Create a policy definition

2. To satisfy the finance team's request for billing by department, multiple resource groups have been created and the resource tags applied. What's the next step? 

Create a management group

Create an Azure policy

Review the Azure Policy compliance page

3. How can you ensure that only cost-effective virtual machine SKU sizes are deployed? 

Periodically inspect the deployment to see which SKU sizes are used

Create an Azure RBAC role that defines the allowed virtual machine SKU sizes

Create a policy in Azure Policy that specifies the allowed SKU sizes

4. Which option can you use to manage governance across multiple Azure subscriptions? 

Azure initiatives

Resource groups

Management groups

Summary and resources

Azure Policy is a service in Azure that enables you to create, assign, and manage policies. Azure Policy helps you define and implement your governance strategy by using policies to control and audit your resources.

In this module, you have learned about Azure Policy and how it allows you to control and audit your resources. You explored how to implement Azure policy definitions and initiatives for your corporate departments. You learned how to create management groups, scope policies, and manage spending budgets. You reviewed how Azure policies can be scoped to meet compliance regulations.

The main takeaways from this module are:

Azure Policy is a powerful service in Azure that enables you to enforce rules and ensure compliance with corporate standards and service level agreements.
Management groups provide a way to efficiently manage access, policies, and compliance across multiple subscriptions, allowing for unified policy and access management.
Creating policy definitions and initiative definitions allows you to define conventions for resources and control the scope of policies, ensuring resource compliance.
The Compliance feature in Azure Policy helps you determine the compliance state of your resources and evaluate whether they're compliant or compliant.
Learn more with Azure documentation
Azure Policy documentation. This collection of articles is your starting point for all things Azure policy.

Azure Policy built-in policy definitions. This page is an index of Azure Policy built-in policy definitions.

Azure built-in policy initiatives. This page is an index of Azure Policy built-in initiative definitions.

Quickstart: Create a policy assignment to identify noncompliant resources. This quickstart steps you through the process of creating a policy assignment to identify virtual machines that aren't using managed disks.

Learn more with self-paced training
Introduction to Azure Policy. This module introduces you to Azure Policy and describes its characteristics, capabilities, and use cases.




Point 5: Configure role-based access control

Learn how to use role-based access control (RBAC) to ensure resources are protected, but users can still access the resources they need.

Learning objectives
In this module, you learn how to:

Identify features and use cases for role-based access control.
List and create role definitions.
Create role assignments.
Identify differences between Azure RBAC and Microsoft Entra roles.
Manage access to subscriptions with RBAC.
Review built-in Azure RBAC roles.

1- Introduction

Azure Administrators need to secure access to their Azure resources like virtual machines (VMs), websites, networks, and storage. Administrators need mechanisms to help them manage who can access their resources, and what actions are allowed. Organizations that do business in the cloud recognize that securing their resources is a critical function of their infrastructure.

In this module, your business is investigating how to ensure their corporate data and assets are protected. They want secure protection that enables them to control access to their data and resources by specifying roles and access privileges for employees and business partners. You're responsible for researching how to use role-based access control (RBAC) to accomplish these tasks. You need to ensure the company assets are protected, and also support user access to the resources.

The goal of this module is to understand the features and use cases for Azure role-based access control (RBAC). You learn how to create role definitions and role assignments, and find and use built-in Azure RBAC roles. Additionally, you explore how to use RBAC to manage access to subscriptions. You also review the differences between Azure RBAC and Entra ID roles.

Learning objectives
In this module, you learn how to:

Understand the concepts and principles of Azure RBAC.
Create role definitions and role assignments.
Identify differences between Azure RBAC and Microsoft Entra roles.
Use RBAC to manage access to resources.
Review and select the best built-in Azure role for a scenario.
Prerequisites
Familiarity with Azure. Having a general understanding of Azure services, concepts, and terminology that helps you grasp RBAC more effectively.

Identity concepts. A basic understanding of Microsoft Entra ID, which is Microsoft's cloud-based identity and access management service, is essential. Knowledge of concepts like users, groups, roles, and permissions are helpful.

Azure Resource Management. Understanding how resources are organized, deployed, and managed provides context for RBAC implementation.

Access Control Models. Knowledge of access control models, such as discretionary access control (DAC) and mandatory access control (MAC). This knowledge helps you understand the principles behind RBAC and its advantages over traditional access control mechanisms.

Next unit: Implement role-based access control

2- Implement role-based access control

Secure access management for cloud resources is critical for businesses that operate in the cloud. Role-based access control (RBAC) is a mechanism that can help you manage who can access your Azure resources. RBAC lets you determine what operations specific users can do on specific resources, and control what areas of a resource each user can access.

Azure RBAC is an authorization system built on Azure Resource Manager. Azure RBAC provides fine-grained access management of resources in Azure.

Things to know about Azure RBAC
Here are some things you can do with Azure RBAC:

Allow an application to access all resources in a resource group.

Allow one user to manage VMs in a subscription, and allow another user to manage virtual networks.

Allow a database administrator (DBA) group to manage SQL databases in a subscription.

Allow a user to manage all resources in a resource group, such as VMs, websites, and subnets.

Azure RBAC concepts
The following table describes the core concepts of Azure RBAC.

Concept	Description	Examples
Security principal	An object that represents something that requests access to resources.	User, group, service principal, managed identity
Role definition	A set of permissions that lists the allowed operations. Azure RBAC comes with built-in role definitions, but you can also create your own custom role definitions.	Some built-in role definitions: Reader, Contributor, Owner, User Access Administrator
Scope	The boundary for the requested level of access, or "how much" access is granted.	Management group, subscription, resource group, resource
Role assignment	An assignment attaches a role definition to a security principal at a particular scope. Users can grant the access described in a role definition by creating (attaching) an assignment for the role.	- Assign the User Access Administrator role to an admin group scoped to a management group
- Assign the Contributor role to a user scoped to a subscription
Things to consider when using Azure RBAC
As you think about how you can implement roles and scope assignments within your organization, consider these points:

Consider your requestors. Plan your strategy to accommodate for all types of access to your resources. Security principals are created for anything that requests access to your resources. Determine who are the requestors in your organization. Requestors can be internal or external users, groups of users, applications and services, resources, and so on.

Consider your roles. Examine the types of job responsibilities and work scenarios in your organization. Roles are commonly built around the requirements to fulfill job tasks or complete work goals. Certain users like administrators, corporate controllers, and engineers can require a level of access beyond what most users need. Some roles can be defined to provide the same access for all members of a team or department for specific resources or applications.

Consider scope of permissions. Think about how you can ensure security by controlling the scope of permissions for role assignments. Outline the types of permissions and levels of scope that you need to support. You can apply different scope levels for a single role to support requestors in different scenarios.

Consider built-in or custom definitions. Review the built-in role definitions in Azure RBAC. Built-in roles can be used as-is, or adjusted to meet the specific requirements for your organization. You can also create custom role definitions from scratch.

Next unit: Create a role definition

3- Create a role definition

A role definition consists of sets of permissions that are defined in a JSON file. Each permission set has a name, such as Actions or NotActions that describe the permissions. Some examples of permission sets include:

Actions permissions identify what actions are allowed.

NotActions permissions specify what actions aren't allowed.

DataActions permissions indicate how data can be changed or used.

AssignableScopes permissions list the scopes where a role definition can be assigned.

The following diagram shows details for the Contributor role in Azure RBAC.

Diagram that shows built-in roles in Azure RBAC and custom roles. Permission sets are shown for the built-in Contributor role, including Actions, Not Actions, and Data Actions.

The Actions permissions show the Contributor role has all action privileges. The asterisk "*" wildcard means "all." The NotActions permissions narrow the privileges provided by the Actions set, and deny three actions:

Authorization/*/Delete: Not authorized to delete or remove for "all."
Authorization/*/Write: Not authorized to write or change for "all."
Authorization/elevateAccess/Action: Not authorized to increase the level or scope of access privileges.
The Contributor role also has two permissions to specify how data can be affected:

"NotDataActions": []: No specific actions are listed. Therefore, all actions can affect the data.
"AssignableScopes": ["/"]: The role can be assigned for all scopes that affect data.
Things to know about role definitions
Review the following characteristics of role definitions:

Azure RBAC provides built-in roles and permissions sets. You can also create custom roles and permissions.

The Owner built-in role has the highest level of access privilege in Azure.

The system subtracts NotActions permissions from Actions permissions to determine the effective permissions for a role.

The AssignableScopes permissions for a role can be management groups, subscriptions, resource groups, or resources.

Role permissions
Use the Actions and NotActions permissions together to grant and deny the exact privileges for each role. The Actions permissions can provide the breadth of access and the NotActions permissions can narrow the access.

The following table shows how the Actions or NotActions permissions are used in the definitions for three built-in roles: Owner, Contributor, and Reader. The permissions are narrowed from the Owner role to the Contributor and Reader roles to limit access.

Role name	Description	Actions permissions	NotActions permissions
Owner	Grants full access to manage all resources, including the ability to assign roles in Azure RBAC.	*	n/a
Contributor	Grants full access to manage all resources, but does not allow you to assign roles in Azure RBAC, manage assignments in Azure Blueprints, or share image galleries.	*	- Microsoft.Authorization/*/Delete
- Microsoft.Authorization/*/Write
- Microsoft.Authorization/elevateAccess/Action
Reader	View all resources, but does not allow you to make any changes.	/*/read	n/a
Role scopes
After you define the role permissions, you use the AssignableScopes permissions to specify how the role can be assigned. Let's look at a few examples.

Scope a role as available for assignment in two subscriptions:

"/subscriptions/c276fc76-9cd4-44c9-99a7-4fd71546436e", "/subscriptions/e91d47c4-76f3-4271-a796-21b4ecfe3624"

Scope a role as available for assignment only in the Network resource group:

"/subscriptions/c276fc76-9cd4-44c9-99a7-4fd71546436e/resourceGroups/Network"

Scope a role as available for assignment for all requestors:

"/"

Things to consider when creating roles
Consider the following points about creating role definitions in Azure RBAC:

Consider using built-in roles. Review the list of built-in role definitions in Azure RBAC. There are over 100 predefined role definitions to choose from, such as Owner, Backup Operator, Website Contributor, and SQL Security Manager. Built-in roles are defined for several categories of services, tasks, and users, including General, Networking, Storage, Databases, and more.

Consider creating custom definitions. Define your own custom roles to meet specific business scenarios for your organization. You can modify the permissions for a built-in role to meet the specific requirements for your organization. You can also create custom role definitions from scratch.

Consider limiting access scope. Assign your roles with the minimum level of scope required to perform the job duties. Some users like administrators require full access to corporate resources to maintain the infrastructure. Other users in the organization can require write access to personal or team resource, and read-only access to shared company resources.

Consider controlling changes to data. Identify data or resources that should only be modified in specific scenarios and apply tight access control. Limit users to the least of amount of access they need to get their work done. A well-planned access management strategy helps to maintain your infrastructure and prevent security issues.

Consider applying deny assignments. Determine if you need to implement the deny assignment feature. Similar to a role assignment, a deny assignment attaches a set of deny actions to a user, group, or service principal at a particular scope for the purpose of denying access. Deny assignments block users from performing specific Azure resource actions even if a role assignment grants them access.

Next unit: Create a role assignment

4- Create a role assignment

A role assignment is the process of scoping a role definition to limit permissions for a requestor, such as a user, group, service principal, or managed identity.

Things to know about role assignments
Review the following characteristics of role assignments:

The purpose of a role assignment is to control access.

The scope limits which permissions defined for a role are available for the assigned requestor.

Access is revoked by removing a role assignment.

A resource inherits role assignments from its parent resource.

The effective permissions for a requestor are a combination of the permissions for the requestor's assigned roles, and the permissions for the roles assigned to the requested resources.

Things to consider when assigning scope levels for roles
The following diagram shows an example of how scopes can be applied for a role to grant varying levels of access for different users. Think about how you can implement scopes for your roles to create meaningful assignments for your organization.

Diagram that shows how a role assignment is created for a service principal, role definition, and access scope level.

This scenario has the following access management configuration:

Three security principals are supported: user, group, service principal.

Six built-in roles are implemented, and two custom roles are defined: Reader Support Tickets and Virtual Machine Operator.

The built-in Contributor role has two sets of permissions: Actions and NotActions.

The Contributor role is assigned at different scopes to the Marketing and Pharma-sales resource groups:

Marketing users are granted access to create or manage any Azure resource in the Pharma-sales resource group.

Marketing users aren't granted access to resources outside the Pharma-sales resource group.

Next unit: Compare Azure roles to Microsoft Entra roles

5- Compare Azure roles to Microsoft Entra roles

Three types of roles are available for access management in Azure:

Classic subscription administrator roles

Azure role-based access control (RBAC) roles

Microsoft Entra administrator roles

To better understand how these different roles are defined and implemented in Azure, it helps to know some of the history.

When Azure was initially released, access to resources was managed with just three administrator roles: Account Administrator, Service Administrator, and Co-Administrator. Access was controlled by assigning admin roles to subscriptions.

Later, role-based access control (RBAC) for Azure resources was added. Azure RBAC is a newer authorization system that provides fine-grained access management to Azure resources. RBAC includes many built-in roles that can be assigned at different scopes. The Azure RBAC model also lets you create your own custom roles.

In addition to Azure RBAC roles, Microsoft Entra ID provides built-in administrator roles to manage Microsoft Entra resources like users, groups, and domains.

Azure RBAC roles	Microsoft Entra ID admin roles
Access management	Manages access to Azure resources	Manages access to Microsoft Entra resources
Scope assignment	Scope can be specified at multiple levels, including management groups, subscriptions, resource groups, and resources	Scope is specified at the tenant level
Role definitions	Roles can be defined via the Azure portal, the Azure CLI, Azure PowerShell, Azure Resource Manager templates, and the REST API	Roles can be defined via the Azure admin portal, Microsoft 365 admin portal, and Microsoft Graph PowerShell

Next unit: Apply role-based access control

6- Apply role-based access control

Built-in role definitions are defined for several categories of services, tasks, and users. You can assign built-in roles at different scopes to support various scenarios, and build custom roles from the base definitions.

Microsoft Entra ID also provides built-in roles to manage resources in Microsoft Entra ID, including users, groups, and domains. Microsoft Entra ID offers administrator roles that you can implement for your organization, such as Global admin, Application admin, and Application developer.

The following diagram illustrates how you can apply Microsoft Entra administrator roles and Azure roles in your organization.

Diagram that shows how Microsoft Entra admin roles and Azure roles can be used together to authenticate users and control access to resources.

Microsoft Entra admin roles are used to manage resources in Microsoft Entra ID, such as users, groups, and domains. These roles are defined for the Microsoft Entra tenant at the root level of the configuration.

Azure RBAC roles provide more granular access management for Azure resources. These roles are defined for a requestor or resource and can be applied at multiple levels: the root, management groups, subscriptions, resource groups, or resources.

Next unit: Review fundamental Azure RBAC roles

7- Review fundamental Azure RBAC roles

Azure provides over 100 pre-defined role definitions. Roles can grant access to data within an object. If a user has read data access to a storage account, then they can read the blobs or messages in the storage account.

The following table describes four built-in role definitions that are considered fundamental.

Fundamental role	Description
Owner	The Owner role has full access to all resources, including the right to delegate access to others. The Service Administrator and Co-Administrators roles are assigned the Owner role at the subscription scope.
Contributor	The Contributor role can create and manage all types of Azure resources. This role can't grant access to others.
Reader	The Reader role can view existing Azure resources.
User Access Administrator	The User Access Administrator role can manage user access to Azure resources.

Next unit: Interactive lab simulation

8- Interactive lab simulation

Lab scenario
Your organization is setting up a new Help Desk. You've been tasked to configure the appropriate user account permissions. The specific requirements are:

As the organization grows, ensure it will be easy to manage multiple subscriptions.
Ensure the Help Desk members can create support requests.
Ensure the Help Desk members can view resource groups, but not the resources in the resource groups.
Architecture diagram
Architecture diagram as explained in the text.

Objectives
Task 1:Implement management groups. This will make it easy to manage multiple subscriptions.
Ensure you have the necessary permissions to access the root management group.
Create a management group and add your subscription.
Task 2: Create a custom RBAC role for the Help Desk users.
Create a JSON file that defines the custom Support Request Contributor role permissions.
Use PowerShell to upload the new custom role.
Task 3: Assign RBAC roles.
Create a new user, az104-02-aaduser1, and assign them the Support Request Contributor (Custom) role.
Test the user permissions. The user should be able to view resource groups and create support requests.
 Note

Click on the thumbnail image to start the lab simulation. When you're done, be sure to return to this page so you can continue learning.

Screenshot of the simulation page.

Next unit: Knowledge check

Your company has decided to implement Azure role-based access control (RBAC) to secure their resources and manage user access. You're reviewing the scenarios to support, and have a list of issues to address:

Not all users have access to the same resources. A new employee should have only limited resource access.

Most administrators require full access to all corporate resources. A few admins need limited access to specific resources so they can read the settings, but not make changes.

How are scopes and permissions applied for Azure resources, including the custom role definition?

Your manager has asked if there are differences between Azure roles and Microsoft Entra roles.

Answer the following questions
Choose the best response for each of the questions below. Then select Check your answers.


1. You have three virtual machines (VM1, VM2, VM3) in a resource group. A new admin is hired, and they need to be able to modify settings on VM3. They shouldn't be able to make changes to VM1 or VM2. How can you implement RBAC to minimize administrative overhead? 

Assign the admin to the Contributor role on the resource group.

Assign the admin to the Contributor role on VM3.

Move VM3 to a new resource group, and then assign the admin to the Owner role on VM3.
2. What is the purpose of the 'AssignableScopes' permissions in a role definition? 

Specifies the actions that aren't allowed

Specifies the scopes where a role definition can be assigned

Specifies the actions that are allowed
3. Explain the main differences between Azure roles and Microsoft Entra roles. 

Azure roles apply to Azure resources. Microsoft Entra roles apply to Microsoft Entra resources such as users, groups, and domains.

Azure roles can be assigned at the root level.

Microsoft Entra roles are used to manage access to Azure resources.

Summary and resources

Azure role-based access control (RBAC) is a system that enables granular access management of Azure resources. Azure Administrators use Azure RBAC to segregate duties within a team, and grant users the specific access they need to perform their jobs.

In this module, you identified the features and use cases for RBAC. You discovered how to create role definitions and role assignments, and find and use built-in Azure RBAC roles. You explored how to use RBAC to manage access to subscriptions with RBAC. You reviewed the differences between Azure RBAC and Microsoft Entra roles.

The main takeaways from this module are:

Azure RBAC is a system that enables granular access management of Azure resources. It allows you to segregate duties within a team and grant users specific access based on their job requirements.
Role definitions in Azure RBAC define sets of permissions that list the allowed operations. You can use built-in role definitions or create custom role definitions to meet the specific requirements of your organization.
Role assignments attach role definitions to security principals at a particular scope. This assignment determines the level of access granted to the requestor. Access can be revoked by removing a role assignment.
Azure RBAC roles can be assigned at different scopes, including management groups, subscriptions, resource groups, and resources. The scope limits the permissions available to the assigned requestor.
Azure RBAC roles and Entra ID administrator roles can be used together to manage access to both Azure resources and Entra ID resources.
Learn more with Azure documentation
Understand Microsoft Entra role-based access control. This article describes how to understand Microsoft Entra role-based access control.

Understand roles in Microsoft Entra ID. This article explains what Microsoft Entra roles are and how they can be used.

Azure built-in roles. This article lists the Azure built-in roles. If you are looking for administrator roles visit Microsoft Entra built-in roles.

Understand Azure role assignments. This article describes the details of role assignments.

Learn more with optional hands-on exercises
Secure your Azure resources with RBAC (subscription required).

Create custom roles for Azure resources with RBAC.





Point 6: Create Azure users and groups in Microsoft Entra ID

Create users in Microsoft Entra ID. Understand different types of groups. Create a group and add members. Manage business-to-business guest accounts.

Learning objectives
In this module, you'll learn to:

Add users to Microsoft Entra ID.
Manage app and resource access by using Microsoft Entra groups.
Give guest users access in Microsoft Entra business to business (B2B).

1- Introduction

You're a global administrator in Microsoft Entra ID for a marketing organization. Your organization recently added a small developer team to build a new website hosted on Azure. You're also partnering with an external organization to design the website. You've been asked to add the new developer team to the organization's Microsoft Entra ID. To make it easier for the teams to collaborate on the website, you'll create guest accounts in Microsoft Entra ID for the external design organization.

Learning objectives
By the end of this module, you'll be able to:

Add users to Microsoft Entra ID.
Manage app and resource access by using Microsoft Entra groups.
Give guest users access in Microsoft Entra business to business (B2B).
Prerequisites
A paid Azure account with an active subscription

Next unit: What are user accounts in Microsoft Entra ID?

2- What are user accounts in Microsoft Entra ID?

In Microsoft Entra ID, all user accounts are granted a set of default permissions. A user's account access consists of the user type, their role assignments, and their ownership of individual objects.

There are different types of user accounts in Microsoft Entra ID. Each type has a level of access specific to the scope of work expected to be done under each type of user account. Administrators have the highest level of access, followed by the member user accounts in the Microsoft Entra organization. Guest users have the most restricted level of access.

Permissions and roles
Microsoft Entra ID uses permissions to help you control the access rights granted to a user or group. This is done through roles. Microsoft Entra ID has many roles with different permissions attached to them. When a user is assigned a specific role, they inherit permissions from that role. For example, a user assigned to the User Administrator role can create and delete user accounts.

Understanding when to assign the correct type of role to the right user is a fundamental and crucial step in maintaining privacy and security compliance. If the wrong role is assigned to the wrong user, the permissions that come with that role can allow the user to cause serious damage to an organization.

Administrator roles
Administrator roles in Microsoft Entra ID allow users elevated access to control who's allowed to do what. You assign these roles to a limited group of users to manage identity tasks in a Microsoft Entra organization. You can assign administrator roles that allow a user to create or edit users, assign administrative roles to others, reset user passwords, manage user licenses, and more.

If your user account has the User Administrator or Global Administrator role, you can create a new user in Microsoft Entra ID by using the Azure portal, the Azure CLI, or PowerShell. In PowerShell, run the cmdlet New-MgUser. In the Azure CLI, use az ad user create.

Member users
A member user account is a native member of the Microsoft Entra organization that has a set of default permissions, like being able to manage their profile information. When someone new joins your organization, they typically have this type of account created for them.

Anyone who isn't a guest user or isn't assigned an administrator role falls into this type. A member user role is meant for users who are considered internal to an organization and are members of the Microsoft Entra organization. However, these users shouldn't be able to manage other users by (for example) creating and deleting users. Member users don't have the same restrictions that are typically placed on guest users.

Guest users
Guest users have restricted Microsoft Entra organization permissions. When you invite someone to collaborate with your organization, you add them to your Microsoft Entra organization as a guest user. Then, you can either send an invitation email that contains a redemption link or send a direct link to an app you want to share. Guest users sign in with their own work, school, or social identities. By default, Microsoft Entra member users can invite guest users. Someone with the User Administrator role can disable this default.

Your organization might need to work with external partners. To collaborate with your organization, these partners often need to have a certain level of access to specific resources. For this sort of situation, it's a good idea to use guest user accounts. You'll then make sure partners have the right level of access to do their work, without having a higher level of access than they need.

Add user accounts
You can add individual user accounts through the Azure portal, Azure PowerShell, or the Azure CLI.

If you want to use the Azure CLI, run the following cmdlet:

Azure CLI

Copy
# create a new user
az ad user create
This command creates a new user by using the Azure CLI.

For Azure PowerShell, run the following cmdlet:

PowerShell

Copy
# create a new user
New-MgUser
You can bulk create member users and guests accounts. The following example shows how to bulk invite guest users:

PowerShell

Copy
$invitations = import-csv c:\bulkinvite\invitations.csv

$messageInfo = [Microsoft.Graph.PowerShell.Models.MicrosoftGraphInvitation]@{ `
   CustomizedMessageBody = "Hello. You are invited to the Contoso organization." }

foreach ($email in $invitations)
   {New-MgInvitation `
      -InviteRedirectUrl https://myapps.microsoft.com ` 
      -InvitedUserDisplayName $email.Name `
      -InvitedUserEmailAddress $email.InvitedUserEmailAddress `
      -InvitedUserMessageInfo $messageInfo `
      -SendInvitationMessage 
   }
You create the comma-separated values (CSV) file with the list of all the users you want to add. An invitation is sent to each user in that CSV file.

Delete user accounts
You can also delete user accounts through the Azure portal, Azure PowerShell, or the Azure CLI. In PowerShell, run the cmdlet Remove-MgUser. In the Azure CLI, run the cmdlet az ad user delete.

When you delete a user, the account remains in a suspended state for 30 days. During that 30-day window, you can restore the user account.

Check your knowledge

1. If you delete a user account by mistake, can it be restored? 

When a user account is deleted, it's gone forever and can't be restored.

The user account can be restored, but only if it was created within the last 30 days.

The user account can be restored, but only if it was deleted within the last 30 days.

2. What kind of account would you create to allow an external organization easy access? 

A guest user account for each member of the external team.

An external account for each member of the external team.

An administrator account for each member of the external team.


3- Exercise - Add and delete users in Microsoft Entra ID

You need to add member user accounts for the new developer team in your organization.

In this exercise, you'll create a new Microsoft Entra organization to hold all of your user accounts. You'll also create a user account, delete a user account, and learn how to recover a deleted user account.


Create a Microsoft Entra organization
To hold all of the users you create in this exercise, create a new organization.

 Note

You need to have a paid Azure subscription to create a Microsoft Entra organization. You can't create a Microsoft Entra organization with a free subscription, but you can review the steps below to see how to create one.

Sign in to the Azure portal.

On the Azure portal home page, under Azure services, select Create a resource.

In Create a resource, in the left menu under Categories, select Identity. Under Popular Azure services, select Create under Microsoft Entra ID.

Screenshot that shows the create link for Microsoft Entra ID under Azure services.

In Create a tenant, on the Basics tab, enter the following value for the setting.

Setting	Value
Tenant type	
Select a tenant type	Microsoft Entra ID
Select Next : Configuration, and enter the following values for each setting.

Setting	Value
Directory details	
Organization name	Enter Contoso Marketing Company
Initial domain name	Enter contosomarketingXXXX where you replace XXXX with numbers or letters to make your domain name unique
Location	Select your location from the drop-down
Select Next : Review + create.

After validation passes, select Create. The Help us prove you're not a robot pane appears.

Enter the appropriate match to the request and select Submit. Wait for your tenant creation to complete.

On the Help us prove you're not a robot pane, select the Click here to navigate to your new tenant: Contoso Marketing Company link.

Screenshot that shows the link to manage your new organization.

The Overview pane for Contoso Marketing Company appears.


Get a free trial for Microsoft Entra ID P1 or P2
To complete all the exercises in this module, you'll need to activate a free trial for Microsoft Entra ID P1 or P2.

In the left menu pane, under Manage, select Licenses. The Overview pane for Licenses appears.

On the right side of the pane, under Quick tasks, select Get a free trial.

Screenshot that shows the link to create free trial.

The Activate pane appears.

Under Microsoft Entra ID P2, expand Free trial, then select Activate. If you don't have that option, that's okay. You can complete most of the exercises without it. After the premium license activates, the Overview pane for Licenses reappears.

Return to the Overview pane for the Contoso Marketing Company Microsoft Entra ID. On the Overview tab, under the Basic information section, refresh the browser until you see Microsoft Entra ID P2 appear next to License. It might take a couple of minutes.

Screenshot that shows Microsoft Entra ID P2 on the Overview page under Tenant information.

Under My feed, you should also see your role listed as the Global administrator.

Under Basic information, copy the Primary domain name to use in the next section.

Add a new user
Now, let's create a user account.

In the Microsoft Entra organization you created, in the left menu pane, under Manage, select Users. The All users pane appears.

In the top menu bar, select New user, then select Create new user in the drop-down. The New user pane appears for Contoso Marketing Company.

Enter the following values for each setting.

Identity

User principal name: chris@contosomarketingXXXXXX.onmicrosoft.com. The domain name should match the primary domain you copied in the previous section.
Display name: Chris Green
Select Review + Create, then select Create. The All users pane reappears for Contoso Marketing Company - Microsoft Entra ID. The user is now created and registered to your organization.

Delete a user
You can delete users after they're created.

In your All users pane for Microsoft Entra organization, check the box for Chris Green in the list.

In the top menu bar, select Delete. If you don't see that option, select More.

Select OK to confirm deletion.

Recover a deleted user
You can restore deleted users. View the list of the deleted users, and then restore one.

In your All users pane for Microsoft Entra organization, in the left menu pane, select Deleted users under Manage. You now see all of the users that were deleted within the last 30 days.

Check the box next to Chris Green and select Restore users in the top menu bar.

To confirm, select OK. The All users pane reappears.

Verify that Chris Green's account is recovered by selecting All users in the left menu pane. You should see Chris Green restored as a user.

Next unit: Manage app and resource access by using Microsoft Entra groups

4- Manage app and resource access by using Microsoft Entra groups

You want to give all the developers within your organization the same access. You also want to manage who's part of the Developers group and who isn't.

Microsoft Entra ID helps you to manage your cloud-based apps, on-premises apps, and resources by using your organization's groups. Your resources can be part of the Microsoft Entra organization, such as permissions to manage objects through roles, or your resources can be external to the organization, like software as a service (SaaS) apps, Azure services, SharePoint sites, and on-premises resources.


Access management in Microsoft Entra ID
Microsoft Entra roles: Use Microsoft Entra roles to manage Microsoft Entra ID-related resources like users, groups, billing, licensing, application registration, and more.
Role-based access control (RBAC) for Azure resources: Use RBAC roles to manage access to Azure resources like virtual machines, SQL databases, or storage. For example, you could assign an RBAC role to a user to manage and delete SQL databases in a specific resource group or subscription.
Access rights through single user or group assignment
Microsoft Entra ID helps you provide access rights to a single user or to a group of users. You can assign a set of access permissions to all the members of the group. Access permissions range from full access to the ability to create or remove resources.

There are different ways you can assign access rights:

Direct assignment: Assign a user the required access rights by directly assigning a role that has those access rights.
Group assignment: Assign a group the required access rights, and the group members will inherit those rights.
Rule-based assignment: Use rules to determine a group membership based on user or device properties. For a user account or device's group membership to be valid, the user or device must meet the rules. If the rules aren't met, the user account or device's group membership is no longer valid. The rules can be simple. You can select prewritten rules or write your own advanced rules.
In the next exercise, we'll assign users to a Microsoft Entra group and use rule-based assignment to automatically manage their group membership.

Next unit: Exercise - Assign users to Microsoft Entra groups

5- Exercise - Assign users to Microsoft Entra groups

In this exercise, you'll create a Microsoft Entra group to manage the developer team's access. You'll also add a rule for the group to manage the membership automatically.

 Note

This exercise depends on having completed prior exercises in this module. If you haven't done so, complete the exercise in unit 3 before you begin.

Add a new group
Sign in to the Azure portal.

Go to the Microsoft Entra ID you created earlier in this module.

In the left menu pane, under Manage, select Groups. The All groups pane appears for your Microsoft Entra ID.

On the top menu bar, select New group. The New Group pane appears.

Enter the following values for each setting.

Setting	Value
Group type	Security
Group name	Developer group
Group description	Developer team
Select Create. The Groups | All groups pane appears, including the new group in the list of Groups. You might need to refresh to see your new group.

Use direct assignment to add a user to this group
You'll now assign members to the Developer group.

Select Developer group. The Developer group pane appears for your group.

In the left menu pane, under Manage, select Members. The Members pane appears for your developer group.

On the top menu bar, select Add members.

Screenshot that shows Add member button.

The Add members pane appears.

Search for Chris Green and select the check box next to the user.

Select the Select button. You'll see this user in the Direct members list for the Developers group in the Members pane. You might need to refresh to see the users.

Modify the group to use dynamic assignment
You can change the group to use dynamic assignment. Membership then depends on whether a user meets the rules you set for the group.

If you didn't activate the free trial for Microsoft Entra ID P1 or P2, you won't be able to complete this section. That's okay; you can still learn how to change the group to use dynamic assignment.

In the left menu pane, under Manage, select Properties. The Properties pane appears for your developer group.

Change the Membership type to Dynamic User.

Under Dynamic user members, select the Add dynamic query link.

Screenshot that shows the Add dynamic query link.

The Dynamic membership rules pane appears.

On the Configure Rules tab, select the following values for the rule:

Setting	Value
Property	country
Operator	Equals
Value	United States
Screenshot that shows how to assign a dynamic membership rule.

The membership of this group now depends on whether the user is in the United States.

Select another field to enable Save.

On the top menu bar, select Save. The Properties pane reappears for your developer group.

Change group back to assigned
You'll need to assign a guest user to the Developer group in the next exercise, so let's change the membership type back to Assigned.

Change the Membership type to Assigned.

On the top menu bar, select Save.

Next unit: Collaborate by using guest accounts and Microsoft Entra B2B

6- Collaborate by using guest accounts and Microsoft Entra B2B

You want the external team to collaborate with the internal developer team in a process that's easy and secure. With Microsoft Entra business to business (B2B), you can add people from other companies to your Microsoft Entra tenant as guest users.

If your organization has multiple Microsoft Entra tenants, you might also want to use Microsoft Entra B2B to give a user in tenant A access to resources in tenant B. Each Microsoft Entra tenant is distinct and separate from other Microsoft Entra tenants and has its own representation of identities and app registrations.


Guest user access in Microsoft Entra B2B
In any scenario where external users need temporary or restricted access to your organization's resources, give them guest user access. You can grant guest user access with the appropriate restrictions in place, then remove access when the work is done.

You can use the Azure portal to invite B2B collaboration users. Invite guest users to the Microsoft Entra organization, group, or application. After you invite a user, their account is added to Microsoft Entra ID as a guest account.

The guest can get the invitation through email, or you can share the invitation to an application by using a direct link. The guest then redeems their invitation to access the resources.

By default, users and administrators in Microsoft Entra ID can invite guest users, but the Global Administrator can limit or disable this ability.

Collaborate with any partner by using their identities
If your organization has to manage the identities of each external guest user who belongs to a given partner organization, it faces increased responsibilities because it has to secure those identities. There's an increased workload to manage and administer those identities. You also have to sync accounts, manage each account's lifecycle, and track each individual external account to meet your obligations. Your organization has to follow this procedure for every partner organization with which it wants to collaborate. Also, if something happens to those accounts, your organization is liable.

With Microsoft Entra B2B, you don't have to manage your external users' identities. The partner is responsible for managing its own identities. External users continue to use their current identities to collaborate with your organization.

For example, say you work with the external partner Giovanna Carvalho at Proseware. Her organization manages her identity as gcarvalho@proseware.com. You use that identity for the guest account in your organization's Microsoft Entra ID. After Giovanna redeems the guest account invitation, she uses the same identity (name and password) for the guest account as she does for her organization.


Why use Microsoft Entra B2B instead of federation?
With Microsoft Entra B2B, you don't take on the responsibility of managing and authenticating partners' credentials and identities. Your partners can collaborate with you even if they don't have an IT department. For example, you can collaborate with a contractor who only has a personal or business email address and no identity management solution managed by an IT department.

Giving access to external users is much easier than in a federation. You don't need an administrator to create and manage external user accounts. Any authorized user can invite other users. A line manager could, for example, invite external users to collaborate with their team. When collaboration is no longer needed, you can easily remove these external users.

A federation is more complex. A federation is where you have a trust established with another organization, or a collection of domains, for shared access to a set of resources. You might be using an on-premises identity provider and authorization service like Active Directory Federation Services (AD FS) that has an established trust with Microsoft Entra ID. To get access to resources, all users have to provide their credentials and successfully authenticate against the AD FS server. If you have someone trying to authenticate outside the internal network, you need to set up a web application proxy. The architecture might look something like the following diagram:

Diagram that shows a federation example between an on-premises Active Directory and Microsoft Entra ID.

An on-premises federation with Microsoft Entra ID might be good if your organization wants all authentication to Azure resources to happen in the local environment. Administrators can implement more rigorous levels of access control, but this means that if your local environment is down, users can't access the Azure resources and services they need.

With a B2B collaboration, external teams get the required access to Azure resources and services with the appropriate permissions. There's no need for a federation and trust to be established, and authentication doesn't depend on an on-premises server. Authentication is done directly through Azure. Collaboration becomes simplified, and you don't have to worry about situations where users can't sign in because an on-premises directory isn't available.

Next unit: Exercise - Give guest users access in Microsoft Entra B2B

7- Exercise - Give guest users access in Microsoft Entra B2B

The external and internal developer teams want to work together, so you decide to create guest-user access for the external developer team.

Use the Azure portal to invite business-to-business (B2B) collaboration users. You can invite guest users to a Microsoft Entra organization, group, or application. After you invite a user, their account is added to Microsoft Entra ID with a guest user type.

After you add a guest user to the organization, send them a direct link to a shared app. Have the guest user open the redemption URL in the invitation email.

Add guest users to the organization
Sign in to the Azure portal, and under Azure services, select Microsoft Entra ID. The Overview pane for your Microsoft Entra ID appears.

In the left menu pane, under Manage, select Users. The All users pane appears.

On the top menu bar, select New user, then select Invite external user.

Screenshot that shows the New guest user button.

The New user pane opens.

Enter a display name and an email address to which you have access.

Select Review + invite, then select Invite. An invitation is sent to the email address you provided for the guest user. The All users pane appears. Notice that the user now appears in the list of users and has Guest as User type. You might need to refresh to see the new user.

Add guest users to a group
In your Microsoft Entra organization overview page, in the left menu pane, under Manage, select Groups. The All groups pane appears.

Search for and select Developer group in the list of groups. The Developer group pane appears.

In the left menu pane, under Manage, select Members. The Members pane appears for your developer group.

On the top menu bar, select Add members. The Add members pane appears.

Search for the guest account you added to the organization.

Select the box next to the account and select the Select button. The Members pane for your developer group appears.

You now see the user in the list of members for this group. You might need to refresh to see the new user.

Add guest users to an application
Go to your Microsoft Entra organization, and in the left menu pane, under Manage, select Enterprise applications. The Enterprise applications | All applications pane appears.

On the top menu bar, select New application.

Screenshot that shows the New Application button.

The Browse Microsoft Entra Gallery pane appears.

Search for and select DocuSign. Once the app is added, the Docusign pane appears.

Select Create. The Docusign | Overview pane appears.

In the left menu pane, under Manage, select Users and groups. The Users and groups pane appears for Docusign.

On the top menu bar, select Add user/group.

Screenshot that shows the Docusign application user and groups page.

The Add Assignment pane appears.

Under Users and groups, select the None Selected link. The Users and groups pane appears.

Select the check box for the guest user you added in the previous exercise, then select the Select button. The Add Assignment pane reappears.

Select Assign. The Users and groups pane for Docusign appears. The user is now in the list for this application.

To check that the correct access level is set, select check box for the user in the list.

Screenshot that shows the user selected on the users and groups page.

On the top menu bar, select Edit assignment. The Edit Assignment pane appears.

Under Select a role, select the None Selected link. The Select a role pane appears.

Select DocuSign Sender, and then select the Select button to make sure they have the correct access.

Screenshot that shows role selected for user.

The Edit Assignment pane reappears.

Select Assign. The Users and groups pane appears with the proper Role assigned as DocuSign Sender for the user you selected.

When the invitation arrives, the user accepts it, and can then access the application.

Screenshot that shows the DocuSign app in the browser for the guest user after they've accepted the invitation.

You've now added a guest user to an application.

Resend invitations to guest users
If the guest user didn't receive the first email invitation, you can resend an invitation email.

In your Microsoft Entra organization, in the left menu pane, under Manage, select Users. The All users pane appears.

Select the user. The user's Profile pane appears.

Scroll down to the B2B invitation box and select the Resend invitation link.

Select Resend.

Next unit: Summary

Summary

In this module, you added a user and a group to a Microsoft Entra organization that you created. You invited a guest user from an external development team so that your development team can collaborate with them. You added an enterprise application to your Microsoft Entra organization and allowed a guest user to access that application.

Remember to clean up your resources after you've finished. Delete all of the users and applications you created during this module. You can't delete the tenant you created. If you don't use the tenant going forward, it'll automatically be deleted after a few months.

If you normally use a different Azure tenant, switch back to that tenant. In the upper-right corner of the Azure portal, select your profile. Sign in with a different account or, if your tenant is under the same account, select Switch directory. Under All directories, select the tenant in which you normally work.







Point 7: Secure your Azure resources with Azure role-based access control (Azure RBAC)

Learn how to use Azure RBAC to manage access to resources in Azure.

Learning objectives
In this module, you will:

Verify access to resources for yourself and others.
Grant access to resources.
View activity logs of Azure RBAC changes.

1- Introduction

Securing your Azure resources—such as virtual machines, websites, networks, and storage—is a critical function for any organization using the cloud. You want to ensure that your data and assets are protected, but still grant your employees and partners the access they need to perform their jobs. Azure role-based access control (Azure RBAC) is an authorization system in Azure that helps you manage who has access to Azure resources, what they can do with those resources, and where they have access.

As an example, suppose you work for First Up Consultants, which is an engineering firm that specializes in circuit and electrical design. They've moved their workloads and assets to Azure to make collaboration easier across several offices and other companies. You work in the IT department at First Up Consultants, where you're responsible for keeping the company's assets secure, but still allowing users to access the resources they need. You've heard that Azure RBAC can help you manage resources in Azure.

In this module, you'll learn how to use Azure role-based access control (Azure RBAC) to manage access to resources in Azure.

Learning objectives
In this module, you'll:

Verify access to resources for yourself and others.
Grant access to resources.
View activity logs of Azure RBAC changes.

Next unit: What is Azure RBAC?

2- What is Azure RBAC?

When it comes to identity and access, most organizations that are considering using the public cloud are concerned about two things:

Ensuring that when people leave the organization, they lose access to resources in the cloud.
Striking the right balance between autonomy and central governance; for example, giving project teams the ability to create and manage virtual machines in the cloud while centrally controlling the networks those VMs use to communicate with other resources.
Microsoft Entra ID and Azure role-based access control (Azure RBAC) work together to make it simple to carry out these goals.

Azure subscriptions
First, remember that each Azure subscription is associated with a single Microsoft Entra directory. Users, groups, and applications in that directory can manage resources in the Azure subscription. The subscriptions use Microsoft Entra ID for single sign-on (SSO) and access management. You can extend your on-premises Active Directory to the cloud by using Microsoft Entra Connect. This feature allows your employees to manage their Azure subscriptions by using their existing work identities. When you disable an on-premises Active Directory account, it automatically loses access to all Azure subscriptions connected with Microsoft Entra ID.

What's Azure RBAC?
Azure role-based access control (Azure RBAC) is an authorization system built on Azure Resource Manager that provides fine-grained access management for resources in Azure. With Azure RBAC, you can grant the exact access that users need to do their jobs. For example, you can use Azure RBAC to let one employee manage virtual machines in a subscription while another manages SQL databases within the same subscription.

The following video describes Azure RBAC in detail:


You can grant access by assigning the appropriate Azure role to users, groups, and applications at a certain scope. The scope of a role assignment can be a management group, subscription, a resource group, or a single resource. A role assigned at a parent scope also grants access to the child scopes contained within it. For example, a user with access to a resource group can manage all the resources it contains, like websites, virtual machines, and subnets. The Azure role that you assign dictates what resources the user, group, or application can manage within that scope.

The following diagram depicts how the classic subscription administrator roles, Azure roles, and Microsoft Entra roles are related at a high level. Roles assigned at a higher scope, like an entire subscription, are inherited by child scopes, like service instances.

Diagram that depicts how the classic subscription administrator roles, Azure roles, and Microsoft Entra roles are related at a high level.

In the preceding diagram, a subscription is associated with only one Microsoft Entra tenant. Also note that a resource group can have multiple resources, but it's associated with only one subscription. Although it's not obvious from the diagram, a resource can be bound to only one resource group.

What can I do with Azure RBAC?
Azure RBAC allows you to grant access to Azure resources that you control. Suppose you need to manage access to resources in Azure for the development, engineering, and marketing teams. You’ve started to receive access requests, and you need to quickly learn how access management works for Azure resources.

Here are some scenarios you can implement with Azure RBAC:

Allow one user to manage virtual machines in a subscription and another user to manage virtual networks
Allow a database administrator group to manage SQL databases in a subscription
Allow a user to manage all resources in a resource group, such as virtual machines, websites, and subnets
Allow an application to access all resources in a resource group
Azure RBAC in the Azure portal
In several areas in the Azure portal, you'll see a pane named Access control (IAM), also known as identity and access management. On this pane, you can see who has access to that area and their role. Using this same pane, you can grant or remove access.

The following shows an example of the Access control (IAM) pane for a resource group. In this example, Alain has been assigned the Backup Operator role for this resource group.

Screenshot of the Azure portal showing the Access control Role assignment pane with the Backup operator section highlighted.

How does Azure RBAC work?
You can control access to resources using Azure RBAC by creating role assignments, which control how permissions are enforced. To create a role assignment, you need three elements: a security principal, a role definition, and a scope. You can think of these elements as "who," "what," and "where."

1. Security principal (who)
A security principal is just a fancy name for a user, group, or application to which you want to grant access.

An illustration showing security principal including user, group, and service principal.

2. Role definition (what you can do)
A role definition is a collection of permissions. It's sometimes just called a role. A role definition lists the permissions the role can perform, such as read, write, and delete. Roles can be high-level, like Owner, or specific, like Virtual Machine Contributor.

An illustration listing different built-in and custom roles with zoom-in on the definition for the contributor role.

Azure includes several built-in roles that you can use. The following lists four fundamental built-in roles:

Owner: Has full access to all resources, including the right to delegate access to others
Contributor: Can create and manage all types of Azure resources, but can’t grant access to others
Reader: Can view existing Azure resources
User Access Administrator: Lets you manage user access to Azure resources
If the built-in roles don't meet the specific needs of your organization, you can create your own custom roles.

3. Scope (where)
Scope is the level where the access applies. This is helpful if you want to make someone a Website Contributor, but only for one resource group.

In Azure, you can specify a scope at multiple levels: management group, subscription, resource group, or resource. Scopes are structured in a parent-child relationship. When you grant access at a parent scope, those permissions are inherited by the child scopes. For example, if you assign the Contributor role to a group at the subscription scope, that role is inherited by all resource groups and resources in the subscription.

An illustration showing a hierarchical representation of different Azure levels to apply scope. The hierarchy, starting with the highest level, is in this order: Management group, subscription, resource group, and resource.

Role assignment
Once you have determined the who, what, and where, you can combine those elements to grant access. A role assignment is the process of binding a role to a security principal at a particular scope for the purpose of granting access. To grant access, you'll create a role assignment. To revoke access, you'll remove a role assignment.

The following example shows how the Marketing group has been assigned the Contributor role at the sales resource group scope.

An illustration showing a sample role assignment process for Marketing group, which is a combination of security principal, role definition, and scope. The Marketing group falls under the Group security principal and has a Contributor role assigned for the Resource group scope.

Azure RBAC is an allow model
Azure RBAC is an allow model. This means that when you're assigned a role, Azure RBAC allows you to perform certain actions, such as read, write, or delete. So, if one role assignment grants you read permissions to a resource group and a different role assignment grants you write permissions to the same resource group, you'll have read and write permissions on that resource group.

Azure RBAC has something called NotActions permissions. You can use NotActions to create a set of not allowed permissions. The access a role grants—the effective permissions—is computed by subtracting the NotActions operations from the Actions operations. For example, the Contributor role has both Actions and NotActions. The wildcard (*) in Actions indicates that it can perform all operations on the control plane. You'd then subtract the following operations in NotActions to compute the effective permissions:

Delete roles and role assignments
Create roles and role assignments
Grant the caller User Access Administrator access at the tenant scope
Create or update any blueprint artifacts
Delete any blueprint artifacts

Next unit: Knowledge check - What is Azure RBAC?

3- Knowledge check - What is Azure RBAC?

Check your knowledge

1. What is a role definition in Azure? 

A collection of permissions with a name that is assignable to a user, group, or application

The collection of users, groups, or applications that have permissions to a role

The binding of a role to a security principal at a specific scope, to grant access

2. Suppose an administrator wants to assign a role to allow a user to create and manage Azure resources but not be able to grant access to others. Which of the following built-in roles would support this? 

Owner

Contributor

Reader

User Access Administrator

3. What is the inheritance order for scope in Azure? 

Management group, Resource group, Subscription, Resource

Management group, Subscription, Resource group, Resource

Subscription, Management group, Resource group, Resource

Subscription, Resource group, Management group, Resource


4- Exercise - List access using Azure RBAC and the Azure portal

At First Up Consultants, you've been granted access to a resource group for the marketing team. You want to familiarize yourself with the Azure portal and see what roles are currently assigned.

You need an Azure subscription to complete the exercises. If you don't have an Azure subscription, create a free account and add a subscription before you begin. If you're a student, you can take advantage of the Azure for students offer.

List role assignments for yourself
Follow these steps to see what roles are currently assigned to you.

Sign in to the Azure portal.

On the Profile menu, select the ellipsis (...) to see more links.

Screenshot of user menu with My permissions highlighted.

Select My permissions to open the My permissions pane.

Screenshot of the My permissions pane.

You'll find the roles that you've been assigned and the scope. Your list will look different.

List role assignments for a resource group
Follow these steps to see what roles are assigned at the resource group scope.

In the Search box at the top, search for and select Resource groups.

Screenshot of the Azure portal that shows how to search for resource groups.

In the list of resource groups, select a resource group.

These steps use a resource group named example-group, but your resource group's name will be different.

On the left menu pane, select Access control (IAM).

Screenshot showing Access control (IAM) option on the resource group pane.

Select the Role assignments tab.

This tab shows who has access to the resource group. Notice that some roles are scoped to This resource, while others are (Inherited) from a parent scope.

Screenshot showing Role assignments tab for the selected resource group.

List roles
As you learned in the previous unit, a role is a collection of permissions. Azure has more than 70 built-in roles that you can use in your role assignments. To list the roles:

In the menu bar at the top of the pane, select the Roles tab to list of all the built-in and custom roles.

Select a role's View link in the Details column, then select the Assignments tab to display the number of users and groups assigned to that role.

Screenshot showing a list of Roles and users and groups assigned to each role.

In this unit, you learned how to list the role assignments for yourself in the Azure portal. You also learned how to list the role assignments for a resource group.


Next unit: Exercise - Grant access using Azure RBAC and the Azure portal

5- Exercise - Grant access using Azure RBAC and the Azure portal

A co-worker named Alain at First Up Consultants needs permission to create and manage virtual machines for a project on which he's working. Your manager has asked that you handle this request. Using the best practice to grant users the least privileges to get their work done, you decide to assign Alain the Virtual Machine Contributor role for a resource group.

Grant access
Follow this procedure to assign the Virtual Machine Contributor role to a user at the resource group scope.

Sign in to the Azure portal as an administrator that has permissions to assign roles, such as User Access Administrator or Owner.

In the Search box at the top, search for Resource groups.

Screenshot of the Azure portal that shows how to search for resource groups.

In the list of resource groups, select a resource group.

These steps use a resource group named example-group, but your resource group's name will be different.

On the left menu pane, select Access control (IAM).

Select the Role assignments tab to display the current list of role assignments at this scope.

Screenshot showing Role assignments tab for the selected resource group.

Select Add > Add role assignment.

If you don't have permissions to assign roles, the Add role assignment option will be disabled.

Screenshot that shows Add role assignment menu.

The Add role assignment page opens.

On the Role tab, search for and select Virtual Machine Contributor.

Screenshot that shows Add role assignment and list of roles.

Select Next.

On the Members tab, select Select members.

Search for and select a user.

Screenshot of the add role assignment page that shows the select members option.

Select Select to add the user to the Members list.

Select Next.

On the Review + assign tab, review the role assignment settings.

Select Review + assign to assign the role.

After a few moments, the user is assigned the Virtual Machine Contributor role at the resource group scope. The user can now create and manage virtual machines just within this resource group.

Screenshot that shows the Virtual Machine Contributor role assigned to a user.

Remove access
In Azure RBAC, you can remove a role assignment to remove access.

In the list of role assignments, check the box for the user with the Virtual Machine Contributor role.

Select Remove.

Screenshot that shows the Remove role assignment message.

In the Remove role assignments message that appears, select Yes.

In this unit, you learned how to grant a user access to create and manage virtual machines in a resource group using the Azure portal.


Next unit: Exercise - View activity logs for Azure RBAC changes

6- Exercise - View activity logs for Azure RBAC changes

First Up Consultants reviews Azure role-based access control (Azure RBAC) changes quarterly for auditing and troubleshooting purposes. You know that changes get logged in the Azure Activity Log. Your manager has asked if you can generate a report of the role assignment and custom role changes for the last month.

View activity logs
The easiest way to get started is to view the activity logs with the Azure portal.

Select All services, then search for Activity log.

Screenshot of the Azure portal showing the location of Activity logs option.

Select Activity log to open the activity log.

Screenshot of the Azure portal showing the Activity logs.

Set the Timespan filter to Last month.

Add an Operation filter and type role to filter the list.

Select the following Azure RBAC operations:

Create role assignment (roleAssignments)
Delete role assignment (roleAssignments)
Create or update custom role definition (roleDefinitions)
Delete custom role definition (roleDefinitions)
Screenshot showing a list of Operation filter with the four filters selected.

After a moment, you'll get a list of all the role assignment and role definition operations for the last month. There's also a button at the top of the screen to download the activity log as a CSV file.

Select one of the operations to get the activity log details.

Screenshot showing the details for an activity log.

In this unit, you learned how to use Azure Activity Log to list Azure RBAC changes in the portal and generate a simple report.

Next unit: Knowledge check - Using Azure RBAC

Check your knowledge

1. Suppose a team member can't view resources in a resource group. Where would the administrator go to check the team member's access? 

Check the team member's permissions by going to their Azure profile > My permissions.

Go to the resource group and select Access control (IAM) > Check Access.

Go to one of the resources in the resource group and select Role assignments.

2. Suppose an administrator in another department needs access to a virtual machine managed by your department. What's the best way to grant them access to just that resource? 

At the resource scope, create a role for them with the appropriate access.

At the resource group scope, assign the role with the appropriate access.

At the resource scope, assign the role with the appropriate access.

3. Suppose a developer needs full access to a resource group. If you are following least-privilege best practices, what scope should you specify? 

Resource

Resource group

Subscription

4. Suppose an administrator needs to generate a report of the role assignments for the last week. Where in the Azure portal would they generate that report? 

Search for Activity log and filter on the Create role assignment (roleAssignments) operation.

At the appropriate scope, go to Access control (IAM) > Download role assignments.

At the appropriate scope, go to Access control (IAM) > Role assignments.


Summary

In this module, you learned about Azure role-based access control (Azure RBAC) and how you can use it to secure your Azure resources. To grant access, you assign users a role at a particular scope. Using Azure RBAC, you can grant only the amount of access to users that they need to perform their jobs. Azure RBAC has more than 200 built-in roles, but if your organization needs specific permissions, you can create your own custom roles. Azure keeps track of your Azure RBAC changes in case you need to see what changes were made in the past.




Point 8: Allow users to reset their password with Microsoft Entra self-service password reset

Evaluate self-service password reset to allow users in your organization to reset their passwords or unlock their accounts. Set up, configure, and test self-service password reset.

Learning objectives
In this module, you will:

Decide whether to implement self-service password reset.
Implement self-service password reset to meet your requirements.
Configure self-service password reset to customize the experience.



1- Introduction

Suppose you're an IT administrator for a large retail organization. Your organization has started using Microsoft Entra ID to allow employees to securely sign in and use software as a service (SaaS) apps and access the organization's resources in Microsoft 365. You're overwhelmed with password-reset requests, because you currently reset employees' passwords manually. To get these employees back to being productive quickly and reduce your workload, you decide to evaluate and set up self-service password reset in Microsoft Entra ID.

In this module, you'll learn how Azure supports this feature and how to set it up.

By the end of this module, you'll be able to configure self-service password reset in Microsoft Entra ID.

Learning objectives
In this module, you will:

Decide whether to implement self-service password reset.
Implement self-service password reset to meet your requirements.
Configure self-service password reset to customize the experience.
Prerequisites
Basic understanding of Microsoft Entra ID


Next unit: What is self-service password reset in Microsoft Entra ID?


2- What is self-service password reset in Microsoft Entra ID?

You've been asked to assess ways to reduce help-desk costs in your retail organization. You've noticed that the support staff spends much of their time resetting passwords for users. Users often complain about delays with this process, and these delays impact their productivity. You want to understand how you can configure Azure to allow users to manage their own passwords.

In this unit, you'll learn how self-service password reset (SSPR) works in Microsoft Entra ID.

Why use SSPR?
In Microsoft Entra ID, any user can change their password if they're already signed in. But if they're not signed in and forgot their password or it's expired, they'll need to reset their password. With SSPR, users can reset their passwords in a web browser or from a Windows sign-in screen to regain access to Azure, Microsoft 365, and any other application that uses Microsoft Entra ID for authentication.

SSPR reduces the load on administrators, because users can fix password problems themselves without having to call the help desk. Also, it minimizes the productivity impact of a forgotten or expired password. Users don't have to wait until an administrator is available to reset their password.

How SSPR works
The user initiates a password reset either by going directly to the password-reset portal or by selecting the Can't access your account link on a sign-in page. The reset portal takes these steps:

Localization: The portal checks the browser's locale setting and renders the SSPR page in the appropriate language.
Verification: The user enters their username and passes a captcha to ensure that it's a user and not a bot.
Authentication: The user enters the required data to authenticate their identity. They might, for example, enter a code or answer security questions.
Password reset: If the user passes the authentication tests, they can enter a new password and confirm it.
Notification: A message is sent to the user to confirm the reset.
There are several ways you can customize the SSPR user experience. For example, you can add your company logo to the sign-in page so users know they're in the right place to reset their password.

Authenticate a password reset
It's critical to verify a user's identity before you allow a password reset. Malicious users might exploit any weakness in the system to impersonate that user. Azure supports six different ways to authenticate reset requests.

As an administrator, you can choose the methods to use when you configure SSPR. Enable two or more of these methods so that users can choose the ones they can easily use. The methods are:

Authentication method	How to register	How to authenticate for a password reset
Mobile app notification	Install the Microsoft Authenticator app on your mobile device, then register it on the multifactor authentication setup page.	Azure sends a notification to the app, which you can either verify or deny.
Mobile app code	This method also uses the Authenticator app, and you install and register it in the same way.	Enter the code from the app.
Email	Provide an email address that's external to Azure and Microsoft 365.	Azure sends a code to the address, which you enter in the reset wizard.
Mobile phone	Provide a mobile phone number.	Azure sends a code to the phone in an SMS message, which you enter in the reset wizard. You can also choose to get an automated call.
Office phone	Provide a nonmobile phone number.	You receive an automated call to this number and press #.
Security questions	Select questions such as "In what city was your mother born?" and save their responses.	Answer the questions.
In free and trial Microsoft Entra organizations, phone call options aren't supported.

Require the minimum number of authentication methods
You can specify the minimum number of methods that the user must set up: one or two. For example, you might enable the mobile app code, email, office phone, and security questions methods and specify a minimum of two methods. Users can then choose the two methods they prefer, like mobile app code and email.

For the security-question method, you can specify a minimum number of questions the user must set up to register for this method. You also can specify a minimum number of questions they must answer correctly to reset their password.

After your users register the required information for the minimum number of methods you've specified, they're considered registered for SSPR.

Recommendations
Enable two or more of the authentication reset request methods.
Use the mobile app notification or code as the primary method, but also enable the email or office phone methods to support users without mobile devices.
The mobile phone method isn't a recommended method, because it's possible to send fraudulent SMS messages.
The security-question option is the least recommended method, because the answers to the security questions might be known to other people. Only use the security-question method in combination with at least one other method.
Accounts associated with administrator roles
A strong, two-method authentication policy is always applied to accounts with an administrator role, regardless of your configuration for other users.
The security-question method isn't available to accounts associated with an administrator role.
Configure notifications
Administrators can choose how users are notified of password changes. There are two options you can enable:

Notify users on password resets: The user who resets their own password is notified to their primary and secondary email addresses. If the reset was done by a malicious user, this notification alerts the user, who can take mitigation steps.
Notify all admins when other admins reset their password: All administrators are notified when another administrator resets their password.
License requirements
There are three editions of Microsoft Entra ID: free, Premium P1, and Premium P2. The password-reset functionality you can use depends on your edition.

Any user who is signed in can change their password, regardless of the edition of Microsoft Entra ID.

If you're not signed in and you've forgotten your password or your password has expired, you can use SSPR in Microsoft Entra ID P1 or P2. It's also available with Microsoft 365 Apps for business or Microsoft 365.

In a hybrid situation, where you have Active Directory on-premises and Microsoft Entra ID in the cloud, any password change in the cloud must be written back to the on-premises directory. This writeback support is available in Microsoft Entra ID P1 or P2. It's also available with Microsoft 365 Apps for business.

SSPR deployment options
You can deploy SSPR with password writeback by using Microsoft Entra Connect or cloud sync, depending on user needs. You can deploy each option side-by-side in different domains to target different sets of users. This helps existing users on-premises to write back password changes, while adding an option for users in disconnected domains because of a company merger or split. Users from an existing on-premises domain can use Microsoft Entra Connect, while new users from a merger can use cloud sync in another domain. Cloud sync can also provide higher availability, because it doesn't rely on a single instance of Microsoft Entra Connect. For a feature comparison between the two deployment options, see Comparison between Microsoft Entra Connect and cloud sync.

Check your knowledge

1. When is a user considered registered for SSPR? 

When they've registered at least one of the permitted authentication methods

When they've registered at least the number of methods that you've required to reset a password

When they've set up the minimum number of security questions

2. When you enable SSPR for your Microsoft Entra organization... 

Users can only change their password when they're signed in

Admins can reset their password by using one authentication method

Users can reset their passwords when they can't sign in



3- Implement Microsoft Entra self-service password reset

You've decided to implement self-service password reset (SSPR) in Microsoft Entra ID for your organization. You want to start using SSPR for a group of 20 users in the marketing department as a trial deployment. If everything works well, you'll enable SSPR for your whole organization.

In this unit, you'll learn how to enable SSPR in Microsoft Entra ID.

Prerequisites
Before you start to configure SSPR, you need these things in place:

a Microsoft Entra organization: This organization must have at least a trial license enabled.
a Microsoft Entra account with Global Administrator privileges: You'll use this account to set up SSPR.
A non-administrative user account: You'll use this account to test SSPR. It's important that this account isn't an administrator, because Microsoft Entra imposes extra requirements on administrative accounts for SSPR. This user, and all user accounts, must have a valid license to use SSPR.
A security group with which to test the configuration: The non-administrative user account must be a member of this group. You'll use this security group to limit who you roll SSPR out to.
If you don't already have a Microsoft Entra organization that you can use for this module, we'll set one up in the next unit.

Scope of SSPR rollout
There are three settings for the Self-service password reset enabled property:

Disabled: No users in the Microsoft Entra organization can use SSPR. This value is the default.
Enabled: All users in the Microsoft Entra organization can use SSPR.
Selected: Only the members of the specified security group can use SSPR. You can use this option to enable SSPR for a targeted group of users who can test it and verify that it works as expected. When you're ready to roll it out broadly, set the property to Enabled so that all users have access to SSPR.
Configure SSPR
Here are the high-level steps to configure SSPR:

Go to the Azure portal, go to Microsoft Entra ID > Password reset.

Properties:

Enable SSPR.
You can enable it for all users in the Microsoft Entra organization or for selected users.
To enable for selected users, you must specify the security group. Members of this group can use SSPR.
Screenshot of the Password Reset configuration panel. Properties option is selected allowing user to enable self service password resets.

Authentication methods:

Choose whether to require one or two authentication methods.
Choose the authentication methods that the users can use.
Screenshot of the Password Reset panel's Authentication methods option selected displaying panel with authentication options.

Registration:

Specify whether users are required to register for SSPR when they next sign in.
Specify how often users are asked to reconfirm their authentication information.
Screenshot of the Password Reset panel's Registration option selected displaying panel with registration options.

Notifications: Choose whether to notify users and administrators of password resets.

Screenshot of the Password Reset panel's Notification option selected displaying panel with notification options.

Customization: Provide an email address or web page URL where your users can get help.

Screenshot of the Password Reset panel's Customization option selected displaying panel with helpdesk options.


Next unit: Exercise - Set up self-service password reset


4- Exercise - Set up self-service password reset

In this unit, you'll configure and test self-service password reset (SSPR) by using your mobile phone. You'll need to use your mobile phone to complete the password-reset process in this exercise.


Create a Microsoft Entra organization
For this step, you'll want to create a new directory and sign up for trial Premium subscription for Microsoft Entra ID.

Sign in to the Azure portal.

Select Create a resource > Identity > Microsoft Entra ID.

Screenshot that shows Microsoft Entra ID in the Azure Marketplace.

Select Microsoft Entra ID, then select Next : Configuration.

On the Create tenant page, use these values, select Review + Create, then select Create.

Property	Value
Organization name	Choose any organization name.
Initial domain name	Choose a domain name that's unique within .onmicrosoft.com. Make a note of the domain you choose.
Country or region	United States.
Complete the captcha, then select Submit.

After you create the organization, select the F5 key to refresh the page. In the upper-right corner, select your user account, then select Switch directory.

Select the organization you just created.


Create a Microsoft Entra ID P2 trial subscription
Now activate a trial Premium subscription for the organization so that you can test SSPR.

Go to Microsoft Entra ID > Password reset.
Select Get a free Premium trial to use this feature.
Under Microsoft Entra ID P2, expand Free trial, and select Activate.
Refresh the browser to see the Password reset - Properties page. You might need to refresh a few times.
Create a group
You want to roll out SSPR to a limited set of users first to make sure your SSPR configuration works as expected. Let's begin by creating a security group for the limited rollout.

In the Microsoft Entra organization you created, under Manage, select Groups.

Select New Group.

Enter the following values:

Setting	Value
Group type	Security
Group name	SSPRTesters
Group description	Members are testing the rollout of SSPR
Membership type	Assigned
Select Create.

Screenshot that shows new group form filled out and the create button highlighted.

Create a user account
To test your configuration, create an account that's not associated with an administrator role. You'll also assign the account to the group you created.

In your Microsoft Entra organization, under Manage, select Users.

Select + New user, select Create new user in the drop-down, and use the following values:

Setting	Value
User name	balas
Name	Bala Sandhu
Password	Select the Copy icon next to the autogenerated password, then paste the password to a text editor like Notepad.
Select the Assignments tab.

Select Add group, check the box for the SSPRTesters group, then select the Select button.

Select Review + create, then select Create.

Enable SSPR
Now, you're ready to enable SSPR for the group.

In your Microsoft Entra organization, under Manage, select Password reset.

If the Password reset page still displays the message Get a free Premium trial to use this feature, wait for a few minutes and then refresh the page.

On the Properties page, select Selected. Select the No groups selected link, select the box next to the SSPRTesters group, and then select the Select button.

Select Save.

Screenshot of the Password Reset properties panel wwith SSPR enabled and selected group set to SSPRTesters.

Under Manage, select the Authentication methods, Registration, and Notifications pages to review the default values.

Select Customization.

Select Yes, and then in the Custom helpdesk email or URL text box, enter admin@organization-domain-name.onmicrosoft.com. Replace "organization-domain-name" with the domain name of the Microsoft Entra organization you created. If you've forgotten the domain name, hover over your profile in the upper-right corner of the Azure portal.

Select Save.

Register for SSPR
Now that the SSPR configuration is complete, register a mobile phone number for the user you created.

 Note

If you get a message that says "The administrator has not enabled this feature," use private/incognito mode in your web browser.

In a new browser window, go to https://aka.ms/ssprsetup.

Sign in with the user name balas@organization-domain-name.onmicrosoft.com and the password that you noted earlier. Remember to replace "organization-domain-name" with the domain name of the Microsoft Entra organization you created.

If you're asked to update your password, enter a new password of your choice. Make sure you note the new password.

Select the Security info tab on the left, then select + Add sign-in method.

In the Add a method box, select Phone.

Enter your mobile phone details.

Screenshot that shows mobile phone registration form for SSPR.

Select the Text me a code radio button, then select Next.

When you receive the code on your mobile phone, enter the code in the text box and select Next.

Select Done.

Test SSPR
Now, let's test whether the user can reset their password.

In a new browser window, go to https://aka.ms/sspr.

For User ID, type balas@organization-domain-name.onmicrosoft.com. Replace "organization-domain-name" with the domain you used for your Microsoft Entra organization.

Screenshot that shows the password reset dialog.

Complete the captcha and select Next.

Enter your mobile phone number, then select Text.

When the text arrives, in the Enter your verification code text box, enter the code you were sent. Select Next.

Enter a new password, then select Finish. Make sure you note the new password.

Close the browser window.


Next unit: Exercise - Customize directory branding


5- Exercise - Customize directory branding

Suppose you've been asked to display your retail organization's branding on the Azure sign-in page to reassure users that they're passing credentials to a legitimate system.

Here, you'll learn how to configure this custom branding.

To complete this exercise, you must have two image files:

A page background image. This must be a PNG or JPG file, 1920 x 1080 pixels, and smaller than 300 KB.
A company logo image. This must be a PNG or JPG file, 32 x 32 pixels, and smaller than 5 KB.

Customize Microsoft Entra organization branding
Let's use Microsoft Entra ID to set up the custom branding.

Sign in to the Azure portal.

Go to your Microsoft Entra organization by selecting Microsoft Entra ID. If you're not in the right Microsoft Entra organization, go to your Azure profile in the upper-right corner and select Switch directory to find your organization.

Under Manage, select Company branding > Customize.

Next to Favicon, select Browse. Select your logo image.

Next to Background image, select Browse. Select your page background image.

Select a Page background color or accept the default.

Screenshot that shows the configure company branding form.

Select Review + Create, then select Create.

Test the organization's branding
Now, let's use the account that we created in the last exercise to test the branding.

In a new browser window, go to https://login.microsoft.com.

Select the account for Bala Sandhu. Your custom branding is displayed.

Screenshot that shows the customized sign-in page.

Select Forgot my password.

Screenshot that shows organization logo on password reset page.

Next unit: Summary

Summary

In this module, you've learned how you can use self-service password reset (SSPR) in Microsoft Entra ID to allow users to reset their forgotten or expired passwords. An administrator doesn't have to do the password reset. SSPR is secured by authentication methods of your choice. These methods can include a mobile authentication app, a code sent to you by an SMS text message, or security questions.

SSPR helps reduce the amount of work required from administrators. It also minimizes the productivity impact for users when they forget their password.

Clean up
Remember to clean up after you've finished.

Delete the user you created in Microsoft Entra ID: Go to Microsoft Entra ID > Manage > Users. Check the box next to the user and select Delete. Select OK.
Delete the group you created in Microsoft Entra ID. Go to Microsoft Entra ID > Manage > Groups. Check the box next to the group and select Delete. Select OK.
Turn off self-service password reset. Go to Microsoft Entra ID > Manage > Password reset. Under Self service password reset enabled, select None. Select Save.
If you created a Premium trial Microsoft Entra tenant for this module, you can delete the tenant 30 days after the trial has expired.

Learn more
Tutorial: Enable users to unlock their account or reset passwords using Microsoft Entra self-service password reset
How it works: Microsoft Entra self-service password reset
Deploy Microsoft Entra self-service password reset


Azure Administrator Associate

Chapter 3: Configure and manage virtual networks for Azure administrators

Learn how to configure and manage Azure network capabilities like connectivity services, application protection, application delivery, and network monitoring services.

Modules in this learning path

Configure virtual networks

Learn to configure virtual networks and subnets, including IP addressing.


Configure network security groups

Learn how to implement network security groups, and ensure network security group rules are correctly applied.


Configure Azure Virtual Network peering

Learn to configure an Azure Virtual Network peering connection and address transit and connectivity concerns.


Configure network routing and endpoints

Learn how to configure network routes, including endpoints and private links.


Configure Azure Load Balancer

Learn how to configure an internal or public load balancer.


Configure Azure Application Gateway

Learn how to configure Azure Application Gateway.

Design an IP addressing schema for your Azure deployment

A good Azure IP addressing schema provides flexibility, room for growth, and integration with on-premises networks. The schema ensures that communication works for deployed resources, minimizes public exposure of systems, and gives the organization flexibility in its network. If not properly designed, systems might not be able to communicate, and additional work will be required to remediate.


Distribute your services across Azure virtual networks and integrate them by using virtual network peering

Use virtual network peering to enable communication across virtual networks in a way that's secure and minimally complex.


Host your domain on Azure DNS

Create a DNS zone for your domain name. Create DNS records to map the domain to an IP address. Test that the domain name resolves to your web server.


Manage and control traffic flow in your Azure deployment with routes

Learn how to control Azure virtual network traffic by implementing custom routes.


Improve application scalability and resiliency by using Azure Load Balancer

Discuss the different load balancers in Azure and how to choose the right Azure load balancer solution to meet your requirements.





Point 1: Configure virtual networks

Learn to configure virtual networks and subnets, including IP addressing.

Learning objectives
In this module, you learn how to:

Describe Azure virtual network features and components.
Identify features and usage cases for subnets and subnetting.
Identify usage cases for private and public IP addresses.
Create a virtual network and assign IP address.

1- Introduction

Azure virtual networks are an essential component for creating private networks in Azure. They allow different Azure resources to securely communicate with each other, the internet, and on-premises networks.

Suppose you work for a company in the healthcare industry. Your company is looking to migrate their on-premises infrastructure to Azure. They want to ensure secure communication between their resources in Azure and their on-premises network. The company is also concerned about scalability and availability. By using Azure virtual networks, they can create a private network in Azure and securely connect their resources.

The topics covered in this module include subnetting, creating virtual networks, and using private and public IP addresses. You also learn about the different scenarios where virtual networks can be used, such as creating a dedicated private cloud-only network or extending a data center securely. The module provides a detailed explanation of subnets and their benefits, and how to plan IP addressing for Azure resources.

By the end of this module, you have a clear understanding of how to create and configure virtual networks in Azure. You're able to effectively use subnets, assign IP addresses, and ensure secure communication between your Azure resources and on-premises network.

Learning objectives
In this module, you learn how to:

Describe Azure virtual network features and components.
Identify features and usage cases for subnets and subnetting.
Identify usage cases for private and public IP addresses.
Create a virtual network and assign an IP address.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Basic knowledge of virtual networking in cloud environments.
Familiarity with IP address formats and subnetting.


Next unit: Plan virtual networks

2- Plan virtual networks

A major incentive for adopting cloud solutions like Azure is to enable information technology departments to transition server resources to the cloud. Moving resources to the cloud can save money and simplify administrative operations. Relocating resources removes the need to maintain expensive datacenters with uninterruptible power supplies, generators, multiple fail-safes, or clustered database servers. For small and medium-sized companies, which might not have the expertise to maintain their own robust infrastructure, moving to the cloud is particularly appealing.

After resources are moved to Azure, they require the same networking functionality as an on-premises deployment. In specific scenarios, the resources require some level of network isolation. Azure network services offer a range of components with functionalities and capabilities, as shown in the following image:

Screenshot that shows the main components of Azure network services.

Things to know about Azure virtual networks
You can implement Azure Virtual Network to create a virtual representation of your network in the cloud. Let's examine some characteristics of virtual networks in Azure.

An Azure virtual network is a logical isolation of the Azure cloud resources.

You can use virtual networks to provision and manage virtual private networks (VPNs) in Azure.

Each virtual network has its own Classless Inter-Domain Routing (CIDR) block and can be linked to other virtual networks and on-premises networks.

You can link virtual networks with an on-premises IT infrastructure to create hybrid or cross-premises solutions, when the CIDR blocks of the connecting networks don't overlap.

You control the DNS server settings for virtual networks, and segmentation of the virtual network into subnets.

The following illustration depicts a virtual network that has a subnet containing two virtual machines. The virtual network has connections to an on-premises infrastructure and a separate virtual network.

Diagram of a virtual network with a subnet of two virtual machines. The network connects to an on-premises infrastructure and separate virtual network.

Things to consider when using virtual networks
Virtual networks can be used in many ways. As you think about the configuration plan for your virtual networks and subnets, consider the following scenarios.

Scenario	Description
Create a dedicated private cloud-only virtual network	Sometimes you don't require a cross-premises configuration for your solution. When you create a virtual network, your services and virtual machines within your virtual network can communicate directly and securely with each other in the cloud. You can still configure endpoint connections for the virtual machines and services that require internet communication, as part of your solution.
Securely extend your data center with virtual networks	You can build traditional site-to-site VPNs to securely scale your datacenter capacity. Site-to-site VPNs use IPSEC to provide a secure connection between your corporate VPN gateway and Azure.
Enable hybrid cloud scenarios	Virtual networks give you the flexibility to support a range of hybrid cloud scenarios. You can securely connect cloud-based applications to any type of on-premises system, such as mainframes and Unix systems.


Next unit: Create subnets

3- Create subnets

Subnets provide a way for you to implement logical divisions within your virtual network. Your network can be segmented into subnets to help improve security, increase performance, and make it easier to manage.

Things to know about subnets
There are certain conditions for the IP addresses in a virtual network when you apply segmentation with subnets.

Each subnet contains a range of IP addresses that fall within the virtual network address space.

The address range for a subnet must be unique within the address space for the virtual network.

The range for one subnet can't overlap with other subnet IP address ranges in the same virtual network.

The IP address space for a subnet must be specified by using CIDR notation.

You can segment a virtual network into one or more subnets in the Azure portal. Characteristics about the IP addresses for the subnets are listed.

Screenshot that shows multiple subnets for a virtual network in the Azure portal.

Reserved addresses
For each subnet, Azure reserves five IP addresses. The first four addresses and the last address are reserved.

Let's examine the reserved addresses in an IP address range of 192.168.1.0/24.

Reserved address	Reason
192.168.1.0	This value identifies the virtual network address.
192.168.1.1	Azure configures this address as the default gateway.
192.168.1.2 and 192.168.1.3	Azure maps these Azure DNS IP addresses to the virtual network space.
192.168.1.255	This value supplies the virtual network broadcast address.
Things to consider when using subnets
When you plan for adding subnet segments within your virtual network, there are several factors to consider. Review the following scenarios.

Consider service requirements. Each service directly deployed into a virtual network has specific requirements for routing and the types of traffic that must be allowed into and out of associated subnets. A service might require or create their own subnet. There must be enough unallocated space to meet the service requirements. Suppose you connect a virtual network to an on-premises network by using Azure VPN Gateway. The virtual network must have a dedicated subnet for the gateway.

Consider network virtual appliances. Azure routes network traffic between all subnets in a virtual network, by default. You can override Azure's default routing to prevent Azure routing between subnets. You can also override the default to route traffic between subnets through a network virtual appliance. If you require traffic between resources in the same virtual network to flow through a network virtual appliance, deploy the resources to different subnets.

Consider service endpoints. You can limit access to Azure resources like an Azure storage account or Azure SQL database to specific subnets with a virtual network service endpoint. You can also deny access to the resources from the internet. You might create multiple subnets, and then enable a service endpoint for some subnets, but not others.

Consider network security groups. You can associate zero or one network security group to each subnet in a virtual network. You can associate the same or a different network security group to each subnet. Each network security group contains rules that allow or deny traffic to and from sources and destinations.

Consider private links. Azure Private Link provides private connectivity from a virtual network to Azure platform as a service (PaaS), customer-owned, or Microsoft partner services. Private Link simplifies the network architecture and secures the connection between endpoints in Azure. The service eliminates data exposure to the public internet.


Next unit: Create virtual networks

4- Create virtual networks

You can create new virtual networks at any time. You can also add virtual networks when you create a virtual machine.

Things to know about creating virtual networks
Review the following requirements for creating a virtual network.

When you create a virtual network, you need to define the IP address space for the network.

Plan to use an IP address space that's not already in use in your organization.

The address space for the network can be either on-premises or in the cloud, but not both.

Once you create the IP address space, it can't be changed. If you plan your address space for cloud-only virtual networks, you might later decide to connect an on-premises site.

To create a virtual network, you need to define at least one subnet.

Each subnet contains a range of IP addresses that fall within the virtual network address space.

The address range for each subnet must be unique within the address space for the virtual network.

The range for one subnet can't overlap with other subnet IP address ranges in the same virtual network.

You can create a virtual network in the Azure portal. Provide the Azure subscription, resource group, virtual network name, and service region for the network.

Screenshot that shows how to create a virtual network in the Azure portal.

 Note

Default limits on Azure networking resources can change periodically. Be sure to consult the Azure networking documentation for the latest information.


Next unit: Plan IP addressing

5- Plan IP addressing

You can assign IP addresses to Azure resources to communicate with other Azure resources, your on-premises network, and the internet. There are two types of Azure IP addresses: private and public.

Private IP addresses enable communication within an Azure virtual network and your on-premises network. You create a private IP address for your resource when you use a VPN gateway or Azure ExpressRoute circuit to extend your network to Azure.

Public IP addresses allow your resource to communicate with the internet. You can create a public IP address to connect with Azure public-facing services.

The following illustration shows a virtual machine resource that has a private IP address and a public IP address.

Illustration of a resource with a private IP address and a public IP address.

Things to know about IP addresses
Let's take a closer look at the characteristics of IP addresses.

IP addresses can be statically assigned or dynamically assigned.

You can separate dynamically and statically assigned IP resources into different subnets.

Static IP addresses don't change and are best for certain situations, such as:

DNS name resolution, where a change in the IP address requires updating host records.
IP address-based security models that require apps or services to have a static IP address.
TLS/SSL certificates linked to an IP address.
Firewall rules that allow or deny traffic by using IP address ranges.
Role-based virtual machines such as Domain Controllers and DNS servers.


Next unit: Create public IP addressing

6- Create public IP addressing

You can create a public IP address for your resource in the Azure portal.

Screenshot that shows how to create a public IP address in the Azure portal.

Things to consider when creating a public IP address
To create a public IP address, configure the following settings:

IP Version: Select to create an IPv4 or IPv6 address, or Both addresses.

SKU: Select the SKU for the public IP address, including Basic or Standard. The value must match the SKU of the Azure load balancer with which the address is used.

Name: Enter a name to identify the IP address. The name must be unique within the resource group you select.

IP address assignment: Identify the type of IP address assignment to use.

Dynamic addresses are assigned after a public IP address is associated to an Azure resource and is started for the first time. Dynamic addresses can change if a resource such as a virtual machine is stopped (deallocated) and then restarted through Azure. The address remains the same if a virtual machine is rebooted or stopped from within the guest OS. When a public IP address resource is removed from a resource, the dynamic address is released.

Static addresses are assigned when a public IP address is created. Static addresses aren't released until a public IP address resource is deleted. If the address isn't associated to a resource, you can change the assignment method after the address is created. If the address is associated to a resource, you might not be able to change the assignment method.

 Note

If you select IPv6 for the IP version, the assignment method must be Dynamic for the Basic SKU. Standard SKU addresses are Static for both IPv4 and IPv6 addresses.


Next unit: Associate public IP addresses

7- Associate public IP addresses

A public IP address resource can be associated with virtual machine network interfaces, internet-facing load balancers, VPN gateways, and application gateways. You can associate your resource with both dynamic and static public IP addresses.

Things to consider when associating public IP addresses
The following table summarizes how you can associate public IP addresses for different types of resources.

Resource	Public IP address association	Dynamic IP address	Static IP address
Virtual machine	NIC	Yes	Yes
Load balancer	Front-end configuration	Yes	Yes
VPN gateway	VPN gateway IP configuration	Yes	Yes *
Application gateway	Front-end configuration	Yes	Yes *
* Static IP addresses are available on certain SKUs only.

Public IP address SKUs
When you create a public IP address, you select the Basic or Standard SKU. Your SKU choice affects the IP assignment method, security, available resources, and redundancy options.

The following table summarizes the differences between the SKU types for public IP addresses.

Feature	Basic SKU	Standard SKU
IP assignment	Static or Dynamic	Static
Security	Open by default	Secure by default, closed to inbound traffic
Resources	Network interfaces, VPN gateways, Application gateways, and internet-facing load balancers	Network interfaces or public standard load balancers
Redundancy	Not zone redundant	Zone redundant by default


Next unit: Allocate or assign private IP addresses

8- Allocate or assign private IP addresses

A private IP address resource can be associated with virtual machine network interfaces, internal load balancers, and application gateways. Azure can provide an IP address (dynamic assignment) or you can assign the IP address (static assignment).

Things to consider when associating private IP addresses
The following table summarizes how you can associate private IP addresses for different types of resources.

Resource	Private IP address association	Dynamic IP address	Static IP address
Virtual machine	NIC	Yes	Yes
Internal load balancer	Front-end configuration	Yes	Yes
Application gateway	Front-end configuration	Yes	Yes
Private IP address assignment
A private IP address is allocated from the address range of the virtual network subnet that a resource is deployed in. There are two options: dynamic and static.

Dynamic: Azure assigns the next available unassigned or unreserved IP address in the subnet's address range. Dynamic assignment is the default allocation method.

Suppose addresses 10.0.0.4 through 10.0.0.9 are already assigned to other resources. In this case, Azure assigns the address 10.0.0.10 to a new resource.

Static: You select and assign any unassigned or unreserved IP address in the subnet's address range.

Suppose a subnet's address range is 10.0.0.0/16, and addresses 10.0.0.4 through 10.0.0.9 are already assigned to other resources. In this scenario, you can assign any address between 10.0.0.10 and 10.0.255.254.


Next unit: Interactive lab simulation

9- Interactive lab simulation

Lab scenario
Your organization is migrating network infrastructure and virtual machines to Azure. As the Azure Administrator you need to:

Configure Azure virtual networks and subnets.
Connect remotely to Azure virtual machines by using RDP.
Verify virtual machines in the same virtual network can communicate.
Architecture diagram
Diagram of the architecture as explained in the text.

Objectives
Task 1: Create a virtual network.
Create a virtual network, vnet1, with an IP address space of 10.1.0.0/16.
Create a subnet, default, with an IP address space of 10.1.0.0/24.
Task 2: Create two virtual machines.
Create a virtual machine, vm1, in vnet1 and allow inbound RDP.
Create a second virtual machine, vm2, in vnet1 and allow inbound RDP.
Ensure both virtual machines are deployed and running before continuing.
Task 3: Test the virtual machine connections.
Connect to vm1 with RDP.
Connect to vm2 with RDP.
Disable the public and private Windows Firewall on both virtual machines.
Use Azure PowerShell to confirm vm1 can ping vm2.
 Note

Select the thumbnail image to start the lab simulation. When you're done, be sure to return to this page so you can continue learning.

Screenshot of the simulation page.

Next unit: Knowledge check

Your company is migrating to Azure and replicating their on-premises network in the cloud. They're developing a plan to use Azure Virtual Network to organize company resources into virtual networks and subnets. You're working on the design for the company IP address schema, mapping out which ranges can be assigned, and which ranges can be denied traffic.

Answer these questions
Choose the best response for each of the questions. Then select Check your answers.


1. Which of the following statements about Azure virtual networks is correct? 

Each virtual network has only one subnet.

Azure virtual networks can't be configured to communicate with on-premises resources.

Azure virtual networks enable communication between Azure resources.

2. Which situation is appropriate for a public IP address? 

Virtual machines accessed from the Internet.

Virtual machines used as internal load balancers.

Virtual machines running production workloads.

3. When does a dynamic IP address change? 

In Azure, when a virtual machine is stopped and then restarted.

In the guest OS, when a virtual machine is rebooted or stopped.

Each time the virtual machine is accessed.


Summary and resources

In this module, you learned about Azure virtual networks and their importance in creating private networks in Azure. You explored the benefits of using virtual networks, such as scalability, availability, and isolation. You learned how to create virtual networks with subnetting and how to determine which resources require public or private IP addresses.

The main takeaways from this module are:

Azure virtual networks allow different Azure resources to securely communicate with each other, the internet, and on-premises networks.

Subnets within virtual networks provide logical divisions, improving security, performance, and management.

When creating virtual networks, ensure that the IP address space is unique and doesn't overlap with other subnets.

IP addresses can provide public or private access to resources.

Learn more with documentation
What is Azure Virtual Network?. This article is your starting point to learn about virtual networks.

Public IP addresses. This article reviews the basics of when to use public IP addresses.

Private IP addresses. This article reviews the basics of when to use private IP addresses.

Learn more with self-paced training
Introduction to Azure Virtual Networks. Learn how to design and implement core Azure Networking infrastructure.

Design an IP addressing schema for your Azure deployment (sandbox). Learn about network IP addressing and integration.

Implement Windows Server IaaS virtual machine IP addressing and routing. Learn about IP addressing and virtual networks for virtual machines.



Point 2: Configure network security groups

Learn how to implement network security groups, and ensure network security group rules are correctly applied.

Learning objectives
In this module, you learn how to:

Determine when to use network security groups.
Create network security groups.
Implement and evaluate network security group rules.
Describe the function of application security groups.


1- Introduction

Network security groups are a way to limit network traffic to resources in your virtual network. Network security groups contain a list of security rules that allow or deny inbound or outbound network traffic.

Suppose your company has several locations and wants to migrate to a cloud based solution. The company only considers moving key systems onto the cloud platform if stringent security requirements can be met. These requirements include tight control over which computers have network access to the app servers. You need to secure both virtual machine networking and Azure services networking. Your goal is to prevent unwanted or unsecured network traffic from being able to reach key systems.

In this module, you learn how to create a network security group, configure inbound and outbound port rules, and verify secure connectivity.

The goal of this module is to teach you how to control network traffic with network security groups.

Learning objectives
In this module, you learn how to:

Determine when to use network security groups.
Create network security groups.
Implement and evaluate network security group rules.
Describe the function of application security groups.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Familiarity with Azure virtual networks and resources such as virtual machines.
Working knowledge of the Azure portal so you can configure the network security groups.
Basic understanding of traffic routing and traffic control strategies.


Next unit: Implement network security groups

2- Implement network security groups

You can limit network traffic to resources in your virtual network by using a network security group. You can assign a network security group to a subnet or a network interface, and define security rules in the group to control network traffic.

Things to know about network security groups
Let's look at the characteristics of network security groups.

A network security group contains a list of security rules that allow or deny inbound or outbound network traffic.

A network security group can be associated to a subnet or a network interface.

A network security group can be associated multiple times.

You create a network security group and define security rules in the Azure portal.

Network security groups are defined for your virtual machines in the Azure portal. The Overview page for a virtual machine provides information about the associated network security groups. You can see details such as the assigned subnets, assigned network interfaces, and the defined security rules.

Screenshot that shows details for a network security group for a virtual machine in the Azure portal.

Network security groups and subnets
You can assign network security groups to a subnet and create a protected screened subnet (also referred to as a demilitarized zone or DMZ). A DMZ acts as a buffer between resources within your virtual network and the internet.

Use the network security group to restrict traffic flow to all machines that reside within the subnet.

Each subnet can have a maximum of one associated network security group.

Network security groups and network interfaces
You can assign network security groups to a network interface card (NIC).

Define network security group rules to control all traffic that flows through a NIC.

Each network interface that exists in a subnet can have zero, or one, associated network security groups.


Next unit: Determine network security group rules

3- Determine network security group rules

Security rules in network security groups enable you to filter network traffic. You can define rules to control the traffic flow in and out of virtual network subnets and network interfaces.

Things to know about security rules
Let's review the characteristics of security rules in network security groups.

Azure creates several default security rules within each network security group, including inbound traffic and outbound traffic. Examples of default rules include DenyAllInbound traffic and AllowInternetOutbound traffic.

Azure creates the default security rules in each network security group that you create.

You can add more security rules to a network security group by specifying conditions for any of the following settings:

Name
Priority
Port
Protocol (Any, TCP, UDP)
Source (Any, IP addresses, Service tag)
Destination (Any, IP addresses, Virtual network)
Action (Allow or Deny)
Each security rule is assigned a Priority value. All security rules for a network security group are processed in priority order. When a rule has a low Priority value, the rule has a higher priority or precedence in terms of order processing.

You can't remove the default security rules.

You can override a default security rule by creating another security rule that has a higher Priority setting for your network security group.

Inbound traffic rules
Azure defines three default inbound security rules for your network security group. These rules deny all inbound traffic except traffic from your virtual network and Azure load balancers. The following image shows the default inbound security rules for a network security group in the Azure portal.

Screenshot that shows default inbound security rules for a network security group in the Azure portal.

Outbound traffic rules
Azure defines three default outbound security rules for your network security group. These rules only allow outbound traffic to the internet and your virtual network. The following image shows the default outbound security rules for a network security group in the Azure portal.

Screenshot that shows default outbound security rules for a network security group in the Azure portal.


Next unit: Determine network security group effective rules

4- Determine network security group effective rules

Each network security group and its defined security rules are evaluated independently. Azure processes the conditions in each rule defined for each virtual machine in your configuration.

For inbound traffic, Azure first processes network security group security rules for any associated subnets and then any associated network interfaces.
For outbound traffic, the process is reversed. Azure first evaluates network security group security rules for any associated network interfaces followed by any associated subnets.
For both the inbound and outbound evaluation process, Azure also checks how to apply the rules for intra-subnet traffic.
How Azure ends up applying your defined security rules for a virtual machine determines the overall effectiveness of your rules.

Things to know about effective security rules
Let's explore how network security group rules are defined and processed within a virtual network to yield the effective rules.

Consider the following virtual network configuration that shows network security groups (NSGs) controlling traffic to virtual machines (VMs). The configuration requires security rules to manage network traffic to and from the internet over TCP port 80 via the network interface.

Diagram that shows how network security group security rules control traffic to virtual machines.

In this virtual network configuration, there are three subnets. Subnet 1 contains two virtual machines: VM 1 and VM 2. Subnet 2 and Subnet 3 each contain one virtual machine: VM 3 and VM 4, respectively. Each VM has a network interface card (NIC).

Azure evaluates each NSG configuration to determine the effective security rules:

Evaluation	Subnet NSG	NIC NSG	Inbound rules	Outbound rules
VM 1	Subnet 1
NSG 1	NIC
NSG 2	NSG 1 subnet rules have precedence over NSG 2 NIC rules	NSG 2 NIC rules have precedence over NSG 1 subnet rules
VM 2	Subnet 1
NSG 1	NIC
none	NSG 1 subnet rules apply to both subnet and NIC	Azure default rules apply to NIC
and NSG 1 subnet rules apply to subnet only
VM 3	Subnet 2
none	NIC
NSG 2	Azure default rules apply to subnet
and NSG 2 rules apply to NIC	NSG 2 NIC rules apply to NIC and subnet
VM 4	Subnet 3
none	NIC
none	Azure default rules apply to both subnet and NIC
and all inbound traffic is allowed	Azure default rules apply to both subnet and NIC
and all outbound traffic is allowed
Inbound traffic effective rules
Azure processes rules for inbound traffic for all VMs in the configuration. Azure identifies if the VMs are members of an NSG, and if they have an associated subnet or NIC.

When an NSG is created, Azure creates the default security rule DenyAllInbound for the group. The default behavior is to deny all inbound traffic from the internet. If an NSG has a subnet or NIC, the rules for the subnet or NIC can override the default Azure security rules.

NSG inbound rules for a subnet in a VM take precedence over NSG inbound rules for a NIC in the same VM.

Outbound traffic effective rules
Azure processes rules for outbound traffic by first examining NSG associations for NICs in all VMs.

When an NSG is created, Azure creates the default security rule AllowInternetOutbound for the group. The default behavior is to allow all outbound traffic to the internet. If an NSG has a subnet or NIC, the rules for the subnet or NIC can override the default Azure security rules.

NSG outbound rules for a NIC in a VM take precedence over NSG outbound rules for a subnet in the same VM.

Things to consider when creating effective rules
Review the following considerations regarding creating effective security rules for machines in your virtual network.

Consider allowing all traffic. If you place your virtual machine within a subnet or utilize a network interface, you don't have to associate the subnet or NIC with a network security group. This approach allows all network traffic through the subnet or NIC according to the default Azure security rules. If you're not concerned about controlling traffic to your resource at a specific level, then don't associate your resource at that level to a network security group.

Consider importance of allow rules. When you create a network security group, you must define an allow rule for both the subnet and network interface in the group to ensure traffic can get through. If you have a subnet or NIC in your network security group, you must define an allow rule at each level. Otherwise, the traffic is denied for any level that doesn't provide an allow rule definition.

Consider intra-subnet traffic. The security rules for a network security group that's associated to a subnet can affect traffic between all virtual machines in the subnet. By default, Azure allows virtual machines in the same subnet to send traffic to each other (referred to as intra-subnet traffic). You can prohibit intra-subnet traffic by defining a rule in the network security group to deny all inbound and outbound traffic. This rule prevents all virtual machines in your subnet from communicating with each other.

Consider rule priority. The security rules for a network security group are processed in priority order. To ensure a particular security rule is always processed, assign the lowest possible priority value to the rule. It's a good practice to leave gaps in your priority numbering, such as 100, 200, 300, and so. The gaps in the numbering allow you to add new rules without having to edit existing rules.

View effective security rules
If you have several network security groups and aren't sure which security rules are being applied, you can use the Effective security rules link in the Azure portal. You can use the link to verify which security rules are applied to your machines, subnets, and network interfaces.

Screenshot of the Networking page in the Azure portal showing the Effective security rules link highlighted.


Next unit: Create network security group rules

5- Create network security group rules

It's easy to add security rules to control inbound and outbound traffic in the Azure portal. You can configure your virtual network security group rule settings, and select from a large variety of communication services, including HTTPS, RDP, FTP, and DNS.

Things to know about configuring security rules
Let's look at some of the properties you need to specify to create your security rules. As you review these settings, think about the traffic rules you need to create and what services can fulfill your network requirements.

Screenshot that shows how to configure source and destination settings to create a security rule in the Azure portal.

Source: Identifies how the security rule controls inbound traffic. The value specifies a specific source IP address range that's allowed or denied. The source filter can be any resource, an IP address range, an application security group, or a default tag.

Destination: Identifies how the security rule controls outbound traffic. The value specifies a specific destination IP address range that's allowed or denied. The destination filter value is similar to the source filter. The value can be any resource, an IP address range, an application security group, or a default tag.

Service: Specifies the destination protocol and port range for the security rule. You can choose a predefined service like RDP or SSH or provide a custom port range. There are a large number of services to select from.

Screenshot that shows service rule options for a security rule in the Azure portal.

Priority: Assigns the priority order value for the security rule. Rules are processed according to the priority order of all rules for a network security group, including a subnet and network interface. The lower the priority value, the higher priority for the rule.

Screenshot that shows how to set the priority value for a security rule in the Azure portal.


Next unit: Implement application security groups

6- Implement application security groups

You can implement application security groups in your Azure virtual network to logically group your virtual machines by workload. You can then define your network security group rules based on your application security groups.

Things to know about using application security groups
Application security groups work in the same way as network security groups, but they provide an application-centric way of looking at your infrastructure. You join your virtual machines to an application security group. Then you use the application security group as a source or destination in the network security group rules.

Let's examine how to implement application security groups by creating a configuration for an online retailer. In our example scenario, we need to control network traffic to virtual machines in application security groups.

Diagram that shows how application security groups combine with network security groups to protect applications.

Scenario requirements
Here are the scenario requirements for our example configuration:

We have six virtual machines in our configuration with two web servers and two database servers.
Customers access the online catalog hosted on our web servers.
The web servers must be accessible from the internet over HTTP port 80 and HTTPS port 443.
Inventory information is stored on our database servers.
The database servers must be accessible over HTTPS port 1433.
Only our web servers should have access to our database servers.
Solution
For our scenario, we need to build the following configuration:

Create application security groups for the virtual machines.

Create an application security group named WebASG to group our web server machines.

Create an application security group named DBASG to group our database server machines.

Assign the network interfaces for the virtual machines.

For each virtual machine server, assign its NIC to the appropriate application security group.
Create the network security group and security rules.

Rule 1: Set Priority to 100. Allow access from the internet to machines in the WebASG group from HTTP port 80 and HTTPS port 443.

Rule 1 has the lowest priority value, so it has precedence over the other rules in the group. Customer access to our online catalog is paramount in our design.

Rule 2: Set Priority to 110. Allow access from machines in the WebASG group to machines in the DBASG group over HTTPS port 1433.

Rule 3: Set Priority to 120. Deny (X) access from anywhere to machines in the DBASG group over HTTPS port 1433.

The combination of Rule 2 and Rule 3 ensures that only our web servers can access our database servers. This security configuration protects our inventory databases from outside attack.

Things to consider when using application security groups
There are several advantages to implementing application security groups in your virtual networks.

Consider IP address maintenance. When you control network traffic by using application security groups, you don't need to configure inbound and outbound traffic for specific IP addresses. If you have many virtual machines in your configuration, it can be difficult to specify all of the affected IP addresses. As you maintain your configuration, the number of your servers can change. These changes can require you to modify how you support different IP addresses in your security rules.

Consider no subnets. By organizing your virtual machines into application security groups, you don't need to also distribute your servers across specific subnets. You can arrange your servers by application and purpose to achieve logical groupings.

Consider simplified rules. Application security groups help to eliminate the need for multiple rule sets. You don't need to create a separate rule for each virtual machine. You can dynamically apply new rules to designated application security groups. New security rules are automatically applied to all the virtual machines in the specified application security group.

Consider workload support. A configuration that implements application security groups is easy to maintain and understand because the organization is based on workload usage. Application security groups provide logical arrangements for your applications, services, data storage, and workloads.


Next unit: Interactive lab simulation

7- Interactive lab simulation

Lab scenario
Your organization wants to ensure that access to virtual machines is restricted. As the Azure Administrator, you need to:

Create and configure network security groups.
Associate network security groups to virtual machines.
Deny and allow access to the virtual machines by using network security groups.
Architecture diagram
Diagram showing the architecture as explained in the text.

Objectives
Task 1: Create a virtual machine to test network security.
Create a Windows Server virtual machine.
Don't configure any inbound port rules or NIC network security groups.
Verify the virtual machine is created.
Review the Inbound port rules tab, and note there are no network security groups associated with the virtual machine.
Task 2: Create a network security group, and associate the group with the virtual machine.
Create a network security group.
Associate the network security group with the virtual machine network interface (NIC).
Task 3: Configure an inbound security port rule to allow RDP.
Verify you can't connect to the virtual machine by using RDP.
Add an inbound port rule to allow RDP to the virtual machine on port 3389.
Verify you can now connect to the virtual machine with RDP.
Task 4: Configure an outbound security port rule to deny internet access
Verify you can access the internet from the virtual machine.
Add an outbound port rule to deny internet access from the virtual machine.
Verify you can no longer access the internet from the virtual machine.
 Note

Select the thumbnail image to start the lab simulation. When you're done, be sure to return to this page so you can continue learning.

Screenshot of the simulation page.

Next unit: Knowledge check

Your company is migrating several sites to Azure. You're responsible for implementing network security groups and designing effective security rules to control network traffic. You need to ensure that virtual machine networking and Azure services networking are both secure.

The infrastructure team has two network security group security rules for inbound traffic to the back-end web servers. There's an allow rule with a priority of 200, and a deny rule with a priority of 150.

The IT team wants to apply new and pre-existing Azure service tags for the virtual machine IP addresses.

You're exploring how to use default rules to apply security to inbound traffic from virtual machines within your virtual network.

Answer the following questions
Choose the best response for each of the following questions. Then select Check your answers.


1. Which of the security rules defined by the infrastructure team takes precedence? 

The allowed rule takes precedence.

The deny rule takes precedence.

The rule that was created first takes precedence.
2. How do Application Security Groups (ASGs) enhance network security within Azure Virtual Networks? 

By encrypting virtual network traffic.

By applying anti-virus software.

By grouping virtual machines according to their functions.
3. What happens to network traffic that doesn't match any NSG rules? 

It's allowed by default.

It's denied by default.

It's postponed until the rules change.


Summary and resources

In this module, you learned about network security groups (NSGs) in Azure. NSGs are used to limit network traffic to resources in your virtual network by containing a list of security rules. You can associate NSGs with subnets or network interfaces and define rules to control inbound and outbound traffic.

You also learned how NSG rules are evaluated and processed. Lastly, you learned how application security groups, allow for grouping virtual machines based on workload.

The main takeaways from this module are:

Network security groups are essential for controlling network traffic in Azure virtual networks.

NSG rules are evaluated and processed based on priority and can be created for subnets and network interfaces.

Effective NSG rules can be achieved by considering rule precedence, intra-subnet traffic, and managing rule priority.

Application security groups provide an application-centric view of infrastructure and simplify rule management.

Learn more
Read about network security groups. This article describes the properties of a network security group rule, the default security rules that are applied, and the rule properties that you can modify.

Filter network traffic with network security groups in the Azure portal. Learn how to create a network security group and an application security group.

Create, change, or delete a network security group. Learn how to work with network and application security groups.

Application security groups. Learn about application security groups and traffic control with rules.




Point 3: Configure Azure Virtual Network peering

Learn to configure an Azure Virtual Network peering connection and address transit and connectivity concerns.

Learning objectives
In this module, you learn how to:

Identify usage cases and product features of Azure Virtual Network peering.

Configure your network to implement Azure VPN Gateway for transit connectivity.

Extend peering by using a hub and spoke network with user-defined routes and service chaining.


1- Introduction

Azure Virtual Network peering lets you connect virtual networks in the same or different regions. Azure Virtual Network peering provides secure communication between resources in the peered networks.

Suppose your engineering company is migrating services to Azure. The company is deploying services into separate Azure virtual networks. Private connectivity between the virtual networks isn't yet configured. Several business units identified services in the virtual networks that need to communicate with each other.

You're responsible for implementing an Azure Virtual Network peering solution and enabling connectivity between the virtual networks. Two of your strategy goals include preventing exposure of the services to the internet, and keeping the integration as simple as possible. Your solution should address transit and connectivity concerns.

The goal of this module is to successfully implement Azure Virtual Network peering.

Learning objectives
In this module, you learn how to:

Identify usage cases and product features of Azure Virtual Network peering.
Configure your network to implement Azure VPN Gateway for transit connectivity.
Extend peering by using a hub and spoke network with user-defined routes and service chaining.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Basic understanding of cloud networking including virtual networks and virtual machines.

Familiarity with the command line connectivity testing tools.


Next unit: Determine Azure Virtual Network peering uses


2- Determine Azure Virtual Network peering uses

Perhaps the simplest and quickest way to connect your virtual networks is to use Azure Virtual Network peering. Virtual Network peering enables you to seamlessly connect two Azure virtual networks. After the networks are peered, the two virtual networks operate as a single network, for connectivity purposes.

Things to know about Azure Virtual Network peering
Let's examine some prominent characteristics of Azure Virtual Network peering.

There are two types of Azure Virtual Network peering: regional and global.

Diagram that demonstrates the two types of Azure Virtual Network peering: global and regional.

Regional virtual network peering connects Azure virtual networks that exist in the same region.

Global virtual network peering connects Azure virtual networks that exist in different regions.

You can create a regional peering of virtual networks in the same Azure public cloud region, or in the same China cloud region, or in the same Microsoft Azure Government cloud region.

You can create a global peering of virtual networks in any Azure public cloud region, or in any China cloud region.

Global peering of virtual networks in different Azure Government cloud regions isn't permitted.

After you create a peering between virtual networks, the individual virtual networks are still managed as separate resources.

Things to consider when using Azure Virtual Network peering
Consider the following benefits of using Azure Virtual Network peering.

Benefit	Description
Private network connections	When you implement Azure Virtual Network peering, network traffic between peered virtual networks is private. Traffic between the virtual networks is kept on the Microsoft Azure backbone network. No public internet, gateways, or encryption is required in the communication between the virtual networks.
Strong performance	Because Azure Virtual Network peering utilizes the Azure infrastructure, you gain a low-latency, high-bandwidth connection between resources in different virtual networks.
Simplified communication	Azure Virtual Network peering lets resources in one virtual network communicate with resources in a different virtual network, after the virtual networks are peered.
Seamless data transfer	You can create an Azure Virtual Network peering configuration to transfer data across Azure subscriptions, deployment models, and across Azure regions.
No resource disruptions	Azure Virtual Network peering doesn't require downtime for resources in either virtual network when creating the peering, or after the peering is created.


Next unit: Determine gateway transit and connectivity

3- Determine gateway transit and connectivity

When virtual networks are peered, you can configure Azure VPN Gateway in the peered virtual network as a transit point. In this scenario, a peered virtual network uses the remote VPN gateway to gain access to other resources.

Consider a scenario where three virtual networks in the same region are connected by virtual network peering. Virtual network A and virtual network B are each peered with a hub virtual network. The hub virtual network contains several resources, including a gateway subnet and an Azure VPN gateway. The VPN gateway is configured to allow VPN gateway transit. Virtual network B accesses resources in the hub, including the gateway subnet, by using a remote VPN gateway.

Diagram of a regional virtual network peering. One network allows VPN gateway transit and uses a remote VPN gateway to access resources in a hub virtual network.

Things to know about Azure VPN Gateway
Let's take a closer look at how Azure VPN Gateway is implemented with Azure Virtual Network peering.

A virtual network can have only one VPN gateway.

Gateway transit is supported for both regional and global virtual network peering.

When you allow VPN gateway transit, the virtual network can communicate to resources outside the peering. In our sample illustration, the gateway subnet gateway within the hub virtual network can complete tasks such as:

Use a site-to-site VPN to connect to an on-premises network.
Use a vnet-to-vnet connection to another virtual network.
Use a point-to-site VPN to connect to a client.
Gateway transit allows peered virtual networks to share the gateway and get access to resources. With this implementation, you don't need to deploy a VPN gateway in the peer virtual network.

You can apply network security groups in a virtual network to block or allow access to other virtual networks or subnets. When you configure virtual network peering, you can choose to open or close the network security group rules between the virtual networks.

Next unit: Create virtual network peering

4- Create virtual network peering

Azure Virtual Network peering can be configured for virtual networks by using PowerShell, the Azure CLI, and in the Azure portal. In this module, we review the steps to create the peering in the Azure portal for virtual networks deployed through Azure Resource Manager.

Things to know about creating virtual network peering
There are a few points to review before we look at how to create the peering in the Azure portal.

To implement virtual network peering, your Azure account must be assigned to the Network Contributor or Classic Network Contributor role. Alternatively, your Azure account can be assigned to a custom role that can complete the necessary peering actions. For details, see Permissions.

To create a peering, you need two virtual networks.

The second virtual network in the peering is referred to as the remote network.

Initially, the virtual machines in your virtual networks can't communicate with each other. After the peering is established, the machines can communicate within the peered network based on your configuration settings.

How to connect virtual networks across Azure regions with Azure Global VNet peering

How to check your peering status
In the Azure portal, you can check the connectivity status of the virtual networks in your virtual network peering. The status conditions depend on how your virtual networks are deployed.

 Important

Your peering isn't successfully established until both virtual networks in the peering have a status of Connected.

For deployment with the Azure Resource Manager, the two primary status conditions are Initiated and Connected. For the classic deployment model, the Updating status condition is also used.

When you create the initial peering to the second (remote) virtual network from the first virtual network, the peering status for the first virtual network is Initiated.

When you create the subsequent peering from the second virtual network to the first virtual network, the peering status for both the first and remote virtual networks is Connected. In the Azure portal, you can see the status for the first virtual network change from Initiated to Connected.

Next unit: Extend peering with user-defined routes and service chaining

5- Extend peering with user-defined routes and service chaining

Virtual network peering is nontransitive. The communication capabilities in a peering are available to only the virtual networks and resources in the peering. Other mechanisms have to be used to enable traffic to and from resources and networks outside the private peering network.

Suppose you have three virtual networks: A, B, and C. You establish virtual network peering between networks A and B, and also between networks B and C. You don't set up peering between networks A and C. The virtual network peering capabilities that you set up between networks B and C don't automatically enable peering communication capabilities between networks A and C.

Things to know about extending peering
There are a few ways to extend the capabilities of your peering for resources and virtual networks outside your peering network:

Hub and spoke networks
User-defined routes
Service chaining
You can implement these mechanisms and create a multi-level hub and spoke architecture. These options can help overcome the limit on the number of virtual network peerings per virtual network.

The following diagram shows a hub and spoke virtual network with an NVA and VPN gateway. The hub and spoke network is accessible to other virtual networks via user-defined routes and service chaining.

Diagram that shows a hub virtual network with an NVA and VPN gateway that are accessible to other virtual networks.

Mechanism	Description
Hub and spoke network	When you deploy a hub-and-spoke network, the hub virtual network can host infrastructure components like a network virtual appliance (NVA) or Azure VPN gateway. All the spoke virtual networks can then peer with the hub virtual network. Traffic can flow through NVAs or VPN gateways in the hub virtual network.
User-defined route (UDR)	Virtual network peering enables the next hop in a user-defined route to be the IP address of a virtual machine in the peered virtual network, or a VPN gateway.
Service chaining	Service chaining is used to direct traffic from one virtual network to a virtual appliance or gateway. A user-defined route defines the peered networks.


Next unit: Interactive lab simulation

6- Interactive lab simulation

Lab scenario
Your organization has three datacenters connected with a mesh wide-area network. As the Azure Administrator, you need to implement the on-premises infrastructure in Azure.

There are two offices, New York and Boston, in one region.
There's one office, Seattle, in another region.
All the offices need to be networked together so they can share information.
This simulation focuses on the connectivity of the offices, and not creating the individual Azure resources.
Architecture diagram
Architecture diagram as explained in the text.

Objectives
 Note

You may find slight differences between the interactive simulation and the Azure environment, but the core concepts and ideas being demonstrated are the same.

Task 1: Create the infrastructure environment. In this task, you deploy three virtual machines. Virtual machines are deployed in different regions and virtual networks.
Use a template to create the virtual networks and virtual machines in the different regions. You can review the lab templates.
Use Azure PowerShell to deploy the template.
Task 2: Configure local and global virtual network peering.
Create a local virtual network peering between the two virtual networks in the same region.
Create a global virtual network peering between virtual networks in different regions.
Task 3: Test intersite connectivity between virtual machines on the three virtual networks.
Test the virtual machine connections in the same region.
Test the virtual machine connections in different regions.
 Note

select the thumbnail image to start the lab simulation. When you're done, be sure to return to this page so you can continue learning.

Screenshot of the simulation page.

Next unit: Knowledge check

Your company is implementing an Azure Virtual Network peering solution to enable connectivity between virtual networks. You're working on the plan to support shared access to gateways and resources, and to control internet communication. A few teams submitted questions and configuration requests for your input.

The IT team needs information about how to check virtual network peering status and verify peered connections.

The Marketing department has resources in virtual networks in different regions.

Answer the following questions
Choose the best response for each of the following questions. Then select Check your answers.


1. When virtual networks are successfully peered, what's the peering status for both virtual networks in the peering? 

Initiated

Connected

Peered

2. What approach enables peered virtual networks to share the gateway and get access to resources? 

Point-to-site connectivity

Transitivity

Gateway transit

3. How is Azure Virtual Network peering best described? 

Traffic between virtual networks is kept on the Microsoft backbone network.

Virtual network peering disrupts other resources.

Peered virtual networks must be in the same region.


Summary and resources

In this module, you learned Azure Virtual Network peering lets you connect virtual networks in a hub and spoke topology. You learned how to configure your virtual networks with Azure VPN Gateway for transit connectivity. You explored how to extend peering with user-defined routes and service chaining.

The main takeaways from this module are:

Azure Virtual Network peering allows for the connection of virtual networks in a hub and spoke topology.

There are two types of peering: regional and global. Regional peering connects virtual networks in the same region. Global peering connects virtual networks in different regions.

Network traffic between peered virtual networks is private and kept on the Azure backbone network.

You can configure Azure VPN Gateway in the peered virtual network as a transit point to access resources in another network.

Network security groups can be applied to block or allow access between virtual networks when configuring virtual network peering.

Learn more
Azure Virtual Network peering. This article is your starting point for learning about virtual network peering.

Create, change, or delete a virtual network peering. This article reviews how to create a virtual network peering and what each setting means.





Point 4: Configure network routing and endpoints

Learn how to configure network routes, including endpoints and private links.

Learning objectives
In this module, you learn how to:

Implement system routes and user-defined routes.
Configure a custom route.
Implement service endpoints.
Identify features and usage cases for Azure Private Link and endpoint services.

1- Introduction

Administrators use network routes to control the flow of traffic through a network. Azure virtual networking provides capabilities to help you customize your network routes, establish service endpoints, and access private links.

In this module, suppose your company recently suffered a security incident that exposed customer personal information. This security incident has resulted in the loss of customers' confidential data and confidence. To address this scenario, the IT team has recommended implementing network virtual appliances (NVAs). You need to ensure traffic is properly routed through the virtual appliances. You're exploring other security options like service endpoints and private links.

Learning objectives
In this module, you learn how to:

Implement system routes and user-defined routes.
Configure a custom route.
Implement service endpoints.
Identify features and usage cases for Azure Private Link and endpoint services.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator. The module concepts are covered in:

Configure and manage virtual networking (25–30%)

Implement and manage virtual networking.
Configure user-defined network routes.
Configure endpoints on subnets.
Configure private endpoints.
Prerequisites
Familiarity with network routing.


Next unit: Review system routes

2- Review system routes

Azure uses system routes to direct network traffic between virtual machines, on-premises networks, and the internet. Information about the system routes is recorded in a route table.

Things to know about system routes
Let's take a closer look at how Azure implements system routes.

Azure uses system routes to control traffic for virtual machines in several scenarios:

Traffic between virtual machines in the same subnet
Traffic between virtual machines in different subnets in the same virtual network
Traffic from virtual machines to the internet
A route table contains a set of rules (called routes) that specifies how packets should be routed in a virtual network.

Route tables record information about the system routes, where the tables are associated to subnets.

Each packet leaving a subnet is handled based on the associated route table.

Packets are matched to routes by using the destination. The destination can be an IP address, a virtual network gateway, a virtual appliance, or the internet.

When a matching route can't be found, the packet is dropped.

Business scenario
Suppose you have a virtual network with two subnets. In this configuration, you can use Azure system routes to control communication between the subnets and between subnets and the internet. A front-end subnet can use a system route to access the internet. A back-end subnet can use a system route to access the front-end subnet. Both subnets access a route table. The following illustration highlights this scenario:

Diagram that shows two subnets that use system routes as described in the text.

Next unit: Identify user-defined routes

3- Identify user-defined routes

Azure automatically handles all network traffic routing, but in some cases, a custom configuration is preferable. In these situations, you can configure user-defined routes (UDRs) and next hop targets.

Things to know about user-defined routes
Let's examine the characteristics of user-defined routes.

UDRs control network traffic by defining routes that specify the next hop of the traffic flow.

The next hop can be one of the following targets:

Virtual network gateway
Virtual network
Internet
Network virtual appliance (NVA)
Similar to system routes, UDRs also access route tables.

Each route table can be associated to multiple subnets.

Each subnet can be associated to one route table only.

There are no charges for creating route tables in Microsoft Azure.

Business scenario
Suppose you have a virtual machine that performs a network function like routing, firewalling, or WAN optimization. You want to direct certain subnet traffic to the NVA. To accomplish this configuration, you can place an NVA between subnets or between one subnet and the internet. The subnet can use a UDR to access the NVA and then the internet. The subnet can use another UDR and NVA to access the back-end subnet. The following illustration highlights this scenario:

Diagram that shows two subnets that use a UDR to access an NVA as described in the text.

Next unit: Determine service endpoint uses

4- Determine service endpoint uses

A virtual network service endpoint provides the identity of your virtual network to the Azure service. After service endpoints are enabled in your virtual network, you can secure Azure service resources to your virtual network by adding a virtual network rule to the resources.

Today, Azure service traffic from a virtual network uses public IP addresses as source IP addresses. With service endpoints, service traffic switches to use virtual network private addresses as the source IP addresses when accessing the Azure service from a virtual network. This switch allows you to access the services without the need for reserved public IP addresses that are typically used in IP firewalls.

Things to know about service endpoints
Review the following characteristics of service endpoints.

Service endpoints can extend your virtual network identity to your Azure services to secure your service resources.

You secure your Azure service resources to your virtual network by using virtual network rules.

Virtual network rules can remove public internet access to resources, and allow traffic only from your virtual network.

Service endpoints always take service traffic directly from your virtual network to the service on the Microsoft Azure backbone network.

Service endpoints are configured through the subnet. No extra overhead is required to maintain the endpoints.

The following illustration shows a virtual machine connecting to the Azure service through a service endpoint. A virtual machine in a subnet accesses an Azure Storage account through a service endpoint. Virtual network rules allow the virtual machine to access the Azure service resource, but not communicate with the internet.

Diagram of a virtual machine in a subnet connecting to an Azure service through a service endpoint.

Things to consider when using service endpoints
There are several scenarios where using service endpoints can be advantageous. Review the following points and think about how you can implement service endpoints in your configuration.

Consider improved security for resources. Implement service endpoints to improve the security of your Azure service resources. When service endpoints are enabled in your virtual network, you secure Azure service resources to your virtual network with virtual network rules. The rule improves security by fully removing public internet access to resources, and allowing traffic only from your virtual network.

Consider optimal routing for service traffic. Routes in your virtual network that force internet traffic to your on-premises or network virtual appliances also typically force Azure service traffic to take the same route as the internet traffic. This traffic control process is known as forced-tunneling. Service endpoints provide optimal routing for Azure service traffic to allow you to circumvent forced tunneling.

Consider direct traffic to the Microsoft network. Use service endpoints to keep traffic on the Azure backbone network. This approach allows you to continue auditing and monitoring outbound internet traffic from your virtual networks, through forced-tunneling, without impacting service traffic. Learn more about user-defined routes and forced-tunneling.

Consider easy configuration and maintenance. Configure service endpoints in your subnets for simple setup and low maintenance. You no longer need reserved public IP addresses in your virtual networks to secure Azure resources through an IP firewall. There are no NAT or gateway devices required to set up the service endpoints.

 Note

With service endpoints, the virtual machine IP addresses switch from public to private IPv4 addresses. Existing Azure service firewall rules that use Azure public IP addresses stop working after the switch. Ensure Azure service firewall rules allow for this switch before you set up service endpoints. You might also experience temporary interruption to service traffic from this subnet while configuring service endpoints.


Next unit: Determine service endpoint services

5- Determine service endpoint services

It's easy to add a service endpoint to the virtual network. In the Azure portal, you select the Azure service for which to create the endpoint. In this unit, we examine several services, including Azure Cosmos DB, Event Hubs, Key Vault, and SQL Database.

Screenshot of the Service endpoints page in the Azure portal.

 Note

Adding service endpoints can take up to 15 minutes to complete. Each service endpoint integration has its own Azure documentation page.

Service	Availability	Description
Azure Storage	Generally available in all Azure regions	This endpoint gives traffic an optimal route to the Azure Storage service. Each Storage account supports up to 100 virtual network rules.
Azure SQL Database and Azure SQL Data Warehouse	Generally available in all Azure regions	A firewall security feature controls whether your database accepts communication from particular subnets in virtual networks. This feature applies to the database server for your single databases and elastic pool in SQL Database or your databases in SQL Data Warehouse.
Azure Database for PostgreSQL and Azure Database for MySQL	Generally available in Azure regions where database service is available	Virtual network service endpoints and rules extend the private address space of a virtual network to your Azure Database for PostgreSQL server and Azure Database for MySQL server.
Azure Cosmos DB	Generally available in all Azure regions	You can configure the Azure Cosmos DB account to allow access only from a specific subnet of virtual network. Enable service endpoints to access Azure Cosmos DB on the subnet within a virtual network. Traffic from the subnet is sent to Azure Cosmos DB with the identity of the subnet and virtual network. After the Azure Cosmos DB service endpoint is enabled, you can limit access to the subnet by adding it to your Azure Cosmos DB account.
Azure Key Vault	Generally available in all Azure regions	The virtual network service endpoints for Key Vault allow you to restrict access to a specified virtual network. The endpoints also allow you to restrict access to a list of IPv4 (internet protocol version 4) address ranges. Any user connecting to your key vault from outside those sources is denied access.
Azure Service Bus and Azure Event Hubs	Generally available in all Azure regions	The integration of Service Bus with virtual network service endpoints enables secure access to messaging capabilities from workloads like virtual machines that are bound to virtual networks. The network traffic path is secured on both ends.



Next unit: Identify private link uses

6- Identify private link uses

Azure Private Link provides private connectivity from a virtual network to Azure platform as a service (PaaS), customer-owned, or Microsoft partner services. It simplifies the network architecture and secures the connection between endpoints in Azure by eliminating data exposure to the public internet.

Things to know about Azure Private Link
Let's examine the characteristics of Azure Private Link and network routing configurations.

Azure Private Link keeps all traffic on the Microsoft global network. There's no public internet access.

Private Link is global and there are no regional restrictions. You can connect privately to services running in other Azure regions.

Services delivered on Azure can be brought into your private virtual network by mapping your network to a private endpoint.

Private Link can privately deliver your own services in your customer's virtual networks.

All traffic to the service can be routed through the private endpoint. No gateways, NAT devices, Azure ExpressRoute or VPN connections, or public IP addresses are required.

The following Illustration demonstrates a network routing configuration with Azure Private Link. The service connects to a network security group (NSG) private endpoint by using Azure SQL Database. This configuration prevents a direct connection.

Diagram that shows a network routing configuration with Azure Private Link as described in the text.

Things to consider when using Azure Private Link
There are many benefits to working with Azure Private Link. Review the following points and consider how you can implement the service for your scenarios.

Consider private connectivity to services on Azure. Connect privately to services running in other Azure regions. Traffic remains on the Microsoft network with no public internet access.

Consider integration with on-premises and peered networks. Access private endpoints over private peering or VPN tunnels from on-premises or peered virtual networks. Microsoft hosts the traffic, so you don't need to set up public peering or use the internet to migrate your workloads to the cloud.

Consider protection against data exfiltration for Azure resources. Map private endpoints to Azure PaaS resources. When there's a security incident within your network, only the mapped resources are accessible. This implementation eliminates the threat of data exfiltration.

Consider services delivered directly to customer virtual networks. Privately consume Azure PaaS, Microsoft partner, and your own services in your virtual networks on Azure. Private Link works across tenants to help unify your experience across services. Send, approve, or reject requests directly without permissions or role-based access controls.

Next unit: Interactive lab simulation

7- Interactive lab simulation

Lab scenario
Your organization is exploring Azure virtual networking capabilities. As the Azure Administrator you've been tasked to implement the following requirements:

Create and configure a virtual network in Azure.
Deploy two virtual machines into different subnets of the virtual network.
Ensure the virtual machines have public IP addresses that won't change over time.
Protect the virtual machine public endpoints from being accessible from the internet.
Ensure internal Azure virtual machines names and IP addresses can be resolved.
Ensure a publicly available domain name can be resolved by external queries.
Architecture diagram
Architecture diagram as explained in the text.

 Note

Tasks 1 - 4 focus on IP addresses and access.

Objectives
 Note

You may find slight differences between the interactive simulation and the Azure environment, but the core concepts and ideas being demonstrated are the same.

Task 1: Create and configure a virtual network in Azure.
Create a virtual network, az104-04-vnet1.
Add two subnets, Subnet0 and Subnet1, to the virtual network.
Task 2: Deploy virtual machines into different subnets of the virtual network.
Review a JSON template that will deploy two virtual machines, VM0 and VM1.
Use Azure PowerShell to deploy the template.
Task 3: Configure private and public IP addresses of Azure virtual machines. Ensure the IP addresses don't change over time.
Associate the VM0 NIC with a static public IP address, az104-04-pip0.
Associate the VM1 NIC with a static public IP address, az104-04-pip1.
Task 4: Configure network security groups. Protect the virtual machine public endpoints from being accessible from the internet.
Verify you can't use RDP to connect to a virtual machine.
Create a network security group.
Configure inbound security rules to allow RDP.
Associate the network security group with the virtual machine NICs.
Confirm that you can now use RDP to connect to a virtual machine.
Task 5: Configure Azure DNS for internal name resolution. Ensure internal Azure virtual machines names and IP addresses can be resolved.
Create a private DNS zone for your organization.
Add a virtual network link to the virtual network.
Verify the virtual machines DNS records are registered.
Verify internal DNS name resolution is working.
Task 6: Configure Azure DNS for external name resolution. Ensure a publicly available domain name can be resolved by external queries.
Create a DNS zone for a publicly available domain name.
Add a DNS record for each virtual machine.
Verify external DNS name resolution is working.
 Note

Select the thumbnail image to start the lab simulation. When you're done, be sure to return to this page so you can continue learning.

Screenshot of the simulation page.

Next unit: Knowledge check

Your company is implementing network virtual appliances, and you're developing the plan to ensure traffic is properly routed to the appropriate appliance. You're working out to implement custom network routes, service endpoints, and private links to ensure the security of your customer data. A few teams have submitted configuration requirements and questions for your consideration:

Your company needs to extend their private address space in Azure by providing a direct connection to Azure resources.

The IT team needs a list of the valid next hop types.

The infrastructure team has requested an overview of Azure routing capabilities and requirements.

Answer the following questions
Choose the best response for each of the following questions. Then select Check your answers.


1. Which statement best describes Azure routing? 

Administrators can create system routes.

When the next hop type is none, traffic is dropped.

Azure gateways are needed to route traffic between subnets.

2. What's a valid next hop type? 

Load Balancer

ExpressRoute

Internet

3. How can you extend the company's private address space with direct connections to Azure resources? 

Virtual network endpoints

User-defined routes

Virtual appliances


Summary and resources

Network routes control the flow of traffic through your network. You can customize these routes, implement service endpoints, and work with private links.

In this module, you learned how to implement system routes and user-defined routes. You identified features and usage cases for Azure Private Link and endpoint services. You explored how to configure a custom route, and discovered how to work with service endpoints.

Learn more
Peruse virtual network traffic routing documentation.

Read about Virtual network traffic routing.

Route network traffic with a route table by using the Azure portal.

Configure BGP for Azure VPN Gateway by using PowerShell.

Create custom routes.

View all routes for a subnet and diagnose virtual machine routing problems.

Determine the next hop type between a virtual machine and a destination IP address.

Explore user-defined routes and forced-tunneling in Azure Firewall.

Learn more with self-paced training
Complete an introduction to Azure Private Link.
Learn more with optional hands-on exercises
Manage and control traffic flow in your Azure deployment with routes (sandbox).





Point 5: Configure Azure Load Balancer

Learn how to configure an internal or public load balancer.

Learning objectives
In this module, you learn how to:

Identify features and usage cases for Azure Load Balancer.

Implement public and internal Azure load balancers.

Compare features of load balancer SKUs and configuration differences.

Configure back-end pools, load-balancing rules, session persistence, and health probes.


1- Introduction

Many applications need to be resilient to failure and scale easily when demand increases. Administrators can address these requirements by using Azure Load Balancer.

Suppose your healthcare organization is launching a new portal application for patients to schedule appointments. The application has a patient portal, web application frontend, and business tier database. The database is used by the frontend to retrieve and save patient information.

The new portal needs to be available around the clock to handle failures. The portal must adjust to fluctuations in load by adding and removing resources to match the load. You need a solution to distribute work to virtual machines across the system as virtual machines are added. The solution should detect failures and reroute jobs to virtual machines as needed. Improved resiliency and scalability are required to help ensure patients can schedule appointments from any location.

You're responsible for configuring the load balancers to distribute incoming network traffic across a group of back-end servers. You need to scale your applications while maintaining throughput and keeping response times low.

The goal of this module is to equip you to implement an Azure load balancer.

Learning objectives
In this module, you learn how to:

Identify features and usage cases for Azure Load Balancer.
Implement public and internal Azure load balancers.
Compare features of load balancer SKUs and configuration differences.
Configure back-end pools, load-balancing rules, session persistence, and health probes.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Basic knowledge of virtual networks and routing.
Familiarity with the Azure portal so you can configure the load balancer.


Next unit: Determine Azure Load Balancer uses

2- Determine Azure Load Balancer uses

Azure Load Balancer delivers high availability and network performance to your applications. Administrators use load balancing to efficiently distribute incoming network traffic across back-end servers and resources. A load balancer is implemented by using load-balancing rules and health probes.

The following diagram shows how Azure Load Balancer works. The frontend exchanges information with a load balancer. The load balancer uses rules and health probes to communicate with the backend.

Diagram that shows how a load balancer works as described in the text.

Things to know about Azure Load Balancer
Let's take a closer look at how Azure Load Balancer operates.

Azure Load Balancer can be used for inbound and outbound scenarios.

You can implement a public or internal load balancer, or use both types in a combination configuration.

To implement a load balancer, you configure four components:

Front-end IP configuration
Back-end pools
Health probes
Load-balancing rules
The front-end configuration specifies the public IP or internal IP that your load balancer responds to.

The back-end pools are your services and resources, including Azure Virtual Machines or instances in Azure Virtual Machine Scale Sets.

Load-balancing rules determine how traffic is distributed to back-end resources.

Health probes ensure the resources in the backend are healthy.

Load Balancer scales up to millions of TCP and UDP application flows.

Next unit: Implement a public load balancer


3- Implement a public load balancer

Administrators use public load balancers to map the public IP addresses and port numbers of incoming traffic to the private IP addresses and port numbers of virtual machines. The mapping can also be configured for response traffic from the virtual machines.

Load-balancing rules are used to specify how to distribute specific types of traffic across multiple virtual machines or services. You can use this approach to share the load of incoming web request traffic across multiple web servers.

Business scenario
Consider a scenario where internet traffic attempts to reach virtual machines in a web tier subnet that implements a public load balancer. Internet clients send webpage requests to the public IP address of a web app on TCP port 80. Azure Load Balancer intercepts the traffic and distributes the requests across the virtual machines in the load-balanced set according to the defined load-balancing rules. The following illustration highlights this scenario:

Diagram showing how a public load balancer works as described in the text.

Next unit: Implement an internal load balancer


4- Implement an internal load balancer

Administrators use internal load balancers to direct traffic to resources that reside in a virtual network, or to resources that use a VPN to access Azure infrastructure. In this configuration, front-end IP addresses and virtual networks are never directly exposed to an internet endpoint. Internal line-of-business applications run in Azure and are accessed from within Azure or from on-premises resources.

Business scenario
Suppose you have an Azure SQL Database tier subnet with several virtual machines, and you implement an internal load balancer. Database requests need to be distributed to the backend. The internal load balancer receives the database requests and uses the load-balancing rules to determine how to distribute the requests to the back-end SQL servers. The SQL servers respond on port 1433. The following illustration highlights this scenario:

Diagram showing how an internal load balancer works as described in the text.

Things to consider when using an internal load balancer
You can implement an internal load balancer to achieve several types of load balancing.

Within virtual network: Establish load balancing from your virtual machines in the virtual network to a set of virtual machines that reside within the same virtual network.

For cross-premises virtual network: Apply load balancing from your on-premises computers to a set of virtual machines that reside within the same virtual network.

For multi-tier applications: Implement load balancing for your internet-facing multi-tier applications when the back-end tiers aren't internet-facing. The back-end tiers require traffic load-balancing from the internet-facing tier.

For line-of-business applications: Add load balancing for your line-of-business applications hosted in Azure without having to add other load balancer hardware or software. This scenario includes on-premises servers that are in the set of computers whose traffic is load-balanced.

With public load balancer: Configure a public load balancer in front of your internal load balancer to create a multi-tier application.


Next unit: Determine load balancer SKUs

5- Determine load balancer SKUs

When you create an Azure load balancer in the Azure portal, you select the type of load balancer to create (internal or public) and the Stock Keeping Unit (SKU). Azure Load Balancer supports three SKU options: Basic, Standard, and Gateway. Each SKU provides different features, scenario scaling, and pricing.

Screenshot that shows how to create an Azure load balancer in the Azure portal.

Things to know about Azure Load Balancer SKUs
Let's review some points to consider when choosing the SKU type for your load balancer.

Standard Load Balancer is the newest product. It's essentially a superset of Basic Load Balancer.

The Standard SKU offers an expanded and more granular feature set than the Basic SKU.

The Basic SKU can be upgraded to the Standard SKU. But, new designs and architectures should use the Standard SKU.

The Gateway SKU supports high performance and high availability scenarios with third-party network virtual appliances (NVAs).

Compare Basic and Standard SKU features
The following table provides a brief comparison of how features are implemented in the Standard and Basic SKUs.

Feature	Basic SKU	Standard SKU
Health probes	HTTP, TCP	HTTPS, HTTP, TCP
Availability zones	Not available	Zone-redundant and zonal frontends for inbound and outbound traffic
Multiple frontends	Inbound only	Inbound and outbound
Security	- Open by default
- (Optional) Control through network security groups (NSGs)	- Closed to inbound flows unless allowed by an NSG
- Internal traffic from the virtual network to the internal load balancer is allowed


Next unit: Create back-end pools


6- Create back-end pools

Each load balancer has one or more back-end pools that are used for distributing traffic. The back-end pools contain the IP addresses of the virtual NICs that are connected to your load balancer. You configure these pool settings in the Azure portal.

Screenshot that shows how to configure back-end pools in the Azure portal.

Things to know about back-end pools
The SKU type that you select determines which endpoint configurations are supported for the pool along with the number of pool instances allowed.

The Basic SKU allows up to 300 pools, and the Standard SKU allows up to 1,000 pools.

When you configure the back-end pools, you can connect to availability sets, virtual machines, or Azure Virtual Machine Scale Sets.

For the Basic SKU, you can select virtual machines in a single availability set or virtual machines in an instance of Azure Virtual Machine Scale Sets.

For the Standard SKU, you can select virtual machines or Virtual Machine Scale Sets in a single virtual network. Your configuration can include a combination of virtual machines, availability sets, and Virtual Machine Scale Sets.

Next unit: Create health probes

7- Create health probes

A health probe allows your load balancer to monitor the status of your application. The probe dynamically adds or removes virtual machines from your load balancer rotation based on the machine response to health checks. When a probe fails to respond, the load balancer stops sending new connections to the unhealthy instance.

The following image shows how to create a health probe in the Azure portal. A custom HTTP health probe is configured to run on TCP port 80. The probe is defined to check the health of the virtual machine instances at 5-second intervals.

Screenshot that shows how to create a health probe in the Azure portal.

Things to know about health probes
There are two main ways to configure a custom health probe: HTTP and TCP.

In an HTTP probe, the load balancer probes your back-end pool endpoints every 15 seconds. A virtual machine instance is considered healthy if it responds with an HTTP 200 message within the specified timeout period (default is 31 seconds). If any status other than HTTP 200 is returned, the instance is considered unhealthy, and the probe fails.

A TCP probe relies on establishing a successful TCP session to a defined probe port. If the specified listener on the virtual machine exists, the probe succeeds. If the connection is refused, the probe fails.

To configure a probe, you specify values for the following settings:

Port: Back-end port
URI: URI for requesting the health status from the backend
Interval: Amount of time between probe attempts (default is 15 seconds)
Unhealthy threshold: Number of failures that must occur for the instance to be considered unhealthy
A Guest agent probe is a third option that uses the guest agent inside the virtual machine. This option isn't recommended when an HTTP or TCP custom probe configuration is possible.

Next unit: Create load balancer rules


8- Create load balancer rules

You can define load-balancing rules to specify how traffic is distributed to your back-end pools. Each rule maps a front-end IP address and port combination to a set of back-end IP address and port combinations.

Screenshot that shows how to create load-balancing rules in the Azure portal.

Things to know about load-balancing rules
Let's take a closer look at how to configure load-balancing rules for your back-end pools.

To configure a load-balancing rule, you need to have a frontend, backend, and health probe for your load balancer.

To define a rule in the Azure portal, you configure several settings:

IP version (IPv4 or IPv6)
Front-end IP address, *Port, and Protocol (TCP or UDP)
Back-end pool and Back-end port
Health probe
Session persistence
By default, Azure Load Balancer distributes network traffic equally among multiple virtual machines.

Azure Load Balancer uses a five-tuple hash to map traffic to available servers. The tuple consists of the source IP address, source port, destination IP address, destination port, and protocol type. The load balancer provides stickiness only within a transport session.

Session persistence specifies how to handle traffic from a client. By default, successive requests from a client go to any virtual machine in your pool.

You can modify the session persistence behavior as follows:

None (default): Any virtual machine can handle the request.
Client IP: Successive requests from the same client IP address go to the same virtual machine.
Client IP and protocol: Successive requests from the same client IP address and protocol combination go to the same virtual machine.
 Note

Maintaining session persistence information is important for applications that implement a shopping cart. Can you think of other applications that might benefit from session persistence?

Load-balancing rules can be used in combination with NAT rules.

Consider a scenario where you use NAT from a load balancer's public address to TCP port 3389 on a specific virtual machine. By combining your NAT rule with load-balancing rules, you can enable remote desktop access from outside of Azure.

Next unit: Interactive lab simulation

9- Interactive lab simulation


Lab scenario
Your organization is migrating hub and spoke network topologies to Azure. As the Azure Administrator you need to:

Replicate the on-premises functionality in Azure.
Configure virtual network peering and traffic routing.
Implement load balancer and application gateway functionality.
Test to ensure traffic management is flowing as intended.
Architecture diagram
Architecture diagram as explained in the text.

Objectives
Task 1: Provision the lab environment. In this task, you deploy four virtual machines into the same Azure region. The first two reside in a hub virtual network, while the remaining two reside in a separate spoke virtual network.
Review an Azure Resource Manager template.
This template includes the virtual machines and virtual networks in the underlying architecture.
Use Azure PowerShell to install the Network Watcher extension on the Azure virtual machines.
Task 2: Configure the hub and spoke network topology. In this task, you configure local peering between the virtual networks you deployed in the previous tasks in order to create a hub and spoke network topology.
Configure virtual network peering between the virtual networks.
Ensure forwarded traffic is allowed to facilitate routing between spoke virtual networks.
Task 3: Test transitivity of virtual network peering. In this task, you test transitivity of virtual network peering by using Network Watcher.
Use Network Watcher to verify peered networks are reachable.
Use Network Watcher to verify unpeered networks are unreachable.
Task 4: Configure routing in the hub and spoke topology. In this task, you configure and test routing between the two spoke virtual networks.
Enable IP forwarding on a virtual machine.
Install the remote access Windows feature with associated tools.
Create routing tables and associate them with the appropriate subnets.
Use Network Watcher to verify traffic routed through the virtual machine.
Task 5: Implement Azure Load Balancer. In this task, you implement an Azure load balancer in front of the two Azure virtual machines in the hub virtual network.
Create a load balancer with a public IP address.
Create a back-end pool that includes the virtual machines.
Add a load balancing rule to alternate between virtual machines in the back-end pool.
Test to confirm that the load balancer is working correctly.
Task 6: Implement Azure Application Gateway. In this task, you implement an Azure application gateway in front of the two Azure virtual machines in the spoke virtual networks.
Create a dedicated subnet for the application gateway.
Create an application gateway with a public IP address.
Configure the application gateway back-end pool to include the virtual machines.
Test to ensure traffic is balanced across the back-end virtual machines.
 Note

Select the thumbnail image to start the lab simulation. When you're done, be sure to return to this page so you can continue learning.

Screenshot of the simulation page.

Next unit: Knowledge check

You're configuring load balancers to distribute incoming network traffic across a group of back-end resources and virtual machines. Your solution must scale your applications while maintaining throughput and keeping response times low. A few teams submitted configuration requirements and questions for your consideration:

Your company wants to provide customers with a virtual network in the cloud.

The IT team is interested in default traffic distribution for load balancing, and has questions about hashes and affinities.

You're investigating configuration options for internal and external load balancers.

Answer the following questions
Choose the best response for each of the following questions. Then select Check your answers.


1. What's the default distribution type for traffic through a load balancer? 

Source IP affinity

Three-tuple hash

Five-tuple hash

2. Which configuration is required for an internal load balancer? 

Virtual machines must be in the same virtual network.

Virtual machines must be publicly accessible.

Virtual machines must be in an availability set.


Summary and resources

In this module, you learned about Azure Load Balancer and its features. Azure Load Balancer distributed workloads and network traffic across virtual machines, making applications more resilient and scalable. You learned about load balancer SKUs, back-end pools, load-balancing rules, session persistence, and health probes.

The main takeaways from this module are:

Azure Load Balancer helps distribute network traffic across servers and resources.

Load balancing can be used for inbound and outbound scenarios.

There are public and internal load balancers.

Load-balancing rules specify how traffic is distributed to your back-end pools.

Back-end pools contain the IP addresses of the virtual NICs that are connected to your load balancer.

Health probes dynamically add or remove virtual machines based on virtual machine health checks.

Learn more with documentation
Azure Load Balancer documentation. This collection of articles is your starting point for all things load balancer.

Create a public load balancer for virtual machines in the Azure portal. This article reviews creating a public load balancer for a backend pool with two virtual machines.

Learn more with self-paced training
Introduction to Azure Load Balancer. Learn what Azure Load Balancer does, how it works, and when you should choose to use Load Balancer as a solution

Improve application scalability and resiliency by using Azure Load Balancer (sandbox). Learn about the different load balancers in Azure and how to choose the right Azure load balancer solution.

Load balance non-HTTP(S) traffic in Azure. Learn the different load balancer options in Azure and how to choose and implement the right Azure solution for non-HTTP(S) traffic.





Point 6: Configure Azure Application Gateway

Learn how to configure Azure Application Gateway.

Learning objectives
In this module, you learn how to:

Identify features and usage cases for Azure Application Gateway.

Implement an Azure application gateway, including selecting a routing method.

Configure gateway components, such as listeners, health probes, and routing rules.


1- Introduction

Azure Application Gateway is a load balancer for web traffic. Administrators implement an application gateway to manage traffic to their web apps.

Suppose you work for the motor vehicle department of a governmental organization. The department runs several public websites that enable drivers to register their vehicles, and renew their licenses online. The vehicle registration website has been running on a single server, and has suffered multiple outages because of server failures. The outages have resulted in frustrated drivers trying to register their vehicles before their registrations expire.

You're responsible for improving the resiliency of the site by adding multiple web servers to distribute the load. You want to centralize the site on a single load-balancing service, and simplify the URLs for site visitors. You're researching how to implement Azure Application Gateway.

Learning objectives
In this module, you learn how to:

Identify features and usage cases for Azure Application Gateway.
Implement an Azure application gateway, including selecting a routing method.
Configure gateway components, such as listeners, health probes, and routing rules.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator. The module concepts are covered in:

Configure and manage virtual networking (25–30%)

Configure load balancing.
Configure Azure Application Gateway.
Prerequisites
None.


Next unit: Implement Azure Application Gateway

2- Implement Azure Application Gateway

Administrators use Azure Application Gateway to manage requests from client applications to their web apps. An application gateway listens for incoming traffic to web apps and checks for messages sent via protocols like HTTP. Gateway rules direct the traffic to resources in a back-end pool.

Business scenario
Consider a scenario where internet client applications request access to resources in a load-balanced back-end pool. The requests can be managed by implementing Azure Application Gateway to listen for HTTP(S) messages. Messages can be handled by load-balancing rules to direct client request traffic to the appropriate resources in the pool. The following diagram illustrates this scenario:

Diagram that illustrates how Azure Application Gateway manages requests from client applications to resources in a back-end pool, as described in the text.

Things to know about Azure Application Gateway
Let's examine some of the benefits of using Azure Application Gateway to manage internet traffic to your web applications.

Benefit	Description
Application layer routing	Use application layer routing to direct traffic to a back-end pool of web servers based on the URL of a request. The back-end pool can include Azure virtual machines, Azure Virtual Machine Scale Sets, Azure App Service, and even on-premises servers.
Round-robin load balancing	Employ round-robin load balancing to distribute incoming traffic across multiple servers. Send load-balance requests to the servers in each back-end pool. Client requests are forwarded in a cycle through a group of servers to create an effective balance for the server load.
Session stickiness	Apply session stickiness to your application gateway to ensure client requests in the same session are routed to the same back-end server.
Supported protocols	Build an application gateway to support the HTTP, HTTPS, HTTP/2, or WebSocket protocols.
Firewall protection	Implement a web application firewall to protect against web application vulnerabilities.
Encryption	Support end-to-end request encryption for your web applications.
Load autoscaling	Dynamically adjust capacity as your web traffic load changes.


Next unit: Determine Azure Application Gateway routing

3- Determine Azure Application Gateway routing

Clients send requests to your web apps by specifying the IP address or DNS name of your application gateway. Your gateway directs the requests to a selected web server in your back-end pool according to a set of rules. You define the rules for your gateway to identify the allowable routes for the request traffic.

Things to know about traffic routing
Let's take a closer look at your routing options for Azure Application Gateway.

Azure Application Gateway offers two primary methods for routing traffic:

Path-based routing sends requests with different URL paths to different pools of back-end servers.

Multi-site routing configures more than one web application on the same application gateway instance.

You can configure your application gateway to redirect traffic.

Application Gateway can redirect traffic received at one listener to another listener, or to an external site. This approach is commonly used by web apps to automatically redirect HTTP requests to communicate via HTTPS. The redirection ensures all communication between your web app and clients occurs over an encrypted path.

You can implement Application Gateway to rewrite HTTP headers.

HTTP headers allow the client and server to pass parameter information with the request or the response. In this scenario, you can translate URLs or query string parameters, and modify request and response headers. Add conditions to ensure URLs or headers are rewritten only for certain conditions.

Application Gateway allows you to create custom error pages instead of displaying default error pages. You can use your own branding and layout by using a custom error page.

Path-based routing
You can implement path-based routing to direct requests for specific URL paths to the appropriate back-end pool. Consider a scenario where your web app receives requests for videos or images. You can use path-based routing to direct requests for the /video/\* path to a back-end pool of servers that are optimized to handle video streaming. Image requests for the /images/\* path can be directed to a pool of servers that handle image retrieval. The following illustration demonstrates this routing method:

Diagram that shows a path-based routing approach.

Multi-site routing
When you need to support multiple web apps on the same application gateway instance, multi-site routing is the best option. Multi-site configurations are useful for supporting multi-tenant applications, where each tenant has its own set of virtual machines or other resources hosting a web application.

In this configuration, you register multiple DNS names (CNAMEs) for the IP address of your application gateway and specify the name of each site. Application Gateway uses separate listeners to wait for requests for each site. Each listener passes the request to a different rule, which can route the requests to servers in a different back-end pool.

Consider a scenario where you need to support traffic to two sites on the same gateway. You can direct all requests for the http://contoso.com site to servers in one back-end pool, and requests for the http://fabrikam.com site to another back-end pool. The following illustration demonstrates this routing method.

Diagram that shows a multiple site routing approach.

Next unit: Configure Azure Application Gateway components


4-  Configure Azure Application Gateway components

Azure Application Gateway has a series of components that combine to route requests to a pool of web servers and to check the health of these web servers. These components include the frontend IP address, back-end pools, routing rules, health probes, and listeners. As an option, the gateway can also implement a firewall.

Things to know about Application Gateway components
Let's explore how the components of an application gateway work together.

The front-end IP address receives the client requests.

An optional Web Application Firewall checks incoming traffic for common threats before the requests reach the listeners.

One or more listeners receive the traffic and route the requests to the back-end pool.

Routing rules define how to analyze the request to direct the request to the appropriate back-end pool.

A back-end pool contains web servers for resources like virtual machines or Virtual Machine Scale Sets. Each pool has a load balancer to distribute the workload across the resources.

Health probes determine which back-end pool servers are available for load-balancing.

The following flowchart demonstrates how the Application Gateway components work together to direct traffic requests between the frontend and back-end pools in your configuration.

Flowchart that demonstrates how Application Gateway components direct traffic requests between the frontend and back-end pools.

Front-end IP address
Client requests are received through your front-end IP address. Your application gateway can have a public or private IP address, or both. You can have only one public IP address and only one private IP address.

Web Application Firewall (optional)
You can enable Azure Web Application Firewall for Azure Application Gateway to handle incoming requests before they reach your listener. The firewall checks each request for threats based on the Open Web Application Security Project (OWASP). Common threats include SQL-injection, cross-site scripting, command injection, HTTP request smuggling and response splitting, and remote file inclusion. Other threats can come from bots, crawlers, scanners, and HTTP protocol violations and anomalies.

OWASP defines a set of generic rules for detecting attacks. These rules are referred to as the Core Rule Set (CRS). The rule sets are under continuous review as attacks evolve in sophistication. Azure Web Application Firewall supports two rule sets: CRS 2.2.9 and CRS 3.0. CRS 3.0 is the default and more recent of these rule sets. If necessary, you can opt to select only specific rules in a rule set to target certain threats. Additionally, you can customize the firewall to specify which elements in a request to examine, and limit the size of messages to prevent massive uploads from overwhelming your servers.

Listeners
Listeners accept traffic arriving on a specified combination of protocol, port, host, and IP address. Each listener routes requests to a back-end pool of servers according to your routing rules. A listener can be Basic or Multi-site. A Basic listener only routes a request based on the path in the URL. A Multi-site listener can also route requests by using the hostname element of the URL. Listeners also handle TLS/SSL certificates for securing your application between the user and Application Gateway.

Routing rules
A routing rule binds your listeners to the back-end pools. A rule specifies how to interpret the hostname and path elements in the URL of a request, and then direct the request to the appropriate back-end pool. A routing rule also has an associated set of HTTP settings. These HTTP settings indicate whether (and how) traffic is encrypted between Application Gateway and the back-end servers. Other configuration information includes protocol, session stickiness, connection draining, request timeout period, and health probes.

Back-end pools
A back-end pool references a collection of web servers. You provide the IP address of each web server and the port on which it listens for requests when configuring the pool. Each pool can specify a fixed set of virtual machines, Virtual Machine Scale Sets, an app hosted by Azure App Services, or a collection of on-premises servers. Each back-end pool has an associated load balancer that distributes work across the pool.

Health probes
Health probes determine which servers in your back-end pool are available for load-balancing. Application Gateway uses a health probe to send a request to a server. When the server returns an HTTP response with a status code between 200 and 399, the server is considered healthy. If you don't configure a health probe, Application Gateway creates a default probe that waits for 30 seconds before identifying a server as unavailable (unhealthy).

Next unit: Knowledge check

Your IT department is adding several web servers to distribute the load on the motor vehicle department's registration website. You're working on the configuration plan to centralize the site by using Azure Application Gateway. A few teams have submitted configuration requirements and questions for your consideration:

The web team is installing the application gateway. They want to ensure incoming requests are checked for common security threats like cross-site scripting and crawlers.

The server team has requested information about how Application Gateway routes requests to web servers.

You need to present a high-level comparison of load-balancing strategies to the executive committee.

Answer the following questions
Choose the best response for each of the following questions. Then select Check your answers.


1. What criteria does Azure Application Gateway use to route requests to a web server? 

The region where the servers hosting the web application are located.

The hostname, port, and path in the URL of the request.

The users authentication information.

2. Which load-balancing strategy does Azure Application Gateway implement? 

Requests are distributed to the server in the back-end pool with the lightest load.

Each server in the back-end pool is polled in turn, and the request is sent to the first server that responds.

Requests are distributed to each available server in a back-end pool in turn via a round-robin technique.

3. How can the concerns about security threats be addressed? 

Install Azure Web Application Firewall.

Install an internal load balancer.

Install Azure Firewall.


Summary and resources

Azure Application Gateway provides load balancing and application routing capabilities across multiple web sites. Several routing methods are available, including multi-site and path-based. Application Gateway also provides Azure Web Application Firewall to supply built-in security features.

In this module, you identified features and usage cases for Azure Application Gateway. You explored Application Gateway components, including listeners, firewalls, health probes, and routing rules. You learned how to implement an application gateway, including selecting the appropriate routing method.

Learn more
Read about Azure Application Gateway.

Examine Application Gateway components.

Discover Application Gateway features.

Read about Azure Web Application Firewall on Application Gateway.

Explore Application Gateway redirection routing.

Configure an application gateway to host multiple web sites.

Rewrite HTTP headers and URL with Application Gateway.

Learn more with self-paced training
Complete an introduction to Azure Application Gateway.
Learn more with optional hands-on exercises
Load balance HTTP(S) traffic in Azure. Azure subscription required.

Load balance your web service traffic with Azure Application Gateway. Azure subscription required.

Encrypt network traffic end-to-end with Azure Application Gateway. Azure subscription required.





Point 7: Design an IP addressing schema for your Azure deployment

A good Azure IP addressing schema provides flexibility, room for growth, and integration with on-premises networks. The schema ensures that communication works for deployed resources, minimizes public exposure of systems, and gives the organization flexibility in its network. If not properly designed, systems might not be able to communicate, and additional work will be required to remediate.

Learning objectives
In this module, you will:

Identify the private IP addressing capabilities of Azure virtual networks.
Identify the public IP addressing capabilities of Azure.
Identify the requirements for IP addressing when integrating with on-premises networks.


1- Introduction

Imagine you're the solution architect for a manufacturing company. Your company is beginning a project to move many services out of its existing datacenter and into the Azure cloud. The company wants to integrate the existing network with Azure. You need to plan the public and private IP addresses for the network carefully so you don’t run out of addresses and have capacity for future growth. A good IP addressing scheme provides flexibility, room for growth, and integration with on-premises networks.

In this module, you learn about the public and private IP addressing capabilities of Azure virtual networks. Also, you learn how to gather the necessary requirements for planning an IP address scheme. This module covers the on-premises integration methods of point-to-site and site-to-site, and also virtual network-to-virtual network peering. You'll also design and implement virtual networks and configure and verify virtual network peering. By the end of this module, you'll understand how to plan IP addressing for an Azure network and how to integrate Azure with an on-premises network.

Learning objectives
In this module, you'll:

Identify the private IP addressing capabilities of Azure virtual networks.
Identify the public IP addressing capabilities of Azure.
Identify the requirements for IP addressing when integrating with on-premises networks.
Prerequisites
Knowledge of basic networking concepts, network subnets, and IP addressing
Familiarity with Azure virtual networking


Next unit: Network IP addressing and integration

2- Network IP addressing and integration

To integrate resources in an Azure virtual network with resources in your on-premises network, you must understand how you can connect those resources and how to configure IP addresses.

Your manufacturing company wants to migrate a business-critical database to Azure. Client applications on desktop computers, laptops, and mobile devices need constant access to the database as if the database remained in the on-premises network. You want to move the database server without affecting users.

In this unit, you look at a typical on-premises network design and compare it to a typical Azure network design. You'll also learn about requirements for IP addressing when integrating an Azure network with on-premises networks.

On-premises IP addressing
A typical on-premises network design includes these components:

Routers
Firewalls
Switches
Network segmentation
Diagram of a typical on-premises network design.

The preceding diagram shows a simplified version of a typical on-premises network. On the routers facing the internet service provider (ISP), you have public IP addresses that your outbound internet traffic uses as their source. These addresses also are used for inbound traffic across the internet. The ISP might issue you a block of IP addresses to assign to your devices, or you might have your own block of public IP addresses that your organization owns and controls. You can assign these addresses to systems that you would like to make accessible from the internet, such as web servers.

The perimeter network and internal zone have private IP addresses. In the perimeter network and internal zone, the IP addresses that are assigned to these devices aren't accessible over the internet. The administrator has full control over the IP address assignment, name resolution, security settings, and security rules. There are three ranges of nonroutable IP addresses that are designed for internal networks that won't be sent over internet routers:

10.0.0.0 to 10.255.255.255
172.16.0.0 to 172.31.255.255
192.168.0.0 to 192.168.255.255
The administrator can add or remove on-premises subnets to accommodate network devices and services. The number of subnets and IP addresses you can have in your on-premises network depends on the Classless Inter-Domain Routing (CIDR) for the IP address block.

Azure IP addressing
Azure virtual networks use private IP addresses. The ranges of private IP addresses are the same as for on-premises IP addressing. Like on-premises networks, the administrator has full control over the IP address assignment, name resolution, security settings, and security rules in an Azure virtual network. The administrator can add or remove subnets depending on the CIDR for the IP address block.

A typical Azure network design usually has these components:

Virtual networks
Subnets
Network security groups
Firewalls
Load balancers
Diagram of a typical Azure network design.

In Azure, the network design has features and functions that are similar to an on-premises network, but the network's structure is different. The Azure network doesn't follow the typical on-premises hierarchical network design. The Azure network allows you to scale up and scale down infrastructure based on demand. Provisioning in the Azure network happens in a matter of seconds. There are no hardware devices like routers or switches. The entire infrastructure is virtual, and you can slice it into chunks that suit your requirements.

In Azure, you'd typically implement a network security group and a firewall. You'd use subnets to isolate front-end services, including web servers and DNS, and back-end services like databases and storage systems. Network security groups filter internal and external traffic at the network layer. A firewall has more extensive capabilities for network-layer filtering and application-layer filtering. By deploying both network security groups and a firewall, you get improved isolation of resources for a secure network architecture.

Basic properties of Azure virtual networks
A virtual network is your network in the cloud. You can divide your virtual network into multiple subnets. Each subnet contains a portion of the IP-address space assigned to your virtual network. You can add, remove, expand, or shrink a subnet if there are no VMs or services deployed in it.

By default, all subnets in an Azure virtual network can communicate with each other. However, you can use a network security group to deny communication between subnets. Regarding sizing, the smallest supported subnet uses a /29 subnet mask, and the largest supported subnet uses a /2 subnet mask. The smallest subnet has eight IP addresses, and the largest subnet has 1,073,741,824 IP addresses.

Integrate Azure with on-premises networks
Before you start integrating Azure with on-premises networks, it's important to identify the current private IP address scheme the on-premises network uses. There can be no IP address overlap for interconnected networks.

For example, you can't use 192.168.0.0/16 on your on-premises network and use 192.168.10.0/24 on your Azure virtual network. These ranges both contain the same IP addresses so traffic can't be routed between them.

You can, however, have the same class range for multiple networks. For example, you can use the 10.10.0.0/16 address space for your on-premises network and the 10.20.0.0/16 address space for your Azure network because they don't overlap.

It's vital to check for overlaps when you're planning an IP address scheme. If there's an overlap of IP addresses, you can't integrate your on-premises network with your Azure network.

Check your knowledge

1. What are some of the typical components involved in a network design? 

A dedicated leased line

A virtual network, subnet, firewall, and load balancer

A dedicated network security group

2. Which of the following IP address ranges is routable over the internet? 

10.0.0.0 to 10.255.255.255

215.11.0.0 to 215.11.255.255

172.16.0.0 to 172.31.255.255

192.168.0.1 to 192.168.255.255


3- Public and private IP addressing in Azure

 You work for a manufacturing company, and you're moving resources into Azure. The database server must be accessible for clients in your on-premises network. Public resources—like web servers—must be accessible from the internet. You want to ensure that you plan IP addresses that support both these requirements.

In this unit, you explore the constraints and limitations for public and private IP addresses in Azure. Also, you look at the capabilities that are available in Azure to reassign IP addresses in your network.

IP address types
In Azure, you can use two types of IP addresses:

Public IP addresses
Private IP addresses
You can allocate both types of IP addresses in one of two ways:

Dynamic
Static
Let's take a closer look at how the IP address types work together.

Public IP addresses
Use a public IP address for public-facing services. A public address can be either static or dynamic. A public IP address can be assigned to a VM, an internet-facing load balancer, a VPN gateway, or an application gateway.

Dynamic public IP addresses are assigned addresses that can change over the lifespan of the Azure resource. The dynamic IP address is allocated when you create or start a VM. The IP address is released when you stop or delete the VM. In each Azure region, public IP addresses are assigned from a unique pool of addresses. The default allocation method is dynamic.

Static public IP addresses are assigned addresses that don't change over the lifespan of the Azure resource. To ensure that the IP address for the resource remains the same, you can set the allocation method to static. In this case, an IP address is assigned immediately, and is released only when you delete the resource or change the IP allocation method to dynamic.

SKUs for public IP addresses
For public IP addresses, there are two SKUs from which to choose: Basic and Standard. All public IP addresses created before the introduction of SKUs are Basic SKU public IP addresses. With the introduction of SKUs, you can choose the scale, features, and pricing for load balancing internet traffic.

Both Basic and Standard SKUs have:

A default inbound originated flow idle timeout of four minutes, which is adjustable to up to 30 minutes.
A fixed outbound originated flow idle timeout of four minutes.
Basic SKU
You can assign Basic public IPs by using static or dynamic allocation methods. You can assign Basic public IPs to any Azure resource that can be assigned a public IP address, including network interfaces, VPN gateways, application gateways, and internet-facing load balancers.

By default, Basic SKU IP addresses:

Are open. Network security groups are recommended, but optional, for restricting inbound or outbound traffic.
Are available for inbound only traffic.
Are available when using instance meta data service (IMDS).
Don't support Availability Zones.
Don't support routing preferences.
Standard SKU
By default, Standard SKU IP addresses:

Always use static allocation.
Are secure, and thus closed to inbound traffic. You must enable inbound traffic by using a network security group.
Are zone-redundant and optionally zonal (they can be created as zonal and guaranteed in a specific availability zone).
Can be assigned to network interfaces, Standard public load balancers, application gateways, or VPN gateways.
Can be utilized with the routing preference to enable more granular control of how traffic is routed between Azure and the Internet.
Can be used as anycast frontend IPs for cross-region load balancers.
For more information, see SKU comparison, Load Balancer overview, and components.

Public IP address prefix
In Azure, a public IP address prefix is a reserved, static range of public IP addresses. Azure assigns an IP address from a pool of available addresses that's unique to each region in each Azure cloud. When you define a Public IP address prefix, associated public IP addresses are assigned from a pool for an Azure region.

In a region with Availability Zones, Public IP address prefixes can be created as zone-redundant or associated with a specific availability zone.

The benefit of a public IP address prefix is that you can specify firewall rules for a known range of IP addresses. If your business needs to have datacenters in different regions, you need a different public IP address range for each region. You can assign the addresses from a public IP address prefix to any Azure resource that supports public IP addresses.

You can create a public IP address prefix by specifying a name and prefix size. The prefix size is the number of reserved addresses available for use.

Public IP address prefixes consist of IPv4 or IPv6 addresses.
You can use technology like Azure Traffic Manager to balance region-specific instances.
You can only bring your own public IP addresses from on-premises networks into Azure by using a Custom IP address prefix.
You can't specify addresses when you create a prefix; Azure assigns them. After a prefix is created, the IP addresses are fixed in a contiguous range.
Public IP addresses can't be moved between regions; all IP addresses are region-specific.
Private IP addresses
Private IP addresses are used for communication within an Azure Virtual Network, including virtual networks and your on-premises networks. You can set private IP addresses to dynamic (DHCP lease) or static (DHCP reservation).

Dynamic private IP addresses are assigned through a DHCP lease and can change over the lifespan of the Azure resource.

Static private IP addresses are assigned through a DHCP reservation and don't change throughout the lifespan of the Azure resource. Static private IP addresses persist if a resource is stopped or deallocated.

IP addressing for Azure virtual networks
In Azure, a virtual network is a fundamental component that acts as an organization's network. The administrator has full control over IP address assignment, security settings, and security rules. When you create a virtual network, you define a scope of IP addresses. Private IP addressing works the same way as it does in an on-premises network. You choose the private IP addresses that the Internet Assigned Numbers Authority (IANA) reserves based on your network requirements:

10.0.0.0/8
172.16.0.0/12
192.168.0.0/16
A subnet is a range of IP address within the virtual network. You can divide a virtual network into multiple subnets. Each subnet must have a unique address range, which is specified in classless interdomain routing (CIDR) format. CIDR is a way to represent a block of network IP addresses. An IPv4 CIDR, specified as part of the IP address, shows the length of the network prefix.

Consider, for example, CIDR 192.168.10.0/24. "192.168.10.0" is the network address, and "24" indicates that the first 24 bits are part of the network address, leaving the last 8 bits for specific host addresses. A subnet's address range can't overlap with other subnets in the virtual network or with the on-premises network.

For all subnets in Azure, the first three IP addresses are reserved by default. For protocol conformance, the first and last IP addresses of all subnets also are reserved. In Azure, an internal DHCP service assigns and maintains the lease of IP addresses. The .1, .2, .3, and last IP addresses aren't visible or configurable by the Azure customer. These addresses are reserved and used by internal Azure services.

In Azure virtual networks, IP addresses can be allocated to the following types of resources:

Virtual machine network interfaces
Load balancers
Application gateways
Check your knowledge

1. Which of the following resources can you assign a public IP address to? 

A virtual machine

Azure Data Lake

Azure Key Vault

2. What must a virtual machine have to communicate with the other resources in the same virtual network? 

Load balancer

Network security group

Network interface


4- Plan IP addressing for your networks

In your manufacturing company, you asked the operations and engineering teams about their requirements for the number of virtual machines in Azure. Also, you asked them about their plans for expansion. Based on the results of this survey, you want to plan an IP addressing scheme that you won't have to change in the foreseeable future.

In this unit, you explore the requirements for a network IP address scheme. You learn about classless inter-domain routing (CIDR) and how you can use it to slice an IP block to meet your addressing needs. In the next unit, there's an exercise that shows how to plan IP addressing for Azure virtual networks.

Gather your requirements
Before planning your network IP address scheme, you must gather the requirements for your infrastructure. These requirements also will help you prepare for future growth by reserving extra IP addresses and subnets.

Here are two of the questions you might ask to discover the requirements:

How many devices do you have on the network?
How many devices are you planning to add to the network in the future?
When your network expands, you don't want to redesign the IP address scheme. Here are some other questions you could ask:

Based on the services running on the infrastructure, what devices do you need to separate?
How many subnets do you need?
How many devices per subnet do you have?
How many devices are you planning to add to the subnets in future?
Are all subnets going to be the same size?
How many subnets do you want or plan to add in future?
You need to isolate some services. Isolating services provides another layer of security, but also requires good planning. For example, public devices can access your front-end servers, but the back-end servers need to be isolated. Subnets help isolate the network in Azure. However, all subnets within a virtual network can communicate with each other in Azure by default. To provide further isolation, you can use a network security group. You might isolate services depending on the data and its security requirements. For example, you might choose to isolate HR data and the company's financial data from customer databases.

When you know the requirements, you have a greater understanding of the total number of devices on the network per subnet and how many subnets you need. CIDR allows more flexible allocation of IP addresses than was possible with the original system of IP address classes. Depending on your requirements, you determine the required subnets and hosts out of the block of IP Addresses.

Remember that Azure uses the first three addresses on each subnet. The subnets' first and last IP addresses also are reserved for protocol conformance. Therefore, the number of possible addresses on an Azure subnet is (2^n)-5, where n represents the number of host bits.


Next unit: Exercise - Design and implement IP addressing for Azure virtual networks

5- Exercise - Design and implement IP addressing for Azure virtual networks

Now, you're ready to create and deploy some virtual networks with the IP addresses based on your design.

In this unit, you deployed three virtual networks and subnets to support resources in those virtual networks.

The CoreServicesVnet virtual network is deployed in the US West region. This virtual network has the largest number of resources. It has connectivity to on-premises networks through a VPN connection. This network has web services, databases, and other systems that are key to business operations. Shared services, such as domain controllers and DNS, are located here as well. A large amount of growth is anticipated, so a large address space is necessary for this virtual network.

The ManufacturingVnet virtual network is deployed in the North Europe region, near the location of your organization's manufacturing facilities. This virtual network contains systems for the manufacturing facilities' operations. The organization is anticipating a large number of internal connected devices from which their systems retrieve data (such as temperature) and need an IP address space for expansion.

The ResearchVnet virtual network is deployed in the West India region, near the location of the organization's research and development team that uses this virtual network. The team has a small, stable set of resources with no expectation of future growth. The team needs a few IP addresses for a few virtual machines for their work.

A diagram of virtual networks that you need to create.

You create the following resources:

Virtual network	Region	Virtual network address space	Subnet	Subnet address space
CoreServicesVnet	West US	10.20.0.0/16	-	-
GatewaySubnet	10.20.0.0/27
SharedServicesSubnet	10.20.10.0/24
DatabaseSubnet	10.20.20.0/24
PublicWebServiceSubnet	10.20.30.0/24
ManufacturingVnet	North Europe	10.30.0.0/16	-	-
ManufacturingSystemSubnet	10.30.10.0/24
SensorSubnet1	10.30.20.0/24
SensorSubnet2	10.30.21.0/24
SensorSubnet3	10.30.22.0/24
ResearchVnet	West India	10.40.40.0/24	-	-
ResearchSystemSubnet	10.40.40.0/24
These virtual networks and subnets are structured in a way that accommodates existing resources, yet allows for projected growth. Let's create these virtual networks and subnets to lay the foundation for our networking infrastructure.

Create the CoreServicesVnet virtual network
In Azure Cloud Shell, run the following command to create the CoreServicesVnet virtual network:

Azure CLI

Copy
az network vnet create \
    --resource-group "[sandbox resource group name]" \
    --name CoreServicesVnet \
    --address-prefixes 10.20.0.0/16 \
    --location westus
Now, let's create the subnets that we need for the planned resources in the virtual network:

Azure CLI

Copy
az network vnet subnet create \
    --resource-group "[sandbox resource group name]" \
    --vnet-name CoreServicesVnet \
    --name GatewaySubnet \
    --address-prefixes 10.20.0.0/27

az network vnet subnet create \
    --resource-group "[sandbox resource group name]" \
    --vnet-name CoreServicesVnet \
    --name SharedServicesSubnet \
    --address-prefixes 10.20.10.0/24

az network vnet subnet create \
    --resource-group "[sandbox resource group name]" \
    --vnet-name CoreServicesVnet \
    --name DatabaseSubnet \
    --address-prefixes 10.20.20.0/24

az network vnet subnet create \
    --resource-group "[sandbox resource group name]" \
    --vnet-name CoreServicesVnet \
    --name PublicWebServiceSubnet \
    --address-prefixes 10.20.30.0/24
Let's take a look at the resources created. Run this command to show all the subnets that we configured:

Azure CLI

Copy
az network vnet subnet list \
    --resource-group "[sandbox resource group name]" \
    --vnet-name CoreServicesVnet \
    --output table
You should see the following subnets listed:

Output

Copy
AddressPrefix    Name                    PrivateEndpointNetworkPolicies    PrivateLinkServiceNetworkPolicies    ProvisioningState    ResourceGroup
---------------  ----------------------  --------------------------------  -----------------------------------  -------------------  -------------------------------------------
10.20.0.0/27     GatewaySubnet           Enabled                           Enabled                              Succeeded            [sandbox resource group name]
10.20.10.0/24    SharedServicesSubnet    Enabled                           Enabled                              Succeeded            [sandbox resource group name]
10.20.20.0/24    DatabaseSubnet          Enabled                           Enabled                              Succeeded            [sandbox resource group name]
10.20.30.0/24    PublicWebServiceSubnet  Enabled                           Enabled                              Succeeded            [sandbox resource group name]
Create the ManufacturingVnet virtual network
In Cloud Shell, run the following command to create the ManufacturingVnet virtual network:

Azure CLI

Copy
az network vnet create \
    --resource-group "[sandbox resource group name]" \
    --name ManufacturingVnet \
    --address-prefixes 10.30.0.0/16 \
    --location northeurope
Now, let's create the subnets that we need for the planned resources in the virtual network:

Azure CLI

Copy
az network vnet subnet create \
    --resource-group "[sandbox resource group name]" \
    --vnet-name ManufacturingVnet \
    --name ManufacturingSystemSubnet \
    --address-prefixes 10.30.10.0/24

az network vnet subnet create \
    --resource-group "[sandbox resource group name]" \
    --vnet-name ManufacturingVnet \
    --name SensorSubnet1 \
    --address-prefixes 10.30.20.0/24

az network vnet subnet create \
    --resource-group "[sandbox resource group name]" \
    --vnet-name ManufacturingVnet \
    --name SensorSubnet2 \
    --address-prefixes 10.30.21.0/24

az network vnet subnet create \
    --resource-group "[sandbox resource group name]" \
    --vnet-name ManufacturingVnet \
    --name SensorSubnet3 \
    --address-prefixes 10.30.22.0/24
Let's take a look at the resources created. Run this command to show all the subnets that we configured:

Azure CLI

Copy
az network vnet subnet list \
    --resource-group "[sandbox resource group name]" \
    --vnet-name ManufacturingVnet \
    --output table
You should see the following subnets listed:

Azure CLI

Copy
AddressPrefix    Name                       PrivateEndpointNetworkPolicies    PrivateLinkServiceNetworkPolicies    ProvisioningState    ResourceGroup
---------------  -------------------------  --------------------------------  -----------------------------------  -------------------  -------------------------------------------
10.30.10.0/24    ManufacturingSystemSubnet  Enabled                           Enabled                              Succeeded            [sandbox resource group name]
10.30.20.0/24    SensorSubnet1              Enabled                           Enabled                              Succeeded            [sandbox resource group name]
10.30.21.0/24    SensorSubnet2              Enabled                           Enabled                              Succeeded            [sandbox resource group name]
10.30.22.0/24    SensorSubnet3              Enabled                           Enabled                              Succeeded            [sandbox resource group name]
Create the ResearchVnet virtual network
In Cloud Shell, run the following command to create the ResearchVnet virtual network:

Azure CLI

Copy
az network vnet create \
    --resource-group "[sandbox resource group name]" \
    --name ResearchVnet \
    --address-prefixes 10.40.40.0/24 \
    --location westindia
Now, let's create the subnets that we need for the planned resources in the virtual network:

Azure CLI

Copy
az network vnet subnet create \
    --resource-group "[sandbox resource group name]" \
    --vnet-name ResearchVnet \
    --name ResearchSystemSubnet \
    --address-prefixes 10.40.40.0/24
Let's take a look at the final virtual network. Run this command to show all the subnets that we configured:

Azure CLI

Copy
az network vnet subnet list \
    --resource-group "[sandbox resource group name]" \
    --vnet-name ResearchVnet \
    --output table
You should see the following subnets listed:

Azure CLI

Copy
AddressPrefix    Name                  PrivateEndpointNetworkPolicies    PrivateLinkServiceNetworkPolicies    ProvisioningState    ResourceGroup
---------------  --------------------  --------------------------------  -----------------------------------  -------------------  -------------------------------------------
10.40.40.0/24    ResearchSystemSubnet  Enabled                           Enabled                              Succeeded            [sandbox resource group name]
With the virtual networks and subnets created, you have the infrastructure on which you can deploy resources.

You can further integrate these networks through virtual network peering and through Azure VPN Gateway to connect to on-premises networks. You can use network security groups to filter traffic and control access within and between virtual networks.

Next unit: Summary

Summary

In this module, you have:

Identified the private and public IP addressing capabilities of Azure virtual networks.
Identified how to integrate on-premises networks with Azure.
Planned an IP address scheme for your Azure infrastructure and created virtual networks.
Now that you understand how to plan IP addressing for Azure networks, you understand the private and public IP addressing capabilities of Azure virtual networks. You can use this information to plan out the IP addressing for your own Azure infrastructure.

Learn more
For more information about IP addressing in Azure, see the following articles:

Public IP addresses
Public IP address prefix





Point 8: Distribute your services across Azure virtual networks and integrate them by using virtual network peering

Use virtual network peering to enable communication across virtual networks in a way that's secure and minimally complex.

Learning objectives
In this module, you will:

Identify use cases for virtual network peering.
Identify the features and limitations of virtual network peering.
Configure peering connections between virtual networks.


1- Introduction

Imagine you're the solution architect for an engineering company that has been migrating services into Azure. The company has deployed services into separate virtual networks, but has yet to configure private connectivity between them.

Several business units have identified services in these virtual networks that need to communicate with each other. You need to enable this connectivity, but you don't want to expose these services to the internet. You also want to keep the integration as simple as possible.

In this module, you'll learn about virtual network connection options and why virtual network peering is suited for this scenario. You'll create three virtual networks and configure virtual network peering between them. You'll then test your configuration to make sure it meets your connectivity goals.

Learning objectives
In this module, you'll:

Identify use cases for virtual network peering.
Identify the features and limitations of virtual network peering.
Configure peering connections between virtual networks.


Next unit: Connect services by using virtual network peering


2- Connect services by using virtual network peering

You can use virtual network peering to directly connect Azure virtual networks together. When you use peering to connect virtual networks, virtual machines (VMs) in these networks can communicate with each other as if they're in the same network.

With peered virtual networks, traffic between virtual machines is routed through the Azure network. The traffic uses only private IP addresses. It doesn't rely on internet connectivity, gateways, or encrypted connections. The traffic is always private, and it takes advantage of the high bandwidth and low latency of the Azure backbone network.

A basic diagram of two virtual networks that are connected by virtual network peering.

The two types of peering connections are created in the same way:

Virtual network peering connects virtual networks in the same Azure region, such as two virtual networks in North Europe.
Global virtual network peering connects virtual networks that are in different Azure regions, such as a virtual network in North Europe and a virtual network in West Europe.
Virtual network peering doesn't affect or disrupt any resources that you've already deployed to the virtual networks. When you use virtual network peering, consider the key features defined in the following sections.

Reciprocal connections
When you create a virtual network peering connection with Azure PowerShell or Azure CLI, only one side of the peering gets created. To complete the virtual network peering configuration, you'll need to configure the peering in reverse direction to establish connectivity. When you create the virtual network peering connection through the Azure portal, the configuration for both side is completed at the same time.

Think of how you'd connect two network switches together. You'd connect a cable to each switch and maybe configure some settings so that the switches can communicate. Virtual network peering requires similar connections in each virtual network. Reciprocal connections provide this functionality.

Cross-subscription virtual network peering
You can use virtual network peering even when both virtual networks are in different subscriptions. This setup might be necessary for mergers and acquisitions, or to connect virtual networks in subscriptions that different departments manage. Virtual networks can be in different subscriptions, and the subscriptions can use the same or different Microsoft Entra tenants.

When you use virtual network peering across subscriptions, you might find that an administrator of one subscription doesn't administer the peer network's subscription. The administrator might not be able to configure both ends of the connection. To peer the virtual networks when both subscriptions are in different Microsoft Entra tenants, the administrators of each subscription must grant the peer subscription's administrator the Network Contributor role on their virtual network.

Transitivity
Virtual network peering is nontransitive. Only virtual networks that are directly peered can communicate with each other. Virtual networks can't communicate with peers of their peers.

Suppose, for example, that your three virtual networks (A, B, C) are peered like this: A <-> B <-> C. Resources in A can't communicate with resources in C because that traffic can't transit through virtual network B. If you need communication between virtual network A and virtual network C, you must explicitly peer these two virtual networks.

Gateway transit
You can connect to your on-premises network from a peered virtual network if you enable gateways transit from a virtual network that has a VPN gateway. Using gateway transit, you can enable on-premises connectivity without deploying virtual network gateways to all your virtual networks. This method can reduce the overall cost and complexity of your network. By using virtual network peering with gateway transit, you can configure a single virtual network as a hub network. Connect this hub network to your on-premises datacenter and share its virtual network gateway with peers.

To enable gateway transit, configure the Allow gateway transit option in the hub virtual network where you deployed the gateway connection to your on-premises network. Also configure the Use remote gateways option in any spoke virtual networks.

 Note

If you want to enable the Use remote gateways option in a spoke network peering, you can't deploy a virtual network gateway in the spoke virtual network.

Overlapping address spaces
IP address spaces of connected networks within Azure, between Azure and your on-premises network can't overlap. This is also true for peered virtual networks. Keep this rule in mind when you're planning your network design. In any networks you connect through virtual network peering, VPN, or ExpressRoute, assign different address spaces that don't overlap.

Diagram of a comparison of overlapping and non-overlapping network addressing.

Alternative connectivity methods
Virtual network peering is the least complex way to connect virtual networks together. Other methods focus primarily on connectivity between on-premises and Azure networks rather than connections between virtual networks.

You can also connect virtual networks together through an ExpressRoute circuit. ExpressRoute is a dedicated, private connection between an on-premises datacenter and the Azure backbone network. The virtual networks that connect to an ExpressRoute circuit are part of the same routing domain and can communicate with each other. ExpressRoute connections don't go over the public internet, so your communications with Azure services are as secure as possible.

VPNs use the internet to connect your on-premises datacenter to the Azure backbone through an encrypted tunnel. You can use a site-to-site configuration to connect virtual networks together through VPN gateways. VPN gateways have higher latency than virtual network peering setups. They're more complex and can cost more to manage.

When virtual networks are connected through both a gateway and virtual network peering, traffic flows through the peering configuration.

When to choose virtual network peering
Virtual network peering can be a great way to enable network connectivity between services that are in different virtual networks. Because it's easy to implement and deploy, and it works well across regions and subscriptions, virtual network peering should be your first choice when you need to integrate Azure virtual networks.

Peering might not be your best option if you have existing VPN or ExpressRoute connections or services behind Azure Basic Load Balancers that would be accessed from a peered virtual network. In these cases, you should research alternatives.


Next unit: Exercise - Prepare virtual networks for peering by using Azure CLI commands


3- Exercise - Prepare virtual networks for peering by using Azure CLI commands


Let's say your company is now ready to implement virtual network peering. You want to connect systems that are deployed in different virtual networks. To test this plan, you'll start by creating virtual networks to support the services your company is already running in Azure. You need three virtual networks:

The Sales virtual network is deployed in North Europe. Sales systems use this virtual network to process data that's added after a customer is engaged. The Sales team wants access to Marketing data.
The Marketing virtual network is deployed in North Europe. Marketing systems use this virtual network. Members of the Marketing team regularly chat with the Sales team. To share their data with the Sales team, they must download it because the Sales and Marketing systems aren't connected.
The Research virtual network is deployed in West Europe. Research systems use this virtual network. Members of the Research team have a logical working relationship with Marketing, but they don't want the Sales team to have direct access to their data.
A diagram of virtual networks you need to create.

You'll create the following resources:

Virtual network	Region	Virtual network address space	Subnet	Subnet address space
SalesVNet	North Europe	10.1.0.0/16	Apps	10.1.1.0/24
MarketingVNet	North Europe	10.2.0.0/16	Apps	10.2.1.0/24
ResearchVNet	West Europe	10.3.0.0/16	Data	10.3.1.0/24
Create the virtual networks
In Cloud Shell, run the following command to create the virtual network and subnet for the Sales systems:

Azure CLI

Copy
az network vnet create \
    --resource-group "[sandbox resource group name]" \
    --name SalesVNet \
    --address-prefixes 10.1.0.0/16 \
    --subnet-name Apps \
    --subnet-prefixes 10.1.1.0/24 \
    --location northeurope
Run the following command to create the virtual network and subnet for the Marketing systems:

Azure CLI

Copy
az network vnet create \
    --resource-group "[sandbox resource group name]" \
    --name MarketingVNet \
    --address-prefixes 10.2.0.0/16 \
    --subnet-name Apps \
    --subnet-prefixes 10.2.1.0/24 \
    --location northeurope
Run the following command to create the virtual network and subnet for the Research systems:

Azure CLI

Copy
az network vnet create \
    --resource-group "[sandbox resource group name]" \
    --name ResearchVNet \
    --address-prefixes 10.3.0.0/16 \
    --subnet-name Data \
    --subnet-prefixes 10.3.1.0/24 \
    --location westeurope
Confirm the virtual network configuration
Let's take a quick look at what you created.

In Cloud Shell, run the following command to view the virtual networks:

Azure CLI

Copy
az network vnet list --query "[?contains(provisioningState, 'Succeeded')]" --output table
You should get an output like this:

Output

Copy
Location     Name           EnableDdosProtection    ProvisioningState    ResourceGuid                          ResourceGroup
-----------  -------------  ----------------------  -------------------  ------------------------------------  ------------------------------------------
westeurope   ResearchVNet   False                   Succeeded            9fe09fe0-d6cd-4043-aba8-b5e850a91251  learn-cb081b92-bc67-49cf-a965-1aeb40a2e25c
northeurope  SalesVNet      False                   Succeeded            8f030706-cce4-4a7b-8da2-a9f738887ffd  learn-cb081b92-bc67-49cf-a965-1aeb40a2e25c
northeurope  MarketingVNet  False                   Succeeded            ffbf8430-b0eb-4c3d-aa94-3b3156b90bed  learn-cb081b92-bc67-49cf-a965-1aeb40a2e25c
Create virtual machines in each virtual network
Now, you'll deploy some Ubuntu virtual machines (VMs) in each of the virtual networks. These VMs simulate the services in each virtual network. In the final unit of this module, you'll use these VMs to test connectivity between the virtual networks.

In Cloud Shell, run the following command, replacing <password> with a password that meets the requirements for Linux VMs, to create an Ubuntu VM in the Apps subnet of SalesVNet. Note this password for later use.

Azure CLI

Copy
az vm create \
    --resource-group "[sandbox resource group name]" \
    --no-wait \
    --name SalesVM \
    --location northeurope \
    --vnet-name SalesVNet \
    --subnet Apps \
    --image Ubuntu2204 \
    --admin-username azureuser \
    --admin-password <password>
 Note

The --no-wait parameter in this command lets you continue working in Cloud Shell while the VM is building.

Run the following command, replacing <password> with a password that meets the requirements for Linux VMs, to create another Ubuntu VM in the Apps subnet of MarketingVNet. Note this password for later use. The VM may take a minute or two to be created.

Azure CLI

Copy
az vm create \
    --resource-group "[sandbox resource group name]" \
    --no-wait \
    --name MarketingVM \
    --location northeurope \
    --vnet-name MarketingVNet \
    --subnet Apps \
    --image Ubuntu2204 \
    --admin-username azureuser \
    --admin-password <password>
Run the following command, replacing <password> with a password that meets the requirements for Linux VMs, to create an Ubuntu VM in the Data subnet of ResearchVNet. Note this password for later use.

Azure CLI

Copy
az vm create \
    --resource-group "[sandbox resource group name]" \
    --no-wait \
    --name ResearchVM \
    --location westeurope \
    --vnet-name ResearchVNet \
    --subnet Data \
    --image Ubuntu2204 \
    --admin-username azureuser \
    --admin-password <password>
The VMs might take several minutes to reach a running state.

To confirm that the VMs are running, run the following command. The Linux watch command is configured to refresh every five seconds.

Bash

Copy
watch -d -n 5 "az vm list \
    --resource-group "[sandbox resource group name]" \
    --show-details \
    --query '[*].{Name:name, ProvisioningState:provisioningState, PowerState:powerState}' \
    --output table"
A ProvisioningState of Succeeded and a PowerState of VM running indicates a successful deployment for the VM.

When your VMs are running, you're ready to move on. Press Ctrl-c to stop the command and continue on with the exercise.


Next unit: Exercise - Configure virtual network peering connections by using Azure CLI commands


4- Exercise - Configure virtual network peering connections by using Azure CLI commands

You've created virtual networks and run virtual machines (VMs) within them. However, the virtual networks have no connectivity, and none of these systems can communicate with each other.

To enable communication, you need to create peering connections for the virtual networks. To satisfy your company's requirements, you'll configure a hub and spoke topology and permit virtual network access when you create the peering connections.

Create virtual network peering connections
Follow these steps to create connections between the virtual networks and to configure the behavior of each connection.

In Cloud Shell, run the following command to create the peering connection between the SalesVNet and MarketingVNet virtual networks. This command also permits virtual network access across this peering connection.

Azure CLI

Copy
az network vnet peering create \
    --name SalesVNet-To-MarketingVNet \
    --remote-vnet MarketingVNet \
    --resource-group "[sandbox resource group name]" \
    --vnet-name SalesVNet \
    --allow-vnet-access
Run the following command to create a reciprocal connection from MarketingVNet to SalesVNet. This step completes the connection between these virtual networks.

Azure CLI

Copy
az network vnet peering create \
    --name MarketingVNet-To-SalesVNet \
    --remote-vnet SalesVNet \
    --resource-group "[sandbox resource group name]" \
    --vnet-name MarketingVNet \
    --allow-vnet-access
Now that you have connections between Sales and Marketing, create connections between Marketing and Research.

In Cloud Shell, run the following command to create the peering connection between the MarketingVNet and ResearchVNet virtual networks:

Azure CLI

Copy
az network vnet peering create \
    --name MarketingVNet-To-ResearchVNet \
    --remote-vnet ResearchVNet \
    --resource-group "[sandbox resource group name]" \
    --vnet-name MarketingVNet \
    --allow-vnet-access
Run the following command to create the reciprocal connection between ResearchVNet and MarketingVNet:

Azure CLI

Copy
az network vnet peering create \
    --name ResearchVNet-To-MarketingVNet \
    --remote-vnet MarketingVNet \
    --resource-group "[sandbox resource group name]" \
    --vnet-name ResearchVNet \
    --allow-vnet-access
Check the virtual network peering connections
Now that you've created the peering connections between the virtual networks, make sure the connections work.

In Cloud Shell, run the following command to check the connection between SalesVNet and MarketingVNet:

Azure CLI

Copy
az network vnet peering list \
    --resource-group "[sandbox resource group name]" \
    --vnet-name SalesVNet \
    --query "[].{Name:name, Resource:resourceGroup, PeeringState:peeringState, AllowVnetAccess:allowVirtualNetworkAccess}"\
    --output table
You've created only one connection from SalesVNet, so you get only one result. In the PeeringState column, make sure the status is Connected.

Run the following command to check the peering connection between the ResearchVNet and MarketingVNet virtual networks:

Azure CLI

Copy
az network vnet peering list \
    --resource-group "[sandbox resource group name]" \
    --vnet-name ResearchVNet \
    --query "[].{Name:name, Resource:resourceGroup, PeeringState:peeringState, AllowVnetAccess:allowVirtualNetworkAccess}"\
    --output table
Again, you've created only one connection from ResearchVNet, so you get only one result. In the PeeringState column, make sure the status is Connected.

Run the following command to check the peering connections for the MarketingVNet virtual network.

Azure CLI

Copy
az network vnet peering list \
    --resource-group "[sandbox resource group name]" \
    --vnet-name MarketingVNet \
    --query "[].{Name:name, Resource:resourceGroup, PeeringState:peeringState, AllowVnetAccess:allowVirtualNetworkAccess}"\
    --output table
Remember that you created connections from Marketing to Sales and from Marketing to Research, so you should get two connections. In the PeeringState column, make sure the status of both connections is Connected.

Your peering connections between the virtual networks should now look like this:

Diagram of the resulting virtual network peering connections.

Check effective routes
You can further check the peering connection by looking at the routes that apply to the network interfaces of the VMs.

Run the following command to look at the routes that apply to the SalesVM network interface:

Azure CLI

Copy
az network nic show-effective-route-table \
    --resource-group "[sandbox resource group name]" \
    --name SalesVMVMNic \
    --output table
The output table shows the effective routes for the VM's network interface. For SalesVMVMNic, you should have a route to 10.2.0.0/16 with Next Hop Type of VNetPeering. This is the network route for the peering connection from SalesVNet to MarketingVNet.

Output

Copy
Source    State    Address Prefix    Next Hop Type    Next Hop IP
--------  -------  ----------------  ---------------  -------------
Default   Active   10.1.0.0/16       VnetLocal
Default   Active   10.2.0.0/16       VNetPeering
Default   Active   0.0.0.0/0         Internet
Default   Active   10.0.0.0/8        None
Default   Active   100.64.0.0/10     None
Default   Active   192.168.0.0/16    None
Run the following command to look at the routes for MarketingVM:

Azure CLI

Copy
az network nic show-effective-route-table \
    --resource-group "[sandbox resource group name]" \
    --name MarketingVMVMNic \
    --output table
The output table shows the effective routes for the VM's network interface. For MarketingVMVMNic, you should have a route to 10.1.0.0/16 with a next hop type of VNetPeering and a route to 10.3.0.0/16 with a next hop type of VNetGlobalPeering. These are the network routes for the peering connection from MarketingVNet to SalesVNet and from MarketingVNet to ResearchVNet.

Output

Copy
Source    State    Address Prefix    Next Hop Type      Next Hop IP
--------  -------  ----------------  -----------------  -------------
Default   Active   10.2.0.0/16       VnetLocal
Default   Active   10.1.0.0/16       VNetPeering
Default   Active   0.0.0.0/0         Internet
Default   Active   10.0.0.0/8        None
Default   Active   100.64.0.0/10     None
Default   Active   192.168.0.0/16    None
Default   Active   10.3.0.0/16       VNetGlobalPeering
Run the following command to look at the routes for ResearchVM:

Azure CLI

Copy
az network nic show-effective-route-table \
    --resource-group "[sandbox resource group name]" \
    --name ResearchVMVMNic \
    --output table
The output table shows the effective routes for the VM's network interface. For ResearchVMVMNic, you should have a route to 10.2.0.0/16 with a next hop type of VNetGlobalPeering. This is the network route for the peering connection from ResearchVNet to MarketingVNet.

Output

Copy
Source    State    Address Prefix    Next Hop Type      Next Hop IP
--------  -------  ----------------  -----------------  -------------
Default   Active   10.3.0.0/16       VnetLocal
Default   Active   0.0.0.0/0         Internet
Default   Active   10.0.0.0/8        None
Default   Active   100.64.0.0/10     None
Default   Active   192.168.0.0/16    None
Default   Active   10.2.0.0/16       VNetGlobalPeering
Now that your peering connections are configured, let's take a look at how this affects the communication between VMs.


Next unit: Exercise - Verify virtual network peering by using SSH between Azure virtual machines

5- Exercise - Verify virtual network peering by using SSH between Azure virtual machines

In the previous unit, you configured peering connections between the virtual networks to enable resources to communicate with each other. Your configuration used a hub and spoke topology. MarketingVNet was the hub, and SalesVNet and ResearchVNet were spokes.

Diagram of a hub and spoke topology for virtual networks.

Remember, peering connections are nontransitive. Intermediate virtual networks don't allow connectivity to flow through them to connected virtual networks. SalesVNet can communicate with MarketingVNet. ResearchVNet can communicate with MarketingVNet. MarketingVNet can communicate with both SalesVNet and ResearchVNet. The only communication that's not permitted is between SalesVNet and ResearchVNet. Even though SalesVNet and ResearchVNet are both connected to MarketingVNet, they can't communicate with each other because they're not directly peered to each other.

Let's confirm the connectivity across the peering connections. To do this, you'll first create a connection from Azure Cloud Shell to a target VM's public IP address. Then you'll connect from the target VM to the destination VM by using the destination VM's private IP address.

 Important

To test the virtual network peering connection, connect to the private IP address assigned to each VM.

To connect to your VMs, you'll use SSH (Secure Shell) directly from Cloud Shell. When using SSH, you'll first find the public IP addresses that are assigned to your test VMs.

In Cloud Shell, run the following command to list the IP addresses you'll use to connect to the VMs:

Azure CLI

Copy
az vm list \
    --resource-group "[sandbox resource group name]" \
    --query "[*].{Name:name, PrivateIP:privateIps, PublicIP:publicIps}" \
    --show-details \
    --output table
Record the output. You'll need the IP addresses for the exercises in this unit.

Before you start the tests, think about what you've learned in this module. What results do you expect? Which VMs will and won't be able to communicate with each other?

Test connections from SalesVM
In the first test, you'll use SSH in Cloud Shell to connect to the public IP address of SalesVM. You'll then attempt to connect from SalesVM to MarketingVM and ResearchVM.

In Cloud Shell, run the following command, using SSH to connect to the public IP address of SalesVM. In the command, replace <SalesVM public IP> with the VM's public IP address.

Bash

Copy
ssh -o StrictHostKeyChecking=no azureuser@<SalesVM public IP>
A diagram showing connection to the public IP address of SalesVM.

Sign in with the password that you used to create the VM. The prompt now shows that you're signed in to SalesVM.

In Cloud Shell, run the following command, using SSH to connect to the private IP address of MarketingVM. In the command, replace <MarketingVM private IP> with this VM's private IP address.

Bash

Copy
ssh -o StrictHostKeyChecking=no azureuser@<MarketingVM private IP>
Diagram showing connection from SalesVM to the private IP address of MarketingVM.

The connection attempt should succeed because of the peering connection between the SalesVNet and MarketingVNet virtual networks.

Sign in by using the password you used to create the VM.

Enter exit to close this SSH session and return to the SalesVM prompt.

In Cloud Shell, run the following command, using SSH to connect to the private IP address of ResearchVM. In the command, replace <ResearchVM private IP> with this VM's private IP address.

Bash

Copy
ssh -o StrictHostKeyChecking=no azureuser@<ResearchVM private IP>
The connection attempt should fail because there's no peering connection between the SalesVNet and ResearchVNet virtual networks. Up to 60 seconds might pass before the connection attempt times out. To force the attempt to stop, use Ctrl+C.

Diagram showing the attempt failing to connect from SalesVM to the private IP address of ResearchVM.

Enter exit to close the SSH session and return to Cloud Shell.

Test connections from ResearchVM
In the second test, you'll use SSH in Cloud Shell to connect to the public IP address of ResearchVM. You'll then attempt to connect from ResearchVM to MarketingVM and SalesVM.

In Cloud Shell, run the following command, using SSH to connect to the public IP address of ResearchVM. In the command, replace <ResearchVM public IP> with this VM's public IP address.

Bash

Copy
ssh -o StrictHostKeyChecking=no azureuser@<ResearchVM public IP>
Diagram showing connection to the public IP address of ResearchVM.

Sign in by using the password that you used to create the VM. The prompt now shows that you're signed in to ResearchVM.

In Cloud Shell, run the following command, using SSH to connect to the private IP address of MarketingVM. In the command, replace <MarketingVM private IP> with this VM's private IP address.

Bash

Copy
ssh -o StrictHostKeyChecking=no azureuser@<MarketingVM private IP>
Diagram showing connection to the private IP address of MarketingVM.

The connection attempt should succeed because of the peering connection between the ResearchVNet and MarketingVNet virtual networks.

Sign in by using the password you used to create the VM.

Enter exit to close this SSH session and return to the ResearchVM prompt.

In Cloud Shell, run the following command, using SSH to connect to the private IP address of SalesVM. In the command, replace <SalesVM private IP> with this VM's private IP address.

Bash

Copy
ssh -o StrictHostKeyChecking=no azureuser@<SalesVM private IP>
The connection attempt should fail because there's no peering connection between the ResearchVNet and SalesVNet virtual networks. Up to 60 seconds might pass before the connection attempt times out. To force the attempt to stop, use Ctrl+C.

Diagram showing the attempt failing to connect ResearchVM to the private IP address of SalesVM.

Enter exit to close the SSH session and return to Cloud Shell.

Test connections from Marketing VM
In the final test, you'll use SSH in Cloud Shell to connect to the public IP address of MarketingVM. You'll then attempt to connect from MarketingVM to ResearchVM and SalesVM.

In Cloud Shell, run the following command, using SSH to connect to the public IP address of MarketingVM. In the command, replace <MarketingVM public IP> with this VM's public IP address.

Bash

Copy
ssh -o StrictHostKeyChecking=no azureuser@<MarketingVM public IP>
Diagram that shows connection to the public IP address of MarketingVM.

Sign in by using the password that you used to create the VM. The prompt shows that you're signed in to MarketingVM.

In Cloud Shell, run the following command, using SSH to connect to the private IP address of ResearchVM. In the command, replace <ResearchVM private IP> with this VM's private IP address.

Bash

Copy
ssh -o StrictHostKeyChecking=no azureuser@<ResearchVM private IP>
Diagram that shows Azure Cloud Shell connecting to the Marketing V Net and the Research V Net virtual networks, using a peering connection.

The connection attempt should succeed because of the peering connection between the MarketingVNet and ResearchVNet virtual networks.

Sign in by using the password you used to create the VM.

Enter exit to close this SSH session, and return to the MarketingVM prompt.

In Cloud Shell, run the following command, using SSH to connect to the private IP address of SalesVM. In the command, replace <SalesVM private IP> with this VM's private IP address.

Bash

Copy
ssh -o StrictHostKeyChecking=no azureuser@<SalesVM private IP>
The connection attempt should also succeed because there is a peering connection between the MarketingVNet and SalesVNet virtual networks.

Diagram that shows Azure Cloud Shell connecting to the Marketing V Net and the Sales V Net virtual machines, using a peering connection.

Sign in by using the password you used to create the VM.

Enter exit to close this SSH session, and return to the MarketingVM prompt.

Enter exit to close the SSH session, and return to Cloud Shell.

This is a simple test using SSH. It demonstrates network connectivity between peered virtual networks. It also demonstrates lack of network connectivity for transitive connections.

If these servers were running application services, the server connectivity would allow communication between the services running on the VMs. The connectivity would allow the business to share data across departments as required.

Next unit: Summary

Summary

In this module, you learned how to use peering to connect virtual networks in a hub and spoke topology. You used VMs and SSH to verify connectivity between virtual networks. The peering connections will enable communication for services that run on the VMs.

Now that you understand how to peer virtual networks together, you can use this cost-effective and minimally complex method in your Azure network infrastructure. The method enables low-latency communication between resources in virtual networks. It supports scenarios where resources are in different regions or subscriptions. Virtual network peering should be your first choice when you need to connect virtual networks.

Clean up
The sandbox automatically cleans up your resources when you're finished with this module.

When you're working in your own subscription, it's a good idea at the end of a project to identify whether you still need the resources you created. Resources that you leave running can cost you money. You can delete resources individually or delete the resource group to delete the entire set of resources.





Point 9: Host your domain on Azure DNS

Create a DNS zone for your domain name. Create DNS records to map the domain to an IP address. Test that the domain name resolves to your web server.

Learning objectives
In this module, you will:

Configure Azure DNS to host your domain.


1- Introduction

Azure DNS lets you host your DNS records for your domains on Azure infrastructure. With Azure DNS, you can use the same credentials, APIs, tools, and billing as your other Azure services.

Let's say that your company recently bought the custom domain name wideworldimporters.com from a third-party domain-name registrar. The domain name is for a new website that your organization plans to launch. You need a hosting service for DNS domains. This hosting service would resolve the wideworldimporters.com domain to your web server's IP address.

You're already using Azure to build your website, so you decide to use Azure DNS to manage your domain.

This module shows you how to configure Azure DNS to host your domain. You'll also see how to add an alias and other DNS records to resolve your domain name to a website.

Learning objectives
In this module, you will:

Configure Azure DNS to host your domain.
Prerequisites
Knowledge of networking concepts like name resolution and IP addresses



Next unit: What is Azure DNS?

2-  What is Azure DNS?

Azure DNS is a hosting service for DNS domains that provides name resolution by using Microsoft Azure infrastructure.

In this unit, you'll learn what DNS is and how it works. You will also learn about Azure DNS and why you'd use it.

What is DNS?
DNS, or the Domain Name System, is a protocol within the TCP/IP standard. DNS serves an essential role of translating the human-readable domain names—for example: www.wideworldimports.com—into a known IP address. IP addresses enable computers and network devices to identify and route requests among themselves.

DNS uses a global directory hosted on servers around the world. Microsoft is part of the network that provides a DNS service through Azure DNS.

A DNS server is also known as a DNS name server, or just a name server.

How does DNS work?
A DNS server carries out one of two primary functions:

Maintains a local cache of recently accessed or used domain names and their IP addresses. This cache provides a faster response to a local domain lookup request. If the DNS server can't find the requested domain, it passes the request to another DNS server. This process repeats at each DNS server until either a match is made or the search times out.
Maintains the key-value pair database of IP addresses and any host or subdomain over which the DNS server has authority. This function is often associated with mail, web, and other internet domain services.
DNS server assignment
In order for a computer, server, or other network-enabled device to access web-based resources, it must reference a DNS server.

When you connect by using your on-premises network, the DNS settings come from your server. When you connect by using an external location like a hotel, the DNS settings come from the internet service provider (ISP).

Domain lookup requests
Here's a simplified overview of the process a DNS server uses when it resolves a domain-name lookup request:

Checks to see if the domain name is stored in the short-term cache. If so, the DNS server resolves the domain request.
If the domain isn't in the cache, it contacts one or more DNS servers on the web to see if they have a match. When a match is found, the DNS server updates the local cache and resolves the request.
If the domain isn't found after a reasonable number of DNS checks, the DNS server responds with a domain cannot be found error.
IPv4 and IPv6
Every computer, server, or network-enabled device on your network has an IP address. An IP address is unique within your domain. There are two standards of IP address: IPv4 and IPv6.

IPv4 is composed of four sets of numbers, in the range 0 to 255, each separated by a dot; for example: 127.0.0.1. Today, IPv4 is the most commonly used standard. Yet, with the increase in IoT devices, the IPv4 standard will eventually be unable to keep up.

IPv6 is a relatively new standard and will eventually replace IPv4. It's made up of eight groups of hexadecimal numbers, each separated by a colon; for example: fe80:11a1:ac15:e9gf:e884:edb0:ddee:fea3.

Many network devices are now provisioned with both an IPv4 and an IPv6 address. The DNS name server can resolve domain names to both IPv4 and IPv6 addresses.

DNS settings for your domain
Whether the DNS server for your domain is hosted by a third party or managed in-house, you'll need to configure it for each host type you're using. Host types include web, email, or other services you're using.

As the administrator for your company, you want to set up a DNS server by using Azure DNS. In this instance, the DNS server will act as a start of authority (SOA) for your domain.

DNS record types
Configuration information for your DNS server is stored as a file within a zone on your DNS server. Each file is called a record. The following record types are the most commonly created and used:

A is the host record, and is the most common type of DNS record. It maps the domain or host name to the IP address.
CNAME is a Canonical Name record that's used to create an alias from one domain name to another domain name. If you had different domain names that all accessed the same website, you'd use CNAME.
MX is the mail exchange record. It maps mail requests to your mail server, whether hosted on-premises or in the cloud.
TXT is the text record. It's used to associate text strings with a domain name. Azure and Microsoft 365 use TXT records to verify domain ownership.
Additionally, there are the following record types:

Wildcards
CAA (certificate authority)
NS (name server)
SOA (start of authority)
SPF (sender policy framework)
SRV (server locations)
The SOA and NS records are created automatically when you create a DNS zone by using Azure DNS.

Record sets
Some record types support the concept of record sets, or resource record sets. A record set allows for multiple resources to be defined in a single record. For example, here's an A record that has one domain with two IP addresses:


Copy
www.wideworldimports.com.     3600    IN    A    127.0.0.1
www.wideworldimports.com.     3600    IN    A    127.0.0.2
SOA and CNAME records can't contain record sets.

What is Azure DNS?
Azure DNS allows you to host and manage your domains by using a globally distributed name-server infrastructure. It allows you to manage all of your domains by using your existing Azure credentials.

Azure DNS acts as the SOA for the domain.

You can't use Azure DNS to register a domain name; you need to use a third-party domain registrar for that.

Why use Azure DNS to host your domain?
Azure DNS is built on the Azure Resource Manager service, which offers the following benefits:

Improved security
Ease of use
Private DNS domains
Alias record sets
At this time, Azure DNS doesn't support Domain Name System Security Extensions. If you require this security extension, you should host those portions of your domain with a third-party provider.

Security features
Azure DNS provides the following security features:

Role-based access control, which gives you fine-grained control over users' access to Azure resources. You can monitor their usage and control the resources and services to which they have access.
Activity logs, which let you track changes to a resource and pinpoint where faults occurred.
Resource locking, which gives you a greater level of control to restrict or remove access to resource groups, subscriptions, or any Azure resources.
Ease of use
Azure DNS can manage DNS records for your Azure services and provide DNS for your external resources. Azure DNS uses the same Azure credentials, support contract, and billing as your other Azure services.

You can manage your domains and records by using the Azure portal, Azure PowerShell cmdlets, or the Azure CLI. Applications that require automated DNS management can integrate with the service by using the REST API and SDKs.

Private domains
Azure DNS handles translating external domain names to IP addresses. Azure DNS lets you create private zones. These provide name resolution for virtual machines (VMs) within a virtual network and between virtual networks without having to create a custom DNS solution. This allows you to use your own custom domain names rather than the Azure-provided names.

To publish a private DNS zone to your virtual network, you'll specify the list of virtual networks that are allowed to resolve records within the zone.

Private DNS zones have the following benefits:

There's no need to invest in a DNS solution. DNS zones are supported as part of the Azure infrastructure.
All DNS record types are supported: A, CNAME, TXT, MX, SOA, AAAA, PTR, and SRV.
Host names for VMs in your virtual network are automatically maintained.
Split-horizon DNS support allows the same domain name to exist in both private and public zones. It resolves to the correct one based on the originating request location.
Alias record sets
Alias records sets can point to an Azure resource. For example, you can set up an alias record to direct traffic to an Azure public IP address, an Azure Traffic Manager profile, or an Azure Content Delivery Network endpoint.

The alias record set is supported in the following DNS record types:

A
AAAA
CNAME
Check your knowledge

1. What does Azure DNS allow you to do? 

Manage the security and access to your website.

Register new domain names, removing the need to use a domain registrar.

Manage and host your registered domain and associated records.

2. What security features does Azure DNS provide? 

Role-based access control, activity logs, and resource locking

Role-based access control, activity logs, and Azure threat detection

Role-based access control, activity logs, and Azure infrastructure security

3. What type of DNS record should you create to map one or more IP addresses against a single domain? 

CNAME

A or AAAA

SOA


3- Configure Azure DNS to host your domain

The new company website is in final testing. You're working on the plan to deploy the wideworldimports.com domain by using Azure DNS. You need to understand what steps are involved.

In this unit, you learn how to:

Create and configure a DNS zone for your domain by using Azure DNS.
Understand how to link your domain to an Azure DNS zone.
Create and configure a private DNS zone.
Configure a public DNS zone
You use a DNS zone to host the DNS records for a domain, such as wideworldimports.com.

Step 1: Create a DNS zone in Azure
You used a third-party domain-name registrar to register the wideworldimports.com domain. The domain doesn't point to your organization's website yet.

To host the domain name with Azure DNS, you first need to create a DNS zone for that domain. A DNS zone holds all the DNS entries for your domain.

When creating a DNS zone, you need to supply the following details:

Subscription: The subscription to be used.

Resource group: The name of the resource group to hold your domains. If one doesn't exist, create one to allow for better control and management.

Name: Your domain name, which in this case is wideworldimports.com.

Resource group location: The location defaults to the location of the resource group.

Screenshot of Create DNS zone page.

Step 2: Get your Azure DNS name servers
After you create a DNS zone for the domain, you need to get the name server details from the name servers (NS) record. You use these details to update your domain registrar's information and point to the Azure DNS zone.

Screenshot of the name server details on the DNS zone page.

Step 3: Update the domain registrar setting
As the domain owner, you need to sign in to the domain-management application provided by your domain registrar. In the management application, edit the NS record and change the NS details to match your Azure DNS name server details.

Changing the NS details is called domain delegation. When you delegate the domain, you must use all four name servers provided by Azure DNS.

Step 4: Verify delegation of domain name services
The next step is to verify that the delegated domain now points to the Azure DNS zone you created for the domain. This process can take as few as 10 minutes, but might take longer.

To verify the success of the domain delegation, query the start of authority (SOA) record. The SOA record is automatically created when the Azure DNS zone is set up. You can verify the SOA record using a tool like nslookup.

The SOA record represents your domain and becomes the reference point when other DNS servers are searching for your domain on the internet.

To verify the delegation, use nslookup like this:

dos

Copy
nslookup -type=SOA wideworldimports.com
Step 5: Configure your custom DNS settings
The domain name is wideworldimports.com. When it's used in a browser, the domain resolves to your website. But what if you want to add in web servers or load balancers? These resources need to have their own custom settings in the DNS zone, either as an A record or a CNAME.

A record
Each A record requires the following details:

Name: The name of the custom domain, for example webserver1.
Type: In this instance, it's A.
TTL: Represents the "time-to-live" as a whole unit, where 1 is one second. This value indicates how long the A record lives in a DNS cache before it expires.
IP address: The IP address of the server to which this A record should resolve.
CNAME record
The CNAME is the canonical name, or the alias for an A record. Use CNAME when you have different domain names that all access the same website. For example, you might need a CNAME in the wideworldimports zone if you want both www.wideworldimports.com and wideworldimports.com to resolve to the same IP address.

You'd create the CNAME record in the wideworldimports zone with the following information:

NAME: www
TTL: 600 seconds
Record type: CNAME
If you exposed a web function, you'd create a CNAME record that resolves to the Azure function.

Configure a private DNS zone
Another type of DNS zone that you can configure and host in Azure is a private DNS zone. Private DNS zones are not visible on the Internet, and don't require that you use a domain registrar. You can use private DNS zones to assign DNS names to virtual machines (VMs) in your Azure virtual networks.

Step 1: Create a private DNS zone
In the Azure portal, search for private DNS zones. To create the private zone, you need enter a resource group and the name of the zone. For example, the name might be something like private.wideworldimports.com.

Screenshot of the Create Private DNS zone page.

Step 2: Identify virtual networks
Let's assume that your organization already created your VMs and virtual networks in a production environment. Identify the virtual networks associated with VMs that need name-resolution support. To link the virtual networks to the private zone, you need the virtual network names.

Step 3: Link your virtual network to a private DNS zone
To link the private DNS zone to a virtual network, you create a virtual network link. In the Azure portal, go to the private zone, and select Virtual network links.

Screenshot of the Virtual Network Links page in a private DNS zone.

Select Add to pick the virtual network you want to link to the private zone.

Screenshot of Add virtual network link page.

You add a virtual network link record for each virtual network that needs private name-resolution support.

In the next unit, you learn how to create a public DNS zone.



Next unit: Exercise - Create a DNS zone and an A record by using Azure DNS

4- Exercise - Create a DNS zone and an A record by using Azure DNS

In the previous unit, we described setting up and configuring the wideworldimports.com domain to point to your Azure hosting on Azure DNS.

In this unit, you'll:

Set up an Azure DNS and create a public DNS zone.
Create an A record.
Verify that the A record resolves to an IP address.
Create a DNS zone in Azure DNS
Before you can host the wideworldimports.com domain on your servers, you need to create a DNS zone. The DNS zone holds all the configuration records associated with your domain.

To create your DNS zone:

Sign in to the Azure portal with the account you used to activate the sandbox.

On the Azure home page, under Azure services, select Create a resource. The Create a resource pane appears.

In the Search services and marketplace search box, search for and select DNS zone by Microsoft. The DNS zone pane appears.

Select Create > DNS zone.

Screenshot of DNS zone, with Create highlighted.

The Create DNS zone pane appears.

On the Basics tab, enter the following values for each setting.

Setting	Value
Project details	
Subscription	Concierge subscription
Resource group	From the dropdown list, select [sandbox resource group]
Instance details	
Name	The name needs to be unique in the sandbox. Use wideworldimportsXXXX.com, replacing the Xs with letters or numbers.
Screenshot of Create DNS zone page.

Select Review + create.

After validation passes, select Create. It'll take a few minutes to create the DNS zone.

When deployment is complete, select Go to resource. Your DNS zone pane appears.

By default, the NS and SOA record sets are automatically created and automatically deleted whenever a DNS zone is created or deleted. The NS record set defines the Azure DNS namespaces and contains the four Azure DNS records. You use all four records when you update the registrar.

The SOA record represents your domain, and is used when other DNS servers are searching for your domain.

Make a note of the NS record values. You'll need them in the next section.

Create a DNS record
Now that the DNS zone exists, you need to create the necessary records to support the domain.

The primary record set to create is the A record. The A record set is used to point traffic from a logical domain name to the hosting server's IP address. An A record set can have multiple records. In a record set, the domain name remains constant, while the IP addresses differ.

On the DNS zone pane for wideworldimportsXXXX.com, in the top menu bar, select + Record set.

Screenshot of the DNS zone page, with + Record set highlighted.

The Add record set pane appears.

Enter the following values for each setting.

Setting	Value	Description
Name	www	The host name that you want to resolve to an IP address.
Type	A	The A record is the most commonly used. If you're using IPv6, select the AAAA type.
Alias record set	No	This can only be applied to A, AAAA, and CNAME record types.
TTL	1	The time to live, which specifies the period of time each DNS server caches the resolution before it's purged.
TTL unit	Hours	This value can be seconds, minutes, hours, days, or weeks. Here, you're selecting hours.
IP Address	10.10.10.10	The IP address the record name resolves to. In a real-world scenario, you'd enter the public IP address for your web server.
Select OK to add the record to your zone.

Screenshot of A record set.

Note that it's possible to have more than one IP address set up for your web server. In that case, you'd add all the associated IP addresses as records in the A record set. After it's created, you can update the record set with additional IP addresses.

Verify your global Azure DNS
In a real-world scenario, after you create the public DNS zone, you'd update the NS records of the domain-name registrar to delegate the domain to Azure.

Even though we don't have a registered domain, it's still possible to verify that the DNS zone works as expected by using the nslookup tool.

Use nslookup to verify the configuration
Here's how to use nslookup to verify the DNS zone configuration.

Use Cloud Shell to run the following command. Replace the DNS zone name with the zone you created, and replace <name server address> with one of the NS values you copied after you created the DNS zone.

Bash

Copy
nslookup www.wideworldimportsXXXX.com <name server address>
The command should look something like the following:

Bash

Copy
nslookup www.wideworldimportsXXXX.com ns1-04.azure-dns.com
You should see that your host name www.wideworldimportsXXXX.com resolves to 10.10.10.10.

Screenshot of Cloud Shell, showing the nslookup results.

You've successfully set up a DNS zone and created an A record.



Next unit: Dynamically resolve resource name by using alias record

5- Dynamically resolve resource name by using alias record

You've now successfully delegated the domain from the domain registrar to your Azure DNS and configured an A record to link the domain to your web server.

The next phase of the deployment is to improve resiliency by using a load balancer. Load balancers distribute inbound data requests and traffic across one or more servers. They reduce the load on any one server and improve performance. This technology is well established, you'll use it throughout your on-premises network.

You know that the A record and CNAME record don't support direct connection to Azure resources like your load balancers. You've been tasked with finding out how to link the apex domain with a load balancer.

What is an apex domain?
The apex domain is your domain's highest level. In our case, that's wideworldimports.com. The apex domain is also sometimes referred to as the zone apex or root apex. It's often represented by the @ symbol in your DNS zone records.

If you check the DNS zone for wideworldimports.com, you'll see there are two apex domain records: NS and SOA. The NS and SOA records are automatically created when you created the DNS zone.

CNAME records that you might need for an Azure Traffic Manager profile or Azure Content Delivery Network endpoints aren't supported at the zone apex level. However, other alias records are supported at the zone apex level.

What are alias records?
Azure alias records enable a zone apex domain to reference other Azure resources from the DNS zone. You don't need to create complex redirection policies. You can also use an Azure alias to route all traffic through Traffic Manager.

The Azure alias record can point to the following Azure resources:

A Traffic Manager profile
Azure Content Delivery Network endpoints
A public IP resource
A front-door profile
Alias records provide lifecycle tracking of target resources, ensuring that changes to any target resource are automatically applied to the DNS zone. Alias records also provide support for load-balanced applications in the zone apex.

The alias record set supports the following DNS zone record types:

A: The IPv4 domain name-mapping record.
AAAA: The IPv6 domain name-mapping record.
CNAME: The alias for your domain, which links to the A record.
Uses for alias records
The following are some of the advantages of using alias records:

Prevents dangling DNS records: A dangling DNS record occurs when the DNS zone records aren't up to date with changes to IP addresses. Alias records prevent dangling references by tightly coupling the lifecycle of a DNS record with an Azure resource.
Updates DNS record set automatically when IP addresses change: When the underlying IP address of a resource, service, or application is changed, the alias record ensures that any associated DNS records are automatically refreshed.
Hosts load-balanced applications at the zone apex: Alias records allow for zone apex resource routing to Traffic Manager.
Points zone apex to Azure Content Delivery Network endpoints: With alias records, you can now directly reference your Azure Content Delivery Network instance.
An alias record allows you to link the zone apex (wideworldimports.com) to a load balancer. It creates a link to the Azure resource rather than a direct IP-based connection. So, if the IP address of your load balancer changes, the zone apex record continues to work.

Next unit: Exercise - Create alias records for Azure DNS

6- Exercise - Create alias records for Azure DNS

Your new website's deployment was a huge success. Usage volumes are much higher than anticipated. The single web server on which the website runs is showing signs of strain. Your organization wants to increase the number of servers and distribute the load using a load balancer.

You now know you can use an Azure alias record to provide a dynamic, automatically refreshing link between the zone apex and the load balancer.

In this unit, you'll:

Set up a virtual network with two VMs and a load balancer.
Learn how to configure an Azure alias at the zone apex to direct to the load balancer.
Verify that the domain name resolves to one or either of the VMs on your virtual network.
Set up a virtual network, load balancer, and VMs in Azure
Manually creating a virtual network, load balancer, and two VMs will take some time. To reduce this time, you can use a Bash setup script that's available on GitHub. Follow these instructions to create a test environment for your alias record.

In Azure Cloud Shell, run the following setup script:

Bash

Copy
git clone https://github.com/MicrosoftDocs/mslearn-host-domain-azure-dns.git
To run the setup script, run the following commands:

Bash

Copy
cd mslearn-host-domain-azure-dns
chmod +x setup.sh
./setup.sh
The setup script takes a few minutes to run. The script:

Creates a network security group.
Creates two network interface controllers (NICs) and two VMs.
Creates a virtual network and assigns the VMs.
Creates a public IP address and updates the configuration of the VMs.
Creates a load balancer that references the VMs, including rules for the load balancer.
Links the NICs to the load balancer.
After the script completes, it shows you the public IP address for the load balancer. Copy the IP address to use it later.

Create an alias record in your zone apex
Now that you've created a test environment, you're ready to set up the Azure alias record in your zone apex.

In the Azure portal, select Resource groups. The Resource groups pane appears.

Select the resource group: [sandbox resource group]. The Resource group pane appears.

In the list of resources, select the DNS zone you created in the previous exercise, wideworldimportsXXXX.com. The wideworldimportsXXXX.com DNS zone pane appears.

In the menu bar, select + Record set. The Add record set pane appears.

Enter the following values for each setting to create an alias record.

Setting	Value
Name	Leave the name blank. By leaving it blank, it indicates the DNS zone for wideworldimportsXXXX.com.
Type	A. Even though we're creating an alias, the base record type must still be either A, AAAA, or CNAME.
Alias record set	Yes
Alias type	Azure resource
Azure resource	From the list of resources, select myPublicIP. It may take up to 15 minutes for the deployments to propagate. If this resource isn't listed, wait several minutes, refresh the portal, and try again.
Screenshot of Add record set.

Select OK to add the record to your zone.

When the new alias record is created, it should look something like this:

Screenshot of the DNS zone, with an alias record created.

Verify that the alias resolves to the load balancer
Now, you need to verify that the alias record is set up correctly. In a real-world scenario, you'd have an actual domain, and would've completed the domain delegation to Azure DNS. You'd use the registered domain name for this exercise. Because this unit assumes there's no registered domain, you'll use the public IP address.

In the Azure portal, go to the resource group, select myPublicIP, then select the Copy icon next to the IP address.

Screenshot of the DNS zone with an alias record created.

In a web browser, paste the Public IP address as the URL.

You'll see a basic web page that shows the name of the VM to which the load balancer sent the request.

Next unit: Summary

Your company recently bought the custom domain name wideworldimporters.com from a third-party domain-name registrar. The domain name is for a new website your organization plans to launch. You need a hosting service for DNS domains. This hosting service would resolve the wideworldimporters.com domain to your Azure-based web server's IP address.

Your company wanted to manage all their infrastructure and related domain name information in one place. You've seen how easy it was to manage DNS information by using an Azure DNS zone. First, you created an Azure DNS zone, and then you updated the NS records at your domain registrar to point at it.

You learned the uses of the different record sets, A, AAAA, CNAME, NS, and SOA. You also learned how you can use Azure aliases to override the static A/AAAA/CNAME record to provide a dynamic reference to your resources. Using an Azure DNS zone improved your company's administration of resources, because your staff only needed one place to manage DNS-related tasks.

The Azure DNS zone allows better control and integration with your Azure resources. It's possible to achieve some of the more basic record set functions by using the domain registrar's management console. However, linking to any of your Azure resources becomes difficult or impossible without a high degree of complex redirection.

By using an Azure DNS zone to host your domain, your organization benefits by having all the resources managed through a single, common interface. This includes better integration with existing Azure resources, improved security, and monitoring tools.

Clean up
The sandbox automatically cleans up your resources when you're finished with this module.

When you're working in your own subscription, it's a good idea at the end of a project to identify whether you still need the resources you created. Resources that you leave running can cost you money. You can delete resources individually or delete the resource group to delete the entire set of resources.

Learn more
Quickstart: Create an Azure private DNS zone by using the Azure portal
Overview of DNS zones and records







Point 10: Manage and control traffic flow in your Azure deployment with routes

Learn how to control Azure virtual network traffic by implementing custom routes.

Learning objectives
In this module, you will:

Identify the routing capabilities of an Azure virtual network
Configure routing within a virtual network
Deploy a basic network virtual appliance
Configure routing to send traffic through a network virtual appliance


1- Introduction

A virtual network lets you implement a security perimeter around your resources in the cloud. You can control the information that flows in and out of a virtual network. You can also restrict access to allow only the traffic that originates from trusted sources.

Suppose you're the solution architect for a retail organization. Also suppose your organization recently suffered a security incident that exposed customer information such as names, addresses, and credit card numbers. Malicious actors infiltrated vulnerabilities in your retailer's network infrastructure, which resulted in the loss of customers' confidential information.

As part of a remediation plan, the security team recommends adding network protections in the form of network virtual appliances. The cloud infrastructure team must ensure traffic gets properly routed through the virtual appliances and gets inspected for malicious activity.

You'll learn about Azure routing, and you'll create custom routes to control the traffic flow. You'll also learn to redirect the traffic through the network virtual appliance so you can inspect the traffic before it's allowed through.

Learning objectives
In this module, you'll:

Identify the routing capabilities of an Azure virtual network
Configure routing within a virtual network
Deploy a basic network virtual appliance
Configure routing to send traffic through a network virtual appliance
Prerequisites
Knowledge of basic networking concepts, including subnets and IP addressing
Familiarity with Azure virtual networking


Next unit: Identify routing capabilities of an Azure virtual network

2- Identify routing capabilities of an Azure virtual network

To control traffic flow within your virtual network, you must learn the purpose and benefits of custom routes. You must also learn how to configure the routes to direct traffic flow through a network virtual appliance (NVA).

Azure routing
Network traffic in Azure is automatically routed across Azure subnets, virtual networks, and on-premises networks. System routes control this routing. They're assigned by default to each subnet in a virtual network. With these system routes, any Azure virtual machine that is deployed into a virtual network can communicate with any other in the network. These virtual machines are also potentially accessible from on-premises through a hybrid network or the internet.

You can't create or delete system routes, but you can override the system routes by adding custom routes to control traffic flow to the next hop.

Every subnet has the following default system routes:

Address prefix	Next hop type
Unique to the virtual network	Virtual network
0.0.0.0/0	Internet
10.0.0.0/8	None
172.16.0.0/12	None
192.168.0.0/16	None
100.64.0.0/10	None
The Next hop type column shows the network path taken by traffic sent to each address prefix. The path can be one of the following hop types:

Virtual network: A route is created in the address prefix. The prefix represents each address range created at the virtual-network level. If multiple address ranges are specified, multiple routes are created for each address range.
Internet: The default system route 0.0.0.0/0 routes any address range to the internet, unless you override Azure's default route with a custom route.
None: Any traffic routed to this hop type is dropped and doesn't get routed outside the subnet. By default, the following IPv4 private-address prefixes are created: 10.0.0.0/8, 172.16.0.0/12, and 192.168.0.0/16. The prefix 100.64.0.0/10 for a shared address space is also added. None of these address ranges are globally routable.
The following diagram shows an overview of system routes and shows how traffic flows among subnets and the internet by default. You can see from the diagram that traffic flows freely among the two subnets and the internet.

Diagram of traffic flowing among subnets and the internet.

Within Azure, there are other system routes. Azure creates these routes if the following capabilities are enabled:

Virtual network peering
Service chaining
Virtual network gateway
Virtual network service endpoint
Virtual network peering and service chaining
Virtual network peering and service chaining let virtual networks within Azure be connected to one another. With this connection, virtual machines can communicate with each other within the same region or across regions. This communication in turn creates more routes within the default route table. Service chaining lets you override these routes by creating user-defined routes between peered networks.

The following diagram shows two virtual networks with peering configured. The user-defined routes are configured to route traffic through an NVA or an Azure VPN gateway.

Diagram of virtual network peering with user-defined routes.

Virtual network gateway
Use a virtual network gateway to send encrypted traffic between Azure and on-premises over the internet and to send encrypted traffic between Azure networks. A virtual network gateway contains routing tables and gateway services.

Diagram of the structure of a virtual network gateway.

Virtual network service endpoint
Virtual network endpoints extend your private address space in Azure by providing a direct connection to your Azure resources. This connection restricts the flow of traffic: your Azure virtual machines can access your storage account directly from the private address space and deny access from a public virtual machine. As you enable service endpoints, Azure creates routes in the route table to direct this traffic.

Custom routes
System routes might make it easy for you to quickly get your environment up and running. However, there are many scenarios in which you want to more closely control the traffic flow within your network. For example, you might want to route traffic through an NVA or through a firewall. This control is possible with custom routes.

You have two options for implementing custom routes: create a user-defined route, or use Border Gateway Protocol (BGP) to exchange routes between Azure and on-premises networks.

User-defined routes
You can use a user-defined route to override the default system routes so traffic can be routed through firewalls or NVAs.

For example, you might have a network with two subnets and want to add a virtual machine in the perimeter network to be used as a firewall. You can create a user-defined route so that traffic passes through the firewall and doesn't go directly between the subnets.

When creating user-defined routes, you can specify these next hop types:

Virtual appliance: A virtual appliance is typically a firewall device used to analyze or filter traffic that is entering or leaving your network. You can specify the private IP address of a Network Interface Card (NIC) attached to a virtual machine so that IP forwarding can be enabled. Or you can provide the private IP address of an internal load balancer.
Virtual network gateway: Use to indicate when you want routes for a specific address to be routed to a virtual network gateway. The virtual network gateway is specified as a VPN for the next hop type.
Virtual network: Use to override the default system route within a virtual network.
Internet: Use to route traffic to a specified address prefix that is routed to the internet.
None: Use to drop traffic sent to a specified address prefix.
With user-defined routes, you can't specify the next hop type VirtualNetworkServiceEndpoint, which indicates virtual network peering.

Service tags for user-defined routes
You can specify a service tag as the address prefix for a user-defined route instead of an explicit IP range. A service tag represents a group of IP address prefixes from a given Azure service. Microsoft manages the address prefixes encompassed by the service tag and automatically updates the service tag as addresses change. Thus minimizing the complexity of frequent updates to user-defined routes and reducing the number of routes you need to create.

Border gateway protocol
A network gateway in your on-premises network can exchange routes with a virtual network gateway in Azure by using BGP. BGP is the standard routing protocol that is normally used to exchange routing information among two or more networks. BGP is used to transfer data and information between autonomous systems on the internet, such as different host gateways.

Typically, you use BGP to advertise on-premises routes to Azure when you're connected to an Azure datacenter through Azure ExpressRoute. You can also configure BGP if you connect to an Azure virtual network by using a VPN site-to-site connection.

The following diagram shows a topology with paths that can pass data between Azure VPN Gateway and on-premises networks:

Diagram showing an example of using the Border Gateway Protocol.

BGP offers network stability, because routers can quickly change connections to send packets if a connection path goes down.

Route selection and priority
If multiple routes are available in a route table, Azure uses the route with the longest prefix match. For example, a message is sent to the IP address 10.0.0.2, but two routes are available with the 10.0.0.0/16 and 10.0.0.0/24 prefixes. Azure selects the route with the 10.0.0.0/24 prefix because it's more specific.

The longer the route prefix, the shorter the list of IP addresses available through that prefix. When you use longer prefixes, the routing algorithm can select the intended address more quickly.

You can't configure multiple user-defined routes with the same address prefix.

If there are multiple routes with the same address prefix, Azure selects the route based on the type in the following order of priority:

User-defined routes
BGP routes
System routes
Check your knowledge

1. Why would you use a custom route in a virtual network? 

To load balance the traffic within your virtual network.

To connect to your Azure virtual machines using RDP or SSH.

To control the flow of traffic within your Azure virtual network.

To connect to resources in another virtual network hosted in Azure.

2. Why might you use virtual network peering? 

To connect virtual networks together in the same region or across regions.

To assign public IP addresses to all of your resources across multiple virtual networks.

So that load balancers can control traffic flow across your virtual networks.

To run custom reports that scan and identify what resources are running across all of your virtual networks, as opposed to running reports on each virtual network.


3- Exercise - Create custom routes

As you implement your security strategy, you want to control how network traffic is routed across your Azure infrastructure.

In the following exercise, you use a network virtual appliance (NVA) to help secure and monitor traffic. You want to ensure communication between front-end public servers and internal private servers is always routed through the appliance.

You configure the network so that all traffic flowing from a public subnet to a private subnet will be routed through the NVA. To make this flow happen, you create a custom route for the public subnet to route this traffic to a perimeter-network subnet. Later, you deploy an NVA to the perimeter-network subnet.

Diagram of virtual network, subnets, and route table.

In this exercise, you create the route table, custom route, and subnets. You'll then associate the route table with a subnet.

Create a route table and custom route
The first task is to create a new routing table and then add a custom route for all traffic intended for the private subnet.

 Note

You might get an error that reads: This command is implicitly deprecated. Please ignore this error for this learning module. We are working on it!

In the Cloud Shell window on the right side of the screen, select the More icon (...), then select Settings > Go to Classic version.

In Azure Cloud Shell, run the following command to create a route table.

Azure CLI

Copy
    az network route-table create \
        --name publictable \
        --resource-group "[sandbox resource group name]" \
        --disable-bgp-route-propagation false
Run the following command in Cloud Shell to create a custom route.
Azure CLI

Copy
    az network route-table route create \
        --route-table-name publictable \
        --resource-group "[sandbox resource group name]" \
        --name productionsubnet \
        --address-prefix 10.0.1.0/24 \
        --next-hop-type VirtualAppliance \
        --next-hop-ip-address 10.0.2.4
Create a virtual network and subnets
The next task is to create the vnet virtual network and the three subnets you need: publicsubnet, privatesubnet, and dmzsubnet.

Run the following command to create the vnet virtual network and the publicsubnet subnet.
Azure CLI

Copy
    az network vnet create \
        --name vnet \
        --resource-group "[sandbox resource group name]" \
        --address-prefixes 10.0.0.0/16 \
        --subnet-name publicsubnet \
        --subnet-prefixes 10.0.0.0/24
Run the following command in Cloud Shell to create the privatesubnet subnet.
Azure CLI

Copy
    az network vnet subnet create \
        --name privatesubnet \
        --vnet-name vnet \
        --resource-group "[sandbox resource group name]" \
        --address-prefixes 10.0.1.0/24
Run the following command to create the dmzsubnet subnet.
Azure CLI

Copy
    az network vnet subnet create \
        --name dmzsubnet \
        --vnet-name vnet \
        --resource-group "[sandbox resource group name]" \
        --address-prefixes 10.0.2.0/24
You should now have three subnets. Run the following command to show all of the subnets in the vnet virtual network.
Azure CLI

Copy
    az network vnet subnet list \
        --resource-group "[sandbox resource group name]" \
        --vnet-name vnet \
        --output table
Associate the route table with the public subnet
The final task in this exercise is to associate the route table with the publicsubnet subnet.

Run the following command to associate the route table with the public subnet.

Azure CLI

Copy
    az network vnet subnet update \
        --name publicsubnet \
        --vnet-name vnet \
        --resource-group "[sandbox resource group name]" \
        --route-table publictable



Next unit: What is an NVA?

4- What is an NVA?

A network virtual appliance (NVA) is a virtual appliance that consists of various layers like:

a firewall
a WAN optimizer
application-delivery controllers
routers
load balancers
IDS/IPS
proxies
You can deploy NVAs chosen from providers in Azure Marketplace. Such providers include Cisco, Check Point, Barracuda, Sophos, WatchGuard, and SonicWall. You can use an NVA to filter traffic inbound to a virtual network, to block malicious requests, and to block requests made from unexpected resources.

In the retail-organization example scenario, you must work with the security and network teams. You want to implement a secure environment that scrutinizes all incoming traffic and blocks unauthorized traffic from passing on to the internal network. You also want to secure both virtual-machine networking and Azure-services networking as part of your company's network-security strategy.

Your goal is to prevent unwanted or unsecured network traffic from reaching key systems.

As part of the network-security strategy, you must control the flow of traffic within your virtual network. You also must learn the role of an NVA and the benefit of using an NVA to control traffic flow through an Azure network.

Network virtual appliance
Network virtual appliances (NVAs) are virtual machines that control the flow of network traffic by controlling routing. You'll typically use them to manage traffic flowing from a perimeter-network environment to other networks or subnets.

Visualization of a network architecture with a network virtual appliance.

You can deploy firewall appliances into a virtual network in different configurations. You can put a firewall appliance in a perimeter-network subnet in the virtual network or if you want more control of security, implement a microsegmentation approach.

With the microsegmentation approach, you can create dedicated subnets for the firewall and then deploy web applications and other services in other subnets. All traffic is routed through the firewall and inspected by the NVAs. You'll enable forwarding on the virtual-appliance network interfaces to pass traffic that is accepted by the appropriate subnet.

Microsegmentation lets the firewall inspect all packets at OSI Layer 4 and, for application-aware appliances, Layer 7. When you deploy an NVA to Azure, it acts as a router that forwards requests between subnets on the virtual network.

Some NVAs require multiple network interfaces. One network interface is dedicated to the management network for the appliance. Additional network interfaces manage and control the traffic processing. After you’ve deployed the NVA, you can then configure the appliance to route the traffic through the proper interface.

User-defined routes
For most environments, the default system routes already defined by Azure are enough to get the environments up and running. In certain cases, you should create a routing table and add custom routes. Examples include:

Access to the internet via on-premises network using forced tunneling
Using virtual appliances to control traffic flow
You can create multiple route tables in Azure. Each route table can be associated with one or more subnets. A subnet can only be associated with one route table.

Network virtual appliances in a highly available architecture
If traffic is routed through an NVA, the NVA becomes a critical piece of your infrastructure. Any NVA failures will directly affect the ability of your services to communicate. It's important to include a highly available architecture in your NVA deployment.

There are several methods of achieving high availability when using NVAs. At the end of this module, you can find more information about using NVAs in highly available scenarios.

Check your knowledge

1. What is the main benefit of using a network virtual appliance? 

To control outbound access to the internet.

To load balance incoming traffic from the internet across multiple Azure virtual machines and across two regions for DR purposes.

To control incoming traffic from the perimeter network and allow only traffic that meets security requirements to pass through.

To control who can access Azure resources from the perimeter network.

2. How might you deploy a network virtual appliance? 

You can configure a Windows virtual machine and enable IP forwarding after routing tables, user-defined routes, and subnets have been updated. Or you can use a partner image from Azure Marketplace.

Using Azure CLI, deploy a Linux virtual machine in Azure, connect this virtual machine to your production virtual network, and assign a public IP address.

Using the Azure portal, deploy a Windows 2016 Server instance. Next, using Azure Application Gateway, add the Windows 2016 Server instance as a target endpoint.

Download a virtual appliance from Azure Marketplace and configure the appliance to connect to the production and perimeter networks.


5- Exercise - Create an NVA and virtual machines

In the next stage of your security implementation, you'll deploy a network virtual appliance (NVA) to secure and monitor traffic between your front-end public servers and internal private servers.

You configure the appliance to forward IP traffic. If IP forwarding isn't enabled, traffic that is routed through your appliance will never be received by its intended destination servers.

In this exercise, you deploy the nva network appliance to the dmzsubnet subnet. Then you enable IP forwarding so that traffic from * and traffic that uses the custom route is sent to the privatesubnet subnet.

Visualization of a Network virtual appliance with IP forwarding enabled.

In the following steps, you'll deploy an NVA. You'll then update the Azure virtual NIC and the network settings within the appliance to enable IP forwarding.

Deploy the network virtual appliance
To build the NVA, deploy an Ubuntu LTS instance.

In Cloud Shell, run the following command to deploy the appliance. Replace <password> with a suitable password of your choice for the azureuser admin account.

Azure CLI

Copy
az vm create \
    --resource-group "[sandbox resource group name]" \
    --name nva \
    --vnet-name vnet \
    --subnet dmzsubnet \
    --image Ubuntu2204 \
    --admin-username azureuser \
    --admin-password <password>
Enable IP forwarding for the Azure network interface
In the next steps, IP forwarding for the nva network appliance is enabled. When traffic flows to the NVA but is meant for another target, the NVA will route that traffic to its correct destination.

Run the following command to get the ID of the NVA network interface.

Azure CLI

Copy
NICID=$(az vm nic list \
    --resource-group "[sandbox resource group name]" \
    --vm-name nva \
    --query "[].{id:id}" --output tsv)

echo $NICID
Run the following command to get the name of the NVA network interface.

Azure CLI

Copy
NICNAME=$(az vm nic show \
    --resource-group "[sandbox resource group name]" \
    --vm-name nva \
    --nic $NICID \
    --query "{name:name}" --output tsv)

echo $NICNAME
Run the following command to enable IP forwarding for the network interface.

Azure CLI

Copy
az network nic update --name $NICNAME \
    --resource-group "[sandbox resource group name]" \
    --ip-forwarding true
Enable IP forwarding in the appliance
Run the following command to save the public IP address of the NVA virtual machine to the variable NVAIP.

Azure CLI

Copy
NVAIP="$(az vm list-ip-addresses \
    --resource-group "[sandbox resource group name]" \
    --name nva \
    --query "[].virtualMachine.network.publicIpAddresses[*].ipAddress" \
    --output tsv)"

echo $NVAIP
Run the following command to enable IP forwarding within the NVA.

Bash

Copy
ssh -t -o StrictHostKeyChecking=no azureuser@$NVAIP 'sudo sysctl -w net.ipv4.ip_forward=1; exit;'
When prompted, enter the password you used when you created the virtual machine.


Next unit: Exercise - Route traffic through the NVA

6- Exercise - Route traffic through the NVA

Now that you've created the network virtual appliance (NVA) and virtual machines (VMs), you'll route the traffic through the NVA.

Visualization of virtual machines and IP addresses.

Create public and private virtual machines
The next steps deploy a VM into the public and private subnets.

Open the Cloud Shell editor and create a file named cloud-init.txt.

Bash

Copy
code cloud-init.txt
Add the following configuration information to the file. With this configuration, the inetutils-traceroute package is installed when you create a new VM. This package contains the traceroute utility that you'll use later in this exercise.

Text

Copy
#cloud-config
package_upgrade: true
packages:
   - inetutils-traceroute
Press Ctrl+S to save the file, and then press Ctrl+Q to close the editor.

In Cloud Shell, run the following command to create the public VM. Replace <password> with a suitable password for the azureuser account.

Azure CLI

Copy
az vm create \
    --resource-group "[sandbox resource group name]" \
    --name public \
    --vnet-name vnet \
    --subnet publicsubnet \
    --image Ubuntu2204 \
    --admin-username azureuser \
    --no-wait \
    --custom-data cloud-init.txt \
    --admin-password <password>
Run the following command to create the private VM. Replace <password> with a suitable password.

Azure CLI

Copy
az vm create \
    --resource-group "[sandbox resource group name]" \
    --name private \
    --vnet-name vnet \
    --subnet privatesubnet \
    --image Ubuntu2204 \
    --admin-username azureuser \
    --no-wait \
    --custom-data cloud-init.txt \
    --admin-password <password>
Run the following Linux watch command to check that the VMs are running. The watch command periodically runs the az vm list command so that you can monitor the progress of the VMs.

Bash

Copy
watch -d -n 5 "az vm list \
    --resource-group "[sandbox resource group name]" \
    --show-details \
    --query '[*].{Name:name, ProvisioningState:provisioningState, PowerState:powerState}' \
    --output table"
A ProvisioningState value of "Succeeded" and a PowerState value of "VM running" indicate a successful deployment. When all three VMs are running, you're ready to move on. Press Ctrl-C to stop the command and continue with the exercise.

Run the following command to save the public IP address of the public VM to a variable named PUBLICIP.

Azure CLI

Copy
PUBLICIP="$(az vm list-ip-addresses \
    --resource-group "[sandbox resource group name]" \
    --name public \
    --query "[].virtualMachine.network.publicIpAddresses[*].ipAddress" \
    --output tsv)"

echo $PUBLICIP
Run the following command to save the public IP address of the private VM to a variable named PRIVATEIP.

Azure CLI

Copy
PRIVATEIP="$(az vm list-ip-addresses \
    --resource-group "[sandbox resource group name]" \
    --name private \
    --query "[].virtualMachine.network.publicIpAddresses[*].ipAddress" \
    --output tsv)"

echo $PRIVATEIP
Test traffic routing through the network virtual appliance
The final steps use the Linux traceroute utility to show how traffic is routed. You'll use the ssh command to run traceroute on each VM. The first test will show the route taken by ICMP packets sent from the public VM to the private VM. The second test will show the route taken by ICMP packets sent from the private VM to the public VM.

Run the following command to trace the route from public to private. When prompted, enter the password for the azureuser account that you specified earlier.

Bash

Copy
ssh -t -o StrictHostKeyChecking=no azureuser@$PUBLICIP 'traceroute private --type=icmp; exit'
If you receive the error message bash: traceroute: command not found, wait a minute and retry the command. The automated installation of traceroute can take a minute or two after VM deployment. After the command succeeds, the output should look similar to the following example:

Text

Copy
traceroute to private.kzffavtrkpeulburui2lgywxwg.gx.internal.cloudapp.net (10.0.1.4), 64 hops max
1   10.0.2.4  0.710ms  0.410ms  0.536ms
2   10.0.1.4  0.966ms  0.981ms  1.268ms
Connection to 52.165.151.216 closed.
Notice that the first hop is to 10.0.2.4. This address is the private IP address of nva. The second hop is to 10.0.1.4, the address of private. In the first exercise, you added this route to the route table and linked the table to the publicsubnet subnet. So now all traffic from public to private is routed through the NVA.

Diagram of route from public to private.

Run the following command to trace the route from private to public. When prompted, enter the password for the azureuser account.

Bash

Copy
ssh -t -o StrictHostKeyChecking=no azureuser@$PRIVATEIP 'traceroute public --type=icmp; exit'
You should see the traffic go directly to public (10.0.0.4) and not through the NVA, as shown in the following command output.

Text

Copy
traceroute to public.kzffavtrkpeulburui2lgywxwg.gx.internal.cloudapp.net (10.0.0.4), 64 hops max
1   10.0.0.4  1.095ms  1.610ms  0.812ms
Connection to 52.173.21.188 closed.
The private VM is using default routes, and traffic is routed directly between the subnets.

Diagram of route from private to public.

You've now configured routing between subnets to direct traffic from the public internet through the dmzsubnet subnet before it reaches the private subnet. In the dmzsubnet subnet, you added a VM that acts as an NVA. You can configure this NVA to detect potentially malicious requests and block them before they reach their intended targets.

Next unit: Summary

Summary

In this module, you learned how to customize routes in an Azure virtual network and how to redirect the traffic flow through a network virtual appliance. You also learned how to create your own custom network virtual appliance by deploying an Azure virtual machine.

Clean up
The sandbox automatically cleans up your resources when you're finished with this module.

When you're working in your own subscription, it's a good idea at the end of a project to identify whether you still need the resources you created. Resources that you leave running can cost you money. You can delete resources individually or delete the resource group to delete the entire set of resources.

Learn more
For more information on using routes in your network infrastructure, see the following articles:

Virtual network traffic routing
Tutorial: Route network traffic with a route table using the Azure portal
Deploy highly available network virtual appliances
Implement a DMZ between Azure and the Internet








Point 11: Improve application scalability and resiliency by using Azure Load Balancer

Discuss the different load balancers in Azure and how to choose the right Azure load balancer solution to meet your requirements.

Learning objectives
In this module, you will:

Identify the features and capabilities of Azure Load Balancer.
Deploy and configure an Azure Load Balancer.


1- Introduction

Many apps need to be resilient to failure and scale easily when demand increases. You can address those needs by using Azure Load Balancer.

Suppose you work for a healthcare organization that's launching a new portal application with which patients can schedule appointments. The application has a patient portal, a web-application front end, and a business-tier database. The front end uses the database to retrieve and save patient information.

The new portal needs to be available around the clock to handle failures. The portal must adjust to load fluctuations by adding and removing resources to match the load. The organization needs a solution that distributes work to virtual machines across the system as virtual machines are added. The solution should detect failures and reroute jobs to virtual machines as needed. Improved resiliency and scalability help ensure that patients can schedule appointments from any location.

By the end of this module, you'll be able to use Azure Load Balancer to build a resilient and scalable app architecture.

Learning objectives
In this module, you'll:

Identify the features and capabilities of Azure Load Balancer.
Deploy and configure an instance of Azure Load Balancer.
Prerequisites
Basic knowledge of networking concepts
Basic knowledge of Azure virtual machines
Familiarity with the Azure portal



Next unit: Azure Load Balancer features and capabilities

2- Azure Load Balancer features and capabilities

With Azure Load Balancer, you can spread user requests across multiple virtual machines or other services, allowing you to scale the app to larger sizes than a single virtual machine can support and ensuring that users get service even when a virtual machine fails.

In your healthcare organization, you can expect large user demand. It's vitally important that each user can book an appointment, even during peak demand or when one or more virtual machines fail. If you use multiple virtual servers for your front end and a load balancer to distribute traffic among them, you achieve a high capacity because all the virtual servers collaborate to satisfy requests. You also improve resilience because the load balancer can automatically reroute traffic when a virtual server fails.

Here, you'll learn how Load Balancer's features can help you create robust app architectures.

Distribute traffic with Azure Load Balancer
Azure Load Balancer is a service you can use to distribute traffic across multiple virtual machines. Use Load Balancer to scale applications and create high availability for your virtual machines and services. Load balancers use a hash-based distribution algorithm. By default, a five-tuple hash is used to map traffic to available servers. The hash is made from the following elements:

Source IP: The IP address of the requesting client.
Source port: The port of the requesting client.
Destination IP: The destination IP of the request.
Destination port: The destination port of the request.
Protocol type: The specified protocol type, TCP or UDP.
Diagram showing an overview of Azure Load Balancer.

Load Balancer supports inbound and outbound scenarios, provides low latency and high throughput, and scales up to millions of flows for TCP and UDP applications.

Load balancers aren't physical instances. Load-balancer objects are used to express how Azure configures its infrastructure to meet your requirements.

With Load Balancer, you can use availability sets and availability zones to ensure that virtual machines are always available:

Configuration	Service level agreement (SLA)	Information
Availability set	99.95%	Protection from hardware failures within datacenters
Availability zone	99.99%	Protection from entire datacenter failure
Availability sets
An availability set is a logical grouping used to isolate virtual machine resources from each other when they're deployed. Azure ensures that the virtual machines you put in an availability set run across multiple physical servers, compute racks, storage units, and network switches. If there's a hardware or software failure, only a subset of your virtual machines is affected. Your overall solution stays operational. Availability sets are essential for building reliable cloud solutions.

Diagram showing an overview of availability sets in Azure.

Availability zones
An availability zone offers groups of one or more datacenters that have independent power, cooling, and networking. The virtual machines in an availability zone are placed in different physical locations within the same region. Use this architecture when you want to ensure that you can continue to serve users when an entire datacenter fail.

Diagram showing an overview of availability zones in Azure.

Availability zones don't support all virtual machine sizes and aren't available in all Azure regions. Check that they're supported in your region before you use them in your architecture.

Select the right Load Balancer product
Two products are available when you create a load balancer in Azure: basic load balancers and standard load balancers.

Basic load balancers allow:

Port forwarding
Automatic reconfiguration
Health probes
Outbound connections through source network address translation (SNAT)
Diagnostics through Azure Log Analytics for public-facing load balancers
You can only use basic load balancers with a single availability set or scale set.

Standard load balancers support all of the basic load balancer features. They also allow:

HTTPS health probes
Availability zones
Diagnostics through Azure Monitor, for multidimensional metrics
High availability (HA) ports
Outbound rules
A guaranteed SLA (99.99% for two or more virtual machines)
Internal and external load balancers
An external load balancer operates by distributing client traffic across multiple virtual machines. An external load balancer permits traffic from the internet. The traffic might come from browsers, mobile apps, or other sources. In a healthcare organization, the balancer distributes the load of all the browsers that run the client healthcare application.

An internal load balancer distributes a load from internal Azure resources to other Azure resources. For example, if you have front-end web servers that need to call business logic that's hosted on multiple middle-tier servers, you can distribute that load evenly by using an internal load balancer. No traffic is allowed from internet sources. In a healthcare organization, a load balancer distributes a load across the internal application tier.

Check your knowledge

1. What is the default distribution type for traffic through a load balancer? 

Source IP affinity

Five-tuple hash

Three-tuple hash

2. What is the main advantage of an availability set? 

It allows virtual machines to be available across datacenter failures.

It allows virtual machines to be available across physical server failures.

It allows virtual machines to be grouped into logical categories.


3- Configure a public load balancer

As the solution architect for the healthcare portal, you need to distribute the load from the client browsers over the virtual machines in your web farm. You need to set up a load balancer and configure the virtual machines to be balanced.

A public load balancer maps the public IP address and port number of incoming traffic to the private IP address and port number of a virtual machine in the back-end pool. The responses are then returned to the client. By applying load-balancing rules, you can distribute specific types of traffic across multiple virtual machines or services.

Distribution modes
By default, Azure Load Balancer distributes network traffic equally among virtual machine instances. The following distribution modes are also possible if a different behavior is required:

Five-tuple hash: The default distribution mode for Load Balancer is a five-tuple hash. The tuple is composed of source IP, source port, destination IP, destination port, and protocol type. Because the source port is included in the hash and the source port changes for each session, clients might be directed to a different virtual machine for each session.

Diagram showing how hash-based distribution works.

Source IP affinity: This distribution mode is also known as session affinity or client IP affinity. To map traffic to the available servers, the source IP affinity mode uses a two-tuple hash (from the source IP address and destination IP address) or a three-tuple hash (from the source IP address, destination IP address, and protocol type). The hash ensures that requests from a specific client are always sent to the same virtual machine behind the load balancer.

Diagram showing how session affinity works.

Choose a distribution mode
In the healthcare-portal example, imagine that a developer requirement of the presentation tier is to use in-memory sessions to store the signed-in user's profile as the user interacts with the portal.

In this scenario, the load balancer must provide source IP affinity to maintain a user's session. The profile is stored only on the virtual machine to which the client first connects, because that IP address is directed to the same server. When you create the load-balancer endpoint, you must specify the distribution mode by using the following PowerShell example:

PowerShell

Copy
$lb = Get-AzLoadBalancer -Name MyLb -ResourceGroupName MyResourceGroup
$lb.LoadBalancingRules[0].LoadDistribution = 'sourceIp'
Set-AzLoadBalancer -LoadBalancer $lb
To add session persistence through the Azure portal:

In the Azure portal, select your Load Balancer resource.

In the Load balancing rules page under the Settings pane, select the relevant load balancing rule.

Screenshot showing how to select a load balancing rule in the Azure portal.

In the load balancing rule settings page change the value for Session persistence from None to Client IP.

Screenshot showing how to set IP affinity in the Azure portal.

Load Balancer and Remote Desktop Gateway
Remote Desktop Gateway is a Windows service that you can use to enable clients on the internet to make Remote Desktop Protocol (RDP) connections through firewalls to Remote Desktop servers on your private network. The default five-tuple hash in Load Balancer is incompatible with this service. If you want to use Load Balancer with your Remote Desktop servers, use source IP affinity.

Load Balancer and media upload
Another use case for source IP affinity is media upload. In many implementations, a client initiates a session through a TCP protocol and connects to a destination IP address. This connection remains open throughout the upload to monitor progress, but the file is uploaded through a separate UDP protocol.

With the five-tuple hash, the load balancer likely sends the TCP and UDP connections to different destination IP addresses and the upload won't finish successfully. Use source IP affinity to resolve this issue.

Next unit: Exercise - Configure a public load balancer

4- Exercise - Configure a public load balancer

Choose your shell
You can configure Azure Load Balancer by using the Azure portal, PowerShell, or the Azure CLI.

In your healthcare organization, you want to load-balance client traffic to provide a consistent response based on the patient portal web servers' health. You have two virtual machines (VMs) in an availability set to act as your healthcare-portal web application.

Here, you create a load balancer resource and use it to distribute a load across the virtual machines.

Deploy the patient portal web application
First, deploy your patient-portal application across two virtual machines in a single availability set. To save time, let's start by running a script to create this application. The script:

Creates a virtual network and network infrastructure for the virtual machines.
Creates two virtual machines in this virtual network.
To deploy the patient portal web application:

Run the following git clone command in Azure Cloud Shell. The command clones the repo that contains the source for the app and runs the setup script from GitHub. Then changes to the directory of the cloned repo.

Bash

Copy
git clone https://github.com/MicrosoftDocs/mslearn-improve-app-scalability-resiliency-with-load-balancer.git
cd mslearn-improve-app-scalability-resiliency-with-load-balancer
As its name suggests, the script generates two virtual machines in a single availability set. It takes about two minutes to run.

Bash

Copy
bash create-high-availability-vm-with-sets.sh [sandbox resource group name]
When the script finishes, on the Azure portal menu or from the Home page, select Resource groups, then select the [sandbox resource group name] resource group. Review the resources created by the script.

Create a load balancer
Let's use the Azure CLI to create the load balancer and its associated resources.

Create a new public IP address.

Azure CLI

Copy
az network public-ip create \
  --resource-group [sandbox resource group name] \
  --allocation-method Static \
  --name myPublicIP
Create the load balancer.

Azure CLI

Copy
az network lb create \
  --resource-group [sandbox resource group name] \
  --name myLoadBalancer \
  --public-ip-address myPublicIP \
  --frontend-ip-name myFrontEndPool \
  --backend-pool-name myBackEndPool
To allow the load balancer to monitor the healthcare portal's status, create a health probe. The health probe dynamically adds or removes virtual machines from the load-balancer rotation based on their response to health checks.

Azure CLI

Copy
az network lb probe create \
  --resource-group [sandbox resource group name] \
  --lb-name myLoadBalancer \
  --name myHealthProbe \
  --protocol tcp \
  --port 80  
Now, you need a load balancer rule to define how traffic is distributed to the virtual machines. You define the front-end IP configuration for the incoming traffic and the back-end IP pool to receive the traffic, along with the required source and destination port. To make sure only healthy virtual machines receive traffic, you also define the health probe to use.

Azure CLI

Copy
az network lb rule create \
  --resource-group [sandbox resource group name] \
  --lb-name myLoadBalancer \
  --name myHTTPRule \
  --protocol tcp \
  --frontend-port 80 \
  --backend-port 80 \
  --frontend-ip-name myFrontEndPool \
  --backend-pool-name myBackEndPool \
  --probe-name myHealthProbe
Connect the virtual machines to the back-end pool by updating the network interfaces you created in the script to use the back-end pool information.

Azure CLI

Copy
az network nic ip-config update \
  --resource-group [sandbox resource group name] \
  --nic-name webNic1 \
  --name ipconfig1 \
  --lb-name myLoadBalancer \
  --lb-address-pools myBackEndPool

az network nic ip-config update \
  --resource-group [sandbox resource group name] \
  --nic-name webNic2 \
  --name ipconfig1 \
  --lb-name myLoadBalancer \
  --lb-address-pools myBackEndPool
Run the following command to get the load balancer's public IP address and your website's URL:

Azure CLI

Copy
echo http://$(az network public-ip show \
                --resource-group [sandbox resource group name] \
                --name myPublicIP \
                --query ipAddress \
                --output tsv)
Test the load balancer configuration
Let's test the load balancer setup to show how it can handle availability and health issues dynamically.

In a new browser tab, go to the public IP address that you noted. A response from one of the virtual machines is displayed in the browser.

Try a "force refresh" by pressing Ctrl+F5 a few times to see that the response is returned randomly from both virtual machines.

On the Azure portal menu or from the Home page, select All resources. Then select webVM1, and select Stop.

Return to the tab that shows the website and force a refresh of the webpage. All requests are returned from webVM2.

Next unit: Internal load balancer

5- Internal load balancer

In addition to balancing requests from users to front-end servers, you can use Azure Load Balancer to distribute traffic from front-end servers evenly among back-end servers.

In your healthcare organization, front-end servers call business logic services hosted on a middle tier. You want to ensure that the middle tier is as scalable and resilient as the front end. You want to use a load balancer to distribute requests from the front-end servers evenly among the middle-tier servers. This way, you can scale out the middle-tier servers to achieve the highest capacity possible. You'll also ensure that the middle tier is resilient to failure. When a server fails, the load balancer automatically reroutes traffic to another server.

Here, you learn how to use load balancers to distribute internal traffic.

Configure an internal load balancer
In the healthcare-portal scenario, a web tier handles requests from users. The web tier connects to databases to retrieve data for users. The database tier is also deployed on two virtual machines. To allow the front-end web portal to continue to serve client requests if a database server fails, you can set up an internal load balancer to distribute traffic to the database servers.

You can configure an internal load balancer in almost the same way as an external load balancer, but with these differences:

When you create the load balancer, select Internal for the Type value. When you select this setting, the load balancer's front-end IP address isn't exposed to the internet.
Assign a private IP address instead of a public IP address for the load balancer's front end.
Place the load balancer in the protected virtual network that contains the virtual machines you want to handle the requests.
The internal load balancer should be visible only to the web tier. All the virtual machines that host the databases are in one subnet. You can use an internal load balancer to distribute traffic to those virtual machines.

Diagram showing internal load balancer.

Choose the distribution mode
In the healthcare portal, the application tier is stateless, so you don't need to use source IP affinity. You can use the default distribution mode of a five-tuple hash. This mode offers the greatest scalability and resilience. The load balancer routes traffic to any healthy server.

Check your knowledge

1. Which configuration is required to configure an internal load balancer? 

Virtual machines must be in the same virtual network.

Virtual machines must be publicly accessible.

Virtual machines must be in an availability set.

2. Which one of the following statements about external load balancers is correct? 

They have a private, front-facing IP address.

They don't have a listener IP address.

They have a public IP address.


Summary

In this module, you learned about Azure Load Balancer and how you can use Load Balancer to minimize the effect of failures and increase resilience and stability. You used this knowledge to create a resilient healthcare portal that can adapt to meet the application requirements of session affinity. You learned how to group virtual machines behind a load balancer to increase availability. By implementing the load balancer in the healthcare portal scenario, you learned about the differences between an internal and an external load balancer. You also discovered how you can configure a load balancer to provide availability across datacenters by using availability zones.

Clean up
The sandbox automatically cleans up your resources when you're finished with this module.

When you're working in your own subscription, it's a good idea at the end of a project to identify whether you still need the resources you created. Resources that you leave running can cost you money. You can delete resources individually or delete the resource group to delete the entire set of resources.

Learn more
Azure Load Balance documentation
What is Azure Load Balancer?
Quickstart: Create a public load balancer to load balance VMs using the Azure portal
Quickstart: Create an internal load balancer to load balance VMs using the Azure portal
What are Azure regions and availability zones?
Azure Administrator Associate

Chapter 4: Implement and manage storage in Azure

Modules in this learning path

Configure storage accounts

Learn how to configure storage accounts, including replication and endpoints.


Configure Azure Blob Storage

Learn how to configure Configure Azure Blob Storage, including tiers and object replication.


Configure Azure Storage security

Learn how to configure common Azure Storage security features like storage access signatures.


Configure Azure Files and Azure File Sync

Learn how to configure Azure Files and Azure File Sync.



Create an Azure Storage account

Create an Azure Storage account with the correct options for your business needs.



Control access to Azure Storage with shared access signatures

Grant access to data stored in your Azure Storage accounts securely by using shared access signatures.


Upload, download, and manage data with Azure Storage Explorer

Azure Storage Explorer allows you to quickly view all the storage services under your account. You can browse through, read, and edit data stored in those services through a user-friendly graphical interface.











Point 1: Configure storage accounts

Learn how to configure storage accounts, including replication and endpoints.

Learning objectives
In this module, you learn how to:

Identify features and usage cases for Azure storage accounts.

Select between different types of Azure Storage and create storage accounts.

Select a storage replication strategy.

Configure secure network access to storage endpoints.


1- Introduction

Azure Storage is Microsoft's cloud storage solution for modern data storage scenarios.

Suppose you work for a large e-commerce company that needs to store and serve a vast number of product images to its customers. The company wants a scalable and reliable solution that can handle high traffic and ensure data durability. They want to quickly restore data if there's an outage.

In this module, you learn how to configure storage accounts and select appropriate storage types in Azure. The module covers topics such as implementing replication strategies, and configuring secure access to storage.

The goal of this module is to provide Azure Administrators with the knowledge and skills to effectively configure and manage Azure storage accounts.

Learning objectives
In this module, you learn how to:

Identify features and usage cases for Azure storage accounts.
Select between different types of Azure Storage and create storage accounts.
Select a storage replication strategy.
Configure secure network access to storage endpoints.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Experience with the Azure portal.
Familiarity with managing different types of data storage.


Next unit: Implement Azure Storage

2- Implement Azure Storage

Azure Storage is Microsoft's cloud storage solution for modern data storage scenarios. Azure Storage offers a massively scalable object store for data objects. It provides a file system service for the cloud, a messaging store for reliable messaging, and a NoSQL store.

Azure Storage is a service that you can use to store files, messages, tables, and other types of information. You use Azure Storage for applications like file shares. Developers use Azure Storage for working data. Working data includes websites, mobile apps, and desktop applications. Azure Storage is also used by IaaS virtual machines, and PaaS cloud services.

Things to know about Azure Storage
You can think of Azure Storage as supporting three categories of data: structured data, unstructured data, and virtual machine data. Review the following categories and think about which types of storage are used in your organization.

Category	Description	Storage examples
Virtual machine data	Virtual machine data storage includes disks and files. Disks are persistent block storage for Azure IaaS virtual machines. Files are fully managed file shares in the cloud.	Storage for virtual machine data is provided through Azure managed disks. Data disks are used by virtual machines to store data like database files, website static content, or custom application code. The number of data disks you can add depends on the virtual machine size. Each data disk has a maximum capacity of 32,767 GB.
Unstructured data	Unstructured data is the least organized. Unstructured data may not have a clear relationship. The format of unstructured data is referred to as nonrelational.	Unstructured data can be stored by using Azure Blob Storage and Azure Data Lake Storage. Blob Storage is a highly scalable, REST-based cloud object store. Azure Data Lake Storage is the Hadoop Distributed File System (HDFS) as a service.
Structured data	Structured data is stored in a relational format that has a shared schema. Structured data is often contained in a database table with rows, columns, and keys. Tables are an autoscaling NoSQL store.	Structured data can be stored by using Azure Table Storage, Azure Cosmos DB, and Azure SQL Database. Azure Cosmos DB is a globally distributed database service. Azure SQL Database is a fully managed database-as-a-service built on SQL.
How to create a storage account

Storage account tiers
General purpose Azure storage accounts have two tiers: Standard and Premium.

Standard storage accounts are backed by magnetic hard disk drives (HDD). A standard storage account provides the lowest cost per GB. You can use Standard tier storage for applications that require bulk storage or where data is infrequently accessed.

Premium storage accounts are backed by solid-state drives (SSD) and offer consistent low-latency performance. You can use Premium tier storage for Azure virtual machine disks with I/O-intensive applications like databases.

 Note

You can't convert a Standard tier storage account to a Premium tier storage account or vice versa. You must create a new storage account with the desired type and copy data, if applicable, to a new storage account.

Things to consider when using Azure Storage
As you think about your configuration plan for Azure Storage, consider these prominent features.

Consider durability and availability. Azure Storage is durable and highly available. Redundancy ensures your data is safe during transient hardware failures. You replicate data across datacenters or geographical regions for protection from local catastrophe or natural disaster. Replicated data remains highly available during an unexpected outage.

Consider secure access. Azure Storage encrypts all data. Azure Storage provides you with fine-grained control over who has access to your data.

Consider scalability. Azure Storage is designed to be massively scalable to meet the data storage and performance needs of modern applications.

Consider manageability. Microsoft Azure handles hardware maintenance, updates, and critical issues for you.

Consider data accessibility. Data in Azure Storage is accessible from anywhere in the world over HTTP or HTTPS. Microsoft provides SDKs for Azure Storage in various languages. You can use .NET, Java, Node.js, Python, PHP, Ruby, Go, and the REST API. Azure Storage supports scripting in Azure PowerShell or the Azure CLI. The Azure portal and Azure Storage Explorer offer easy visual solutions for working with your data.

Next unit: Explore Azure Storage services


3- Explore Azure Storage services

Azure Storage offers four data services that can be accessed by using an Azure storage account:

Azure Blob Storage (containers): A massively scalable object store for text and binary data.

Azure Files: Managed file shares for cloud or on-premises deployments.

Azure Queue Storage: A messaging store for reliable messaging between application components.

Azure Table Storage: A service that stores nonrelational structured data (also known as structured NoSQL data).

Let's examine the details of these services.

Azure Blob Storage (containers)
Azure Blob Storage is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured or nonrelational data, such as text or binary data. Blob Storage is ideal for:

Serving images or documents directly to a browser.
Storing files for distributed access.
Streaming video and audio.
Storing data for backup and restore, disaster recovery, and archiving.
Storing data for analysis by an on-premises or Azure-hosted service.
Objects in Blob Storage can be accessed from anywhere in the world via HTTP or HTTPS. Users or client applications can access blobs via URLs, the Azure Storage REST API, Azure PowerShell, the Azure CLI, or an Azure Storage client library. The storage client libraries are available for multiple languages, including .NET, Java, Node.js, Python, PHP, and Ruby.

 Note

You can access data from Azure Blob Storage by using the NFS protocol.

Azure Files
Azure Files enables you to set up highly available network file shares. Shares can be accessed by using the Server Message Block (SMB) protocol and the Network File System (NFS) protocol. Multiple virtual machines can share the same files with both read and write access. You can also read the files by using the REST interface or the storage client libraries.

File shares can be used for many common scenarios:

Many on-premises applications use file shares. This feature makes it easier to migrate those applications that share data to Azure. If you mount the file share to the same drive letter that the on-premises application uses, the part of your application that accesses the file share should work with minimal, if any, changes.
Configuration files can be stored on a file share and accessed from multiple virtual machines. Tools and utilities used by multiple developers in a group can be stored on a file share, ensuring that everybody can find them, and that they use the same version.
Diagnostic logs, metrics, and crash dumps are just three examples of data that can be written to a file share and processed or analyzed later.
The storage account credentials are used to provide authentication for access to the file share. All users who have the share mounted should have full read/write access to the share.

Azure Queue Storage
Azure Queue Storage is used to store and retrieve messages. Queue messages can be up to 64 KB in size, and a queue can contain millions of messages. Queues are used to store lists of messages to be processed asynchronously.

Consider a scenario where you want your customers to be able to upload pictures, and you want to create thumbnails for each picture. You could have your customer wait for you to create the thumbnails while uploading the pictures. An alternative is to use a queue. When the customer finishes the upload, you can write a message to the queue. Then you can use an Azure Function to retrieve the message from the queue and create the thumbnails. Each of the processing parts can be scaled separately, which gives you more control when tuning the configuration.

Azure Table Storage
Azure Table storage is a service that stores non-relational structured data (also known as structured NoSQL data) in the cloud, providing a key/attribute store with a schemaless design. Because Table storage is schemaless, it's easy to adapt your data as the needs of your application evolve. Access to Table storage data is fast and cost-effective for many types of applications, and is typically lower in cost than traditional SQL for similar volumes of data. In addition to the existing Azure Table Storage service, there's a new Azure Cosmos DB Table API offering that provides throughput-optimized tables, global distribution, and automatic secondary indexes.

Things to consider when choosing Azure Storage services
As you think about your configuration plan for Azure Storage, consider the prominent features of the types of Azure Storage and which options support your application needs.

Consider storage optimization for massive data. Azure Blob Storage is optimized for storing massive amounts of unstructured data. Objects in Blob Storage can be accessed from anywhere in the world via HTTP or HTTPS. Blob Storage is ideal for serving data directly to a browser, streaming data, and storing data for backup and restore.

Consider storage with high availability. Azure Files supports highly available network file shares. On-premises apps use file shares for easy migration. By using Azure Files, all users can access shared data and tools. Storage account credentials provide file share authentication to ensure all users who have the file share mounted have the correct read/write access.

Consider storage for messages. Use Azure Queue Storage to store large numbers of messages. Queue Storage is commonly used to create a backlog of work to process asynchronously.

Consider storage for structured data. Azure Table Storage is ideal for storing structured, nonrelational data.

Next unit: Determine storage account types

4 - Determine storage account types

Azure Storage offers several storage account options. Each storage account supports different features and has its own pricing model.

Things to know about storage account types
Review the following options and think about what storage accounts are required to support your applications.

Storage account	Supported services	Recommended usage
Standard general-purpose v2	Blob Storage (including Data Lake Storage), Queue Storage, Table Storage, and Azure Files	Standard storage account for most scenarios, including blobs, file shares, queues, tables, and disks (page blobs).
Premium block blobs	Blob Storage (including Data Lake Storage)	Premium storage account for block blobs and append blobs. Recommended for applications with high transaction rates. Use Premium block blobs if you work with smaller objects or require consistently low storage latency. This storage is designed to scale with your applications.
Premium file shares	Azure Files	Premium storage account for file shares only. Recommended for enterprise or high-performance scale applications. Use Premium file shares if you require support for both Server Message Block (SMB) and NFS file shares.
Premium page blobs	Page blobs only	Premium high-performance storage account for page blobs only. Page blobs are ideal for storing index-based and sparse data structures, such as operating systems, data disks for virtual machines, and databases.
 Note

All storage account types are encrypted by using Storage Service Encryption (SSE) for data at rest.

How to manage your storage account


Next unit: Determine replication strategies

5- Determine replication strategies

The data in your Azure storage account is always replicated to ensure durability and high availability. Azure Storage replication copies your data to protect from planned and unplanned events. These events range from transient hardware failures, network or power outages, massive natural disasters, and so on. You can choose to replicate your data within the same data center, across zonal data centers within the same region, and even across regions. Replication ensures your storage account meets the Service-Level Agreement (SLA) for Azure Storage even if there are failures.

We explore four replication strategies:

Locally redundant storage (LRS)
Zone redundant storage (ZRS)
Geo-redundant storage (GRS)
Geo-zone-redundant storage (GZRS)
Locally redundant storage
Locally redundant storage is the lowest-cost replication option and offers the least durability compared to other strategies. If a data center-level disaster occurs, such as fire or flooding, all replicas might be lost or unrecoverable. Despite its limitations, LRS can be appropriate in several scenarios:

Your application stores data that can be easily reconstructed if data loss occurs.
Your data is constantly changing like in a live feed, and storing the data isn't essential.
Your application is restricted to replicating data only within a country/region due to data governance requirements.
Zone redundant storage
Zone redundant storage synchronously replicates your data across three storage clusters in a single region. Each storage cluster is physically separated from the others and resides in its own availability zone. Each availability zone, and the ZRS cluster within it, is autonomous, and has separate utilities and networking capabilities. Storing your data in a ZRS account ensures you can access and manage your data if a zone becomes unavailable. ZRS provides excellent performance and low latency.

ZRS isn't currently available in all regions.
Changing to ZRS from another data replication option requires the physical data movement from a single storage stamp to multiple stamps within a region.
Geo-redundant storage
Geo-redundant storage replicates your data to a secondary region (hundreds of miles away from the primary location of the source data). GRS provides a higher level of durability even during a regional outage. GRS is designed to provide at least 99.99999999999999% (16 9's) durability. When your storage account has GRS enabled, your data is durable even when there's a complete regional outage or a disaster where the primary region isn't recoverable.

If you implement GRS, you have two related options to choose from:

GRS replicates your data to another data center in a secondary region. The data is available to be read only if Microsoft initiates a failover from the primary to secondary region.

Read-access geo-redundant storage (RA-GRS) is based on GRS. RA-GRS replicates your data to another data center in a secondary region, and also provides you with the option to read from the secondary region. With RA-GRS, you can read from the secondary region regardless of whether Microsoft initiates a failover from the primary to the secondary.

For a storage account with GRS or RA-GRS enabled, all data is first replicated with locally redundant storage. An update is first committed to the primary location and replicated by using LRS. The update is then replicated asynchronously to the secondary region by using GRS. Data in the secondary region uses LRS. Both the primary and secondary regions manage replicas across separate fault domains and upgrade domains within a storage scale unit. The storage scale unit is the basic replication unit within the datacenter. Replication at this level is provided by LRS.

Geo-zone redundant storage
Geo-zone-redundant storage combines the high availability of zone-redundant storage with protection from regional outages as provided by geo-redundant storage. Data in a GZRS storage account is replicated across three Azure availability zones in the primary region, and also replicated to a secondary geographic region for protection from regional disasters. Each Azure region is paired with another region within the same geography, together making a regional pair.

With a GZRS storage account, you can continue to read and write data if an availability zone becomes unavailable or is unrecoverable. Additionally, your data is also durable during a complete regional outage or during a disaster in which the primary region isn't recoverable. GZRS is designed to provide at least 99.99999999999999% (16 9's) durability of objects over a given year. GZRS also offers the same scalability targets as LRS, ZRS, GRS, or RA-GRS. You can optionally enable read access to data in the secondary region with read-access geo-zone-redundant storage (RA-GZRS).

 Tip

Microsoft recommends using GZRS for applications that require consistency, durability, high availability, excellent performance, and resilience for disaster recovery. Enable RA-GZRS for read access to a secondary region when there's a regional disaster.

Things to consider when choosing replication strategies
Let's examine the scope of durability and availability for the different replication strategies. The following table describes several key factors during the replication process, including node unavailability within a data center, and whether the entire data center (zonal or nonzonal) becomes unavailable. The table identifies read access to data in a remote, geo-replicated region during region-wide unavailability, and the supported Azure storage account types.

Node in data center unavailable	Entire data center unavailable	Region-wide outage	Read access during region-wide outage
- LRS
- ZRS
- GRS
- RA-GRS
- GZRS
- RA-GZRS	- ZRS
- GRS
- RA-GRS
- GZRS
- RA-GZRS	- GRS
- RA-GRS
- GZRS
- RA-GZRS	- RA-GRS
- RA-GZRS



Next unit: Access storage

6- Access storage

Every object you store in Azure Storage has a unique URL address. Your storage account name forms the subdomain portion of the URL address. The combination of the subdomain and the domain name, which is specific to each service, forms an endpoint for your storage account.

Let's look at an example. If your storage account name is mystorageaccount, default endpoints for your storage account are formed for the Azure services as shown in the following table:

Service	Default endpoint
Container service	//mystorageaccount.blob.core.windows.net
Table service	//mystorageaccount.table.core.windows.net
Queue service	//mystorageaccount.queue.core.windows.net
File service	//mystorageaccount.file.core.windows.net
We create the URL to access an object in your storage account by appending the object's location in the storage account to the endpoint.

To access the myblob data in the mycontainer location in your storage account, we use the following URL address:

//mystorageaccount.blob.core.windows.net/mycontainer/myblob.

Configure custom domains
You can configure a custom domain to access blob data in your Azure storage account. As we reviewed, the default endpoint for Azure Blob Storage is \<storage-account-name>.blob.core.windows.net. If you map a custom domain and subdomain, such as www.contoso.com, to the blob or web endpoint for your storage account, your users can use that domain to access blob data in your storage account.

 Note

Azure Storage doesn't currently provide native support for HTTPS with custom domains. You can implement an Azure Content Delivery Network (CDN) to access blobs by using custom domains over HTTPS.

There are two ways to configure a custom domain: direct mapping and intermediary domain mapping.

Direct mapping lets you enable a custom domain for a subdomain to an Azure storage account. For this approach, you create a CNAME record that points from the subdomain to the Azure storage account.

The following example shows how a subdomain is mapped to an Azure storage account to create a CNAME record in the domain name system (DNS):

Subdomain: blobs.contoso.com
Azure storage account: \<storage account>\.blob.core.windows.net
Direct CNAME record: contosoblobs.blob.core.windows.net
Intermediary domain mapping is applied to a domain that's already in use within Azure. This approach might result in minor downtime while the domain is being mapped. To avoid downtime, you can use the asverify intermediary domain to validate the domain. By prepending the asverify keyword to your own subdomain, you permit Azure to recognize your custom domain without modifying the DNS record for the domain. After you modify the DNS record for the domain, your domain is mapped to the blob endpoint with no downtime.

The following example shows how a domain in use is mapped to an Azure storage account in the DNS with the asverify intermediary domain:

CNAME record: asverify.blobs.contoso.com
Intermediate CNAME record: asverify.contosoblobs.blob.core.windows.net
Learn more about intermediary domain mapping


Next unit: Secure storage endpoints

7- Secure storage endpoints

In the Azure portal, each Azure service requires certain steps to configure the service endpoints and restrict network access.

To access these settings for your storage account, you use the Firewalls and virtual networks settings. You add the virtual networks that should have access to the service for the account.

Screenshot of the Storage Account Firewalls and virtual networks settings in the Azure portal. One virtual network is selected and the firewall has an IP address range.

Things to know about configuring service endpoints
Here are some points to consider about configuring service access settings:

The Firewalls and virtual networks settings restrict access to your storage account from specific subnets on virtual networks or public IPs.

You can configure the service to allow access to one or more public IP ranges.

Subnets and virtual networks must exist in the same Azure region or region pair as your storage account.

 Important

Be sure to test the service endpoint and verify the endpoint limits access as expected.

How to connect to a storage account using private link

Next unit: Knowledge check

Your organization has diverse requirements for their cloud-hosted data. You're responsible for designing a plan to configure secure access.

The admin team requests your help with implementing a storage replication strategy. They have questions about how to configure storage accounts.

The manufacturing division has sensors that record time-relative data. Only the most recent data is useful. The company wants the lowest cost storage solution for this data.

Answer the following questions
Choose the best response for each question. Then select Check your answers.


1. Which storage solution replicates data to a secondary region? 

Locally redundant storage

Read-access geo-redundant storage

Zone-redundant storage

2. The admin team needs to know the requirements for storage account names. To what extent does a storage account name need to be unique? 

The name must be unique within the containing resource group.

The name must be unique within the organization's subscription.

The name must be globally unique.

3. What's the best storage account solution to support the requirements of the manufacturing division? 

Locally redundant storage

Geo-redundant storage

Zone-redundant storage


Summary and resources

In this module, you learned about Azure Storage and how to create a storage account.

The main takeaways from this module are:

Azure Storage provides a range of storage options for different types of data, including virtual machine data, unstructured data, and structured data.

There are different types of storage accounts available, each with its own features and pricing model. It's important to consider the specific requirements of your application when choosing the right storage account type.

Azure Storage offers four data services: Azure Blob Storage, Azure Files, Azure Queue Storage, and Azure Table Storage. Each service is optimized for different types of data and has its own use cases and benefits.

Replication is an important consideration for ensuring data durability and high availability. Azure Storage offers different replication strategies to choose from based on your requirements.

Configuring custom domains and secure endpoints allow you to access and secure your storage account in Azure.

Learn more with Azure documentation
Storage account overview. This article is your starting point for learning about Azure storage accounts.

Azure storage redundancy. This article reviews how to tradeoff cost and availability when selecting a redundancy option.

Use private endpoints for Azure Storage. This article shows when and how to configure Azure private endpoints for storage.

Learn more with self-paced training
Create an Azure storage account (sandbox). Learn how to create an Azure Storage account with the correct options for your business needs.

Design and implement private access to Azure Services. Learn how to implement private access to Azure Services with Azure Private Link, and virtual network service endpoints.

Provide disaster recovery by replicating storage data across regions and failing over to a secondary location. Learn to initiate storage account failover to the secondary region.



Point 2: Configure Azure Blob Storage

Learn how to configure Configure Azure Blob Storage, including tiers and object replication.

Learning objectives
In this module, you learn how to:

Understand the purpose and benefits of Azure Blob Storage.
Create and configure Azure Blob Storage accounts.
Manage containers and blobs within Azure Blob Storage.
Optimize blob storage performance and scalability.
Implement lifecycle management policies to automate data movement and deletion.
Determine the best pricing plans for your Azure Blob Storage.


1- Introduction

Azure Blob Storage is a service for storing large amounts of unstructured object data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data.

In this module, your media company has an extensive library of video clips that are accessed thousands of times a day. The company relies on you to configure Blob Storage for the video data. You plan to use access tiers to reduce cost and improve performance. You're developing a lifecycle management strategy for the older videos. Your plan also includes configuring object replication for failover.

Learning objectives
In this module, you will:

Understand the purpose and benefits of Azure Blob Storage.
Create and configure Azure Blob Storage accounts.
Manage containers and blobs within Azure Blob Storage.
Optimize blob storage performance and scalability.
Implement lifecycle management policies to automate data movement and deletion.
Determine the best pricing plans for your Azure Blob Storage.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Here are some common prerequisites that can be beneficial for understanding and successfully completing this module.

Basic understanding of cloud computing: Familiarity with cloud computing concepts, such as virtualization, scalability, and pay-as-you-go pricing models, can provide a foundation for understanding how Azure Blob Storage fits into the broader cloud ecosystem.

Knowledge of Azure fundamentals: Having a basic understanding of Microsoft Azure services and concepts, such as Azure Resource Manager, Azure Storage Accounts, and Azure Virtual Networks, can help you navigate and configure blob storage effectively.

Familiarity with storage concepts: Understanding fundamental storage concepts like file systems, directories, files, and data replication can be beneficial when working with blob storage.

Experience with Azure Portal or Azure CLI: Familiarity with the Azure Portal (web-based management interface) or Azure CLI (command-line interface) can help you navigate and configure blob storage resources efficiently.

Basic programming or scripting skills: While not always required, having some knowledge of programming or scripting languages like PowerShell or Python can be advantageous when automating blob storage configuration tasks.



Next unit: Implement Azure Blob Storage

2- Implement Azure Blob Storage

Azure Blob Storage is a service that stores unstructured data in the cloud as objects or blobs. Blob stands for Binary Large Object. Blob Storage is also referred to as object storage or container storage.

Things to know about Azure Blob Storage
Let's examine some configuration characteristics of Blob Storage.

Blob Storage can store any type of text or binary data. Some examples are text documents, images, video files, and application installers.

Blob Storage uses three resources to store and manage your data:

An Azure storage account
Containers in an Azure storage account
Blobs in a container
To implement Blob Storage, you configure several settings:

Blob container options
Blob types and upload options
Blob Storage access tiers
Blob lifecycle rules
Blob object replication options
The following diagram shows the relationship between the Blob Storage resources.

Diagram that shows the Azure Blob Storage architecture.

Things to consider when implementing Azure Blob Storage
There are many common uses for Blob Storage. Consider the following scenarios and think about your own data needs:

Consider browser uploads. Use Blob Storage to serve images or documents directly to a browser.

Consider distributed access. Blob Storage can store files for distributed access, such as during an installation process.

Consider streaming data. Stream video and audio by using Blob Storage.

Consider archiving and recovery. Blob Storage is a great solution for storing data for backup and restore, disaster recovery, and archiving.

Consider application access. You can store data in Blob Storage for analysis by an on-premises or Azure-hosted service.



Next unit: Create blob containers

3- Create blob containers

Azure Blob Storage uses a container resource to group a set of blobs. A blob can't exist by itself in Blob Storage. A blob must be stored in a container resource.

Things to know about containers and blobs
Let's look at the configuration characteristics of containers and blobs.

All blobs must be in a container.

A container can store an unlimited number of blobs.

An Azure storage account can contain an unlimited number of containers.

You can create the container in the Azure portal.

You upload blobs into a container.

How to move content between containers

Configure a container
In the Azure portal, you configure two settings to create a container for an Azure storage account. As you review these details, consider how you might organize containers in your storage account.

Screenshot that shows the container creation page and the public access level choices in the Azure portal.

Name: Enter a name for your container. The name must be unique within the Azure storage account.

The name can contain only lowercase letters, numbers, and hyphens.
The name must begin with a letter or a number.
The minimum length for the name is three characters.
The maximum length for the name is 63 characters.
Public access level: The access level specifies whether the container and its blobs can be accessed publicly. By default, container data is private and visible only to the account owner. There are three access level choices:

Private: (Default) Prohibit anonymous access to the container and blobs.
Blob: Allow anonymous public read access for the blobs only.
Container: Allow anonymous public read and list access to the entire container, including the blobs.
 Note

You can also create a blob container with PowerShell by using the New-AzStorageContainer command.


Next unit: Assign blob access tiers

4- Assign blob access tiers

Azure Storage supports several access tiers for blob data, including Hot, Cool, and Archive. Each access tier is optimized to support a particular pattern of data usage.

Things to know about blob access tiers
Let's examine characteristics of the blob access tiers.

Hot tier
The Hot tier is optimized for frequent reads and writes of objects in the Azure storage account. A good usage case is data that is actively being processed. By default, new storage accounts are created in the Hot tier. This tier has the lowest access costs, but higher storage costs than the Cool and Archive tiers.

Cool tier
The Cool tier is optimized for storing large amounts of data that's infrequently accessed. This tier is intended for data that remains in the Cool tier for at least 30 days. A usage case for the Cool tier is short-term backup and disaster recovery datasets and older media content. This content shouldn't be viewed frequently, but it needs to be immediately available. Storing data in the Cool tier is more cost-effective. Accessing data in the Cool tier can be more expensive than accessing data in the Hot tier.

Cold tier
The Cold tier is also optimized for storing large amounts of data that's infrequently accessed. This tier is intended for data that can remain in the tier for at least 90 days.

Archive tier
The Archive tier is an offline tier that's optimized for data that can tolerate several hours of retrieval latency. Data must remain in the Archive tier for at least 180 days or be subject to an early deletion charge. Data for the Archive tier includes secondary backups, original raw data, and legally required compliance information. This tier is the most cost-effective option for storing data. Accessing data is more expensive in the Archive tier than accessing data in the other tiers.

Compare access tiers
The access options for Azure Blob Storage offer a range of features and support levels to help you optimize your storage costs. As you compare the features and support, think about which access options can best support your application needs.

Comparison	Hot access tier	Cool access tier	Cold access tier	Archive access tier
Availability	99.9%	99%	99%	99%
Availability (RA-GRS reads)	99.99%	99.9%	99.9%	99.9%
Latency (time to first byte)	milliseconds	milliseconds	milliseconds	hours
Minimum storage duration	N/A	30 days	90 days	180 days
Configure the blob access tier
In the Azure portal, you can select the blob access tier for your Azure storage account. You can also change the blob access tier for your account at any time. By selecting the correct access tier for your needs, you can store your blob data in the most cost-effective manner.



Next unit: Add blob lifecycle management rules

5- Add blob lifecycle management rules

Every data set has a unique lifecycle. Early in the lifecycle, users tend to access some of the data in the set, but not all of the data. As the data set ages, access to all of the data in the set tends to dramatically reduce. Some data set stays idle in the cloud and is rarely accessed after it's stored. Some data expires within a few days or months after it's created. Other data is actively read and modified throughout the data set lifetime.

Azure Blob Storage supports lifecycle management for data sets. It offers a rich rule-based policy for GPv2 and Blob Storage accounts. You can use lifecycle policy rules to transition your data to the appropriate access tiers, and set expiration times for the end of a data set's lifecycle.

How to automatically manage Azure Blobs lifecycles | Azure Tips and Tricks

Things to know about lifecycle management
You can use Azure Blob Storage lifecycle management policy rules to accomplish several tasks.

Transition blobs to a cooler storage tier (Hot to Cool, Hot to Archive, Cool to Archive) to optimize for performance and cost.

Delete blobs at the end of their lifecycles.

Define rule-based conditions to run once per day at the Azure storage account level.

Apply rule-based conditions to containers or a subset of blobs.

Business scenario
Consider a scenario where data is frequently accessed in the early stages of the lifecycle, but only occasionally after two weeks. After the first month, the data set is rarely accessed. In this scenario, the Hot tier of Blob Storage is best during the early stages. Cool tier storage is most appropriate for occasional access. Archive tier storage is the best option after the data ages over a month. To achieve this transition, lifecycle management policy rules are available to move aging data to cooler tiers.

Configure lifecycle management policy rules
In the Azure portal, you create lifecycle management policy rules for your Azure storage account by specifying several settings. For each rule, you create If - Then block conditions to transition or expire data based on your specifications. As you review these details, consider how you can set up lifecycle management policy rules for your data sets.

Screenshot that shows how to add a lifecycle management policy rule for blob data in the Azure portal.

If: The If clause sets the evaluation clause for the policy rule. When the If clause evaluates to true, the Then clause is executed. Use the If clause to set the time period to apply to the blob data. The lifecycle management feature checks if the data is accessed or modified according to the specified time.

More than (days ago): The number of days to use in the evaluation condition.
Then: The Then clause sets the action clause for the policy rule. When the If clause evaluates to true, the Then clause is executed. Use the Then clause to set the transition action for the blob data. The lifecycle management feature transitions the data based on the setting.

Move to cool storage: The blob data is transitioned to Cool tier storage.
Move to archive storage: The blob data is transitioned to Archive tier storage.
Delete the blob: The blob data is deleted.
By designing policy rules to adjust storage tiers in respect to the age of data, you can design the least expensive storage options for your needs.


Next unit: Determine blob object replication

6- Determine blob object replication

Object replication copies blobs in a container asynchronously according to policy rules that you configure. During the replication process, the following contents are copied from the source container to the destination container:

The blob contents
The blob metadata and properties
Any versions of data associated with the blob
The following illustration shows an example of asynchronous replication of blob containers between regions.

Diagram that shows asynchronous replication of blob containers between regions.

Things to know about blob object replication
There are several considerations to keep in mind when planning your configuration for blob object replication.

Object replication requires that blob versioning is enabled on both the source and destination accounts.

Object replication doesn't support blob snapshots. Any snapshots on a blob in the source account aren't replicated to the destination account.

Object replication is supported when the source and destination accounts are in the Hot, Cool, or Cold tier. The source and destination accounts can be in different tiers.

When you configure object replication, you create a replication policy that specifies the source Azure storage account and the destination storage account.

A replication policy includes one or more rules that specify a source container and a destination container. The policy identifies the blobs in the source container to replicate.

Things to consider when configuring blob object replication
There are many benefits to using blob object replication. Consider the following scenarios and think about how replication can be a part of your Blob Storage strategy.

Consider latency reductions. Minimize latency with blob object replication. You can reduce latency for read requests by enabling clients to consume data from a region that's in closer physical proximity.

Consider efficiency for compute workloads. Improve efficiency for compute workloads by using blob object replication. With object replication, compute workloads can process the same sets of blobs in different regions.

Consider data distribution. Optimize your configuration for data distribution. You can process or analyze data in a single location and then replicate only the results to other regions.

Consider costs benefits. Manage your configuration and optimize your storage policies to achieve cost benefits. After your data is replicated, you can reduce costs by moving the data to the Archive tier by using lifecycle management policies.


Next unit: Upload blobs

7- Upload blobs

A blob can be any type of data and any size file. Azure Storage offers three types of blobs: block blob, page blob, and append blob.

Things to know about blob types
Let's take a closer look at the characteristics of blob types.

Block blobs. A block blob consists of blocks of data that are assembled to make a blob. Most Blob Storage scenarios use block blobs. Block blobs are ideal for storing text and binary data in the cloud, like files, images, and videos.

Append blobs. An append blob is similar to a block blob because the append blob also consists of blocks of data. The blocks of data in an append blob are optimized for append operations. Append blobs are useful for logging scenarios, where the amount of data can increase as the logging operation continues.

Page blobs. A page blob can be up to 8 TB in size. Page blobs are more efficient for frequent read/write operations. Azure Virtual Machines uses page blobs for operating system disks and data disks.

The block blob type is the default type for a new blob. When you're creating a new blob, if you don't choose a specific type, the new blob is created as a block blob.

After you create a blob, you can't change its type.

Things to consider when using blob upload tools
A common approach for uploading blobs to your Azure storage account is to use Azure Storage Explorer. Many other tools are also available. Review the following options and consider which tools would suit your configuration needs.

Upload tool	Description
AzCopy	An easy-to-use command-line tool for Windows and Linux. You can copy data to and from Blob Storage, across containers, and across storage accounts.
Azure Data Box Disk	A service for transferring on-premises data to Blob Storage when large datasets or network constraints make uploading data over the wire unrealistic. You can use Azure Data Box Disk to request solid-state disks (SSDs) from Microsoft. You can copy your data to those disks and ship them back to Microsoft to be uploaded into Blob Storage.
Azure Import/Export	A service that helps you export large amounts of data from your storage account to hard drives that you provide and that Microsoft then ships back to you with your data.
Business scenario
The following example shows how to upload blob data in Azure Storage Explorer. After you identify the files to upload, you choose the blob type and block size, and the container folder. You also set the authentication method and encryption scope.

Screenshot of the Upload Blob page that shows the Authentication type, blob types, and block size.

How to use Blob versioning
You can enable Blob storage versioning to automatically maintain previous versions of an object. When blob versioning is enabled, you can access earlier versions of a blob to recover your data if it's modified or deleted.



Next unit: Determine Blob Storage pricing

8- Determine Blob Storage pricing

All Azure storage accounts use a pricing model for Azure Blob Storage that's based on the tier of each blob.

Things to know about pricing for Blob Storage
Review the following billing considerations for an Azure storage account and Blob Storage.

Performance tiers. The Blob Storage tier determines the amount of data stored and the cost for storing that data. As the performance tier gets cooler, the per-gigabyte cost decreases.

Data access costs. Data access charges increase as the tier gets cooler. For data in the Cool and Archive tiers, you're billed a per-gigabyte data access charge for reads.

Transaction costs. There's a per-transaction charge for all tiers. The charge increases as the tier gets cooler.

Geo-replication data transfer costs. This charge only applies to accounts that have geo-replication configured, including GRS and RA-GRS. Geo-replication data transfer incurs a per-gigabyte charge.

Outbound data transfer costs. Outbound data transfers (data that's transferred out of an Azure region) incur billing for bandwidth usage on a per-gigabyte basis. This billing is consistent with general-purpose Azure storage accounts.

Changes to the storage tier. If you change the account storage tier from Cool to Hot, you incur a charge equal to reading all the data existing in the storage account. Changing the account storage tier from Hot to Cool incurs a charge equal to writing all the data into the Cool tier (GPv2 accounts only).


Next unit: Interactive lab simulation

9- Interactive lab simulation

Lab scenario
Your organization is migrating storage to Azure. As the Azure Administrator you need to:

Organize content into storage accounts.
Upload and manage images.
Monitor and troubleshoot storage accounts.
Objectives
Task 1: Create a storage account.
Create a storage account in your region with locally redundant storage.
Verify the storage account was created.
Task 2: Work with blob storage.
Create a private blob container.
Upload a file to the container.
Task 3: Monitor the storage container.
Review common storage problems and troubleshooting guides.
Review insights for performance, availability, and capacity.
 Note

Click on the thumbnail image to start the lab simulation. When you're done, be sure to return to this page so you can continue learning.

Screenshot of the simulation page.

Next unit: Knowledge check

Your organization has an extensive video library that's accessed thousands of times a day. You're configuring Azure Blob Storage for the video data. The admin team has asked for guidance about the supported access tiers and object replication for failover. The executive team wants an implementation that can help reduce cost and improve performance.

Answer the following questions
Choose the best response for each question. Then select Check your answers.


1. What statement best describes Azure Blob Storage access tiers? 

The cool access tier is for frequent access of objects in the storage account.

The hot access tier is for storing large amounts of data that's infrequently accessed.

The administrator can switch between hot and cool performance tiers at any time.

2. Which of the following changes between access tiers happens immediately? 

Hot tier to cool tier

Archive tier to cool tier

Archive tier to hot tier

3. How would you describe blob object replication? 

Blob object replication doesn't require versioning to be enabled.

Blob object replication doesn't support blob snapshots.

Blob object replication is supported in the archive tier.


Summary and resources

In this module, you have learned about Azure Blob Storage and how to configure it. You discovered that Blob Storage is Microsoft's object storage solution for the cloud. You learned Azure blob storage is optimized for storing massive amounts of unstructured data like text or binary files. You explored the features of Blob Storage and its use cases. You also learned how to configure Blob Storage, including choosing the appropriate access tiers to reduce cost and improve performance, creating a lifecycle management strategy, and configuring object replication for failover.

The main takeaways from this module are:

Azure Blob Storage is a powerful solution for storing unstructured data in the cloud, such as text documents, images, and videos.
Blob Storage offers different access tiers (Hot, Cool, Cold, and Archive) to optimize performance and cost based on the usage patterns of your data.
You can configure lifecycle management policies to automatically transition data between access tiers and set expiration times for data.
Object replication allows you to asynchronously copy blobs between containers in different regions, providing redundancy and reducing latency for read requests.
By exploring these resources, you can deepen your understanding of Azure Blob Storage and further enhance your skills in configuring and managing it.

Learn more with Azure documentation
Azure Blob Storage documentation - Microsoft Azure's official documentation provides comprehensive information on configuring and managing blob storage. You can find detailed guides, tutorials, and examples to help you navigate through different aspects of blob storage configuration.

Azure Blob Storage Concepts - This article provides an overview of the key concepts related to Azure Blob Storage, including storage accounts, containers, and blobs. It explains how to create and manage these entities and covers various configuration options.

Azure Blob Storage Security - Understanding the security aspects of blob storage is crucial for proper configuration. This article explores authentication, authorization, and encryption options available in Azure Blob Storage. It also covers best practices for securing your blob storage resources.

Azure Blob Storage Performance and Scalability - This article delves into performance considerations when configuring blob storage. It covers topics such as choosing the right storage account type, optimizing data transfer, and leveraging features like Azure CDN and Azure Data Lake Storage.

Azure Blob Storage Lifecycle Management - Blob storage lifecycle management allows you to automate the movement and deletion of data based on predefined rules. This article explains how to configure and manage lifecycle policies to optimize storage costs and improve data management.

Learn more with optional hands-on exercises
Optimize performance and costs by using Blob Storage tiers.






Point 3: Configure Azure Storage security

Learn how to configure common Azure Storage security features like storage access signatures.

Learning objectives
In this module, you learn how to:

Configure a shared access signature (SAS), including the uniform resource identifier (URI) and SAS parameters.

Configure Azure Storage encryption.

Implement customer-managed keys.

Recommend opportunities to improve Azure Storage security.


1- Introduction

Azure Storage provides a comprehensive set of security capabilities that work together to enable developers to build secure applications.

In this module, your company is storing sensitive data in Azure Storage, including personal information. The data is used internally and by external application developers. You're responsible for ensuring the data is secure for all users. You're tasked with providing configuration solutions to grant secure access to the information.

Learning objectives
In this module, you learn how to:

Configure a shared access signature, including the uniform resource identifier (URI) and SAS parameters.
Configure Azure Storage encryption.
Implement customer-managed keys.
Recommend opportunities to improve Azure Storage security.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator. The module concepts are covered in:

Implement and manage storage (15–20%)

Secure storage
Generate shared access signature (SAS) tokens
Manage access keys
Configure Microsoft Entra authentication for an Azure storage account
Prerequisites
None.


Next unit: Review Azure Storage security strategies

2- Review Azure Storage security strategies


Administrators use different strategies to ensure their data is secure. Common approaches include encryption, authentication, authorization, and user access control with credentials, file permissions, and private signatures. Azure Storage offers a suite of security capabilities based on common strategies to help you secure your data.

Things to know about Azure Storage security strategies
Let's look at some characteristics of Azure Storage security.

Encryption. All data written to Azure Storage is automatically encrypted by using Azure Storage encryption.

Authentication. Microsoft Entra ID and role-based access control (RBAC) are supported for Azure Storage for both resource management operations and data operations.

Assign RBAC roles scoped to an Azure storage account to security principals, and use Microsoft Entra ID to authorize resource management operations like key management.
Microsoft Entra integration is supported for data operations on Azure Blob Storage and Azure Queue Storage.
Data in transit. Data can be secured in transit between an application and Azure by using Client-Side Encryption, HTTPS, or SMB 3.0.

Disk encryption. Operating system disks and data disks used by Azure Virtual Machines can be encrypted by using Azure Disk Encryption.

Shared access signatures. Delegated access to the data objects in Azure Storage can be granted by using a shared access signature (SAS).

Authorization. Every request made against a secured resource in Blob Storage, Azure Files, Queue Storage, or Azure Cosmos DB (Azure Table Storage) must be authorized. Authorization ensures that resources in your storage account are accessible only when you want them to be, and to only those users or applications whom you grant access.

Things to consider when using authorization security
Review the following strategies for authorizing requests to Azure Storage. Think about what security strategies would work for your Azure Storage.

Authorization strategy	Description
Microsoft Entra ID	Microsoft Entra ID is Microsoft's cloud-based identity and access management service. With Microsoft Entra ID, you can assign fine-grained access to users, groups, or applications by using role-based access control.
Shared Key	Shared Key authorization relies on your Azure storage account access keys and other parameters to produce an encrypted signature string. The string is passed on the request in the Authorization header.
Shared access signatures	A SAS delegates access to a particular resource in your Azure storage account with specified permissions and for a specified time interval.
Anonymous access to containers and blobs	You can optionally make blob resources public at the container or blob level. A public container or blob is accessible to any user for anonymous read access. Read requests to public containers and blobs don't require authorization.


Next unit: Create shared access signatures

3- Create shared access signatures

A shared access signature (SAS) is a uniform resource identifier (URI) that grants restricted access rights to Azure Storage resources. SAS is a secure way to share your storage resources without compromising your account keys.

You can provide a SAS to clients who shouldn't have access to your storage account key. By distributing a SAS URI to these clients, you grant them access to a resource for a specified period of time.

Things to know about shared access signatures
Let's review some characteristics of a SAS.

A SAS gives you granular control over the type of access you grant to clients who have the SAS.

An account-level SAS can delegate access to multiple Azure Storage services, such as blobs, files, queues, and tables.

You can specify the time interval for which a SAS is valid, including the start time and the expiration time.

You specify the permissions granted by the SAS. A SAS for a blob might grant read and write permissions to that blob, but not delete permissions.

SAS provides account-level and service-level control.

Account-level SAS delegates access to resources in one or more Azure Storage services.

Service-level SAS delegates access to a resource in only one Azure Storage service.

 Note

A stored access policy can provide another level of control when you use a service-level SAS on the server side. You can group SASs and provide other restrictions by using a stored access policy.

There are optional SAS configuration settings:

IP addresses. You can identify an IP address or range of IP addresses from which Azure Storage accepts the SAS. Configure this option to specify a range of IP addresses that belong to your organization.

Protocols. You can specify the protocol over which Azure Storage accepts the SAS. Configure this option to restrict access to clients by using HTTPS.

Configure a shared access signature
In the Azure portal, you configure several settings to create a SAS. As you review these details, consider how you might implement shared access signatures in your storage security solution.

Screenshot of the Create a shared access signature key page.

Signing method: Choose the signing method: Account key or User delegation key.
Signing key: Select the signing key from your list of keys.
Permissions: Select the permissions granted by the SAS, such as read or write.
Start and Expiry date/time: Specify the time interval for which the SAS is valid. Set the start time and the expiry time.
Allowed IP addresses: (Optional) Identify an IP address or range of IP addresses from which Azure Storage accepts the SAS.
Allowed protocols: (Optional) Select the protocol over which Azure Storage accepts the SAS.



Next unit: Identify URI and SAS parameters

4- Identify URI and SAS parameters

When you create your shared access signature (SAS), a uniform resource identifier (URI) is created by using parameters and tokens. The URI consists of your Azure Storage resource URI and the SAS token.

Storage Resource and the S A S Token combine to form the U R I.

Things to know about URI definitions
Let's look at a sample URI definition and examine the parameters. This sample creates a service-level SAS that grants read and write permissions to a blob. Consider how you might configure the parameters to support your Azure Storage resources.

URI

Copy
https://myaccount.blob.core.windows.net/?restype=service&comp=properties&sv=2015-04-05&ss=bf&st=2015-04-29T22%3A18%3A26Z&se=2015-04-30T02%3A23%3A26Z&sr=b&sp=rw&sip=168.1.5.60-168.1.5.70&spr=https&sig=F%6GRVAZ5Cdj2Pw4tgU7IlSTkWgn7bUkkAg8P6HESXwmf%4B
Parameter	Example	Description
Resource URI	https://myaccount.blob.core.windows.net/ ?restype=service &amp;comp=properties	Defines the Azure Storage endpoint and other parameters. This example defines an endpoint for Blob Storage and indicates that the SAS applies to service-level operations. When the URI is used with GET, the Storage properties are retrieved. When the URI is used with SET, the Storage properties are configured.
Storage version	sv=2015-04-05	For Azure Storage version 2012-02-12 and later, this parameter indicates the version to use. This example indicates that version 2015-04-05 (April 5, 2015) should be used.
Storage service	ss=bf	Specifies the Azure Storage to which the SAS applies. This example indicates that the SAS applies to Blob Storage and Azure Files.
Start time	st=2015-04-29T22%3A18%3A26Z	(Optional) Specifies the start time for the SAS in UTC time. This example sets the start time as April 29, 2015 22:18:26 UTC. If you want the SAS to be valid immediately, omit the start time.
Expiry time	se=2015-04-30T02%3A23%3A26Z	Specifies the expiration time for the SAS in UTC time. This example sets the expiry time as April 30, 2015 02:23:26 UTC.
Resource	sr=b	Specifies which resources are accessible via the SAS. This example specifies that the accessible resource is in Blob Storage.
Permissions	sp=rw	Lists the permissions to grant. This example grants access to read and write operations.
IP range	sip=168.1.5.60-168.1.5.70	Specifies a range of IP addresses from which a request is accepted. This example defines the IP address range 168.1.5.60 through 168.1.5.70.
Protocol	spr=https	Specifies the protocols from which Azure Storage accepts the SAS. This example indicates that only requests by using HTTPS are accepted.
Signature	sig=F%6GRVAZ5Cdj2Pw4tgU7Il STkWgn7bUkkAg8P6HESXwmf%4B	Specifies that access to the resource is authenticated by using an HMAC signature. The signature is computed over a string-to-sign with a key by using the SHA256 algorithm, and encoded by using Base64 encoding.


Next unit: Determine Azure Storage encryption

5- Determine Azure Storage encryption

Azure Storage encryption for data at rest protects your data by ensuring your organizational security and compliance commitments are met. The encryption and decryption processes happen automatically. Because your data is secured by default, you don't need to modify your code or applications.

Things to know about Azure Storage encryption
Examine the following characteristics of Azure Storage encryption.

Data is encrypted automatically before it's persisted to Azure Managed Disks, Azure Blob Storage, Azure Queue Storage, Azure Cosmos DB, Azure Table Storage, or Azure Files.

Data is automatically decrypted before it's retrieved.

Azure Storage encryption, encryption at rest, decryption, and key management are transparent to users.

All data written to Azure Storage is encrypted through 256-bit advanced encryption standard (AES) encryption. AES is one of the strongest block ciphers available.

Azure Storage encryption is enabled for all new and existing storage accounts and can't be disabled.

Configure Azure Storage encryption
In the Azure portal, you configure Azure Storage encryption by specifying the encryption type. You can manage the keys yourself, or you can have the keys managed by Microsoft. Consider how you might implement Azure Storage encryption for your storage security.

Screenshot that shows Azure Storage encryption, including keys managed by Microsoft and customer-managed keys.



Next unit: Create customer-managed keys

6- Create customer-managed keys

For your Azure Storage security solution, you can use Azure Key Vault to manage your encryption keys. The Azure Key Vault APIs can be used to generate encryption keys. You can also create your own encryption keys and store them in a key vault.

Things to know about customer-managed keys
Consider the following characteristics of customer-managed keys.

By creating your own keys (referred to as customer-managed keys), you have more flexibility and greater control.

You can create, disable, audit, rotate, and define access controls for your encryption keys.

Customer-managed keys can be used with Azure Storage encryption. You can use a new key or an existing key vault and key. The Azure storage account and the key vault must be in the same region, but they can be in different subscriptions.

Configure customer-managed keys
In the Azure portal, you can configure customer-managed encryption keys. You can create your own keys, or you can have the keys managed by Microsoft. Consider how you might use Azure Key Vault to create your own customer-managed encryption keys.

Screenshot that shows how to create a customer-managed key.

Encryption type: Choose how the encryption key is managed: by Microsoft or by yourself (customer).
Encryption key: Specify an encryption key by entering a URI, or select a key from an existing key vault.




Next unit: Apply Azure Storage security best practices

7- Apply Azure Storage security best practices

We reviewed how to create and work with a shared access signature (SAS) and the benefits it can provide to your storage security solution.

It's important to understand that when you use a SAS in your application, there can be potential risks.

If a SAS is compromised, it can be used by anyone who obtains it, including a malicious user.

If a SAS provided to a client application expires and the application is unable to retrieve a new SAS from your service, the application functionality might be hindered.

Watch this video for more ideas on how to secure your storage. This video is based on Azure Tips and Tricks #272 Azure Security Best Practices.


Recommendations for managing risks
Let's look at some recommendations that can help mitigate risks when working with a SAS.

Recommendation	Description
Always use HTTPS for creation and distribution	If a SAS is passed over HTTP and intercepted, an attacker can intercept and use the SAS. These man-in-the-middle attacks can compromise sensitive data or allow for data corruption by the malicious user.
Reference stored access policies where possible	Stored access policies give you the option to revoke permissions without having to regenerate the Azure storage account keys. Set the storage account key expiration date far in the future.
Set near-term expiry times for an unplanned SAS	If a SAS is compromised, you can mitigate attacks by limiting the SAS validity to a short time. This practice is important if you can't reference a stored access policy. Near-term expiration times also limit the amount of data that can be written to a blob by limiting the time available to upload to it.
Require clients automatically renew the SAS	Require your clients to renew the SAS well before the expiration date. By renewing early, you allow time for retries if the service providing the SAS is unavailable.
Plan carefully for the SAS start time	If you set the start time for a SAS to now, then due to clock skew (differences in current time according to different machines), failures might be observed intermittently for the first few minutes. In general, set the start time to at least 15 minutes in the past. Or, don't set a specific start time, which causes the SAS to be valid immediately in all cases. The same conditions generally apply to the expiry time. You might observe up to 15 minutes of clock skew in either direction on any request. For clients that use a REST API version earlier than 2012-02-12, the maximum duration for a SAS that doesn't reference a stored access policy is 1 hour. Any policies that specify a longer term will fail.
Define minimum access permissions for resources	A security best practice is to provide a user with the minimum required privileges. If a user only needs read access to a single entity, then grant them read access to that single entity, and not read/write/delete access to all entities. This practice also helps lessen the damage if a SAS is compromised because the SAS has less power in the hands of an attacker.
Understand account billing for usage, including a SAS	If you provide write access to a blob, a user might choose to upload a 200-GB blob. If you've given them read access as well, they might choose to download the blob 10 times, which incurs 2 TB in egress costs for you. Again, provide limited permissions to help mitigate the potential actions of malicious users. Use a short-lived SAS to reduce this threat, but be mindful of clock skew on the end time.
Validate data written by using a SAS	When a client application writes data to your Azure storage account, keep in mind there can be problems with the data. If your application requires validated or authorized data, validate the data after it's written, but before it's used. This practice also protects against corrupt or malicious data being written to your account, either by a user who properly acquired the SAS, or by a user exploiting a leaked SAS.
Don't assume a SAS is always the correct choice	In some scenarios, the risks associated with a particular operation against your Azure storage account outweigh the benefits of using a SAS. For such operations, create a middle-tier service that writes to your storage account after performing business rule validation, authentication, and auditing. Also, sometimes it's easier to manage access in other ways. If you want to make all blobs in a container publicly readable, you can make the container Public, rather than providing a SAS to every client for access.
Monitor your applications with Azure Storage Analytics	You can use logging and metrics to observe any spike in authentication failures. You might see spikes from an outage in your SAS provider service or to the inadvertent removal of a stored access policy.



Next unit: Interactive lab simulation

8- Interactive lab simulation

Lab scenario
Your organization is migrating storage to Azure. As the Azure Administrator you need to:

Evaluate the use of Azure storage for storing files. These files are currently residing in on-premises data stores.
Minimize the cost of storage by placing less frequently accessed files in lower-priced storage tiers.
Explore different protection mechanisms that Azure Storage offers, including network access, authentication, authorization, and replication.
Determine to what extent Azure Files service might be suitable for hosting your on-premises file shares.
Architecture diagram
Architecture diagram as explained in the text.

Objectives
Task 1: Create the infrastructure environment.
Use a template to create the virtual networks and virtual machines. You can review the lab template.
Use Azure PowerShell to deploy the template.
Task 2: Create and configure Azure Storage accounts.
Create a storage account.
Configure the storage account to include redundancy and access tiers.
Task 3: Manage blob storage.
Create a private Blob container.
Upload a file into the container.
Task 4: Manage authentication and authorization for Azure Storage.
Generate a shared access signature (SAS) with limited time access.
Verify the SAS is working correctly.
Task 5: Create and configure an Azure Files share.
Create a file and connect to it.
Use Azure PowerShell to add items to the file share.
Task 6: Manage network access for Azure Storage.
Limit access to the Azure storage account from only specific IP addresses.
Confirm access is denied from the Cloud Shell.
 Note

Click on the thumbnail image to start the lab simulation. When you're done, be sure to return to this page so you can continue learning.

Screenshot of the simulation page.

Next unit: Knowledge check

Your company uses an Azure storage account for storing large numbers of video and audio files. Containers are used to store each type of file and access is limited to those media files. Additionally, the files can only be accessed through shared access signatures.

The company wants the ability to revoke access to the files and to change the period for which users can access the files.

The company is planning a delegation model for Azure storage. Applications in the production environment must have unrestricted access to Azure Storage resources.

You're researching how to use network configuration rules, shared access signatures (SAS), and stored access policies to implement secure access to Azure Storage.

Answer the following questions
Choose the best response for each of the questions below. Then select Check your answers.


1. Which solution is the easiest way to implement secure storage for the company's media files? 

Create a shared access signature (SAS) for each user and delete the SAS to prevent access.

Create stored access policies for each container to enable revocation of access or change of duration.

Periodically regenerate the account key to control access to the files.

2. What's the default network rule when configuring network access to an Azure storage account? 

Allow all connections from all networks.

Allow all connection from a private IP address range.

Deny all connections from all networks.

3. What's the best way to implement secure access to Azure Storage for the company's users? 

Use shared access signatures for the production applications.

Use access keys for the production applications.

Use stored access policies for the production applications.



Summary and resources

Azure Administrators must be familiar with how to configure storage security.

In this module, you examined several options for securing Azure Storage. You discovered how to configure shared access signatures (SAS), including the uniform resource identifier (URI) and SAS parameters. You reviewed how to implement customer-managed keys and define stored access policies to configure Azure Storage encryption. You explored opportunities for improving your Azure Storage security solution.





Point 4: Configure Azure Files and Azure File Sync

Learn how to configure Azure Files and Azure File Sync.

Learning objectives
In this module, you learn how to:

Identify storage for file shares versus blob data.

Configure Azure file shares and file share snapshots.

Identify features and use cases of Azure File Sync.

Identify Azure File Sync components and configuration steps.

1- Introduction

Azure Files offers fully managed file shares in the cloud that are accessible via industry standard protocols. Azure File Sync is a service that allows you to cache several Azure Files shares on an on-premises Windows Server or cloud virtual machine.

In this module, your company has a large repository of organizational documents. Offices are located in different geographical regions, and users need the most current versions of the documents. You're researching how to implement Azure Files shares to provide a central location for the documents. You'd like to configure Azure File Sync to keep the documents up to date across the dispersed offices.

Learning objectives
In this module, you learn how to:

Identify storage for file shares.
Compare file shares to blob and disk storage.
Configure Azure file shares, file share snapshots, and soft delete.
Use Azure Storage Explorer to access your file share.
Identify use cases and features of Azure File Sync.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator. The module concepts are covered in:

Implement and manage storage (15–20%).

Configure Azure Files and Azure Blob Storage
Create and configure a files share in Azure storage.
Configure snapshots and soft delete for Azure Files.
Prerequisites
None.


Next unit: Compare storage for file shares and blob data

2- Compare storage for file shares and blob data

Azure Files offers shared storage for applications by using the industry standard Server Message Block and Network File System (NFS) protocols. Azure virtual machines (VMs) and cloud services can share file data across application components by using mounted shares. On-premises applications can also access file data in the share.

Things to know about Azure Files
Let's examine some characteristics of Azure Files.

Azure Files stores data as true directory objects in file shares.

Azure Files provides shared access to files across multiple VMs. Any number of Azure virtual machines or roles can mount and access an Azure file share simultaneously.

Applications that run in Azure VMs or cloud services can mount an Azure file share to access file data. This process is similar to how a desktop application mounts a typical SMB share.

Azure Files offers fully managed file shares in the cloud. Azure file shares can be mounted concurrently by cloud or on-premises deployments of Windows, Linux, and macOS.

Things to consider when using Azure Files
There are many common scenarios for using Azure Files. As you review the following suggestions, think about how Azure Files can provide solutions for your organization.

Consider replacement and supplement options. Replace or supplement traditional on-premises file servers or NAS devices by using Azure Files.

Consider global access. Directly access Azure file shares by using most operating systems, such as Windows, macOS, and Linux, from anywhere in the world.

Consider lift and shift support. Lift and shift applications to the cloud with Azure Files for apps that expect a file share to store file application or user data.

Consider using Azure File Sync. Replicate Azure file shares to Windows Servers by using Azure File Sync. You can replicate on-premises or in the cloud for performance and distributed caching of the data where it's being used. We'll take a closer look at Azure File Sync in a later unit.

Consider shared applications. Store shared application settings such as configuration files in Azure Files.

Consider diagnostic data. Use Azure Files to store diagnostic data such as logs, metrics, and crash dumps in a shared location.

Consider tools and utilities. Azure Files is a good option for storing tools and utilities that are needed for developing or administering Azure VMs or cloud services.

Compare Azure Files to Blob Storage and Azure Disks
It's important to understand when to use Azure Files to store data as file shares rather than using Azure Blob Storage or Azure Disks to store data as blobs. The following table compares different features of these services and common implementation scenarios.

Azure Files (file shares)	Azure Blob Storage (blobs)	Azure Disks (page blobs)
Azure Files provides the SMB and NFS protocols, client libraries, and a REST interface that allows access from anywhere to stored files.	Azure Blob Storage provides client libraries and a REST interface that allows unstructured data to be stored and accessed at a massive scale in block blobs.	Azure Disks is similar to Azure Blob Storage. Azure Disks provides a REST interface to store and access index-based or structured data in page blobs.
- Files in an Azure Files share are true directory objects.
- Data in Azure Files is accessed through file shares across multiple virtual machines.	- Blobs in Azure Blob Storage are a flat namespace.
- Blob data in Azure Blob Storage is accessed through a container.	- Page blobs in Azure Disks are stored as 512-byte pages.
- Page blob data is exclusive to a single virtual machine.
Azure Files is ideal to lift and shift an application to the cloud that already uses the native file system APIs. Share data between the app and other applications running in Azure.

Azure Files is a good option when you want to store development and debugging tools that need to be accessed from many virtual machines.	Azure Blob Storage is ideal for applications that need to support streaming and random-access scenarios.

Azure Blob Storage is a good option when you want to be able to access application data from anywhere.	Azure Disks solutions are ideal when your applications run frequent random read/write operations.

Azure Disks is a good option when you want to store operating system and data disks in Azure Virtual Machines.


Next unit: Manage Azure file shares

3- Manage Azure file shares

Azure Files offers two industry-standard file system protocols for mounting Azure file shares: the Server Message Block (SMB) protocol and the Network File System (NFS) protocol. Azure file shares don't support both the SMB and NFS protocols on the same file share, although you can create SMB and NFS Azure file shares within the same storage account.

Types of Azure file shares
Azure also offers two types of file shares: standard and premium. There are key differences between premium and standard file shares:

The premium tier stores data on modern solid-state drives (SSDs), while the standard tier uses hard disk drives (HDDs).

Standard file shares can be used with SMB and REST protocols only, while premium file shares can be used with SMB, NFS, and REST protocols.

You can easily switch between hot, cool, and transaction optimized tiers of standard file shares, but you can't switch from premium file shares to any of the standard tiers.

Creating Azure SMB file shares
There are two important settings that you need to be aware of when creating and configuring SMB Azure file shares.

Open port 445. Azure Files uses the SMB protocol. SMB communicates over TCP port 445. Be sure port 445 is open. Also, make sure your firewall isn't blocking TCP port 445 from the client machine. If you can't unblock port 445, then a VPN or ExpressRoute connection from on-premises to your Azure network is required, with Azure Files exposed on your internal network using private endpoints.

Enable secure transfer. The Secure transfer required setting enhances the security of your storage account by limiting requests to your storage account from secure connections only. Consider the scenario where you use REST APIs to access your storage account. If you attempt to connect, and secure transfer required is enabled, you must connect by using HTTPS. If you try to connect to your account by using HTTP, and secure transfer required is enabled, the connection is rejected.

Mount an SMB Azure file share on Windows
Azure file shares can be seamlessly used in Windows and Windows Server. You can connect to your Azure file share with Windows or Windows Server in the Azure portal. Specify the Drive where you want to mount the share, and choose the Authentication method. The system supplies you with PowerShell commands to run when you're ready to work with the file share. This video shows you how to mount the SMB file share on Windows.


Mount SMB Azure file share on Linux
You can also connect to Azure file shares from Linux machines. From your virtual machine page, select Connect. SMB Azure file shares can be mounted in Linux distributions by using the CIFS kernel client. File mounting can be done on-demand with the mount command or on-boot (persistent) by creating an entry in /etc/fstab.

Next unit: Create file share snapshots


4- Create file share snapshots


Azure Files provides the capability to take share snapshots of file shares. File share snapshots capture a point-in-time, read-only copy of your data.

Screenshot of a file share snapshot that shows the snapshot name and date it was created.

Things to know about file share snapshots
Let's review some characteristics of file share snapshots.

The Azure Files share snapshot capability is provided at the file share level.

Share snapshots are incremental in nature. Only data changed since the most recent share snapshot is saved.

Incremental snapshots minimize the time required to create share snapshots and saves on storage costs.

Even though share snapshots are saved incrementally, you only need to retain the most recent share snapshot to restore the share.

You can retrieve a share snapshot for an individual file. This level of support helps with restoring individual files rather than having to restore to the entire file share.

If you delete a file share that has share snapshots, all of its snapshots will be deleted along with the share.

Things to consider when using file share snapshots
There are several benefits to using file share snapshots and having access to incremental point-in-time data storage. As you review the following suggestions, think about how you can implement file share snapshots in your Azure Files storage solution.

Benefit	Description
Protect against application error and data corruption	Applications that use file shares perform operations like writing, reading, storage, transmission, and processing. When an application is misconfigured or an unintentional bug is introduced, accidental overwrite or damage can happen to a few data blocks. To help protect against these scenarios, you can take a share snapshot before you deploy new application code. When a bug or application error is introduced with the new deployment, you can go back to a previous version of your data on that file share.
Protect against accidental deletions or unintended changes	Imagine you're working on a text file in a file share. After the text file is closed, you lose the ability to undo your changes. In this scenario, you need to recover a previous version of your file. You can use share snapshots to recover previous versions of the file if it's accidentally renamed or deleted.
Support backup and recovery	After you create a file share, you can periodically create a snapshot of the file share to use it for data backup. A share snapshot, when taken periodically, helps maintain previous versions of data that can be used for future audit requirements or disaster recovery.



Next unit: Implement soft delete for Azure Files

5- Implement soft delete for Azure Files

Azure Files offers soft delete for Server Message Block (SMB) file shares. Soft delete lets you recover deleted files and file shares.

Illustration that depicts how to enable soft delete on an Azure file share.

Things to know about soft delete for Azure Files
Let's take a look at the characteristics of soft delete for Azure Files.

Soft delete for file shares is enabled at the storage account level.

Soft delete transitions content to a soft deleted state instead of being permanently erased.

Soft delete lets you configure the retention period. The retention period is the amount of time that soft deleted file shares are stored and available for recovery.

Soft delete provides a retention period between 1 and 365 days.

Soft delete can be enabled on either new or existing file shares.

Soft delete doesn't work for Network File System (NFS) shares.

Things to consider when using soft delete for Azure Files
There are many advantages to using soft delete for Azure Files. Consider the following scenarios, and think about how you can use soft delete.

Recovery from accidental data loss. Use soft delete to recover data that is deleted or corrupted.

Upgrade scenarios. Use soft delete to restore to a known good state after a failed upgrade attempt.

Ransomware protection. Use soft delete to recover data without paying ransom to cybercriminals.

Long-term retention. Use soft delete to comply with data retention requirements.

Business continuity. Use soft delete to prepare your infrastructure to be highly available for critical workloads.-

Next unit: Use Azure Storage Explorer

6- Use Azure Storage Explorer

Azure Storage Explorer is a standalone application that makes it easy to work with Azure Storage data on Windows, macOS, and Linux. With Azure Storage Explorer, you can access multiple accounts and subscriptions, and manage all your Storage content.

Screenshot of Azure Storage Explorer that shows the Emulator storage account open, which has a folder and several documents. The access tier information is visible.

Things to know about Azure Storage Explorer
Azure Storage Explorer has the following characteristics.

Azure Storage Explorer requires both management (Azure Resource Manager) and data layer permissions to allow full access to your resources. You need Azure Active Directory (Azure AD) permissions to access your storage account, the containers in your account, and the data in the containers.

Azure Storage Explorer lets you connect to different storage accounts.

Connect to storage accounts associated with your Azure subscriptions.
Connect to storage accounts and services that are shared from other Azure subscriptions.
Connect to and manage local storage by using the Azure Storage Emulator.
Screenshot of the Azure Explorer Manage Accounts page.

Things to consider when using Azure Storage Explorer
Azure Storage Explorer supports many scenarios for working with storage accounts in global and national Azure. As you review these options, think about which scenarios apply to your Azure Storage implementation.

Scenario	Description
Connect to an Azure subscription	Manage storage resources that belong to your Azure subscription.
Work with local development storage	Manage local storage by using the Azure Storage Emulator.
Attach to external storage	Manage storage resources that belong to another Azure subscription or that are under national Azure clouds by using the storage account name, key, and endpoints. This scenario is described in more detail in the next section.
Attach a storage account with a SAS	Manage storage resources that belong to another Azure subscription by using a shared access signature (SAS).
Attach a service with a SAS	Manage a specific Azure Storage service (blob container, queue, or table) that belongs to another Azure subscription by using a SAS.
Attach to external storage account
Azure Storage Explorer lets you attach to external storage accounts so storage accounts can be easily shared.

To create the connection, you need the external storage Account name and Account key. In the Azure portal, the account key is called key1.

Screenshot of the Azure Storage Explorer wizard to connect to an external storage account.

To use a storage account name and key from a national Azure cloud, use the Storage endpoints domain drop-down menu to select Other, and then enter the custom storage account endpoint domain.

Access keys
Access keys provide access to the entire storage account. You're provided two access keys so you can maintain connections by using one key while regenerating the other.

 Important

Store your access keys securely. We recommend regenerating your access keys regularly.

When you regenerate your access keys, you must update any Azure resources and applications that access this storage account to use the new keys. This action doesn't interrupt access to disks from your virtual machines.


Next unit: Deploy Azure File Sync

7- Deploy Azure File Sync

Azure File Sync enables you to cache several Azure Files shares on an on-premises Windows Server or cloud virtual machine. You can use Azure File Sync to centralize your organization's file shares in Azure Files, while keeping the flexibility, performance, and compatibility of an on-premises file server.

Illustration that depicts how Azure File Sync can be used to cache an organization's file shares in Azure Files.

Things to know about Azure File Sync
Let's take a look at the characteristics of Azure File Sync.

Azure File Sync transforms Windows Server into a quick cache of your Azure Files shares.

You can use any protocol that's available on Windows Server to access your data locally with Azure File Sync, including SMB, NFS, and FTPS.

Azure File Sync supports as many caches as you need around the world.

Cloud tiering
Cloud tiering is an optional feature of Azure File Sync. Frequently accessed files are cached locally on the server while all other files are tiered to Azure Files based on policy settings.

When a file is tiered, Azure File Sync replaces the file locally with a pointer. A pointer is commonly referred to as a reparse point. The reparse point represents a URL to the file in Azure Files.

When a user opens a tiered file, Azure File Sync seamlessly recalls the file data from Azure Files without the user needing to know that the file is stored in Azure.

Cloud tiering files have greyed icons with an offline O file attribute to let the user know when the file is only in Azure.

Things to consider when using Azure File Sync
There are many advantages to using Azure File Sync. Consider the following scenarios, and think about how you can use Azure File Sync with your Azure Files shares.

Consider application lift and shift. Use Azure File Sync to move applications that require access between Azure and on-premises systems. Provide write access to the same data across Windows Servers and Azure Files.

Consider support for branch offices. Support your branch offices that need to back up files by using Azure File Sync. Use the service to set up a new server that connects to Azure storage.

Consider backup and disaster recovery. After you implement Azure File Sync, Azure Backup backs up your on-premises data. Restore file metadata immediately and recall data as needed for rapid disaster recovery.

Consider file archiving with cloud tiering. Azure File Sync stores only recently accessed data on local servers. Implement cloud tiering so non-used data moves to Azure Files.

Next unit: Knowledge check

Your company maintains a large document repository. You're implementing Azure Files shares to provide a central location for the documents. Users at offices in different geographical regions need access to the latest versions of the documents. You're configuring Azure File Sync to keep the information up to date across multiple offices.

One scenario you're working to resolve involves the manufacturing division. They're running dedicated software in their warehouse to keep track of product stock. The software needs to run on machines in the warehouse, but the management team wants to access the stock data from the main office. Limited bandwidth in the warehouse is causing issues when accessing cloud based solutions. You proposed using cloud tiering, soft delete, and snapshots.

Answer the following questions
Choose the best response for each of the questions. Then select Check your answers.


1. Which statement correctly describes cloud tiering? 

Cloud tiering prioritizes the sync order of file shares.

Cloud tiering sets the frequency at which the sync job runs.

Cloud tiering archives infrequently access files to free up space on the local file share.

2. Which statement correctly describes soft delete? 

Soft delete only retains deleted files for 14 days.

Soft delete can be enabled on either new or existing file shares.

Soft delete provides file protection for SMB and NFS file shares.

3. Which statement correctly describes file snapshots? 

A share snapshot is a point-in-time, read-only copy of your data.

You can't retrieve a share snapshot for an individual file.

The snapshot capability is provided at the storage account level.


Azure Administrators are familiar with Azure Files and the Azure File Sync agent. They know how to implement fully managed file shares in the cloud by using industry standard protocols. They understand how to use Azure File Sync to cache Azure Files shares on an on-premises Windows Server or cloud virtual machine.

In this module, you learned when to use Azure File shares and how this feature compares to blobs and disks. You also reviewed Azure File features such as snapshots and soft delete. You learned how Azure File Sync can be used with on-premises data stores. You also were introduced to the Storage Explorer.

The main takeaways for this module are:

Azure Files provides the SMB and NFS protocols, client libraries, and a REST interface that allows access from anywhere to stored files.

Azure Files is ideal to lift and shift an application to the cloud that already uses the native file system APIs. Share data between the app and other applications running in Azure.

Azure Files offers two industry-standard file system protocols for mounting Azure file shares: the Server Message Block (SMB) protocol and the Network File System (NFS) protocol.

Azure Files offers two types of file shares: standard and premium. The premium tier stores data on modern solid-state drives (SSDs), while the standard tier uses hard disk drives (HDDs).

File share snapshots capture a point-in-time, read-only copy of your data.

Soft delete allows you to recover your file share when it's deleted by an application or other storage account user.

Azure Storage Explorer is a standalone application that makes it easy to work with stored data on Windows, macOS, and Linux.

Azure File Sync enables you to cache file shares on an on-premises Windows Server or cloud virtual machine.

Learn more with documentation
Azure Files documentation. This page is your starting point for all things related to Azure Files.

Azure File Sync documentation. This page is your starting point for all things related to Azure File Sync.

Learn more with self-paced training
Introduction to Azure Files. In this module, you learn how you can meet your storage needs with Azure Files and Azure File Sync.

Implement a hybrid file server infrastructure. In this module, you learn to deploy Azure File Sync and use Storage Migration Services to migrate file servers to Azure.





Point 5: Create an Azure Storage account

Create an Azure Storage account with the correct options for your business needs.

Learning objectives
In this module, you will:

Decide how many storage accounts you need for your project
Determine the appropriate settings for each storage account
Create a storage account using the Azure portal


1- Introduction

Most organizations have diverse requirements for their cloud-hosted data. For example, storing data in a specific region, or needing separate billing for different data categories. Azure storage accounts let you formalize these types of policies and apply them to your Azure data.

Suppose you work at a chocolate manufacturer that produces baking ingredients such as cocoa powder and chocolate chips. You market your products to grocery stores who then sell them to consumers.

Your formulations and manufacturing processes are trade secrets. The spreadsheets, documents, and instructional videos that capture this information are critical to your business and require geographically redundant storage. This data is primarily accessed from your main factory, so you would like to store it in a nearby datacenter. The expense for this storage needs to be billed to the manufacturing department.

You also have a sales group that creates cookie recipes and baking videos to promote your products to consumers. Your priority for this data is low cost, rather than redundancy or location. This storage must be billed to the sales team.

By creating multiple Azure storage accounts, with each one having the appropriate settings for the data it holds, you can handle these types of business requirements.

Learning objectives
In this module, you will:

Decide how many storage accounts you need for your project.
Determine the appropriate settings for each storage account.
Create a storage account using the Azure portal.



Next unit: Decide how many storage accounts you need

2- Decide how many storage accounts you need

Organizations often have multiple storage accounts to enable them to implement different sets of requirements. In the chocolate-manufacturer example, there's one storage account for private business data and one storage account for consumer-facing files. In this unit, you learn the policy factors that each type of storage account controls, which helps you decide how many accounts you need.

What is Azure Storage?
Azure provides many ways to store your data, including multiple database options like Azure SQL Database, Azure Cosmos DB, and Azure Table Storage. Azure offers multiple ways to store and send messages, such as Azure Queues and Event Hubs. You can even store loose files using services like Azure Files and Azure Blobs.

Azure groups four of these data services together under the name Azure Storage. The four services are Azure Blobs, Azure Files, Azure Queues, and Azure Tables. The following illustration shows the elements of Azure Storage.

Illustration identifying the Azure data services that are part of Azure Storage.

These four data services are all primitive, cloud-based storage services, and are often used together in the same application.

What is a storage account?
A storage account is a container that groups a set of Azure Storage services together. Only data services from Azure Storage can be included in a storage account (Azure Blobs, Azure Files, Azure Queues, and Azure Tables). The following illustration shows a storage account containing several data services.

Illustration of an Azure storage account containing a mixed collection of data services.

Combining data services into a single storage account enables you to manage them as a group. The settings you specify when you create the account, or any changes that you make after creation, apply to all services in the storage account. Deleting a storage account deletes all of the data stored inside it.

A storage account is an Azure resource and is part of a resource group. The following illustration shows an Azure subscription containing multiple resource groups, where each group contains one or more storage accounts.

Illustration of an Azure subscription containing multiple resource groups, each with one or more storage accounts.

Other Azure data services, such as Azure SQL and Azure Cosmos DB, are managed as independent Azure resources and can't be included in a storage account. The following illustration shows a typical arrangement: Blobs, Files, Queues, and Tables are contained within storage accounts, while other services aren't.

Illustration of an Azure subscription showing some data services that cannot be placed in a storage account.

Storage account settings
A storage account defines a policy that applies to all the storage services in the account. For example, you could specify that all the contained services will be stored in the West US datacenter, accessible only over https, and billed to the sales department's subscription.

A storage account defines the following settings:

Subscription: The Azure subscription that's billed for the services in the account.

Location: The datacenter that stores the services in the account.

Performance: Determines the data services you can have in your storage account and the type of hardware disks used to store the data.

Standard allows you to have any data service (Blob, File, Queue, Table) and uses magnetic disk drives.
Premium provides more services for storing data. For example, storing unstructured object data as block blobs or append blobs, and specialized file storage used to store and create premium file shares. These storage accounts use solid-state drives (SSD) for storage.
Replication: Determines the strategy used to make copies of your data to protect against hardware failure or natural disaster. At a minimum, Azure automatically maintains three copies of your data within the datacenter associated with the storage account. The minimum replication is called locally redundant storage (LRS), and guards against hardware failure but doesn't protect you from an event that incapacitates the entire datacenter. You can upgrade to one of the other options such as geo-redundant storage (GRS) to get replication at different datacenters across the world.

Access tier: Controls how quickly you're able to access the blobs in a storage account. The Hot access tier is optimized for storing data that's accessed or modified frequently and gives quicker access than Cool, but at increased storage cost. The Cool access tier is optimized for storing data that's infrequently accessed or modified, and has a lower storage cost. Hot access tier applies only to blobs, and serves as the default value for new blobs.

Secure transfer required: A security feature that determines the supported protocols for access. Enabled requires HTTPS, while disabled allows HTTP.

Virtual networks: A security feature that allows inbound access requests only from the virtual network(s) you specify.

How many storage accounts do you need?
A storage account represents a collection of settings like location, replication strategy, and subscription owner. You need one storage account for each group of settings that you want to apply to your data. The following illustration shows two storage accounts that differ in one setting; that one difference is enough to require separate storage accounts.

Illustration showing two storage accounts with different settings.

Typically, your data diversity, cost sensitivity, and tolerance for management overhead determine the number of storage accounts you need.

Data diversity
Organizations often generate data that differs in where it's consumed, how sensitive it is, which group pays the bills, etc. Diversity along any of these vectors can lead to multiple storage accounts. Let's consider two examples:

Do you have data that is specific to a country/region? If so, you might want to store the data in a datacenter in that country/region for performance or compliance reasons. You need one storage account for each geographical region.

Do you have some data that is proprietary and some for public consumption? If so, you could enable virtual networks for the proprietary data and not for the public data. Separating proprietary data and public data requires separate storage accounts.

In general, increased diversity means an increased number of storage accounts.

Cost sensitivity
A storage account by itself has no financial cost; however, the settings you choose for the account do influence the cost of services in the account. Geo-redundant storage costs more than locally redundant storage. Premium performance and the Hot access tier increase the cost of blobs.

You can use multiple storage accounts to reduce costs. For example, you could partition your data into critical and noncritical categories. You could place your critical data into a storage account with geo-redundant storage and put your noncritical data in a different storage account with locally redundant storage.

Tolerance for management overhead
Each storage account requires some time and attention from an administrator to create and maintain. It also increases complexity for anyone who adds data to your cloud storage. Everyone in an administrator role needs to understand the purpose of each storage account so they add new data to the correct account.

Storage accounts are powerful tools to help you obtain the performance and security you need while minimizing costs. A typical strategy is to start with an analysis of your data. Create partitions that share characteristics like location, billing, and replication strategy. Then, create one storage account for each partition.


Next unit: Choose your account settings

3- Choose your account settings

The storage account settings we've already covered apply to the data services in the account. Here, we discuss the three settings that apply to the account itself, rather than to the data stored in the account:

Name
Deployment model
Account kind
These settings affect how you manage your account and the cost of the services within it.

Name
Each storage account has a name. The name must be globally unique within Azure, use only lowercase letters and digits and be between 3 and 24 characters.

Deployment model
A deployment model is the system Azure uses to organize your resources. The model defines the API that you use to create, configure, and manage those resources. Azure provides two deployment models, Resource Manager and Classic. Resource Manager is the current model that uses the Azure Resource Manager API. The Classic model, which is currently being retired, was a legacy offering that used the classic deployment model.

Most Azure resources only work with Resource Manager, which makes it easy to decide which model to choose. However, storage accounts, virtual machines, and virtual networks support both, so you must choose one or the other when you create your storage account.

The key feature difference between the two models is their support for grouping. The Resource Manager model adds the concept of a resource group, which isn't available in the classic model. A resource group lets you deploy and manage a collection of resources as a single unit.

Microsoft recommends that you use the Resource Manager deployment model for all new resources.

Account kind
Storage account kind is a set of policies that determine which data services you can include in the account and the pricing of those services. There are four kinds of storage accounts:

Standard - StorageV2 (general purpose v2): the current offering that supports all storage types and all of the latest features
Premium - Page blobs: Premium storage account type for page blobs only
Premium - Block blobs: Premium storage account type for block blobs and append blobs
Premium - File shares: Premium storage account type for file shares only
Microsoft recommends that you use the Standard - StorageV2 (general purpose v2) option for new storage accounts.

The core advice is to choose the Resource Manager deployment model and the Standard - StorageV2 (general purpose v2) account kind for all your storage accounts. For new resources, there are few reasons to consider the other choices.


Next unit: Choose an account creation tool

4- Choose an account creation tool

There are several tools that create a storage account. Your choice is typically based on if you want a GUI and whether you need automation.

Available tools
The available tools are:

Azure portal
Azure CLI (Command-line interface)
Azure PowerShell
Management client libraries
The portal provides a GUI with explanations for each setting, which makes it easy to use and helpful for learning about the options.

The other tools in this list all support automation. The Azure CLI and Azure PowerShell let you write scripts, while the management libraries allow you to incorporate the creation into a client app.

How to choose a tool
Storage accounts are typically based on an analysis of your data, so they tend to be relatively stable. As a result, storage-account creation is usually a one-time operation done at the start of a project. For one-time activities, the portal is the most common choice.

In the rare cases where you need automation, the decision is between a programmatic API or a scripting solution. Scripts are typically faster to create and less work to maintain because there's no need for an IDE, NuGet packages, or build steps. If you have an existing client application, the management libraries might be an attractive choice; otherwise, scripts are a better option.


Next unit: Exercise - Create a storage account using the Azure portal

5- Exercise - Create a storage account using the Azure portal

In this unit, you use the Azure portal to create a storage account for a fictitious southern California surf report web app. The surf report site lets users upload photos and videos of local beach conditions. Viewers of the site use the content to help them choose the beach with the best surfing conditions.

Your list of design and feature goals is:

Video content must load quickly.
The site must handle unexpected spikes in upload volume.
Outdated content must be removed as surf conditions change so the site always shows current conditions.
You decide to buffer uploaded content in an Azure Queue for processing and then transfer it to an Azure Blob for persistent storage. You need a storage account that can hold both queues and blobs while delivering low-latency access to your content.

Create a storage account using Azure portal
Sign in to the Azure portal using the same account you used to activate the sandbox.

On the resource menu, or from the Home page, select Storage accounts. The Storage accounts pane appears.

On the command bar, select Create. The Create a storage account pane appears.

On the Basics tab, enter the following values for each setting.

Setting	Value
Project details	
Subscription	Concierge Subscription
Resource group	[sandbox resource group name] from the dropdown list.
Instance details	
Storage account name	Enter a unique name. This name is used to generate the public URL to access the data in the account. The name must be unique across all existing storage account names in Azure. Names must have 3 to 24 characters and can contain only lowercase letters and numbers.
Region	Select a location near to you from the dropdown list.
Performance	Standard. This option decides the type of disk storage used to hold the data in the Storage account. Standard uses traditional hard disks, and Premium uses solid-state drives (SSD) for faster access.
Redundancy	Select Locally redundant storage (LRS) from the dropdown list. In our case, the images and videos quickly become out-of-date and are removed from the site. As a result, there's little value to paying extra for Geo-redundant storage (GRS). If a catastrophic event results in data loss, you can restart the site with fresh content from your users.
Select Next. On the Advanced tab, enter the following values for each setting.

Setting	Value
Security	
Require secure transfer for REST API operations	Check. This setting controls whether HTTP can be used for the REST APIs that access data in the storage account. Setting this option to enable forces all clients to use HTTPS. Most of the time, you want to set secure transfer to enable; using HTTPS over the network is considered a best practice.
Allow enabling anonymous access on individual containers	Check. Blob containers, by default, don't permit anonymous access to their content. This setting allows authorized users to selectively enable anonymous access on specific containers.
Enable storage account key access	Check. We want to allow clients to access data via SAS.
Default to Microsoft Entra authorization in the Azure portal	Uncheck. Clients are public, not part of an Active Directory.
Minimum TLS version	Select Version 1.2 from dropdown list. TLS 1.2 is a secure version of TLS, and Azure Storage uses it on public HTTPS endpoints. TLS 1.1 and 1.0 are supported for backwards compatibility. See Warning at end of table.
Permitted scope for copy operations	Accept default
Hierarchical Namespace	
Enable hierarchical namespace	Uncheck. Data Lake hierarchical namespace is for big-data applications that aren't relevant to this module.
Access protocols	
Enable hierarchical namespace	Accept default. Blob and Data Lake Gen2 endpoints are provisioned by default.
Blob storage	
Allow cross-tenant replication	Uncheck. Active Directory isn't being used for this exercise.
Access tier	Hot. This setting is only used for Blob storage. The Hot access tier is ideal for frequently accessed data; the Cool access tier is better for infrequently accessed data. This setting only sets the default value. When you create a Blob, you can set a different value for the data. In our case, we want the videos to load quickly, so we use the high-performance option for our blobs.
Azure Files	
Enable large file shares	Accept default. Large file shares provide support up to a 100 TiB, however this type of storage account can't convert to a Geo-redundant storage offering, and upgrades are permanent.
 Warning

If Enable large file shares is selected, it will enforce additional restrictions, and Azure files service connections without encryption will fail, including scenarios using SMB 2.1 or 3.0 on Linux. Because Azure storage doesn't support SSL for custom domain names, this option cannot be used with a custom domain name.

Select Next. On the Networking tab, enter the following values for each setting.

Setting	Value
Network connectivity	
Network access	Enable public access from all networks. We want to allow public Internet access. Our content is public facing, and we need to allow access from public clients.
Network routing	
Routing preference	Microsoft network routing. We want to make use of the Microsoft global network that is optimized for low-latency path selection.
Select Next. On the Data protection tab, enter the following values for each setting.

Setting	Value
Recovery	
Enable point-in-time restore for containers	Uncheck. Not necessary for this implementation.
Enable soft delete for blobs	Uncheck. Soft delete lets you recover blob data in cases where blobs or blob snapshots are deleted accidentally or overwritten.
Enable soft delete for containers	Uncheck. Soft delete lets you recover your containers that are deleted accidentally.
Enable soft delete for file shares	Uncheck. File share soft delete lets you recover your accidentally deleted file share data more easily.
Tracking	
Enable versioning for blobs	Uncheck. Not necessary for this implementation.
Enable blob change feed	Uncheck. Not necessary for this implementation.
Access control	
Enable version-level immutability support	Uncheck. Not necessary for this implementation.
Select Next. Accept the defaults on the Encryption tab.

Select Next. Here on the Tags tab, you can associate key/value pairs with the account for your categorization to determine if a feature is available to selected Azure resources.

Select Next to validate your options and to ensure all the required fields are selected. If there are issues, this tab identifies them so you can correct them.

When validation passes successfully, select Create to deploy the storage account.

When deployment is complete, which may take up to two minutes, select Go to resource to view Essential details about your new storage account.

You created a storage account with settings driven by your business requirements. For example, you might have selected a West US datacenter because your customers were primarily located in southern California. The typical flow for creating a storage account is: first analyze your data and goals, and then configure the storage account options to match.

Next unit: Knowledge check - Create a storage account


6- Knowledge check - Create a storage account

Knowledge check - Create a storage account

Check your knowledge

1. Suppose you have two video files stored as blobs. One of the videos is business-critical and requires a replication policy that creates multiple copies across geographically diverse datacenters. The other video is noncritical, and a local replication policy is sufficient. Which of the following options would satisfy both data diversity and cost sensitivity consideration. 

Create a single storage account that makes use of Local-redundant storage (LRS) and host both videos from here.

Create a single storage account that makes use of Geo-redundant storage (GRS) and host both videos from here.

Create two storage accounts. The first account makes use of Geo-redundant storage (GRS) and hosts the business-critical video content. The second account makes use of Local-redundant storage (LRS) and hosts the noncritical video content.

2. The name of a storage account must be: 

Unique within the containing resource group.

Unique within your Azure subscription.

Globally unique.

3. In a typical project, when would you create your storage account(s)? 

At the beginning, during project setup.

After deployment, when the project is running.

At the end, during resource cleanup.


Summary

Storage accounts let you create a group of data management rules and apply them all at once to the data stored in the account: blobs, files, tables, and queues.

If you tried to achieve the same thing without storage accounts, the end product would be tedious and error-prone. For example, what are the chances that you could successfully apply the same rules to thousands of blobs?

Instead, you capture the rules in the settings for a storage account, and those rules are automatically applied to every data service in the account.

Clean up
The sandbox automatically cleans up your resources when you're finished with this module.

When you're working in your own subscription, it's a good idea at the end of a project to identify whether you still need the resources you created. Resources that you leave running can cost you money. You can delete resources individually or delete the resource group to delete the entire set of resources.

 Important

When you're working in your own subscription, to avoid unwanted usage charges, you must remove any resources that you create.

Use the following steps in the Azure portal to delete the resource group and all associated resources.

In the resource menu, select Resource groups.

Select the resource group you created.

In the command bar, select Delete resource group.

In the confirmation pane, you're prompted to type the resource group name; you can right-click and drag the title from the Resource group pane.

When the expected name is a match, Delete is available.

Select Delete. It may take several minutes to delete your resource group. Check Notifications in the Global Controls in the upper right corner of the Azure portal to ensure your operation completed.






Point 6: Control access to Azure Storage with shared access signatures

Grant access to data stored in your Azure Storage accounts securely by using shared access signatures.


Learning objectives
In this module, you will:

Identify the features of a shared access signature for Azure Storage.
Identify the features of stored access policies.
Programmatically generate and use a shared access signature to access storage.

1- Introduction

The Azure Storage platform is Microsoft's cloud storage solution for modern data storage solution. Azure Blob Storage is Microsoft's object storage solution for the cloud, and is optimized for storing massive amounts of unstructured data. Every request to access files stored in Azure requires authorization. A shared access signature (SAS) provides secure, delegated access to resources in your storage account.

You work for a healthcare organization that stores patient diagnostic images in blob storage. These images are highly sensitive, and you're developing an application for securely storing and securely accessing content. You're updating your application to integrate with other providers, and you want to authorize access to the images by using a SAS.

In this module, you explore the options available to authorize access to your Azure storage, focusing on SAS and its different variants. You deploy a web app that uses a storage account, and you enhance the web app to use a SAS.

After you finish this module, you'll have a web app that uses multiple SASs that are associated with a stored access policy. The web app shows how patient images are only accessible by using a SAS, and how you can revoke access by updating the stored access policy.

Learning objectives
Identify the features of a shared access signature (SAS) for Azure Storage.
Identify the features of stored access policies.
Programmatically generate and use a SAS to access storage.
Prerequisites
Knowledge of Azure Storage accounts
Familiarity with C#
Familiarity with jQuery and JSON



Next unit: Authorization options for Azure Storage

2- Authorization options for Azure Storage

Before you enhance your company's patient diagnostic image web app, you'd like to understand all the options for secure access. A shared access signature (SAS) provides a secure way of granting access to resources for clients. But it's not the only way to grant access. In some situations, other options might offer better choices for your organization.

Your company could make use of more than just the SAS method of authentication.

In this unit, you look at the different ways to authenticate access to files stored in Azure Storage.

Access Azure Storage
Clients access files stored in Azure Storage over HTTP/HTTPS. Azure checks each client request for authorization to access the stored data. Four options are available for accessing blob storage:

Public access
Microsoft Entra ID
Shared key
Shared access signature (SAS)
Public access
Public access is also known as anonymous public read access for containers and blobs.

There are two separate settings that affect public access:

The Storage Account. Configure the storage account to allow public access by setting the AllowBlobPublicAccess property. When set to true, Blob data is available for public access only if the container's public access setting is also set.

The Container. You can enable anonymous access only if anonymous access has been allowed for the storage account. A container has two possible settings for public access: Public read access for blobs, or public read access for a container and its blobs. Anonymous access is controlled at the container level, not for individual blobs. So, if you want to secure some of the files, you need to put them in a separate container that doesn't permit public read access.

Both storage account and container settings are required to enable anonymous public access. The advantages of this approach are that you don't need to share keys with clients who need access to your files. You also don't need to manage a SAS.


Microsoft Entra ID
Use the Microsoft Entra option to securely access Azure Storage without storing any credentials in your code. AD authorization takes a two-step approach. First, you authenticate a security principal that returns an OAuth 2.0 token if successful. This token is then passed to Azure Storage to enable authorization to the requested resource.

Use this form of authentication if you're running an app with managed identities or using security principals.

Shared key
Azure Storage creates two 512-bit access keys for every storage account that's created. You share these keys to grant clients access to the storage account. These keys grant anyone with access the equivalent of root access to your storage.

We recommend that you manage storage keys with Azure Key Vault because it's easy to rotate keys on a regular schedule to keep your storage account secure.

Shared access signature
A SAS lets you grant granular access to files in Azure Storage, such as read-only or read-write access, expiration time, after which the SAS no longer enables the client to access the chosen resources. A shared access signature is a key that grants permission to a storage resource, and should be protected in the same manner as an account key.

Azure Storage supports three types of shared access signatures:

User delegation SAS: A user delegation SAS is secured with Microsoft Entra credentials, because the OAuth 2.0 token used to sign the SAS is requested on behalf of the user. It can only be used for Blob storage.
Service SAS: A service SAS is secured using a storage account key. A service SAS delegates access to a resource in any one of four Azure Storage services: Blob, Queue, Table, or File.
Account SAS: An account SAS is secured with a storage account key. An account SAS has the same controls as a service SAS, but can also control access to service-level operations, such as Get Service Stats.
You can create a SAS ad-hoc by specifying all the options you need to control, including start time, expiration time, and permissions.

If you plan to create a service SAS, there's also an option to associate it with a stored access policy. A stored access policy can be associated with up to five active SASs. You can control access and expiration at the stored access policy level. This approach is good if you need to have granular control to change the expiration, or to revoke a SAS. The only way to revoke or change an ad-hoc SAS is to change the storage account keys.

Check your knowledge

1. Your organization has an internal system to share patient appointment information and notes. You can secure a user's access based on their membership in a Microsoft Entra group. Which kind of authorization supports this scenario best, and why? 

Use a shared access signature (SAS) token. You use the Microsoft Entra credentials and a user delegation SAS token.

Use Microsoft Entra ID. By using Microsoft Entra ID, you can create a service principal to authenticate the app.

Use a shared key. The Azure Storage account can create and revoke keys that are used in your app.

2. Your public-facing static website stores all its public UI images in blob storage. The website needs to display the graphics without any kind of authorization. Which is the best option? 

Public access

Shared key

Shared access signature


3- Use shared access signatures to delegate access to Azure Storage

By using a shared access signature (SAS), you can delegate access to your resources. Clients don't have direct access to your storage account credentials and, at a granular level, you control what they access.

After you investigate all the authorization options, you decide to look at a SAS in more detail. You want to create and use a SAS in a C# .NET web app. You also want to follow Microsoft's best practices on when and how to use a SAS.

In this unit, you review how a SAS works at a technical level and what C# code you must write to use it.

How shared access signatures work
A SAS has two components: a URI that points to one or more storage resources and a token that indicates how the client may access the resources.

In a single URI, such as https://medicalrecords.blob.core.windows.net/patient-images/patient-116139-nq8z7f.jpg?sp=r&st=2020-01-20T11:42:32Z&se=2020-01-20T19:42:32Z&spr=https&sv=2019-02-02&sr=b&sig=SrW1HZ5Nb6MbRzTbXCaPm%2BJiSEn15tC91Y4umMPwVZs%3D, you can separate the URI from the SAS token as follows:

URI	SAS token
https://medicalrecords.blob.core.windows.net/patient-images/patient-116139-nq8z7f.jpg?	sp=r&st=2020-01-20T11:42:32Z&se=2020-01-20T19:42:32Z&spr=https&sv=2019-02-02&sr=b&sig=SrW1HZ5Nb6MbRzTbXCaPm%2BJiSEn15tC91Y4umMPwVZs%3D
The SAS token contains the following components, or query parameters.

Query Parameter	Field Name	Example	Description
sp	signed permission	sp=r	Indicates one or more operations the client can perform. Values can be compounded: a (add), c (create), d (delete), l (list), r (read), and w (write). sp=r is read only; sp=acdlrw grants all the available rights.
st	start time	st=2020-01-20T11:42:32Z	The date and time when access starts.
se	expiry time	se=2020-01-20T19:42:32Z	The date and time when access ends. Based on the start date, this example grants eight hours of access.
spr	signed protocol	spr=https	The protocol permitted for a request made with the SAS. An optional field that has possible values of both HTTPS and HTTP (the default value), or HTTPS only.
sv	signed version	sv=2019-02-02	The service version of the storage API to use.
sr	scope of resource	sr=b	The kind of storage being accessed. Available values include b (blob), c (container), d (directory), f (file), s (share)
sig	signature	sig=SrW1...wVZs%3D	The cryptographic signature.
The signature is signed with your storage account key when you create a service or account shared access signature. If you use a Microsoft Entra security principal with access to the storage, you create a user delegation shared access signature. You also grant the Microsoft.Storage/storageAccounts/blobServices/generateUserDelegationKey action to the principal.

Create a SAS in .NET
Because your company provides access to third parties, you can't use Microsoft Entra ID to create service principals for each third party that requires access to medical images. Your app uses a storage account key for each individual file. The following steps show how to create a SAS using C# code.

Create a blob container client to connect to the storage account on Azure
C#

Copy
BlobContainerClient container = new BlobContainerClient( "ConnectionString", "Container" );
Retrieve the blob you want to create a SAS token for and create a BlobClient
C#

Copy
foreach (BlobItem blobItem in container.GetBlobs())
{
    BlobClient blob = container.GetBlobClient(blobItem.Name);
}
Create a BlobSasBuilder object for the blob you use to generate the SAS token
C#

Copy
BlobSasBuilder sas = new BlobSasBuilder
{
    BlobContainerName = blob.BlobContainerName,
    BlobName = blob.Name,
    Resource = "b",
    ExpiresOn = DateTimeOffset.UtcNow.AddMinutes(1)
};

// Allow read access
sas.SetPermissions(BlobSasPermissions.Read);
Authenticate a call to the ToSasQueryParameters method of the BlobSasBuilder object
C#

Copy
StorageSharedKeyCredential storageSharedKeyCredential = new StorageSharedKeyCredential( "AccountName", "AccountKey");

sasToken = sas.ToSasQueryParameters(storageSharedKeyCredential).ToString();
Best practices
To reduce the potential risks of using a SAS, Microsoft provides some guidance:

To securely distribute a SAS and help prevent man-in-the-middle attacks, always use HTTPS.
The most secure SAS is user delegation. Use it wherever possible because it removes the need to store your storage account key in code. Microsoft Entra ID must be used to manage credentials; this option might not be possible for your solution.
Try to set your expiration time to the smallest useful value. If a SAS key becomes compromised, it can be exploited for only a short time.
Apply the rule of minimum-required privileges. Only grant the access that's required. For example, in your app, read-only access is sufficient.
There are some situations where a SAS isn't the correct solution. When there's an unacceptable risk of using a SAS, create a middle-tier service to manage users and their access to storage.
The most flexible and secure way to use a service or account SAS is to associate the SAS tokens with a stored access policy. In a later unit, you explore these benefits and how they work.

Next unit: Exercise - Use shared access signatures to delegate access to Azure Storage

4- Exercise - Use shared access signatures to delegate access to Azure Storage

Azure Storage enables you to authorize access to files with a shared key, with a shared access signature (SAS), or via Microsoft Entra ID. With a SAS, you control what a client can do with the files and for how long.

Your company's image diagnostic system accesses its patient images internally via a shared key. You need to create an API to allow third parties access to diagnostic images. You create a test page on your web app to see how a SAS can help you grant secure access to third-party clients.

In this exercise, you create a storage account and upload some example patient images. You deploy your team's existing web app and test that it can access the storage. The last step is to add C# and JavaScript code to generate a SAS token on demand to view the images securely.

Screenshot of your company's patient diagnostic image system showing three images.
Create a storage account and upload images
In the Cloud Shell window on the right side of the screen, select the More icon (...), then select Settings > Go to Classic version.

Using Azure Cloud Shell, enter the following code to create a storage account for patient images. The code generates a storage account name.

Azure CLI

Copy
export STORAGENAME=medicalrecords$RANDOM

az storage account create \
    --name $STORAGENAME \
    --access-tier hot \
    --kind StorageV2 \
    --resource-group "[sandbox resource group name]"
Create a container under the storage account for storing the images.

Azure CLI

Copy
az storage container create \
    --name patient-images \
    --account-name $STORAGENAME \
    --public-access off
Clone your team's existing web app. This repository contains example images used by your team for testing.

Bash

Copy
git clone https://github.com/MicrosoftDocs/mslearn-control-access-to-azure-storage-with-sas.git sas
Use the Azure CLI upload-batch command to upload the images into your storage account.

Azure CLI

Copy
az storage blob upload-batch \
    --source sas \
    --destination patient-images \
    --account-name $STORAGENAME \
    --pattern *.jpg
Test the patient diagnostic image system
Open the appsettings.json file in code editor so we can add the connection string for your storage account.

Bash

Copy
code sas/appsettings.json
In Cloud Shell, enter the following code to obtain the connection string to your storage account.

Azure CLI

Copy
az storage account show-connection-string --name $STORAGENAME
You should see a response in this format:

JSON

Copy
{
  "connectionString": "DefaultEndpointsProtocol=https;EndpointSuffix=core.windows.net;AccountName=<account-name>;AccountKey=<account-key>"
}
Copy the connectionString value, including the quotation marks.

In code editor, replace the ConnectionString value "[connection string]" with the string you copied.

Copy the value for AccountName= in the body of the connection string.

Replace the value for the AccountName parameter with the account name value you copied.

Copy the value for AccountKey= in the body of the connection string (don't include the quotation mark). Make sure to include the == at the end of the value.

Replace the value of the AccountKey parameter with the account key value you copied.

The appsettings.json file should now look similar to this output.

JSON

Copy
{
  "Logging": {
    "LogLevel": {
      "Default": "Warning"
    }
  },
  "AllowedHosts": "*",
  "StorageAccount": {
    "ConnectionString": "DefaultEndpointsProtocol=https;AccountName=<account-name>;AccountKey=<account-key>;EndpointSuffix=core.windows.net",
    "Container" : "patient-images",
    "AccountName":"<account-name>",
    "AccountKey":"<account-key>"
  }  
}
Save and close the code editor by selecting Ctrl+S, and then selecting Ctrl+Q.

Open a port so you can access your web app when it's running in Cloud Shell.

Bash

Copy
curl -X POST http://localhost:8888/openPort/8000;
This command returns a url where your app can be accessed.

JSON

Copy
{"message":"Port 8000 is open","url":"https://gateway11.northeurope.console.azure.com/n/cc-4016c848/cc-4016c848/proxy/8000/"}
Run your app.

Bash

Copy
cd sas
dotnet run
When the app has compiled, the Cloud Shell console displays details similar to the following example.

Bash

Copy
Hosting environment: Development
Content root path: /home/<yourusername>/sas
Now listening on: https://localhost:8001
Now listening on: http://localhost:8000
Application started. Press Ctrl+C to shut down.
In a browser, paste the URL returned by the previous cURL command. Make sure you include the slash (/) at the end of the address.

The URL should be in this format: https://gateway11.northeurope.console.azure.com/n/cc-4016c848/cc-4016c848/proxy/8000/.

If you're prompted to sign in, refresh your browser window. The Lamna Healthcare Patient Diagnostic Image System appears.

Select Get all patients to view a listing of all the images in the storage account.

Add code to create a SAS
In the Cloud Shell, stop the web app by selecting Ctrl+C.

Let's enhance the PatientRecordController class to create an on-demand SAS and return it to the front end of the web app.

Enter the following code to open the PatientRecordController.cs file in the code editor.

Bash

Copy
code Controllers/PatientRecordController.cs
Add the following code to the bottom of the class under the GET PatientRecord/patient-nnnnnn method.

C#

Copy
// GET PatientRecord/patient-nnnnnn/secure
[HttpGet("{Name}/{secure}")]
public PatientRecord Get(string name, string flag)
{
    BlobClient blob = _container.GetBlobClient(name);
    return new PatientRecord { name=blob.Name, imageURI=blob.Uri.AbsoluteUri, sasToken=GetBlobSas(blob) };
}
This method returns the requested patient image with a SAS that can be used to access it.

Add a method that creates the SAS for the blob.

C#

Copy
// Build a SAS token for the given blob
private string GetBlobSas(BlobClient blob)
{
    // Create a user SAS that only allows reading for a minute
    BlobSasBuilder sas = new BlobSasBuilder 
    {
        BlobContainerName = blob.BlobContainerName,
        BlobName = blob.Name,
        Resource = "b",
        ExpiresOn = DateTimeOffset.UtcNow.AddMinutes(1)
    };
    // Allow read access
    sas.SetPermissions(BlobSasPermissions.Read);

    // Use the shared key to access the blob
    var storageSharedKeyCredential = new StorageSharedKeyCredential(
        _iconfiguration.GetValue<string>("StorageAccount:AccountName"),
        _iconfiguration.GetValue<string>("StorageAccount:AccountKey")
    );

    return '?' + sas.ToSasQueryParameters(storageSharedKeyCredential).ToString();
}
This method uses the passed BlobClient object to create a BlobSasBuilder to generate a SAS that is read-only and expires in one minute.

Save the file by selecting Ctrl+S, and then and quit the editor by selecting Ctrl+Q.

Add code to use the SAS
Let's add code to the webpage to request the SAS for the image.

Enter the following command to edit the external.cshtml page.

Bash

Copy
code Pages/external.cshtml

Near the end of the file, in the click listener for #btn-submit, modify the $.get line to add + '/secure':

JavaScript

Copy
$('#btn-submit').click(function(){
    $('#result').empty();
    $.get('api/PatientRecords/' + $('#patientID').val() + '/secure', function (data) {
        var imageURL = data.imageURI + $('#sasKey').val();
        $('#result').html('<img id="patientScan" class="alert alert-success" src="' + imageURL + '" alt="patient scan" onerror="this.classList.remove(\'alert-success\'); this.classList.add(\'alert-danger\')"//>');
    }, 'json');
});
Below the #btn-submit click listener function, at the bottom of the file, above the </script> tag, add the following code:

JavaScript

Copy
$('#btn-getKey').click(function(){
    $.get('api/PatientRecords/' + $('#patientID').val() + '/secure', function (data) {
        $('#sasKey').val(data.sasToken);
    }, 'json');
});
This jQuery code adds a click listener on the btn-getKey button. The code executes an Ajax call to the new secure URL for the given image file. When it returns, it populates the key input box with the SAS.

Save the changes by selecting Ctrl+S, and then and quit the editor by selecting Ctrl+Q.

Test your changes
Run your updated app.

Bash

Copy
dotnet run
In your browser, refresh the tab for your web site. Select Get all patients, and then copy one of the image filenames.

In the menu at the top of the web page, select External companies.

Paste the filename into the Patient image filename field.

Select View scan. The patient scan image isn't accessible because you haven't created a SAS.

 Note

If you are viewing the console in your browser, you'll see the web server returned a 404 error-code message.

Select Get Key, which should populate the Key field with a SAS.

Select View scan. The patient's diagnostic image should appear.

Screenshot of API for external companies showing a patient's image.

In your browser, right-click the image and copy the image address.

Open a new browser tab, paste the copied image address in the address bar, and press Enter. If it's been longer than a minute since you created the SAS, you should see an error message. If it's been less than a minute, the image should display.

 Note

You might need to refresh the page.

XML

Copy
<Error>
    <Code>AuthenticationFailed</Code>
    <Message>Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly, including the signature.
    RequestId:03eda893-f01e-0028-2d73-c5c947000000
    Time:2021-01-07T16:02:55.3752851Z</Message>
    <AuthenticationErrorDetail>Signed expiry time [Tue, 07 Jan 2021 16:02:00 GMT] must be after signed start time [Tue, 07 Jan 2021 16:02:55 GMT]</AuthenticationErrorDetail>
</Error>
 Note

To view this error message from some browsers, you may need to open a new browser window that won't have cached the image.

In Cloud Shell, quit the web app by selecting Ctrl+C.



Next unit: Use stored access policies to delegate access to Azure Storage

5- Use stored access policies to delegate access to Azure Storage

A shared access signature (SAS) is a secure way to give access to clients without having to share your Azure credentials. This ease of use comes with a downside. Anyone with the correct SAS can access the file while it's still valid. The only way you can revoke access to the storage is to regenerate access keys. Regeneration requires you to update all apps that are using the old shared key to use the new one. Another option is to associate the SASs with a stored access policy.

When you added SAS functionality to your app, it highlighted the inflexibility of creating a SAS for each image, each with its own expiration and access controls. You want to update your app to use a stored access policy on the storage container. With the policy in place, you want to test that you can update the expiration and affect all the created SAS tokens.

In this unit, you learn how to:

Use a stored access policy.
Use the C# Storage API to create SAS tokens associated with your new access policy.
Test that the SAS tokens can all be changed by updating the stored access policy in the Azure portal.
What are stored access policies?
You can create a stored access policy on four kinds of storage resources:

Blob containers
File shares
Queues
Tables
The stored access policy you create for a blob container can be used for all the blobs in the container and for the container itself. A stored access policy is created with the following properties:

Identifier: The name you use to reference the stored access policy.
Start time: A DateTimeOffset value for the date and time when the policy might start to be used. This value can be null.
Expiry time: A DateTimeOffset value for the date and time when the policy expires. After this time, requests to the storage will fail with a 403 error-code message.
Permissions: The list of permissions as a string that can be one or all of acdlrw.
Screenshot of the Azure portal showing a stored access policy.

Create stored access policies
You can create a stored access policy with C# code by using the Azure portal or Azure CLI commands.

With C# .NET code
C#

Copy
BlobSignedIdentifier identifier = new BlobSignedIdentifier
{
    Id = "stored access policy identifier",
    AccessPolicy = new BlobAccessPolicy
    {
        ExpiresOn = DateTimeOffset.UtcNow.AddHours(1),
        Permissions = "rw"
    }
};

blobContainer.SetAccessPolicy(permissions: new BlobSignedIdentifier[] { identifier });
With the portal
In the portal, go to the storage account and then go to the blob storage container. On the left, select Access policy. To add a new stored access policy, select + Add policy.

You can then enter all the required parameters.

Screenshot of the options for adding an access policy.

With Azure CLI commands
Azure CLI

Copy
az storage container policy create \
    --name <stored access policy identifier> \
    --container-name <container name> \
    --start <start time UTC datetime> \
    --expiry <expiry time UTC datetime> \
    --permissions <(a)dd, (c)reate, (d)elete, (l)ist, (r)ead, or (w)rite> \
    --account-key <storage account key> \
    --account-name <storage account name> \
Create SAS tokens and associate them with stored access policies
Let's associate the stored access policy you created with any new SAS tokens you need. For your company's patient diagnostic image web app, update the existing code to add the previous code. Then, in the method that creates the SAS token, you reference the new stored access policy.

All of your existing code that's needed to create the SAS token is:

C#

Copy
BlobSasBuilder sas = new BlobSasBuilder
{
    BlobContainerName = blob.BlobContainerName,
    BlobName = blob.Name,
    Resource = "b",
    ExpiresOn = DateTimeOffset.UtcNow.AddMinutes(1)
};
// Allow read access
sas.SetPermissions(BlobSasPermissions.Read);
and it can be replaced by referencing your new access policy.

C#

Copy
// Create a user SAS that only allows reading for a minute
BlobSasBuilder sas = new BlobSasBuilder
{
    Identifier = "stored access policy identifier"
};
You can have up to five stored access policies for a single blob container.


Next unit: Exercise - Use stored access policies to delegate access to Azure Storage

6- Exercise - Use stored access policies to delegate access to Azure Storage

Instead of creating SASs individually, each with its own access permissions and expiration dates, you can associate them with a stored access policy. Changing the policy affects all the SASs associated with it.

Now that you know there's a better way to create and manage your company's SASs. You can update your new test pages to use stored access policies.

In this exercise, you update your web app to create SASs with stored access policies. Then you use Azure CLI commands to change the policies and test that access is revoked.

Add a method to create stored access policies
In Azure Cloud Shell, edit the PatientRecordController.cs file.

Bash

Copy
code ~/sas/Controllers/PatientRecordController.cs
At the bottom of the class, under the GetBlobSas method, write a method to create stored access policies.

C#

Copy
// Use a stored access policy for the SAS
private void CreateStoredAccessPolicy()
{
    // Create a stored access policy for our blobs
    BlobSignedIdentifier identifier = new BlobSignedIdentifier
    {
        Id = _storedPolicyID,
        AccessPolicy = new BlobAccessPolicy
        {
            ExpiresOn = DateTimeOffset.UtcNow.AddHours(1),
            Permissions = "r"
        }
    };

    _container.SetAccessPolicy(permissions: new BlobSignedIdentifier[] { identifier });
} 
This method uses a global variable for the access policy identifier. Add this variable at the top of the class under the declaration for the BlobContainerClient variable named _container.

C#

Copy
private String _storedPolicyID = "patient-images-policy";
The stored access policy is used for each SAS token that's generated, so call the new method on the class instantiation. Add a call at the bottom of the method.

C#

Copy
public PatientRecordsController(ILogger<PatientRecordsController> logger, IConfiguration iconfiguration)
{
    _logger = logger;
    _iconfiguration = iconfiguration; 
    _container = new BlobContainerClient(
        _iconfiguration.GetValue<string>("StorageAccount:ConnectionString"),
        _iconfiguration.GetValue<string>("StorageAccount:Container")
    );
    CreateStoredAccessPolicy();
}
Now GetBlobSas can be simplified to use the access policy. Change the method to use it.

C#

Copy
 // Build a SAS token for the given blob
 private string GetBlobSas()
 {
     // Create a user SAS that only allows reading for a minute
     BlobSasBuilder sas = new BlobSasBuilder 
     {
         Identifier = _storedPolicyID,
         BlobContainerName = _iconfiguration.GetValue<string>("StorageAccount:Container")
     };

     // Use the shared key to access the blob
     var storageSharedKeyCredential = new StorageSharedKeyCredential(
         _iconfiguration.GetValue<string>("StorageAccount:AccountName"),
         _iconfiguration.GetValue<string>("StorageAccount:AccountKey")
     );

     return '?' + sas.ToSasQueryParameters(storageSharedKeyCredential).ToString();
 }
The code that handles the SAS token requests needs a small fix to call the updated method.

C#

Copy
// GET PatientRecord/patient-nnnnnn/secure
[HttpGet("{Name}/{secure}")]
public PatientRecord Get(string name, string flag)
{
    BlobClient blob = _container.GetBlobClient(name);
    return new PatientRecord { name=blob.Name, imageURI=blob.Uri.AbsoluteUri, sasToken=GetBlobSas() };
}
Save your code changes by selecting Ctrl+S and then selecting Ctrl+Q.

Test the new code
In Cloud Shell, build the app.

Bash

Copy
cd ~/sas/
dotnet build
In case the port has closed since you finished the previous exercise, run the curl command to open it again.

Bash

Copy
curl -X POST http://localhost:8888/openPort/8000;
Run the update web app.

Bash

Copy
dotnet run
Go to the web app's URL, and make sure it ends in a slash (/).

On the home page, select Get all patients.

Copy an image filename. An example is patient-32589.jpg.

Select the External companies menu link at the top of the page.

Paste the image filename into the Patient image filename field.

Select Get Key to populate the SAS token.

Select View scan to view the image.

Edit the stored access policy
Sign in to the Azure portal using the same credentials you used to activate the sandbox.

In the Azure portal resource menu, select All resources.

In the list of resources, select the medical records storage account.

On the Overview pane, select Containers, and then select patient-images.

On the patient images menu, under Settings, select Access policy.

Notice that your web app created a patient-images-policy stored access policy.

On the right, select the ... menu, and then select Edit from the pop-up menu.

In the Edit policy, change the Permission from read to list and select OK to confirm.

Select Save on the patient-images | Access policy pane.

Test a new SAS
Return to your web app. On the External companies page, create a new SAS token by selecting Get Key.

Select View scan.

Screenshot of the web app failing to view a patient image.

The image isn't returned from Azure Storage and you get a 403 authentication error.

Next unit: Summary

Summary

Your company needed to control access to highly sensitive images. Your application needed enhancements to enable it to integrate with other providers and provide them with controlled authorization to the images.

You used the security features of Azure Storage to generate unique shared access signature (SAS) tokens for images stored in containers. You then enhanced the flexibility and control of the SAS tokens by associating them with a stored access policy.

Without a SAS and access policies, your company would likely have had to develop a custom middle tier. The cost of this development, and maintaining the middle tier, would be far greater than the elegant solution you used in these exercises.



Point 7: Upload, download, and manage data with Azure Storage Explorer

Azure Storage Explorer allows you to quickly view all the storage services under your account. You can browse through, read, and edit data stored in those services through a user-friendly graphical interface.

Learning objectives
In this module, you will:

Describe the features of Azure Storage Explorer.
Install Storage Explorer.
Use Storage Explorer to connect to Azure Storage services and manipulate stored data.


1- Introduction

Azure Storage Explorer makes it easy to access and edit data in Azure.

Suppose you work for an enterprise that has developed a customer relationship management (CRM) application. The application writes data to Azure Data Lake Storage. Your software engineers occasionally need to view stored data, upload new data, and administer these storage services. They'd like to have a user-friendly tool for these activities.

In this module, you'll learn about the features of Azure Storage Explorer, how to install it, and what it can connect to. Finally, you'll use Storage Explorer to connect to Data Lake Storage to create a database and upload data.

By the end of this module, you'll know how to use Storage Explorer to manipulate data in multiple Azure services.

Learning objectives
Identify the features of Azure Storage Explorer
Install Storage Explorer
Use Storage Explorer to connect to Azure Storage services and manipulate stored data
Prerequisites
Basic knowledge of Azure Storage and Azure Data Lake Storage
Ability to install software locally


Next unit: Connect Azure Storage Explorer to a storage account

2- Connect Azure Storage Explorer to a storage account

Storage accounts provide a flexible solution that keeps data as files, tables, and messages. With Azure Storage Explorer, it's easy to read and manipulate this data.

You want to enable your engineers to manage the data stored in Azure Storage so they can maintain the data that your CRM application uses. You want to assess whether they can use Storage Explorer for this purpose.

Here, you'll learn about Storage Explorer and how you can use it to manage data from multiple storage accounts and subscriptions. You'll learn different ways of using Storage Explorer to connect to your data, Azure Stack, and data held in Azure Data Lake Storage.

What is Storage Explorer?
Storage Explorer is a GUI application developed by Microsoft to simplify accessing and managing data stored in Azure Storage accounts. Storage Explorer is available on Windows, macOS, and Linux.

Some of the benefits of using Storage Explorer are:

It's easy to connect to and manage multiple storage accounts.
The interface lets you connect to Data Lake Storage.
You can also use the interface to update and view entities in your storage accounts.
Storage Explorer is free to download and use.
With Storage Explorer, you can use a range of storage and data operation tasks on any of your Azure Storage accounts. These tasks include edit, download, copy, and delete.

Supported software versions
The Azure Storage Explorer application runs on the following versions of these platforms:

Operating system	Versions
Windows	Windows 11, Windows 10, Windows 8, or Windows 7
macOS	macOS 10.12 Sierra and later
Linux	Ubuntu 18.04 x64, Ubuntu16.04 x64, or Ubuntu 14.04 x64
Azure Storage types
Azure Storage Explorer can access many different data types from services like these:

Azure Blob Storage: Blob storage is used to store unstructured data as a binary large object (blob).
Azure Table Storage: Table storage is used to store NoSQL, semi-structured data.
Azure Queue Storage: Queue storage is used to store messages in a queue, which can then be accessed and processed by applications through HTTP(S) calls.
Azure Files: Azure Files is a file-sharing service that enables access through the Server Message Block protocol, similar to traditional file servers.
Azure Data Lake Storage: Azure Data Lake, based on Apache Hadoop, is designed for large data volumes and can store unstructured and structured data. Azure Data Lake Storage Gen2 is Azure Blob Storage with the hierarchical namespace feature enabled on the account.
Manage multiple storage accounts in multiple subscriptions
If you have multiple storage accounts across multiple subscriptions in your Azure tenant, managing them through the Azure portal can be time consuming. Storage Explorer lets you manage the data stored in multiple Azure Storage accounts and across Azure subscriptions.

Use local emulators
During the development phase of your project, you might not want developers to incur additional costs by using Azure Storage accounts. In those cases, you can use a locally based emulator. Storage Explorer supports two emulators: Azure Storage Emulator and Azurite.

Azure Storage Emulator uses a local instance of Microsoft SQL Server 2012 Express LocalDB. It emulates Azure Table, Queue, and Blob storage.
Azurite, which is based on Node.js, is an open-source emulator that supports most Azure Storage commands through an API.
Storage Explorer requires the emulator to be running before you open it. Connecting to your emulator is no different than connecting to Azure Storage accounts except that you'll choose the Attach to a local emulator connection type.

All locally emulated storage connection types appear in Local & Attached > Storage accounts.

Connecting Storage Explorer to Azure
There are several ways to connect your Storage Explorer application to your Azure Storage accounts.

You need two permissions to access your Azure Storage account: management and data. However, you can use Storage Explorer with only the data-layer permission. The data layer requires the user to be granted, at a minimum, a read data role. The nature of the read/write role should be specific to the type of data stored in the storage account. The data layer is used to access blobs, containers, and other data resources.

The management role grants access to view lists of your various storage accounts, containers, and service endpoints.

Connection types
There are many ways to connect an Azure Storage Explorer instance to your Azure resources. For example:

Add resources by using Microsoft Entra ID
Use a connection string
Use a shared access signature URI
Use a name and key
Attach to a local emulator
Attach to Azure Data Lake Storage by using a URI
We'll explore a few of these connection types, and provide an overview of the required steps to set up the connection.


Add an Azure account by using Microsoft Entra ID
Use this connection type when the user can access the data layer. You can use it only to create a container. Connecting to Azure Storage through Microsoft Entra ID requires more configuration than the other methods. The account that you use to connect to Azure must have the correct permissions and authorization to access the target resources.

To add a resource by using Microsoft Entra ID:

Open Storage Explorer.
Select the Sign in with Azure option and sign in to Azure.
Connect to your Azure Storage account.
Select Add a resource via Microsoft Entra ID, and then choose the Azure tenant and the associated account.
When you're prompted, provide the type of resource that you're connecting to.
Review and verify the connection details, and then select Connect.
It's crucial to select the correct resource type because it changes the information that you need to enter.

Any connections that you create through this approach will appear in the resource tree, in this branch: Local & attached > Storage Accounts > Attached Containers > Blob.

Connect by using a shared access signature URI
A shared access signature (SAS) URI is an unambiguous identifier that's used to access your Azure Storage resources.

With this connection method, you'll use a SAS URI for the required storage account. You'll need a SAS URI whether you want to use a file share, table, queue, or blob container. You can get a SAS URI either from the Azure portal or from Storage Explorer. For more information, see Create an account SAS.

To add a SAS connection:

Open Storage Explorer.
Connect to your Azure Storage account.
Select the connection type: Shared access signature URI (SAS).
Provide a meaningful name for the connection.
Provide the SAS URI.
Review and verify the connection details, and then select Connect.
When you've added a connection, it appears in the resource tree as a new node. You'll find the connection node in this branch: Local & attached > Storage Accounts > Attached Container > Service.

Connect by using a storage account name and key
To connect to a storage account on Azure quickly, you use the account key that's associated with the storage. To find the storage access keys from the Azure portal, go to the correct storage account page and select access keys.

To add a connection:

Open Storage Explorer.
Connect to your Azure Storage account.
Select the connection type: Storage account name and key.
Provide a meaningful name for the connection.
When you're prompted, provide the name of the storage account and either of the account keys needed to access it.
From the provided list, select the storage domain that you want to use.
Review and verify the connection details, and then select Connect.
When the connection is added, it appears in the resource tree as a connection node. The connection node is in this branch: Local & attached > Storage Accounts.

Next unit: Exercise - Connect Azure Storage Explorer to a storage account

3- Exercise - Connect Azure Storage Explorer to a storage account


It's easy to browse through the contents of an Azure Storage account by using Azure Storage Explorer.

Now that you have a better understanding of the features and capabilities of Storage Explorer, you can try it for yourself. Use Storage Explorer to explore some of the files that your CRM system stores in Azure Storage.

Here, you'll try Storage Explorer by downloading, installing, and connecting to an Azure Storage account. You'll create a blob and a queue in your storage account.

Download and install Azure Storage Explorer
First, you need to download and install Storage Explorer.

Browse to the Azure Storage Explorer website.

Select Download now, then select your preferred operating system. The following steps will go through the Windows version of the application. Your steps will be different if you're using a different OS.

Locate the downloaded file and run it. For the Windows version, use the StorageExplorer.exe file.

Accept the license agreement and select Install.

Browse to the location where you want to install Storage Explorer or accept the default location. Select Next.

For Windows installations, select the Start menu folder. Accept the default and select Next.

When the installation is complete, select Finish.

Storage Explorer automatically opens after installation.

Connect to an Azure account
When you first open Storage Explorer, it displays the Connect to Azure Storage wizard.

First, select Connect to Azure resources, and then choose Subscription.

Screenshot that shows the Select resource screen in the Azure Storage wizard.

There are several Azure environment options to select from. Select Azure, then select Next.

Screenshot that shows the Select Azure environment screen in the Connect to Azure Storage wizard.

Your browser opens and an Azure sign-in page appears. Use your Azure credentials to sign in.

Screenshot that shows the Azure sign-in page.

When you've signed in to your Azure instance, the associated Azure account and Azure subscription appear in the Account Management section.

Screenshot that shows the account management panel after signing in to an Azure account.

Confirm that the Concierge Subscription subscription is selected and account details are correct, and then select Open Explorer.

You've now connected Storage Explorer to your Azure subscription. Leave Storage Explorer open while you work through the next steps.

Create a storage account and add a blob
In Azure Cloud Shell, run the following command to create a storage account.

Azure CLI

Copy
az storage account create \
--name  mslearn$RANDOM \
--resource-group "[sandbox resource group name]" \
--sku Standard_GRS \
--kind StorageV2
In the output, note the name of the storage account. After the storage account is created, switch back to Storage Explorer.

If it isn't currently visible, toggle the EXPLORER view so that the pane is shown.

In the EXPLORER pane, select Refresh All, then locate and expand Concierge Subscription.

Screenshot that shows the expansion of Concierge Subscription.

Locate and expand the storage account that you created earlier. It should be named something similar to mslearn12345, ending with a different set of numbers. It has four virtual folders: Blob Containers, File Shares, Queues, and Tables.

 Note

If the storage account you created earlier isn't listed, wait a few moments and select Refresh All; it can take a couple of minutes for the account to appear.

Right-click the Blob Containers virtual folder to access the shortcut menu, and then select Create Blob Container.

Screenshot that shows the shortcut menu options for the Blob Containers folder.

Name the container myblobcontainer, and press Enter.

Each created container appears in a tab to the right of the resource tree.

Screenshot that shows the content and details of the new myblobcontainer blob container.

Upload a blob to the container. In the myblobcontainer pane, select Upload, and then select Upload Files. The Upload Files dialog box appears.

For Selected files, select the ellipsis (...). Browse to a small file on your device and select Open. Select Upload to upload the file.

Screenshot that shows the Upload Files dialog box.

You should now see your file stored in your storage account.

Screenshot that shows the file in the storage account.

From here, you can upload additional files, download files, make copies, and do other administrative tasks.

Create a queue in your Azure Storage account
To create a queue in your storage account:

In the resource tree, find Concierge Subscription and expand the options.

Expand the cloudshell storage account.

Right-click the Queues virtual folder to access the shortcut menu, and then select Create Queue.

An empty and unnamed queue is created inside the Queues folder. The queue won't be created until you give it a name.

 Note

Containers have specific rules that govern how they can be named. They must begin and end with either a letter or a number, must be all lowercase, and can have numbers and hyphens. The name can't contain a double hyphen.

Name this new queue myqueue and press Enter to create the queue. Each created queue appears on a tab to the right of the resource tree.

Screenshot that shows the content and details of the new myblob blob container.

From this view, you can manage the queue's content. If our application used this queue and experienced an issue with processing a message, you could connect to the queue and view the message contents to determine the issue.



Next unit: Connect Azure Storage Explorer to Azure Data Lake Storage

4- Connect Azure Storage Explorer to Azure Data Lake Storage

Azure Storage Explorer doesn't just access Azure Storage. It can also access data in Azure Data Lake Storage.

Use Storage Explorer to manage Data Lake Storage
You've worked through the basics of connecting Storage Explorer to your Azure account. In the CRM system, your developers use Data Lake Storage for big-data storage. You want to use Storage Explorer to connect to it.

Azure Data Lake Storage is used for storing and analyzing large data sets. It supports large data workloads. It's well suited to capture data of any type or size, and at any speed. Data Lake Storage features all the expected enterprise-grade capabilities like security, scalability, reliability, manageability, and availability.

There are two types of Azure Data Lake Storage: Gen1 and Gen2. Azure Data Lake Storage Gen1 has been retired; follow the Azure Data Lake Storage Gen1 migration guidance for existing Gen1 accounts.

You can use Storage Explorer to connect to Data Lake accounts. Just like with storage accounts, you can use Storage Explorer to:

Create, delete, and manage containers.
Upload, manage, and administer blobs.
Let's create Azure Data Lake Storage, then use Storage Explorer to connect to it.

Next unit: Exercise - Connect Azure Storage Explorer to Azure Data Lake Storage

5- Exercise - Connect Azure Storage Explorer to Azure Data Lake Storage

Azure Storage Explorer isn't just about storage accounts. You can also use it to investigate and download data from Azure Data Lake Storage.

You've learned how simple creating and managing blob and queue resources in your Azure Storage account is. Now you want to push your understanding further and learn how the storage account connects to your developers' data lake, which they use to store infrastructure data for the CRM system.

Azure Data Lake Storage Gen2 isn't a dedicated service or account type. It's a set of capabilities that you unlock by enabling the hierarchical namespace feature of an Azure Storage account. Here, you'll learn how to use Storage Explorer to connect to Azure Data Lake Storage Gen2, create a container, and upload data into it.

Create a storage account with Azure Data Lake Storage Gen2 capabilities
Let's look at connecting to a Data Lake Storage Gen2-enabled account. Before you can use Storage Explorer to manage your Data Lake Storage Gen2-enabled account, you need to create the storage account in Azure.

To create the storage account, use the az storage account create command:

Azure CLI

Copy
az storage account create \
    --name dlstoragetest$RANDOM \
    --resource-group [Sandbox resource group] \
    --location westus2 \
    --sku Standard_LRS \
    --kind StorageV2 \
    --hns
 Note

Please give the storage account several minutes to complete.

Connect to your Data Lake Gen2 enabled storage account
Now that you've created a Gen2 enabled storage account, Storage Explorer should automatically connect to it.

In Storage Explorer, in the EXPLORER pane, locate Concierge Subscription and expand it to show all the storage accounts.

 Note

It might take several minutes for the storage account to display in Storage Explorer. If you don't see the storage account, wait a few moments and select Refresh all.

You'll see the dlstoragetest001 (ADLS Gen2) storage account displayed under the storage accounts. Your account will have a different number suffix.

Screenshot that shows the Azure Data Lake Storage Gen2 account.

Create a container
All containers in a Data Lake Gen2 enabled storage account are blobs. To create a new container:

Right-click the dlstoragetest001 storage account, and select Create Blob Container from the shortcut menu.

Screenshot that shows the shortcut menu for adding a container.

Name the new container myfilesystem.

When the container is created, the pane for the container appears. There, you can manage the container contents.

Screenshot that shows the myfilesystem control ribbon and view.

Upload and view blob data
With the new myfilesystem container created, you can now upload files or folders to it.

To upload a file, select the Upload option, then select Upload Files.

Screenshot that shows the upload options.

In the dialog box, use the ellipsis (...) to select the file that you want to upload.

Select the file you want to upload, then select Open.

Select the Upload button.

The file is available to the myfilesystem container.

Screenshot that shows the uploaded file.

You can upload as many files as you want to this folder. Also, you can create an unlimited number of folders. You can then organize and manage the content in your folders, as you do with your file system.

Next unit: Summary

Summary

Your CRM application is a complex system that stores data in Azure Storage in Azure Data Lake Storage. You wanted an easy-to-use tool that helps your engineers to administer the data in the different Azure locations.

You can use Azure Storage Explorer to give your developers a tool that manages and controls data stored in Azure, including Azure Data Lake Storage Gen2.

You can explore your Azure data with many different tools, depending on the data location and your preference. For example, the Azure portal includes a web-based interface that presents the contents of a storage account, but you can also use the Azure CLI at the command line.

In this module, you explored different ways to use Storage Explorer to connect to your storage accounts. You used the Azure CLI to create Data Lake Gen2 Storage enabled storage accounts. Finally, you connected Storage Explorer to Data Lake Storage. Then you uploaded and viewed the data that's stored there.

Learn more
To learn more about Storage Explorer, see the following articles:

Azure Storage Explorer download
Use Azure Storage Explorer to manage directories, files, and ACLs in Azure Data Lake Storage Gen2






















Azure Administrator Associate

Chapter 5: Deploy and manage Azure compute resources


Modules in this learning path


Configure virtual machines

Learn how to configure virtual machines including sizing, storage, and connections.




Configure virtual machine availability

Learn how to configure virtual machine availability including vertical and horizontal scaling.

 Note

This content was partially created with the help of AI. An author reviewed and revised the content as needed. Read more.



Configure Azure App Service plans

Learn how to configure an Azure App Service plan, including pricing and scaling.



Configure Azure App Service

Learn how to configure and monitor Azure App Service instances, including deployment slots.


Configure Azure Container Instances

Learn how to configure Azure Container Instances including container groups.



Manage virtual machines with the Azure CLI

Learn how to use the cross-platform Azure CLI to create, start, stop, and perform other management tasks related to virtual machines in Azure.



Create a Windows virtual machine in Azure

Azure virtual machines (VMs) enable you to create dedicated compute resources in minutes that can be used just like a physical desktop or server machine.



Host a web application with Azure App Service

Azure App Service enables you to build and host web applications in the programming language of your choice without managing infrastructure. Learn how to create a website through the hosted web app platform in Azure App Service.















Point 1: Configure virtual machines

Learn how to configure virtual machines including sizing, storage, and connections.

Learning objectives
In this module, you learn how to:

Determine the responsibilities of cloud service providers and customers in a cloud computing environment.
Identify the key considerations and factors involved in planning for virtual machines. Considerations include workload requirements, resource allocation, and secure access.
Configure virtual machine storage and virtual machine sizing.
Create a virtual machine in the Azure portal.
Practice deploying an Azure virtual machine and verify the configuration.


1- Introduction

Azure Virtual Machines enables you to create on-demand, scalable computing resources. Azure Architects commonly use virtual machines to gain greater control over the computing environment.

Your company is doing consumer research, and your team manages the on-premises servers. The servers you administer run the entire company infrastructure from web servers to databases. However, the hardware is aging and starting to struggle to keep up with some of the new data analysis applications being deployed. Rather than upgrade the hardware, the company has decided to deploy Azure virtual machines. You're responsible for deploying the new virtual machines. Your deployment tasks include correctly sizing the machines, selecting storage, and configuring networking.

In this module, you learn how to configure virtual machine names and locations. You examine virtual machine pricing models and discover how to determine the correct virtual machine size. You learn how to configure virtual machine storage, and how to create a virtual machine in the Azure portal. You select a secure virtual machine connection method, and how to configure Windows and Linux virtual machine connections.

The goal of this module is to learn how to configure and manage virtual machines in Azure.

Learning objectives
In this module, you learn how to:

Determine the responsibilities of cloud service providers and customers in a cloud computing environment.
Identify the key considerations and factors involved in planning for virtual machines. Considerations include workload requirements, resource allocation, and secure access.
Configure virtual machine storage and virtual machine sizing.
Create a virtual machine in the Azure portal.
Practice deploying an Azure virtual machine and verify the configuration.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Cloud computing concepts. Familiarity with Infrastructure as a Service (IaaS), virtualization, and resource provisioning in a cloud environment.

Azure fundamentals. Understanding of basic Azure concepts, including Azure subscriptions, resource groups, and storage accounts.

Networking fundamentals. Knowledge of basic networking concepts, including IP addressing, virtual networks, and subnets.

Azure portal. Ability to create and configure resources in the Azure portal.


Next unit: Review cloud services responsibilities

2- Review cloud services responsibilities

The primary advantage of working with virtual machines is to have more control over installed software and configuration settings. Azure Virtual Machines supports more granular control than other Azure services, such as Azure App Service or Azure Cloud Services.

Things to know about Azure Virtual Machines
Consider the following characteristics of Azure Virtual Machines.

Azure Virtual Machines is the basis of the Azure infrastructure as a service (IaaS) model. IaaS is an instant computing infrastructure, provisioned and managed over the internet.

A virtual machine provides its own operating system, storage, and networking capabilities, and can run a wide range of applications.

You can implement multiple virtual machines, and configure each machine with different software and settings to support separate operation scenarios, such as development, testing, and deployment.

You can use virtual machines to quickly scale up and down with demand and pay only for what you use.

The responsibilities associated with configuring and maintaining virtual machines is shared between Microsoft and the customer. The following chart shows how the responsibilities are handled across the IaaS (virtual machines), PaaS, SaaS, and on-premises offerings.

Diagram of the shared responsibility areas for IaaS, PaaS, SaaS, and on-premises offerings.

Things to consider when using IaaS and virtual machines
Let's look at some scenarios for working with IaaS and virtual machines. Think about how you can implement virtual machines in Azure.

Consider test and development. Teams can quickly set up and dismantle test and development environments, bringing new applications to market faster. IaaS and virtual machines make it quick and economical to scale up dev-test environments up and down.

Consider website hosting. Running websites by using IaaS and virtual machines can be less expensive than traditional web hosting.

Consider storage, backup, and recovery. Virtual machines let organizations avoid the expense for storage and complexity of storage management. Recovery typically requires a skilled staff to manage data and meet legal and compliance requirements. IaaS is useful for handling unpredictable demand and steadily growing storage needs. You can simplify planning and management of backup and recovery systems.

Consider high-performance computing. Virtual machines enable high-performance computing (HPC) on supercomputers, computer grids, or computer clusters. HPC helps solve complex problems involving millions of variables or calculations. You can support scenarios such as earthquake and protein folding simulations, climate and weather predictions, financial modeling, and evaluating product designs.

Consider big data analysis. Big data is a popular term for massive data sets that contain potentially valuable patterns, trends, and associations. Mining data sets to locate or tease out these hidden patterns requires a huge amount of processing power, which IaaS economically provides.

Consider extended datacenters. Add capacity to your datacenter by adding virtual machines in Azure. Avoid the costs of physically adding hardware or space to your physical location. Connect your physical network to the Azure cloud network seamlessly.


Next unit: Plan virtual machines

3- Plan virtual machines

Before you create an Azure virtual machine, it's helpful to make a plan for the machine configuration. You need to consider your preferences for several options, including the machine size and location, storage usage, and associated costs.

Things to know about configuring virtual machines
Let's walk through a checklist of things you need to consider when configuring a virtual machine.

Start with the network.
Choose a name for the virtual machine.
Decide the location for the virtual machine.
Determine the size of the virtual machine.
Review the pricing model and estimate your costs.
Identify which Azure Storage to use with the virtual machine.
Select an operating system for the virtual machine.
Network configuration
Virtual networks are used in Azure to provide private connectivity between Azure Virtual Machines and other Azure services. Virtual machines and services that are part of the same virtual network can access one another. By default, services outside the virtual network can't connect to services within the virtual network. You can, however, configure the network to allow access to the external service, including your on-premises servers.

Network addresses and subnets aren't trivial to change after they're configured. If you plan to connect your private company network to the Azure services, make sure you consider the topology before you put any virtual machines into place.

Virtual machine name
The virtual machine name is used as the computer name, which is configured as part of the operating system. You can specify a name with up to 15 characters on a Windows virtual machine and 64 characters on a Linux virtual machine.

The virtual machine name also defines a manageable Azure resource, and it's not trivial to change later. You should choose names that are meaningful and consistent, so you can easily identify what the virtual machine does. A good convention uses several of the following elements in the machine name:

Name element	Examples	Description
Environment or purpose	dev (development), prod (production), QA (testing)	A portion of the name should identify the environment or purpose for the machine.
Location	uw (US West), je (Japan East), ne (North Europe)	Another portion of the name should specify the region where the machine is deployed.
Instance	1, 02, 005	For multiple machines that have similar names, include an instance number in the name to differentiate the machines in the same category.
Product or service	Outlook, SQL, AzureAD	A portion of the name can specify the product, application, or service that the machine supports.
Role	security, web, messaging	A portion of the name can specify what role the machine supports within the organization.
Let's consider how to name the first development web server for your company that's hosted in the US South Central location. In this scenario, you might use the machine name devusc-webvm01. dev stands for development and usc identifies the location. web indicates the machine as a web server, and the suffix 01 shows the machine is the first in the configuration.

Virtual machine location
Azure has datacenters all over the world filled with servers and disks. These datacenters are grouped into geographic regions like West US, North Europe, Southeast Asia, and so on. The datacenters provide redundancy and availability.

Each virtual machine is in a region where you want the resources like CPU and storage to be allocated. The regional location lets you place your virtual machines as close as possible to your users. The location of the machine can improve performance and ensure you meet any legal, compliance, or tax requirements.

There are two other points to consider about the virtual machine location.

The machine location can limit your available options. Each region has different hardware available, and some configurations aren't available in all regions.

There are price differences between locations. To find the most cost-effective choice, check for your required configuration in different regions.

Virtual machine size
Azure offers different memory and storage options for different virtual machine sizes. The best way to determine the appropriate machine size is to consider the type of workload your machine needs to run. Based on the workload, you can choose from a subset of available virtual machine sizes.

Azure Storage
Azure Managed Disks handle Azure storage account creation and management in the background for you. You specify the disk size and the performance tier (Standard or Premium). Azure creates and manages the disk. As you add disks or scale the virtual machine up and down, you don't have to worry about the storage being used.

Virtual machine pricing options
A subscription is billed two separate costs for every virtual machine: compute and storage. By separating these costs, you can scale them independently and only pay for what you need.

Compute expenses are priced on a per-hour basis but billed on a per-minute basis. If the virtual machine is deployed for 55 minutes, you're charged for only 55 minutes of usage. You're not charged for compute capacity if you stop and deallocate the virtual machine. The hourly price varies based on the virtual machine size and operating system you select. For the compute costs, you're able to choose from two payment options:

Consumption-based: With the consumption-based option, you pay for compute capacity by the second. You're able to increase or decrease compute capacity on demand and start or stop at any time. Use consumption-based pricing if you run applications with short-term or unpredictable workloads that can't be interrupted. An example scenario is if you're doing a quick test or developing an app in a virtual machine.

Reserved Virtual Machine Instances: The Reserved Virtual Machine Instances (RI) option is an advance purchase of a virtual machine for one or three years in a specified region. The commitment is made up front, and in return, you get up to 72% price savings compared to pay-as-you-go pricing. RIs are flexible and can easily be exchanged or returned for an early termination fee. Use this option if the virtual machine has to run continuously, or you need budget predictability, and you can commit to using the virtual machine for at least a year.

Storage costs are charged separately for the Azure Storage used by the virtual machine. The status of the virtual machine has no relation to the Azure Storage charges that are incurred. You're always charged for any Azure Storage used by the disks.

Operating system
Azure provides various operating system images that you can install into the virtual machine, including several versions of Windows and flavors of Linux. Azure bundles the cost of the operating system license into the price.

If you're looking for more than just base operating system images, you can search Azure Marketplace. There are various install images that include not only the operating system but popular software tools, such as WordPress. The image stack consists of a Linux server, Apache web server, a MySQL database, and PHP. Instead of setting up and configuring each component, you can install an Azure Marketplace image and get the entire stack all at once.

If you don't find a suitable operating system image, you can create your own disk image. Your disk image can be uploaded to Azure Storage and used to create an Azure virtual machine. Keep in mind that Azure only supports 64-bit operating systems.

Next unit: Determine virtual machine sizing

4- Determine virtual machine sizing

Rather than specify processing power, memory, and storage capacity independently, Azure provides different virtual machine sizes that offer variations of these elements in different size configurations. Azure provides a wide range of virtual machine size options that allow you to select the appropriate mix of compute, memory, and storage for your needs.

Things to know about virtual machine sizes
The best way to determine the appropriate virtual machine size is to consider the type of workload your virtual machine needs to run. Based on the workload, you can choose from a subset of available virtual machine sizes.

The following table shows size classifications for Azure Virtual Machines workloads and recommended usage scenarios.

Classification	Description	Scenarios
General purpose	General-purpose virtual machines are designed to have a balanced CPU-to-memory ratio.	- Testing and development
- Small to medium databases
- Low to medium traffic web servers
Compute optimized	Compute optimized virtual machines are designed to have a high CPU-to-memory ratio.	- Medium traffic web servers
- Network appliances
- Batch processes
- Application servers
Memory optimized	Memory optimized virtual machines are designed to have a high memory-to-CPU ratio.	- Relational database servers
- Medium to large caches
- In-memory analytics
Storage optimized	Storage optimized virtual machines are designed to have high disk throughput and I/O.	- Big Data
- SQL and NoSQL databases
- Data warehousing
- Large transactional databases
GPU	GPU virtual machines are specialized virtual machines targeted for heavy graphics rendering and video editing. Available with single or multiple GPUs.	- Model training
- Inferencing with deep learning
High performance computes	High performance compute offers the fastest and most powerful CPU virtual machines with optional high-throughput network interfaces (RDMA).	- Workloads that require fast performance
- High traffic networks
Resizing virtual machines
Azure allows you to change the virtual machine size when the existing size no longer meets your needs. You can resize a virtual machine if your current hardware configuration is allowed in the new size. This option provides a fully agile and elastic approach to virtual machine management.

When you stop and deallocate the virtual machine, you can select any size available in your region.

 Important

Be cautious when resizing production virtual machines. Resizing a machine might require a restart that can cause a temporary outage or change configuration settings such as the IP address.

Next unit: Determine virtual machine storage


5- Determine virtual machine storage

Just like any other computer, virtual machines in Azure use disks as a place to store the operating system, applications, and data.

Things to know about virtual machine storage and disks
All Azure virtual machines have at least two disks: an operating system disk and a temporary disk. Virtual machines can also have one or more data disks. All disks are stored as virtual hard disks (VHDs). A VHD is like a physical disk in an on-premises server but, virtualized.

Diagram that shows disks used by an Azure virtual machine, including disks for the OS, data, and temporary storage.

Operating system disk
Every virtual machine has one attached operating system disk. The OS disk has a pre-installed operating system, which is selected when the virtual machine is created. The OS disk is registered as a SATA drive (Serial Advanced Technology Attachment) and labeled as the C: drive by default.

Temporary disk
Data on a temporary disk might be lost during a maintenance event or when you redeploy a virtual machine. During a standard reboot of the virtual machine, the data on the temporary drive should persist. However, there are cases where the data might not persist, such as moving to a new host. Therefore, any data on the temporary drive shouldn't be data that's critical to the system.

On Windows virtual machines, the temporary disk is labeled as the D: drive by default. This drive is used for storing the pagefile.sys file.
On Linux virtual machines, the temporary disk is typically /dev/sdb. This disk is formatted and mounted to /mnt by the Azure Linux Agent.
 Important

Don't store data on the temporary disk. This disk provides temporary storage for applications and processes and is intended to only store data like page or swap files.

Data disks
A data disk is a managed disk that's attached to a virtual machine to store application data, or other data you need to keep. Data disks are registered as SCSI drives and are labeled with a letter you choose. The size of a virtual machine determines how many data disks you can attach and the type of storage you can use to host the data disks.

Things to consider when choosing storage for your virtual machines
Review the following considerations about using Azure Storage and Azure Managed Disks with your virtual machines.

Consider Azure Premium Storage. You can choose Premium Storage to gain high-performance, low-latency disk support for your virtual machines with input/output (I/O)-intensive workloads. Virtual machine disks that use Premium Storage store data on solid-state drives (SSDs). To take advantage of the speed and performance of premium storage disks, you can migrate existing virtual machine disks to Premium Storage.

Consider multiple Storage disks. In Azure, you can attach several Premium Storage disks to a virtual machine. Using multiple disks gives your applications up to 256 TB of storage per virtual machine. With Premium Storage, your applications can achieve 80,000 I/O operations per second (IOPS) per virtual machine, and a disk throughput of up to 2,000 megabytes per second (MB/s) per virtual machine. Read operations completed with Premium Storage yield low latencies.

Consider managed disks. An Azure-managed disk is a VHD. Azure-managed disks are stored as page blobs, which are a random IO storage object in Azure. The disk is described as managed because it's an abstraction over page blobs, blob containers, and Azure storage accounts. With managed disks, you provision the disk, and Azure takes care of the rest. When you choose to use Azure-managed disks with your workloads, Azure creates and manages the disk for you. The available types of disks are Ultra Solid State Drives (SSD), Premium SSD, Standard SSD, and Standard Hard Disk Drives (HDD).

 Note

Managed disks are required for the single instance virtual machine SLA.

Consider migrating to Premium Storage. For the best performance for your application, we recommend that you migrate any virtual machine disk that requires high IOPS to Premium Storage. If your disk doesn't require high IOPS, you can help limit costs by keeping it in standard Azure Storage.

Next unit: Create virtual machines in the Azure portal


6- Create virtual machines in the Azure portal

When you create virtual machines in the Azure portal, one of your first decisions is to specify which image to use. Azure supports Windows and Linux operating systems, and there are server and client platforms to choose from. You can also search Azure Marketplace for other supported images:

Screenshot that shows disk images for virtual machines in Azure Marketplace.

Configure virtual machine image
The Azure portal guides you through the configuration process to create your virtual machine image. The process includes configuring basic and advanced options, and specifying details about the disks, virtual networks, and machine management.

Screenshot that shows the UI for creating a virtual machine in the Azure portal.

The Basics tab contains the project details, administrator account, and inbound port rules.

On the Disks tab, you select the OS disk type and specify your data disks.

The Networking tab provides settings to create virtual networks and load balancing.

On the Management tab, you can enable auto-shutdown and specify backup details.

On the Advanced tab, you can configure agents, scripts, or virtual machine extensions.

Other settings are available on the Monitoring and Tags tabs.

Learn how to reduce costs when creating your virtual machine

Next unit: Connect to virtual machines


7- Connect to virtual machines

There are several ways to access your Azure virtual machines. The Azure portal supports options for connecting your Windows and Linux machines, and making connections by using Azure Bastion. The following diagram shows how you can connect Azure virtual machines with the SSH and RDP protocols, Cloud Shell, and Azure Bastion.

Diagram that shows virtual machine access with the SSH and RDP protocols, Cloud Shell, and Azure Bastion.

Things to know about connecting with Azure Bastion
The Azure Bastion service is a fully platform-managed PaaS service. Azure Bastion provides secure and seamless RDP/SSH connectivity to your virtual machines directly over SSL. When you connect via Azure Bastion, your virtual machines don't need a public IP address. The following example shows a virtual machine connection with Azure Bastion in the Azure portal.


Azure Bastion provides secure RDP and SSH connectivity to all virtual machines in the virtual network. Azure Bastion protects your virtual machines from exposing RDP/SSH ports to the outside world while still providing secure access with RDP/SSH. Azure Bastion lets you connect to the virtual machine directly from the Azure portal. You aren't a client, agent, or another piece of software.

Things to know about connecting Windows-based virtual machines
To connect to a Windows-based virtual machine hosted on Azure, use the Microsoft Remote Desktop application with the remote desktop protocol (RDP). The Remote Desktop app provides a graphical user interface (GUI) session to an Azure virtual machine that runs any supported version of Windows. The following image shows how to use the RDP protocol to connect to a Windows-based virtual machine in the Azure portal.

Screenshot that shows how to use the RDP protocol to connect to a Windows-based virtual machine in the Azure portal.

To create an RDP connection, you specify the IP address for the virtual machine. As an option, you can select the port to use for the connection. The system provides you with a downloadable RDP file to use for the connection.

Things to know about connecting Linux-based virtual machines
To connect to a Linux-based virtual machine, you can use a secure shell protocol (SSH) client. SSH is an encrypted connection protocol that allows secure sign-ins over unsecured connections. Depending on your organization's security policies, you can reuse a single public-private key pair to access multiple Azure virtual machines and services. You don't need a separate pair of keys for each virtual machine or service you wish to access. The following image shows how to use the SSH protocol to connect to a Linux-based virtual machine in the Azure portal.

Screenshot that shows how to use the SSH protocol to connect to a Linux-based virtual machine in the Azure portal.

The public key is placed on your Linux virtual machine, or any other service that you wish to use with public-key cryptography.
The private key remains on your local system.
 Important

Protect your private key. Don't share your private key. Your public key can be shared with anyone, but only you (or your local security infrastructure) should possess your private key.

Next unit: Interactive lab simulation


8- Interactive lab simulation

Lab scenario
Your organization is planning on using virtual machines in Azure. As the Azure Administrator you need to:

Be able to use virtual machine Quickstart templates.
Use templates to create and configure virtual machines.
Be able to monitor virtual machine activity.
Objectives
Task 1: Use the Azure Quickstart Template gallery to deploy a virtual machine.
Browse to the Azure Quickstart Template gallery.
Search for a template that deploys a simple Windows Server virtual machine.
Edit the template and customize the parameters and variables.
Deploy the template to create the virtual machine.
Task 2: Verify and monitor your virtual machine.
In the portal, locate your new virtual machine.
View monitoring data for CPU, network, and data usage.
View activity log information.
 Note

Click on the thumbnail image to start the lab simulation. When you're done, be sure to return to this page so you can continue learning.

Screenshot of the simulation page.

Next unit: Knowledge check



Your organization has diverse requirements for the configuration of their virtual machines. You're responsible for designing a plan to fulfill the various requests.

The plan must support virtual machines that run various network appliances.

You have an established security policy for specific data that prohibits exposing SSH ports to external connections.

The admin team needs to be able to modify network security settings for inbound and outbound traffic on Windows and Linux virtual machines.

Answer the following questions
Choose the best response for each of the questions below. Then select Check your answers.


1. Which virtual machine is best for running a network appliance? 

Memory-optimized virtual machine

Compute-optimized virtual machine

Storage-optimized virtual machine

2. For the security requirements, how can you connect to Azure Linux virtual machines and install software? 

Configure a guest configuration on the virtual machine.

Create a custom script extension.

Configure Azure Bastion.

3. What effect do the default network security settings have on a new virtual machine? 

Outbound requests are allowed. Inbound traffic is allowed only from within the virtual network.

No outbound and inbound requests are allowed.

There are no restrictions. All outbound and inbound requests are allowed.




Summary and resources

Azure Administrators understand how to select and configure virtual machines. Azure Virtual Machines is one of several types of on-demand, scalable computing resources that Azure offers. Virtual machines are typically used when you need greater control over your computing environment.

In this module, you learned how to configure virtual machine names and locations. You examined virtual machine pricing models and discovered how to determine the correct virtual machine size. You discovered how to configure virtual machine storage, and how to create a virtual machine in the Azure portal. You reviewed how to select a secure virtual machine connection method, and how to configure Windows and Linux virtual machine connections.

The main takeaways from this module are:

Azure Virtual Machines is a type of on-demand, scalable computing resource offered by Azure that provides greater control over your computing environment.
When configuring virtual machines, it is important to consider factors such as network configuration, virtual machine name, location, size, storage, pricing options, and operating system.
There are multiple ways to connect to your Azure virtual machines. Remote Desktop Protocol (RDP) connects Windows-based virtual machines. Secure Shell (SSH) protocol connects to Linux-based virtual machines. Azure Bastion is a fully managed PaaS service that provides secure RDP/SSH connectivity to virtual machines directly over SSL.
Learn more with documentation
Azure Virtual Machines documentation. This article is your starting point for all things Azure virtual machines.

Virtual machine selector. This tool helps you find the virtual machines for your needs and budget.

Learn more with self-paced training
Introduction to Azure Virtual Machines (sandbox). Learn about the decisions you make before creating a virtual machine.

Plan and deploy Windows Server IaaS Virtual Machines (demonstration videos). Understand Azure compute and storage in relation to Azure VMs. Deploy Azure VMs by using the Azure portal, Azure CLI, or templates.

Connect to virtual machines through the Azure portal by using Azure Bastion. Learn how to deploy Azure Bastion to securely connect to Azure virtual machines.

Create a Windows virtual machines in Azure (sandbox).

Provision a Linux virtual machine in Microsoft Azure. Learn how to deploy a Linux virtual machine with Bicep, the Azure portal, and the Azure CLI.

Choose the right disk storage for your virtual machine workload. Learn about the variety of disk storage options for virtual machine workloads.




Point 2: Configure virtual machine availability

Learn how to configure virtual machine availability including vertical and horizontal scaling.

Learning objectives
In this module, you learn how to:

Implement availability sets and availability zones.
Implement update and fault domains.
Implement Azure Virtual Machine Scale Sets.
Autoscale virtual machines.



1- Introduction

Managing virtual machines at scale can be challenging, especially when usage patterns vary and demands on applications fluctuate. Azure Administrators need to be able to adjust their virtual machine resources to match changing demands. At the same time, they need to keep their virtual machine configuration consistent to ensure application stability. Achieving these goals means maintaining throughput and responsiveness while minimizing the costs of continually running a large collection of virtual machines.

Your company website uses virtual machines and manages large workloads. The IT department wants to ensure the virtual machines can dynamically adjust to increases and decreases in workloads. They also want to ensure there's a business continuity plan to provide for highly available machines. You're responsible for deploying highly available virtual machines. You decide to use Azure Virtual Machine Scale Sets and the autoscale feature.

In this module, you learn about scaling virtual machines. You learn about availability zones, availability sets, update domains, and fault domains. You also learn about scale sets and autoscale.

The goal of this module is to learn how to successfully respond to changing virtual machine workloads.

Learning objectives
In this module, you learn how to:

Implement availability sets and availability zones.
Implement update and fault domains.
Implement Azure Virtual Machine Scale Sets.
Autoscale virtual machines.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Familiarity with creating and managing Azure virtual machines.
General knowledge of scaling infrastructure resources in fluctuating workloads.



Next unit: Plan for maintenance and downtime

2- Plan for maintenance and downtime

Azure Administrators must be prepared for planned and unplanned failures. Let's explore three scenarios that can lead to your Azure virtual machine being impacted.

Things to know about maintenance planning
An availability plan for Azure virtual machines needs to include strategies for unplanned hardware maintenance, unexpected downtime, and planned maintenance. As you review the following scenarios, think about how these scenarios can affect the example company website.

An unplanned hardware maintenance event occurs when the Azure platform predicts that the hardware or any platform component associated to a physical machine is about to fail. When the platform predicts a failure, it issues an unplanned hardware maintenance event. Azure uses Live Migration technology to migrate your virtual machines from the failing hardware to a healthy physical machine. Live Migration is a virtual machine preserving operation that only pauses the virtual machine for a short time, but performance might be reduced before or after the event.

Unexpected downtime occurs when the hardware or the physical infrastructure for your virtual machine fails unexpectedly. Unexpected downtime can include local network failures, local disk failures, or other rack level failures. When detected, the Azure platform automatically migrates (heals) your virtual machine to a healthy physical machine in the same datacenter. During the healing procedure, virtual machines experience downtime (reboot) and in some cases loss of the temporary drive.

Planned maintenance events are periodic updates made by Microsoft to the underlying Azure platform to improve overall reliability, performance, and security of the platform infrastructure that your virtual machines run on. Most of these updates are performed without any impact to your virtual machines or Cloud Services.

 Note

Microsoft doesn't automatically update your virtual machine operating system or other software. You have complete control and responsibility for those updates. However, the underlying software host and hardware are periodically patched to ensure reliability and high performance.

Next unit: Create availability sets


3- Create availability sets

An availability set is a logical feature you can use to ensure a group of related virtual machines are deployed together. The grouping helps to prevent a single point of failure from affecting all of your machines. The grouping ensures that not all of the machines are upgraded at the same time during a host operating system upgrade in the datacenter.

Things to know about availability sets
Let's review some characteristics of availability sets.

All virtual machines in an availability set should perform the identical set of functionalities.

All virtual machines in an availability set should have the same software installed.

Azure ensures that virtual machines in an availability set run across multiple physical servers, compute racks, storage units, and network switches.

If a hardware or Azure software failure occurs, only a subset of the virtual machines in the availability set are affected. Your application stays up and continues to be available to your customers.

You can create a virtual machine and an availability set at the same time.

A virtual machine can only be added to an availability set when the virtual machine is created. To change the availability set for a virtual machine, you need to delete and then recreate the virtual machine.

You can build availability sets by using the Azure portal, Azure Resource Manager (ARM) templates, scripting, or API tools.

Microsoft provides robust Service Level Agreements (SLAs) for Azure virtual machines and availability sets. For details, see SLA for Azure Virtual Machines.

 Note

Adding your virtual machines to an availability set won't protect your applications from operating system or application-specific failures. You'll need to explore other disaster recovery and backup techniques to provide application-level protection.

Things to consider when using availability sets
Availability sets are an essential capability when you want to build reliable cloud solutions. In your planning for availability sets, keep the following general principles in mind:

Consider redundancy. To achieve redundancy in your configuration, place multiple virtual machines in an availability set.

Consider separation of application tiers. Each application tier exercised in your configuration should be located in a separate availability set. The separation helps to mitigate single point of failure on all machines.

Consider load balancing. For high availability and network performance, create a load-balanced availability set by using Azure Load Balancer. Load Balancer distributes incoming traffic across working instances of services that are defined in your load-balanced availability set.

Consider managed disks. You can use Azure managed disks with your Azure virtual machines in availability sets for block-level storage.

Next unit: Review update domains and fault domains


4- Review update domains and fault domains

Azure Virtual Machine Availability Sets implements two node concepts to help Azure maintain high availability and fault tolerance when deploying and upgrading applications: update domains and fault domains. Each virtual machine in an availability set is placed in one update domain and one fault domain.

Things to know about update domains
An update domain is a group of nodes that are upgraded together during the process of a service upgrade (or rollout). An update domain allows Azure to perform incremental or rolling upgrades across a deployment. Here are some other characteristics of update domains.

Each update domain contains a set of virtual machines and associated physical hardware that can be updated and rebooted at the same time.

During planned maintenance, only one update domain is rebooted at a time.

By default, there are five (non-user-configurable) update domains.

You can configure up to 20 update domains.

Things to know about fault domains
A fault domain is a group of nodes that represent a physical unit of failure. Think of a fault domain as nodes that belong to the same physical rack.

A fault domain defines a group of virtual machines that share a common set of hardware (or switches) that share a single point of failure. An example is a server rack serviced by a set of power or networking switches.

Two fault domains work together to mitigate against hardware failures, network outages, power interruptions, or software updates.

Let's look at a scenario with two fault domains that have two virtual machines each. The virtual machines in each fault domain are contained in different availability sets. The web availability set contains two virtual machines with one machine from each fault domain. The SQL availability set contains two different virtual machines with one from each fault domain.

Illustration that shows two fault domains with two virtual machines each. The virtual machines in each fault domain are contained in different availability sets.

Next unit: Review availability zones


5- Review availability zones

Availability zones are a high-availability offering that protects your applications and data from datacenter failures. An availability zone in an Azure region is a combination of a fault domain and an update domain.

Consider a scenario where you create three or more virtual machines across three zones in an Azure region. Your virtual machines are effectively distributed across three fault domains and three update domains. The Azure platform recognizes this distribution across update domains to make sure that virtual machines in different zones aren't updated at the same time.

You can use availability zones to build high-availability into your application architecture by colocating your compute, storage, networking, and data resources within a zone and replicating in other zones.

Things to know about availability zones
Review the following characteristics of availability zones.

Availability zones are unique physical locations within an Azure region.

Each zone is made up of one or more datacenters that are equipped with independent power, cooling, and networking.

To ensure resiliency, there's a minimum of three separate zones in all enabled regions.

The physical separation of availability zones within a region protects applications and data from datacenter failures.

Zone-redundant services replicate your applications and data across availability zones to protect against single-points-of-failure.

Things to consider when using availability zones
Azure services that support availability zones are divided into two categories.

Category	Description	Examples
Zonal services	Azure zonal services pin each resource to a specific zone.	- Azure Virtual Machines
- Azure managed disks
- Standard IP addresses
Zone-redundant services	For Azure services that are zone-redundant, the platform replicates automatically across all zones.	- Azure Storage that's zone-redundant
- Azure SQL Database
 Tip

To achieve comprehensive business continuity on Azure, build your application architecture by using a combination of availability zones with Azure region pairs.

Next unit: Compare vertical and horizontal scaling


6- Compare vertical and horizontal scaling

A robust virtual machine configuration includes support for scalability. Scalability allows throughput for a virtual machine in proportion to the availability of the associated hardware resources. A scalable virtual machine can handle increases in requests without adversely affecting response time and throughput. For most scaling operations, there are two implementation options: vertical and horizontal.

Things to know about vertical scaling
Vertical scaling, also known as scale up and scale down, involves increasing or decreasing the virtual machine size in response to a workload. Vertical scaling makes a virtual machine more (scale up) or less (scale down) powerful.

Illustration that shows vertical scaling where a single virtual machine increases or decreases in size by scaling up or scaling down.

Here are some scenarios where using vertical scaling can be advantageous:

If you have a service built on a virtual machine that's under-utilized such as on the weekend, you can use vertical scaling to decrease the virtual machine size and reduce your monthly costs.

You can implement vertical scaling to increase your virtual machine size to support larger demand without having to create extra virtual machines.

Things to know about horizontal scaling
Horizontal scaling is used to adjust the number of virtual machines in your configuration to support the changing workload. When you implement horizontal scaling, there's an increase (scale out) or decrease (scale in) in the number of virtual machine instances.

Illustration that shows horizontal scaling where virtual machines are added to scale out the system to support the workload.

Things to consider when using vertical and horizontal scaling
Review the following considerations regarding vertical and horizontal scaling. Think about which implementation might be required to support your company website.

Consider limitations. Generally speaking, horizontal scaling has fewer limitations than vertical scaling. A vertical scaling implementation depends on the availability of larger hardware, which quickly hits an upper limit and can vary by region. Vertical scaling also usually requires a virtual machine to stop and restart, which can temporarily limit access to applications or data.

Consider flexibility. When operating in the cloud, horizontal scaling is more flexible. A horizontal scaling implementation allows you to run potentially thousands of virtual machines to manage changes in workload and throughput.

Consider reprovisioning. Reprovisioning is the process of removing an existing virtual machine and replacing it with a new machine. A robust availability plan considers where reprovisioning might be required and plans for interruptions to service. If reprovisioning might be required, determine if any data needs to be maintained and migrated to the new machine.


7- Implement Azure Virtual Machine Scale Sets

Azure Virtual Machine Scale Sets are an Azure Compute resource that you can use to deploy and manage a set of identical virtual machines. When you implement Virtual Machine Scale Sets and configure all your virtual machines in the same way, you gain true autoscaling. Virtual Machine Scale Sets automatically increases the number of your virtual machine instances as application demand increases, and reduces the number of machine instances as demand decreases.

With Virtual Machine Scale Sets, you don't need to pre-provision your virtual machines. It's easier to build large-scale services that target large compute, big data, and containerized workloads. As workloads increase, more virtual machine instances can be added. As workloads decrease, virtual machines instances can be removed. The process of adding and removing machines can be manual or automated, or a combination of both.

Things to know about Azure Virtual Machine Scale Sets
Review the following characteristics of Azure Virtual Machine Scale Sets.

All virtual machine instances are created from the same base operating system image and configuration. This approach lets you easily manage hundreds of virtual machines without extra configuration tasks or network management.

Virtual Machine Scale Sets support the use of Azure Load Balancer for basic layer-4 traffic distribution, and Azure Application Gateway for more advanced layer-7 traffic distribution and SSL termination.

You can use Virtual Machine Scale Sets to run multiple instances of your application. If one of the virtual machine instances has a problem, customers continue to access your application through another virtual machine instance with minimal interruption.

Customer demand for your application might change throughout the day or week. To meet customer demand, Virtual Machine Scale Sets implements autoscaling to automatically increase and decrease the number of virtual machines.

Virtual Machine Scale Sets support up to 1,000 virtual machine instances. If you create and upload your own custom virtual machine images, the limit is 600 virtual machine instances.

Next unit: Create Virtual Machine Scale Sets


8- Create Virtual Machine Scale Sets

You can implement Azure Virtual Machine Scale Sets in the Azure portal. You specify the number of virtual machines and their sizes, and indicate preferences for using Azure Spot instances, Azure managed disks, and allocation policies.

In the Azure portal, there are several settings to configure to create an Azure Virtual Machine Scale Sets implementation.

Screenshot that shows how to create Virtual Machine Scale Sets in the Azure portal.

Orchestration mode: Choose how the scale set manages virtual machines. In flexible orchestration mode, you manually create and add a virtual machine of any configuration to the scale set. In uniform orchestration mode, you define a virtual machine model and Azure generates identical instances based on that model.

Image: Choose the base operating system or application for the virtual machine (VM).

VM Architecture: Azure provides a choice of x64 or Arm64-based virtual machines to run your applications.

Run with Azure Spot discount: Azure Spot offers unused Azure capacity at a discounted rate versus pay as you go prices. Workloads should be tolerant to infrastructure loss as Azure may recall capacity.

Size: Select a VM size to support the workload that you want to run. The size that you choose then determines factors such as processing power, memory, and storage capacity. Azure offers a wide variety of sizes to support many types of uses. Azure charges an hourly price based on the VM's size and operating system.

Under the Advanced tab, you can also select the following:

Enable scaling beyond 100 instances: Identify your scaling allocation preference. If you select No, your Virtual Machine Scale Sets implementation is limited to one placement group with a maximum capacity of 100. If you select Yes, your implementation can span multiple placement groups with capacity up to 1,000. Selecting Yes also changes the availability characteristics of your implementation.

Spreading algorithm: Microsoft recommends allocating Max spreading for your implementation. This approach provides the optimal spreading.

Next unit: Implement autoscale

9- Implement autoscale

An Azure Virtual Machine Scale Sets implementation can automatically increase or decrease the number of virtual machine instances that run your application. This process is known as autoscaling. Autoscaling allows you to dynamically scale your configuration to meet changing workload demands.

Illustration of a Virtual Machine Scale Sets implementation with a minimum of two virtual machines and a maximum of five machines that autoscale depending on workload demands.

Autoscaling minimizes the number of unnecessary virtual machine instances that run your application when demand is low. Your customers continue to receive an acceptable level of performance as demand grows and more virtual machine instances are automatically added.

Things to consider when using autoscaling
Review the following considerations about autoscaling. Think about how this process can benefit your company website implementation.

Consider automatic adjusted capacity. You can create autoscaling rules to define the acceptable performance for a positive customer experience. When the defined thresholds are met, the autoscale rules act to adjust the capacity of your Virtual Machine Scale Sets implementation.

Consider scale out. If your application demand increases, the load on the virtual machine instances in your implementation increases. If the increased load is consistent, rather than a brief demand, you can configure autoscale rules to increase the number of virtual machine instances in your implementation.

Consider scale in. On an evening or weekend, your application demand might decrease. If the decreased load is consistent over a period of time, you can configure autoscale rules to decrease the number of virtual machine instances in your implementation. The scale-in action reduces the cost to run your Virtual Machine Scale Sets implementation as you only run the number of instances required to meet the current demand.

Consider scheduled events. You can implement autoscaling and schedule events to automatically increase or decrease the capacity of your implementation at fixed times.

Consider overhead. Using Azure Virtual Machine Scale Sets with autoscaling reduces your management overhead to monitor and optimize the performance of your application.

Next unit: Configure autoscale


10- Configure autoscale

When you create an Azure Virtual Machine Scale Sets implementation in the Azure portal, you can enable autoscaling. For optimal performance, you should define a minimum, maximum, and default number of virtual machine instances to use during the autoscale process.

In the Azure portal, there are several settings to configure to enable autoscaling with Azure Virtual Machine Scale Sets.

Screenshot of the settings for configuring virtual machine instances and autoscale in the Azure portal.

Scaling policy: Manual scale maintains a fixed instance count. Custom autoscale scales the capacity on any schedule, based on any metrics.

Minimum number of VMs: Specify the minimum number of virtual machines that should be available when autoscaling is applied on your Virtual Machine Scale Sets implementation.

Maximum number of VMs: Specify the maximum number of virtual machines that can be available when autoscaling is applied on your implementation.

Scale out

CPU threshold: Specify the CPU usage percentage threshold to trigger the scale-out autoscale rule.

Duration in minutes: Duration in minutes is the amount of time that Autoscale engine looks back for metrics. For example, 10 minutes means that every time autoscale runs, it queries metrics for the past 10 minutes. This delay allows your metrics to stabilize and avoids reacting to transient spikes.

Number of VMs to increase by: Specify the number of virtual machines to add to your Virtual Machine Scale Sets implementation when the scale-out autoscale rule is triggered.

Scale in

Scale in CPU threshold: Specify the CPU usage percentage threshold to trigger the scale-in autoscale rule.

Number of VMs to decrease by: Specify the number of virtual machines to remove from your implementation when the scale-in autoscale rule is triggered.

Scale in policy: The scale-in policy feature provides users a way to configure the order in which virtual machines are scaled-in.

Next unit: Interactive lab simulation


11- Interactive lab simulation

Lab scenario
Your organization is deploying virtual machines in Azure. As the Azure Administrator you need to:

Determine different virtual machine compute and storage options.
Implement Virtual Machine Scale Sets, including storage resiliency and scalability options.
Explore using Azure Virtual Machine Custom Script extensions to automatically configure virtual machines.
Architecture diagram
Architecture diagram as explained in the text.

Objectives
 Note

The lab files are available in the GitHub.

Task 1: Deploy zone-resilient Azure virtual machines by using the Azure portal and an Azure Resource Manager template.
Create a virtual machine in the Azure portal.
Review the template and deploy a second virtual machine.

Task 2: Configure Azure virtual machines by using virtual machine extensions.
Create a blob storage container.
Upload an Azure PowerShell script. This script installs the Windows Server Web Server role on a virtual machine.
Use the custom script extension feature to run the script on a virtual machine. Export the template.
Configure the exported template to install the role on a different virtual machine.

Task 3: Scale compute and storage for Azure virtual machines. In this task, you scale compute for Azure virtual machines by changing their size and scale their storage by attaching and configuring their data disks.
Resize the virtual machine.
Create and attach a new disk to the virtual machine.
Use Azure PowerShell to initialize and partition the new disk.
Customize the template to resize the virtual machine and change the disk configuration.

Task 4: Register the Microsoft Insights and Microsoft Alerts Management resource providers.

Task 5: Deploy zone-resilient Azure Virtual Machine Scale Sets by using the Azure portal.
Use the Azure portal to create a Virtual Machine Scale Set.
Configure the virtual network to include an inbound rule to allow HTTP.
Configure load balancing and manual scaling.
Deploy the virtual scale set.

Task 6: Configure Azure Virtual Machine Scale Sets by using virtual machine extensions.
Upload an Azure PowerShell script to install the install Windows Server Web Server role.
Run the script on the virtual machines using the custom script extension feature.
Confirm the Internet Information Service (IIS) is now available on the virtual machines.

Task 7: Scale compute and storage for Azure Virtual Machine Scale Sets.
Confirm the virtual machines in the scale set are in different regions.
Configure autoscale based on a metric.
Use Azure PowerShell to start an infinite loop that sends the HTTP requests to the web sites hosted on the instances of Azure Virtual Machine Scale Sets
Verify a new resource is provisioned.
 Note

Click on the thumbnail image to start the lab simulation. When you're done, be sure to return to this page so you can continue learning.

Screenshot of the simulation page.

Next unit: Knowledge check

Your organization has diverse requirements for the configuration and availability of their virtual machines. You're responsible for helping with the configuration to fulfill requests and resolve issues.

The Admin team is testing an implementation of Azure Virtual Machine Scale Sets with five virtual machines. During testing, monitoring alerts show all virtual machines running at maximum capacity. However, you discover that when the CPU is fully consumed more virtual machines aren't deploying in the scale set.

The DevOps team wants to configure Azure Virtual Machine Scale Sets for their production servers. Thursday evening is typically the busiest time in preparation for delivery to customers by COB on Friday. Conversely, early Monday is generally the quietest time. You need a plan to add more machines when the workload is high.

As load increases on applications hosted in Azure Virtual Machine Scale Sets, you want to increase the CPU capacity of the existing instances rather than deploy more instances.

Answer the following questions
Choose the best response for each of the following questions. Then select Check your answers.


1. How can you ensure more virtual machines are deployed for the Admin team when the CPU is 75% consumed? 

Manually increase the instance count.

Change the CPU percentage to 50%.

Enable the autoscale option.

2. Which Virtual Machine Scale Sets feature can be configured to add more DevOps machines during peak production? 

Schedule-based rules

Autoscale

Metric-based rules

3. What types of scaling can you use to increase the CPU capacity for your existing Virtual Machine Scale Sets instances? 

Horizontal scaling

Vertical scaling

Load balancing


Summary and resources

Azure provides several high availability options for virtual machines. You can achieve high availability by using availability sets, availability zones, and Azure Virtual Machine Scale Sets.

In this module, you learned how to configure virtual machine availability by using availability sets and availability zones with update and fault domains. You discovered how to autoscale virtual machines and configure vertical and horizontal scaling. You reviewed how to implement Virtual Machine Scale Sets, including storage resiliency and scalability options. You explored how to use Azure Virtual Machine Custom Script extensions to automatically configure virtual machines.

The main takeaways from this module are:

Azure Virtual Machine Scale Sets allow for the deployment and management of a group of identical virtual machines, making it easier to build large-scale services.

Autoscaling with Virtual Machine Scale Sets helps optimize performance by automatically adjusting the number of instances based on workload demands.

Availability sets and availability zones are important features in Azure for achieving high availability and fault tolerance for virtual machines.

Learn more with documentation
Availability options for Azure Virtual Machines. This article provides an overview of the availability options for Azure virtual machines (VMs).

Autoscale with Azure Virtual Machine Scale Sets. This article reviews when use Virtual Machine Scale Sets.

Create virtual machines in a scale set using Azure portal. This article steps through using Azure portal to create a Virtual Machine Scale Set.

Learn more with self-paced training
Introduction to Azure Virtual Machines (sandbox). Learn about the decisions you make before creating a virtual machine, the options to create and manage the VM, and the extensions and services you use to manage your VM.

Implement scale and high availability with Windows Server VM. You learn how to implement scaling for virtual machine scale sets and load-balanced VMs. You also learn how to implement Azure Site Recovery.

Introduction to Azure Virtual Machine Scale Sets. Learn about what Azure Virtual Machine Scale Sets do, how they work, and when you should use Azure Virtual Machine Scale Sets as a solution for your organization.







Point 3: Configure Azure App Service plans

Learn how to configure an Azure App Service plan, including pricing and scaling.

Learning objectives
In this module, you learn how to:

Identify features and usage cases for Azure App Service.
Select an appropriate Azure App Service plan pricing tier.
Scale an Azure App Service plan.


1- Introduction

Azure Administrators need to be able to scale a web application. Scaling enables an application to remain responsive during periods of high demand. Scaling also helps to save money by reducing the resources required when demand drops.

Suppose you work for a large chain of hotels. You're responsible for maintaining the hotel website. Customers visit the website to make new reservations and view details for their current bookings. At certain times of the year, the volume of website traffic grows because customers are browsing hotels for vacations during national/regional holidays. At other times, traffic declines. These website usage patterns are predictable.

In this module, you learn to implementing Azure App Service plans. You learn how different App Service plans provide different pricing and scaling options. You learn how changing the plan affects performance.

The goal of this module is to ensure you can determine the best App Service plan for your application.

Learning objectives
In this module, you learn how to:

Identify features and usage cases for Azure App Service.
Select an appropriate Azure App Service plan pricing tier.
Scale an Azure App Service plan.
Scale out an Azure App Service plan.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Basic knowledge of scaling and performance concepts.
Familiarity with the Azure portal so you can configure the correct App Service plan.



Next unit: Implement Azure App Service plans

2- Implement Azure App Service plans

In Azure App Service, an application runs in an Azure App Service plan. An App Service plan defines a set of compute resources for a web application to run. The compute resources are analogous to a server farm in conventional web hosting. One or more applications can be configured to run on the same computing resources (or in the same App Service plan).

Things to know about App Service plans
Let's take a closer look at how to implement and use an App Service plan with your virtual machines.

When you create an App Service plan in a region, a set of compute resources is created for the plan in the specified region. Any applications that you place into the plan run on the compute resources defined by the plan.

Each App Service plan defines three settings:

Region: The region for the App Service plan, such as West US, Central India, North Europe, and so on.
Number of VM instances: The number of virtual machine instances to allocate for the plan.
Size of VM instances: The size of the virtual machine instances in the plan, including Small, Medium, or Large.
You can continue to add new applications to an existing plan as long as the plan has enough resources to handle the increasing load.

How applications run and scale in App Service plans
The Azure App Service plan is the scale unit of App Service applications. Depending on the pricing tier for your Azure App Service plan, your applications run and scale in a different manner. If your plan is configured to run five virtual machine instances, then all applications in the plan run on all five instances. If your plan is configured for autoscaling, then all applications in the plan are scaled out together based on the autoscale settings.

Here's a summary of how applications run and scale in Azure App Service plan pricing tiers:

Free or Shared tier:

Applications run by receiving CPU minutes on a shared virtual machine instance.
Applications can't scale out.
Basic, Standard, Premium, or Isolated tier:

Applications run on all virtual machine instances configured in the App Service plan.
Multiple applications in the same plan share the same virtual machine instances.
If you have multiple deployment slots for an application, all deployment slots run on the same virtual machine instances.
If you enable diagnostic logs, perform backups, or run WebJobs, these tasks use CPU cycles and memory on the same virtual machine instances.
Things to consider when using App Service plans
Review the following considerations about using Azure App Service plans to run and scale your applications. Think about what conditions might apply to running and scaling the hotel website.

Consider cost savings. Because you pay for the computing resources that your App Service plan allocates, you can potentially save money by placing multiple applications into the same App Service plan.

Consider multiple applications in one plan. Create a single plan to support multiple applications, to make it easier to configure and maintain shared virtual machine instances. Because the applications share the same virtual machine instances, you need to carefully manage your plan resources and capacity.

Consider plan capacity. Before you add a new application to an existing plan, determine the resource requirements for the new application and identify the remaining capacity of your plan.

 Important

Overloading an App Service plan can potentially cause downtime for new and existing applications.

Consider application isolation. Isolate your application into a new App Service plan when:

The application is resource-intensive.
You want to scale the application independently from the other applications in the existing plan.
The application needs resource in a different geographical region.


Next unit: Determine Azure App Service plan pricing

3- Determine Azure App Service plan pricing

The pricing tier of an Azure App Service plan determines what App Service features you get and how much you pay for the plan.

Things to know about App Service plan pricing tiers
There are six categories of pricing tiers for an Azure App Service plan. Examine the following plan details and think about which plans can support the hotel website requirements.

Feature	Free	Shared	Basic	Standard	Premium	Isolated
Usage	Development, Testing	Development, Testing	Dedicated development, Testing	Production workloads	Enhanced scale, performance	High performance, security, isolation
Web, mobile, or API applications	10	100	Unlimited	Unlimited	Unlimited	Unlimited
Disk space	1 GB	1 GB	10 GB	50 GB	250 GB	1 TB
Auto scale	n/a	n/a	n/a	Supported	Supported	Supported
Deployment slots	n/a	n/a	n/a	5	20	20
Max instances	n/a	n/a	Up to 3	Up to 10	Up to 30	Up to 100
Free and Shared
The Free and Shared service plans are base tiers that run on the same Azure virtual machines as other applications. Some applications might belong to other customers. These tiers are intended to be used for development and testing purposes only. No SLA is provided for the Free and Shared service plans. Free and Shared plans are metered on a per application basis.

Basic
The Basic service plan is designed for applications that have lower traffic requirements, and don't need advanced auto scale and traffic management features. Pricing is based on the size and number of instances you run. Built-in network load-balancing support automatically distributes traffic across instances. The Basic service plan with Linux runtime environments supports Web App for Containers.

Standard
The Standard service plan is designed for running production workloads. Pricing is based on the size and number of instances you run. Built-in network load-balancing support automatically distributes traffic across instances. The Standard plan includes auto scale that can automatically adjust the number of virtual machine instances running to match your traffic needs. The Standard service plan with Linux runtime environments supports Web App for Containers.

Premium
The Premium service plan is designed to provide enhanced performance for production applications. The upgraded Premium plan, Premium v2, offers Dv2-series virtual machines with faster processors, SSD storage, and double memory-to-core ratio compared to the Standard tier. The new Premium plan also supports higher scale via increased instance count while still providing all the advanced capabilities of the Standard tier. The first generation of Premium plan is still available to support existing customer scaling needs.

Isolated
The Isolated service plan is designed to run mission critical workloads that are required to run in a virtual network. The Isolated plan allows customers to run their applications in a private, dedicated environment in an Azure datacenter. The plan offers Dv2-series virtual machines with faster processors, SSD storage, and a double memory-to-core ratio compared to the Standard tier. The private environment used with an Isolated plan is called the App Service Environment. The plan can scale to 100 instances with more available upon request.


Next unit: Scale up and scale out Azure App Service

4- Scale up and scale out Azure App Service

There are two methods for scaling your Azure App Service plan and applications: scale up and scale out. You can scale your applications manually or automatically, which is referred to as autoscale.

Watch the following video about how to implement automatic scaling for your Azure App Service plan and applications.


Things to know about Azure App Service scaling
Let's examine the details of scaling for your Azure App Service plan and App Service applications.

The scale up method increases the amount of CPU, memory, and disk space. Scaling up gives you extra features like dedicated virtual machines, custom domains and certificates, staging slots, autoscaling, and more. You scale up by changing the pricing tier of the Azure App Service plan where your application is placed.

The scale-out method increases the number of virtual machine instances that run your application. You can scale out to as many as 30 instances, depending on your App Service plan pricing tier. Take advantage of App Service Environments in the Isolated tier to further increase your scale-out count to 100 instances. The scale instance count can be configured manually or automatically (autoscale).

With autoscale, you can automatically increase the scale instance count for the scale-out method. Autoscale is based on predefined rules and schedules.

Your App Service plan can be scaled up and down at any time by changing the pricing tier of the plan.

Things to consider when using Azure App Service scaling
Review the following benefits of implementing scaling for your App Service plan and applications. Think about the scaling advantages for your hotel website.

Consider manually adjusting plan tiers. Start your plan at a lower pricing tier and scale up as needed to acquire more App Service features. Scale down when features are no longer needed, and control your overall costs.

Consider a scenario where you start testing your web app by using the Azure App Service Free tier, where you pay nothing to use the service. After a while, you decide to add a custom DNS name to your web app, so you scale your plan up to the Shared tier. Next, you discover you need to create an SSL binding, so you scale your plan up to the Basic tier. Later, you determine a need for staging environments, so you scale up to the Standard tier. When you need more cores, memory, or storage, you can scale up to a bigger virtual machine size in the same tier.

The same scaling process works in reverse. If you decide you no longer need capabilities or features of a higher tier, scale your plan down to a lower tier and save money.

Consider autoscale to support users and reduce costs. Keep serving your users when your application is experiencing high throughput. Implement autoscale to control how many features and support are offered at a given time based on your preference settings and rule conditions. Autoscale helps you save money when the load on your application decreases by automatically reducing your subscribed features.

Consider no redeployment. When you change your scale settings, you don't need to change your code or redeploy your applications. Changing your plan scale settings takes only seconds to apply. Your changes affect all applications in your App Service plan.

Consider scaling for other Azure services. If your App Service application depends on other Azure services, such as Azure SQL Database or Azure Storage, you can scale these resources separately. These resources aren't managed by your App Service plan.

Next unit: Configure Azure App Service autoscale


5- Configure Azure App Service autoscale

The autoscale process allows you to have the right amount of resources running to handle the load on your application. You can add resources to support increases in load and save money by removing idle resources.

Things to know about autoscale
Let's take a closer look at how to use autoscale for your Azure App Service plan and applications.

To use autoscale, you specify the minimum, and maximum number of instances to run by using a set of rules and conditions.

When your application runs under autoscale conditions, the number of virtual machine instances are automatically adjusted based on your rules. When rule conditions are met, one or more autoscale actions are triggered.

An autoscale setting is read by the autoscale engine to determine whether to scale out or in. Autoscale settings are grouped into profiles.

Autoscale rules include a trigger and a scale action (in or out). The trigger can be metric-based or time-based.

Screenshot that shows how to create an autoscale condition in the Azure portal, including settings for the scale mode and instance count.

Metric-based rules measure application load and add or remove virtual machines based on the load, such as "do this action when CPU usage is above 50%." Example metrics include CPU time, Average response time, and Requests.

Time-based rules (or, schedule-based) allow you to scale when you see time patterns in your load and want to scale before a possible load increase or decrease occurs. An example is "trigger a webhook every 8:00 AM on Saturday in a given time zone."

The autoscale engine uses notification settings.

A notification setting defines what notifications should occur when an autoscale event occurs based on satisfying the criteria of an autoscale setting profile. Autoscale can notify one or more email addresses or make calls to one or more webhooks.

Things to consider when configuring autoscale
There are several considerations to keep in mind when you configure autoscale for your Azure App Service plan and applications.

Minimum instance count. Set a minimum instance count to make sure your application is always running even when there's no load.

Maximum instance count. Set a maximum instance count to limit your total possible hourly cost.

Adequate scale margin. Make sure your maximum and minimum instance count values are different, and set an adequate margin between the two values. You can automatically scale between the minimum and maximum by using rules you create.

Scale rule combinations. Always use a scale-out and scale-in rule combination that performs an increase and decrease. If you don't set a scale-out rule, your application might fail, or performance might degrade under increased loads. If you don't set a scale-in rule, you can experience unnecessary and extensive costs when the load decreases.

Metric statistics. Carefully choose the appropriate statistic for your diagnostic metrics, including Average, Minimum, Maximum, and Total.

Default instance count. Always select a safe default instance count. The default instance count is important because autoscale scales your service to the count you specify when metrics aren't available.

Notifications. Always configure autoscale notifications. It's important to maintain awareness of how your application is performing as the load changes.


6- Knowledge check

You're developing a strategy to implement Azure App Service plans to enable scaling requirements for the hotel website. Various teams in your organization have submitted requests and questions for your consideration.

The Admin team has requested information about scaling options for their virtual machines. They prefer an option that can increase CPU and disk space rather than having to add more virtual machines.

The Production team manages a web app that requires scaling to 5 instances and 100 GB of disk storage. They'd like a cost-efficient scaling solution.

In your website configuration, you need a rule to trigger a webhook at 8:00 AM on Saturdays.

Answer the following questions
Choose the best response for each of the questions below. Then select Check your answers.


1. Which App Service Plan can you implement to support the Production team's requirements? 

Basic

Standard

Premium

2. What scaling option provides more CPU, memory, or disk space without adding more virtual machines? 

Scale up

Scale out

Scale back

3. Triggering a webhook at 8:00 AM on Saturday is an example of what type of rule? 

A metric-based rule.

A time-based rule.

An app-insight rule.


Summary and resources

In this module, you learned about Azure App Service plans and how they're used to define the compute resources for running applications in Azure App Service. These plans can be configured with a specific region, number of virtual machine instances, and size of virtual machine instances. The pricing tier of the App Service plan determines the features and cost. Pricing tiers include Free and Shared plans for development and testing purposes. Pricing tiers also include Isolated plans for mission-critical workloads.

You learned about scaling in Azure App Service. Scale up involves increasing the CPU, memory, and disk space by changing the pricing tier. Scale out increases the number of virtual machine instances running the application. Autoscaling allows you to automatically adjust the number of resources based on the load on your application. Autoscale can be configured with metric-based or time-based rules.

The main takeaways from this module are:

Azure App Service plans are used to define the compute resources for running web applications in Azure App Service.
The pricing tier of the App Service plan determines the features and cost, with options ranging from Free and Shared plans to Isolated plans.
Scaling in Azure App Service can be done through scale up (changing the pricing tier) or scale out (increasing the number of virtual machine instances).
Autoscaling allows for automatic adjustment of resources based on application load, with metric-based and time-based rules.
Learn more with documentation
Azure App Service plans. This article provides an overview of App Service plans.

Manage an App Service plan in Azure. This guide shows how to create and manage an App Service plan.

Scale up an app in Azure App Service. This article shows you how to scale your app in Azure App Service.

Learn more with self-paced training
Scale apps in Azure App Service. Learn how autoscale operates in App Service. Learn to identify autoscale factors, enable autoscale, and create autoscale conditions.

Scale an App Service web app to efficiently meet demand with App Service scale up and scale out. Learn how to respond to changing demand by incrementally increasing the resources available.





Point 4: Configure Azure App Service

Learn how to configure and monitor Azure App Service instances, including deployment slots.


Learning objectives
In this module, you learn how to:

Identify features and usage cases for Azure App Service.
Create an app with Azure App Service.
Configure deployment settings, specifically deployment slots.
Secure your Azure App Service app.
Configure custom domain names.
Back up and restore your Azure App Service app.
Configure Azure Application Insights.


1- Introduction

Azure Administrators are interested in solutions that make it easier to deploy and manage their web, mobile, and API applications.

Your company provides consumer research, and your team manages the on-premises servers. The servers you administer run the entire company infrastructure from web servers to databases. The hardware is aging and starting to struggle to keep up with some of the new data analysis applications. Rather than upgrade the hardware, the company decided to deploy Azure App Service.

In this module, you learn how to configure and manage Azure App Service. You learn about configuration settings, deployment slots, and custom domain names. You learn about application backup, recovery, and monitoring.

The goal of this module is to provide you with the knowledge and skills to effectively use Azure App Services.

Learning objectives
In this module, you learn how to:

Identify features and usage cases for Azure App Service.
Create an app with App Service.
Configure deployment settings, specifically deployment slots.
Secure your App Service app.
Configure custom domain names.
Back up and restore your App Service app.
Configure Azure Application Insights.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Working knowledge of the Azure portal, so you can configure the service.

Familiarity with cloud-based services, specifically web hosting services.


Next unit: Implement Azure App Service

2- Implement Azure App Service

Azure App Service brings together everything you need to create websites, mobile backends, and web APIs for any platform or device. Applications run and scale with ease in both Windows and Linux-based environments.

App Service provides Quickstarts for several products to help you easily create and deploy your Windows and Linux apps:

Illustration that shows products for which you can use an App Service quickstart to develop and deploy your web, mobile, and API apps.

App Service benefits
There are many advantages to using App Service to develop and deploy your web, mobile, and API apps. Review the following table and think about what features can help you host your App Service instances.

Benefit	Description
Multiple languages and frameworks	App Service has first-class support for ASP.NET, Java, Ruby, Node.js, PHP, and Python. You can also run PowerShell and other scripts or executables as background services.
DevOps optimization	App Service supports continuous integration and deployment with Azure DevOps, GitHub, BitBucket, Docker Hub, and Azure Container Registry. You can promote updates through test and staging environments. Manage your apps in App Service by using Azure PowerShell or the cross-platform command-line interface (CLI).
Global scale with high availability	App Service helps you scale up or out manually or automatically. You can host your apps anywhere within the Microsoft global datacenter infrastructure, and the App Service SLA offers high availability.
Connections to SaaS platforms and on-premises data	App Service lets you choose from more than 50 connectors for enterprise systems (such as SAP), SaaS services (such as Salesforce), and internet services (such as Facebook). You can access on-premises data by using Hybrid Connections and Azure Virtual Networks.
Security and compliance	App Service is ISO, SOC, and PCI compliant. You can authenticate users with Microsoft Entra ID or with social logins via Google, Facebook, Twitter, or Microsoft. Create IP address restrictions and manage service identities.
Application templates	Choose from an extensive list of application templates in the Azure Marketplace, such as WordPress, Joomla, and Drupal.
Visual Studio integration	App Service offers dedicated tools in Visual Studio to help streamline the work of creating, deploying, and debugging.
API and mobile features	App Service provides turn-key CORS support for RESTful API scenarios. You can simplify your mobile app scenarios by enabling authentication, offline data sync, push notifications, and more.
Serverless code	App Service lets you run a code snippet or script on-demand without having to explicitly provision or manage infrastructure. You pay only for the compute time your code actually uses.


Next unit: Create an app with App Service

3- Create an app with App Service

You can use the Web Apps, Mobile Apps, or API Apps features of Azure App Service, and create your own apps in the Azure portal.

Watch the following video to learn how to create an app with Azure App Service.

How to create App Services in the Azure portal

Things to know about configuration settings
Let's examine some of the basic configuration settings you need to create an app with App Service.

Name: The name for your app must be unique. The name identifies and locates your app in Azure. An example name is webappces1.azurewebsites.net. You can map a custom domain name, if you prefer to use that option instead.

Publish: App Service hosts (publishes) your app as code or as a Docker Container.

Runtime stack: App Service uses a software stack to run your app, including the language and SDK versions. For Linux apps and custom container apps, you can set an optional start-up command or file. Your choices for the stack include .NET Core, .NET Framework, Node.js, PHP, Python, and Ruby. Various versions of each product are available for Linux and Windows.

Operating system: The operating system for your app runtime stack can be Linux or Windows.

Region: The region location that you choose for your app affects the App Service plans that are available.

App Service plan: Your app needs to be associated with an Azure App Service plan to establish available resources, features, and capacity. You can choose from pricing tiers that are available for the region location you selected.

Post-creation settings
After your app is created, other configuration settings become available in the Azure portal, including app deployment options and path mapping.

Screenshot that shows other configuration options for an app with the App Service in the Azure portal.

Some of the extra configuration settings can be included in the developer's code, while others can be configured in your app. Here are a few of the extra application settings.

Always On: You can keep your app loaded even when there's no traffic. This setting is required for continuous WebJobs or for WebJobs that are triggered by using a CRON expression.

ARR affinity: In a multi-instance deployment, you can ensure your app client is routed to the same instance for the life of the session.

Connection strings: Connection strings for your app are encrypted at rest and transmitted over an encrypted channel.

Next unit: Explore continuous integration and deployment


4- Explore continuous integration and deployment

The Azure portal provides out-of-the-box continuous integration and deployment with Azure DevOps, GitHub, Bitbucket, FTP, or a local Git repository on your development machine. You can connect your web app with any of the above sources and App Service handles the rest for you. App Service auto-synchronizes your code and any future changes to the code into your web app. With Azure DevOps, you can also define your own build and release process. Compile your source code, run tests, and build and deploy the release into your web app every time you commit the code. All of the operations happen implicitly without any need for human administration.

Illustration that shows two developers sharing a single GitHub source to produce a website built with Azure App Service.

Things to know about continuous deployment
When you create your web app with App Service, you can choose automated or manual deployment. As you review these options, consider which deployment method to implement for your App Service apps.

Automated deployment (continuous integration) is a process used to push out new features and bug fixes in a fast and repetitive pattern with minimal impact on end users. Azure supports automated deployment directly from several sources:

Azure DevOps: Push your code to Azure DevOps (previously known as Visual Studio Team Services), build your code in the cloud, run the tests, generate a release from the code, and finally, push your code to an Azure web app.

GitHub: Azure supports automated deployment directly from GitHub. When you connect your GitHub repository to Azure for automated deployment, any changes you push to your production branch on GitHub are automatically deployed for you.

Bitbucket: With its similarities to GitHub, you can configure an automated deployment with Bitbucket.

Manual deployment enables you to manually push your code to Azure. There are several options for manually pushing your code:

Git: The App Service Web Apps feature offers a Git URL that you can add as a remote repository. Pushing to the remote repository deploys your app.

CLI: The webapp up command is a feature of the command-line interface that packages your app and deploys it. Deployment can include creating a new App Service web app.

Visual Studio: Visual Studio features an App Service deployment wizard that can walk you through the deployment process.

FTP/S: FTP or FTPS is a traditional way of pushing your code to many hosting environments, including App Service.


Next unit: Create deployment slots


5- Create deployment slots

When you deploy your web app, web app on Linux, mobile backend, or API app to Azure App Service, you can use a separate deployment slot instead of the default production slot.

Things to know about deployment slots
Let's take a closer look at the characteristics of deployment slots.

Deployment slots are live apps that have their own hostnames.

Deployment slots are available in the Standard, Premium, and Isolated App Service pricing tiers. Your app needs to be running in one of these tiers to use deployment slots.

The Standard, Premium, and Isolated tiers offer different numbers of deployment slots.

App content and configuration elements can be swapped between two deployment slots, including the production slot.

Screenshot that shows how to work with deployment slots in the Azure portal.

Things to consider when using deployment slots
There are several advantages to using deployment slots with your App Service app. Review the following benefits and think about how they can support your App Service implementation.

Consider validation. You can validate changes to your app in a staging deployment slot before swapping the app changes with the content in the production slot.

Consider reductions in downtime. Deploying an app to a slot first and swapping it into production ensures that all instances of the slot are warmed up before being swapped into production. This option eliminates downtime when you deploy your app. The traffic redirection is seamless, and no requests are dropped because of swap operations. The entire workflow can be automated by configuring Auto swap when pre-swap validation isn't needed.

Consider restoring to last known good site. After a swap, the slot with the previously staged app now has the previous production app. If the changes swapped into the production slot aren't as you expected, you can perform the same swap immediately to return to your "last known good site."

Consider Auto swap. Auto swap streamlines Azure DevOps scenarios where you want to deploy your app continuously with zero cold starts and zero downtime for app customers. When Auto swap is enabled from a slot into production, every time you push your code changes to that slot, App Service automatically swaps the app into production after it's warmed up in the source slot. Auto swap isn't currently supported for Web Apps on Linux.


Next unit: Add deployment slots


6- Add deployment slots

Deployment slots are configured in the Azure portal. You can swap your app content and configuration elements between deployment slots, including the production slot.

How to use deployment slots in Azure App Service

Things to know about creating deployment slots
Let's review some details about how deployment slots are configured.

New deployment slots can be empty or cloned.

Deployment slot settings fall into three categories:

Slot-specific app settings and connection strings (if applicable)
Continuous deployment settings (when enabled)
Azure App Service authentication settings (when enabled)
When you clone a configuration from another deployment slot, the cloned configuration is editable. Some configuration elements follow the content across the swap. Other slot-specific configuration elements stay in the source slot after the swap.

Swapped settings versus slot-specific settings
The following table lists the settings that are swapped between deployment slots, and settings that remain in the source slot (slot-specific). As you review these settings, consider which features are required for your App Service apps.

Swapped settings	Slot-specific settings
General settings, such as framework version, 32/64-bit, web sockets
App settings *
Connection strings *
Handler mappings
Public certificates
WebJobs content
Hybrid connections **
Service endpoints **
Azure Content Delivery Network **
Path mapping	Custom domain names
Nonpublic certificates and TLS/SSL settings
Scale settings
Always On
IP restrictions
WebJobs schedulers
Diagnostic settings
Cross-origin resource sharing (CORS)
Virtual network integration
Managed identities
Settings that end with the suffix _EXTENSION_VERSION
* Setting can be configured to be slot-specific.

** Feature isn't currently available.

Next unit: Secure your App Service app

7- Secure your App Service app

Azure App Service provides built-in authentication and authorization support. You can sign in users and access data by writing minimal or no code in your web app, API, and mobile backend, and also your Azure Functions apps.

Secure authentication and authorization require deep understanding of security, including federation, encryption, JSON web tokens (JWT) management, grant types, and so on. App Service provides these utilities so you can spend more time and energy on providing business value to your customer.

 Note

You aren't required to use Azure App Service for authentication and authorization. Many web frameworks are bundled with security features, and you can use your preferred service.

Things to know about app security with App Service
Let's take a closer look at how App Service helps you provide security for your app.

The authentication and authorization security module in Azure App Service runs in the same environment as your application code, yet separately.

The security module is configured by using app settings. No SDKs, specific languages, or changes to your application code are required.

When you enable the security module, every incoming HTTP request passes through the module before it's handled by your application code.

The security module handles several tasks for your app:

Authenticate users with the specified provider
Validate, store, and refresh tokens
Manage the authenticated session
Inject identity information into request headers
Things to consider when using App Service for app security
You configure authentication and authorization security in App Service by selecting features In the Azure portal. Review the following options and think about what security can benefit your App Service apps implementation.

Allow Anonymous requests (no action). Defer authorization of unauthenticated traffic to your application code. For authenticated requests, App Service also passes along authentication information in the HTTP headers. This feature provides more flexibility for handling anonymous requests. With this feature, you can present multiple sign-in providers to your users.

Allow only authenticated requests. Redirect all anonymous requests to /.auth/login/<provider> for the provider you choose. The feature is equivalent to Log in with <provider>. If the anonymous request comes from a native mobile app, the returned response is an HTTP 401 Unauthorized message. With this feature, you don't need to write any authentication code in your app.

 Important

This feature restricts access to all calls to your app. Restricting access to all calls might not be desirable if your app requires a public home page, as is the case for many single-page apps.

Logging and tracing. View authentication and authorization traces directly in your log files. If you see an authentication error that you didn’t expect, you can conveniently find all the details by looking in your existing application logs. If you enable failed request tracing, you can see exactly how the security module participated in a failed request. In the trace logs, look for references to a module named EasyAuthModule_32/64.

Next unit: Create custom domain names


8-  Create custom domain names

When you create a web app, Azure assigns the app to a subdomain of azurewebsites.net. Suppose your web app is named contoso. Azure creates a URL for your web app as contoso.azurewebsites.net. Azure also assigns a virtual IP address for your app. For a production web app, you might want users to see a custom domain name.

How to add and secure a custom domain on your App Service web app

Steps to configure a custom domain name for your app
There are three steps to create a custom domain name. The following steps outline how to create a domain name in the Azure portal.

 Important

To map a custom DNS name to your app, you need a paid tier of an App Service plan for your app.

Reserve your domain name. The easiest way to set up a custom domain is to buy one directly in the Azure portal. (This name isn't the Azure assigned name of \*.azurewebsites.net.) The registration process enables you to manage your web app's domain name directly in the Azure portal instead of going to a third-party site. Configuring the domain name in your web app is also a simple process in the Azure portal.

Create DNS records to map the domain to your Azure web app. The Domain Name System (DNS) uses data records to map domain names to IP addresses. There are several types of DNS records.

For web apps, you create either an A (Address) record or a CNAME (Canonical Name) record.

An A record maps a domain name to an IP address.
A CNAME record maps a domain name to another domain name. DNS uses the second name to look up the address. Users still see the first domain name in their browser. As an example, you could map contoso.com to your webapp.azurewebsites.net URL.
If the IP address changes, a CNAME entry is still valid, whereas an A record must be updated.

Some domain registrars don't allow CNAME records for the root domain or for wildcard domains. In such cases, you must use an A record.

Enable the custom domain. After you have your domain and create your DNS record, use the Azure portal to validate your custom domain and add it to your web app. Be sure to test your domain before publishing.

Next unit: Back up and restore your App Service app



9- Back up and restore your App Service app

The Backup and Restore feature in Azure App Service lets you easily create backups manually or on a schedule. You can configure the backups to be retained for a specific or indefinite amount of time. You can restore your app or site to a snapshot of a previous state by overwriting the existing content or restoring to another app or site.

Watch the following video on how to configure a backup for your App Service instance. This video is based on Azure Tips and Tricks #28 - Configure a backup for Azure App Service.


Things to know about Backup and Restore
Examine the following details about the Backup and Restore feature. Think about how you can implement this feature for your App Service apps.

To use the Backup and Restore feature, you need the Standard or Premium tier App Service plan for your app or site.

You need an Azure storage account and container in the same subscription as the app to back up.

Azure App Service can back up the following information to the Azure storage account and container you configured for your app:

App configuration settings
File content
Any database connected to your app (SQL Database, Azure Database for MySQL, Azure Database for PostgreSQL, MySQL in-app)
In your storage account, each backup consists of a Zip file and XML file:

The Zip file contains the back-up data for your app or site.
The XML file contains a manifest of the Zip file contents.
You can configure backups manually or on a schedule.

Full backups are the default.

Partial backups are supported. You can specify files and folders to exclude from a backup.

You restore partial backups of your app or site the same way you restore a regular backup.

Backups can hold up to 10 GB of app and database content.

Backups for your app or site are visible on the Containers page of your storage account and app (or site) in the Azure portal.

Things to consider when creating backups and restoring backups
Let's review some considerations about creating a backup for your app or site, and restoring data and content from a backup.

Consider full backups. Do a full backup to easily save all configuration settings, all file content, and all database content connected with your app or site.

When you restore a full backup, all content on the site is replaced with whatever is in the backup. If a file is on the site, but not in the backup, the file is deleted.

Consider partial backups. Specify a partial backup so you can choose exactly which files to back up.

When you restore a partial backup, any content located in an excluded folder or file is left as-is.

Consider browsing back-up files. Unzip and browse the Zip and XML files associated with your backup to access your backups. This option lets you view the content without actually performing an app or site restore.

Consider firewall on back-up destination. If your storage account is enabled with a firewall, you can't use the storage account as the destination for your backups.

Next unit: Use Azure Application Insights



10- Use Azure Application Insights

Azure Application Insights is a feature of Azure Monitor that lets you monitor your live applications. You can integrate Application Insights with your App Service configure to automatically detect performance anomalies in your apps.

Application Insights is designed to help you continuously improve the performance and usability of your apps. The feature offers powerful analytics tools to help you diagnose issues and understand what users actually do with your apps.

Things to know about Application Insights
Let's examine some characteristics of Application Insights for Azure Monitor.

Application Insights works on various platforms including .NET, Node.js and Java EE.

The feature can be used for configurations that are hosted on-premises, in a hybrid environment, or in any public cloud.

Application Insights integrates with your Azure DevOps process, and has connection points to many development tools.

You can monitor and analyze data from mobile apps by integrating with Visual Studio App Center.

Diagram that shows Azure Application Insights receiving information from web pages, client apps, and web services, which is transferred to Alerts, Power BI, and Visual Studio.

Things to consider when using Application Insights
Application Insights is ideal for supporting your development team. The feature helps developers understand how your app is performing and how it's being used. Consider monitoring the following items in your App Service configuration scenario.

Consider Request rates, response times, and failure rates. Find out which pages are most popular, at what times of day, and where your users are. See which pages perform best. If your response times and failure rates go high when there are more requests, then perhaps you have a resourcing problem.

Consider Dependency rates, response times, and failure rates. Use Application Insights to discover if external services are degrading your app performance.

Consider Exceptions. Analyze the aggregated statistics, or pick specific instances and drill into the stack trace and related requests. Both server and browser exceptions are reported.

Consider Page views and load performance. Collect the number of page views reported by your users' browsers and analyze the load performance.

Consider User and session counts. Application Insights can help you keep track of the number of users and sessions connected to your app.

Consider Performance counters. Add Application Insights performance counters from your Windows or Linux server machines. Monitor performance output for the CPU, memory, network usage, and so on.

Consider Host diagnostics. Integrate diagnostics from Docker or Azure into your app Application Insights.

Consider Diagnostic trace logs. Implement trace logs from your app to help correlate trace events with requests and diagnose issues.

Consider Custom events and metrics. Write your own custom events and metric tracking algorithms as client or server code. Track business events such as number of items sold, or number of games won.

Next unit: Interactive lab simulation


11- Interactive lab simulation

Lab scenario
Your organization is migrating on-premises web apps to Azure. As the Azure Administrator you need to:

Host web sites running on Windows servers by using the PHP runtime stack.
Implement Azure DevOps practices by using Azure Web Apps deployment slots.
Architecture diagram
Architecture diagram as explained in the text.

Objectives
Task 1: Create an Azure web app.
Create a web app by using the Azure portal.
The web app should run on Windows and use the PHP runtime stack.
Task 2: Create a staging deployment slot.
Verify there's a production deployment slot.
Create a new staging deployment slot.
Task 3: Configure Web App deployment settings.
Deploy your web app from a local Git session.
Provide the authentication credentials.
Task 4: Deploy code to the staging deployment slot.
Use Azure PowerShell to clone the remote repository and set the local path.
Add the remote Git session by using the authentication credentials.
Display the default web page in a new browser tab.
Push the sample web app code from the local repository to the Azure Web App staging deployment slot.
Task 5: Swap the staging slots.
Swap the deployment slots.
Verify the default web page is replaced with the Hello World page.
Task 6: Configure and test autoscaling of your Azure web app.
Configure a custom autoscale rule on the production deployment slot.
The scale rule should use the CPU percentage to increase the resource count.
Use Azure PowerShell to start an infinite loop that sends the HTTP requests to your web app.
Confirm the resource count automatically scales.
 Note

Select the thumbnail image to start the lab simulation. When you're done, be sure to return to this page so you can continue learning.

 Note

You may find slight differences between the interactive simulation and the hosted lab, but the core concepts and ideas being demonstrated are the same.

Screenshot of the simulation page.

Next unit: Knowledge check


12- Knowledge check

You're developing a strategy to implement web apps for your company by using Azure App Service. Various teams in your organization have submitted requests for your consideration as part of the implementation plan.

The Production team needs information about cloning configurations across deployment slots.

The Marketing team needs to know which research web pages are most popular, at what times of day, and where users are located.

You're investigating automated deployment sources.

Answer the following questions
Choose the best response for each of the following questions. Then select Check your answers.


1. When you clone a configuration from another deployment slot, which configuration setting follows the content across the swap? 

Custom domain names

Connection strings

Scale settings

2. How can you support the Marketing team requests about research web page usage? 

Continuous deployment

Application logging

Azure Application Insights

3. Which option is a valid automated deployment source? 

GitHub

JavaScript code

SharePoint


Summary and resources

Azure App Service is an HTTP-based service for hosting web applications. With App Service, you can develop web apps in your favorite language. The service lets you easily run and scale your web apps on Windows and Linux-based environments.

In this module, you reviewed the features and usage cases for Azure App Service. You learned how to create, secure, and back up your web apps. You explored how to configure deployment settings, including deployment slots, and custom domain names for your web apps. You discovered how to use Azure Application Insights to monitor web apps.

The main takeaways from this module are:

Azure App Service lets you develop and deploy web, mobile, and API apps.

Azure App Service configuration settings include runtime stack, operating system, region and App Service plan.

Deployment slots help you manage different app stages. For example, development, test, stage, and production.

The default Azure App Service domain name can be customized for your organization.

Azure Application Insights is a feature of Azure Monitor that lets you monitor your live applications. You can integrate Application Insights with your App Service configure to automatically detect performance anomalies in your apps.

Application Insights lets you continuously monitor the performance and usability of your apps.

Learn more with documentation
App Service overview. This article provides an overview of the App Service and why you would use this service.

Configure an App Service app. This article explains how to configure common settings for web apps, mobile back end, or API app.

Set up staging environments in Azure App Service. The article covers deployment slots and swap operations.

Learn more with self-paced training
Stage a web app deployment for testing and rollback by using App Service deployment slots. Learn to use deployment slots to streamline deployment and roll back.

Explore Azure App Service deployment slots. Learn how slot swapping works and how to route traffic to different slots.

Host a web application with Azure App Service. Learn how to create a website through the hosted web app platform in Azure App Service.






Point 5: Configure Azure Container Instances

Learn how to configure Azure Container Instances including container groups.


Learning objectives
In this module, you learn how to:

Identify when to use containers versus virtual machines.
Identify the features and usage cases of Azure Container Instances.
Implement Azure container groups.


1- Introduction

Containers and virtual machines are both forms of virtualization, but there are some key differences between them.

To provide context, let's consider a scenario: You're an Azure Administrator responsible for deploying and managing applications in a cloud environment. Your organization is looking for a solution that offers fast startup times, easy management, and the ability to run applications in isolated containers. You want to understand the benefits of using Azure Container Instances and how it compares to virtual machines.

In this module, you learn when to use Azure Container Instances instead of virtual machines. You also get an overview of features and use cases.

The goal of this module is to introduce you to Azure Container Instances.

Learning objectives
In this module, you learn how to:

Identify when to use containers versus virtual machines.
Identify the features and usage cases of Azure Container Instances.
Implement Azure container groups.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Working knowledge of containerization concepts and terminology.
Familiarity with cloud computing and experience with the Azure portal.


Next unit: Compare containers to virtual machines


2- Compare containers to virtual machines

Hardware virtualization makes it possible to run multiple isolated instances of operating systems concurrently on the same physical hardware. Containers represent the next stage in the virtualization of computing resources.

Container-based virtualization allows you to virtualize the operating system. This approach lets you run multiple applications within the same instance of an operating system, while maintaining isolation between the applications. The containers within a virtual machine provide functionality similar to that of virtual machines within a physical server.

Things to know about containers versus virtual machines
To better understand container-based virtualization, let's compare containers and virtual machines.

Compare	Containers	Virtual machines
Isolation	A container typically provides lightweight isolation from the host and other containers, but a container doesn't provide as strong a security boundary as a virtual machine.	A virtual machine provides complete isolation from the host operating system and other virtual machines. This separation is useful when a strong security boundary is critical, such as hosting apps from competing companies on the same server or cluster.
Operating system	Containers run the user mode portion of an operating system and can be tailored to contain just the needed services for your app. This approach helps you use fewer system resources.	Virtual machines run a complete operating system including the kernel, which requires more system resources (CPU, memory, and storage).
Deployment	You can deploy individual containers by using Docker via the command line. You can deploy multiple containers by using an orchestrator such as Azure Kubernetes Service.	You can deploy individual virtual machines by using Windows Admin Center or Hyper-V Manager. You can deploy multiple virtual machines by using PowerShell or System Center Virtual Machine Manager.
Persistent storage	Containers use Azure Disks for local storage for a single node, or Azure Files (SMB shares) for storage shared by multiple nodes or servers.	Virtual machines use a virtual hard disk (VHD) for local storage for a single machine, or an SMB file share for storage shared by multiple servers.
Fault tolerance	If a cluster node fails, the orchestrator on another cluster node rapidly recreates any containers running on the node.	Virtual machines can fail over to another server in a cluster, where the virtual machine's operating system restarts on the new server.
Things to consider when using containers
Containers offer several advantages over physical and virtual machines. Review the following benefits and consider how you can implement containers for the internal apps for your company.

Consider flexibility and speed. Gain increased flexibility and speed when developing and sharing your containerized application code.

Consider testing. Choose containers for your configuration to allow for simplified testing of your apps.

Consider app deployment. Implement containers to gain streamlined and accelerated deployment of your apps.

Consider workload density. Support higher workload density and improve your resource utilization by working with containers.

Understand container images
All containers are created from container images. A container image is a lightweight, standalone, executable package of software that encapsulates everything needed to run an application. It includes the following components:

Code: The application’s source code.
Runtime: The environment required to execute the application.
System tools: Utilities necessary for the application to function.
System libraries: Shared libraries used by the application.
Settings: Configuration parameters specific to the application.
When you create a container image, it becomes a portable unit that can run consistently across different computing environments. These images are the building blocks for containers, which are instances of these images running at runtime.

Next unit: Review Azure Container Instances


3- Review Azure Container Instances

Containers are becoming the preferred way to package, deploy, and manage cloud applications. There are many options for teams to build and deploy cloud native and containerized applications on Azure. In this unit, we review Azure Container Instances (ACI).

Azure Container Instances offers the fastest and simplest way to run a container in Azure, without having to manage any virtual machines and without having to adopt a higher-level service. Azure Container Instances is a great solution for any scenario that can operate in isolated containers, including simple applications, task automation, and build jobs, because it provides a single pod of Hyper-V isolated containers on demand.

The following illustration shows a web server container built with Azure Container Instances. The container is running on a virtual machine in a virtual network.

Diagram that shows a web server container running on a virtual machine in a virtual network.

Things to know about Azure Container Instances
Let's review some of the benefits of using Azure Container Instances. As you review these points, think about how you can implement Container Instances for your internal applications.

Fast startup times. Containers can start in seconds without the need to provision and manage virtual machines.

Public IP connectivity and DNS names. Containers can be directly exposed to the internet with an IP address and FQDN (fully qualified domain name).

Custom sizes. Container nodes can be scaled dynamically to match actual resource demands for an application.

Persistent storage. Containers support direct mounting of Azure Files file shares.

Linux and Windows containers. Container Instances can schedule both Windows and Linux containers. Specify the operating system type when you create your container groups.

Coscheduled groups. Container Instances supports scheduling of multi-container groups that share host machine resources.

Virtual network deployment. Container Instances can be deployed into an Azure virtual network.

Next unit: Implement container groups


4- Implement container groups

The top-level resource in Azure Container Instances is the container group. A container group is a collection of containers that get scheduled on the same host machine. The containers in a container group share a lifecycle, resources, local network, and storage volumes.

Things to know about container groups
Let's review some of details about container groups for Azure Container Instances.

A container group is similar to a pod in Kubernetes. A pod typically has a 1:1 mapping with a container, but a pod can contain multiple containers. The containers in a multi-container pod can share related resources.

Azure Container Instances allocates resources to a multi-container group by adding together the resource requests of all containers in the group. Resources can include items such as CPUs, memory, and GPUs.

Consider a container group that has two containers that each require CPU resources. Each container requests one CPU. Azure Container Instances allocates two CPUs for the container group.

There are two common ways to deploy a multi-container group: Azure Resource Manager (ARM) templates and YAML files.

ARM template. An ARM template is recommended for deploying other Azure service resources when you deploy your container instances, such as an Azure Files file share.

YAML file. Due to the concise nature of the YAML format, a YAML file is recommended when your deployment includes only container instances.

Container groups can share an external-facing IP address, one or more ports on the IP address, and a DNS label with an FQDN.

External client access. You must expose the port on the IP address and from the container to enable external clients to reach a container in your group.

Port mapping. Port mapping isn't supported because containers in a group share a port namespace.

Deleted groups. When a container group is deleted, its IP address and FQDN are released.

Configuration example
Consider the following example of a multi-container group with two containers.

Diagram that depicts an Azure Container Instances multi-container group that has two containers.

The multi-container group has the following characteristics and configuration:

The container group is scheduled on a single host machine, and is assigned a DNS name label.
The container group exposes a single public IP address with one exposed port.
One container in the group listens on port 80. The other container listens on port 1433.
The group includes two Azure Files file shares as volume mounts. Each container in the group mounts one of the file shares locally.
Things to consider when using container groups
Multi-container groups are useful when you want to divide a single functional task into a few container images. Different teams can deliver the images, and the images can have separate resource requirements.

Consider the following scenarios for working with multi-container groups. Think about what options can support your internal apps for the online retailer.

Consider web app updates. Support updates to your web apps by implementing a multi-container group. One container in the group serves the web app and another container pulls the latest content from source control.

Consider log data collection. Use a multi-container group to capture logging and metrics data about your app. Your application container outputs logs and metrics. A logging container collects the output data and writes the data to long-term storage.

Consider app monitoring. Enable monitoring for your app with a multi-container group. A monitoring container periodically makes a request to your application container to ensure your app is running and responding correctly. The monitoring container raises an alert if it identifies possible issues with your app.

Consider front-end and back-end support. Create a multi-container group to hold your front-end container and back-end container. The front-end container can serve a web app. The back-end container can run a service to retrieve data.

Next unit: Review Azure Container Apps


5- Review Azure Container Apps

There are many options for teams to build and deploy cloud native and containerized applications on Azure. Let's understand which scenarios and use cases are best suited for Azure Container Apps and how it compares to other container options on Azure.

Things to know about Azure Container Apps
Azure Container Apps is a serverless platform that allows you to maintain less infrastructure and save costs while running containerized applications. Instead of worrying about server configuration, container orchestration, and deployment details, Container Apps provides all the up-to-date server resources required to keep your applications stable and secure.

Common uses of Azure Container Apps include:

Deploying API endpoints
Hosting background processing jobs
Handling event-driven processing
Running microservices
Additionally, applications built on Azure Container Apps can dynamically scale based on the following characteristics:

HTTP traffic
Event-driven processing
CPU or memory load
Any KEDA-supported scaler
Things to consider when using Azure Container Apps
Azure Container Apps enables you to build serverless microservices and jobs based on containers. Distinctive features of Container Apps include:

Optimized for running general purpose containers, especially for applications that span many microservices deployed in containers.
Powered by Kubernetes and open-source technologies like Dapr, KEDA, and envoy.
Supports Kubernetes-style apps and microservices with features like service discovery and traffic splitting.
Enables event-driven application architectures by supporting scale based on traffic and pulling from event sources like queues, including scale to zero.
Supports running on demand, scheduled, and event-driven jobs.
Azure Container Apps doesn't provide direct access to the underlying Kubernetes APIs. If you would like to build Kubernetes-style applications and don't require direct access to all the native Kubernetes APIs and cluster management, Container Apps provides a fully managed experience based on best-practices. For these reasons, many teams may prefer to start building container microservices with Azure Container Apps.

Compare container management solutions
Azure Container Instances (ACI) can be managed in several ways. Azure Container Apps (ACA) is one way, and Azure Kubernetes Service (AKS) is another. Here’s a comparison table for when to use ACA and AKS.

Feature	Azure Container Apps (ACA)	Azure Kubernetes Service (AKS)
Overview	ACA is a serverless container platform that simplifies the deployment and management of microservices-based applications by abstracting away the underlying infrastructure.	AKS simplifies deploying a managed Kubernetes cluster in Azure by offloading the operational overhead to Azure. It’s suitable for complex applications that require orchestration.
Deployment	ACA provides a PaaS experience with quick deployment and management capabilities.	AKS offers more control and customization options for Kubernetes environments, making it suitable for complex applications and microservices.
Management	ACA builds upon AKS and offers a simplified PaaS experience for running containers, with additional features, like Dapr for microservices.	AKS provides a more granular control over the Kubernetes environment, suitable for teams with Kubernetes expertise.
Scalability	ACA supports both HTTP-based autoscaling and event-driven scaling, making it ideal for applications that need to respond quickly to changes in demand.	AKS offers horizontal pod autoscaling and cluster autoscaling, providing robust scalability options for containerized applications.
Use Cases	ACA is designed for microservices and serverless applications that benefit from rapid scaling and simplified management.	AKS is best for complex, long-running applications that require full Kubernetes features and tight integration with other Azure services.
Integration	ACA integrates with Azure Logic Apps, Functions, and Event Grid for event-driven architectures.	AKS provides features like Azure Policy for Kubernetes, Azure Monitor for containers, and Azure Defender for Kubernetes for comprehensive security and governance.



Next unit: Interactive lab simulation


6- Interactive lab simulation

Lab scenario
Your organization needs a new platform for its virtualized workloads. As the Azure Administrator you need to:

Evaluate Azure Container Instances.
Identify and test your app container by using Docker images.
Architecture diagram
Architecture diagram as explained in the text.

Objectives
Task 1: Deploy an Azure Container Instances using a Docker image.
Create a new container instance by using a Linux image.
Configure a valid globally unique DNS host name. This host name is used to test the container.
Task 2: Review the functionality of Azure Container Instances.
Confirm the container instance is running.
Verify that the Welcome to Azure Container Instances page is displayed.
 Note

Select the thumbnail image to start the lab simulation. When you're done, be sure to return to this page so you can continue learning.

Screenshot of the simulation page.

Next unit: Knowledge check


7- Knowledge check

You're working on the app deployment and management strategy for your company's online retail site. You're reviewing a few scenarios as part of the implementation planning.

The Admin team requested input about where containers on Windows can be more advantageous than virtual machines.

You're examining features of Azure Container Instances to support site-related apps hosted on-premises in Azure to share hardware resources, network usage, and storage volumes.

The development team needs a container management solution that simplifies deployment and provides orchestration.

Answer the following questions
Choose the best response for each of the following questions. Then select Check your answers.


1. Why should you select virtual machines over containers for your configuration? 

Virtual machines run the user mode portion of an operating system and can be tailored to contain just the needed services for your app.

Virtual machines provide complete isolation from the host operating system and other virtual machines.

Virtual machines use Azure Disks for local storage for a single node.

2. Which of the following options is a feature of Azure Container Instances? 

Container Instances require several minutes to load.

Container Instances use Azure Blob Storage for retrieve and persist state.

Billing for Container Instances occurs when containers are in use.

3. What container management solution simplifies deployment? 

Azure Container Apps

Container groups

Container Instances



Summary and resources

In this module, you learned how to identify when to use Azure Container Instances versus Azure virtual machines. You explored the features and usage cases of Azure Container Instances. You discovered how to implement Azure container groups.

The main takeaways from this module are:

Containers provide lightweight isolation and use fewer system resources compared to virtual machines.
Containers can be deployed individually using Docker or with an orchestrator like Azure Container Apps.
Containers use Azure Disks or Azure Files for storage.
A container group is a collection of containers that get scheduled on the same host machine.
Containers can be rapidly recreated on another cluster node if a node fails.
Learn more
Containers versus virtual machines. This article reviews the key similarities and differences between containers and virtual machines (VMs), and when you might want to use each.

Quickstart: Deploy a container instance in Azure using the Azure portal. In this quickstart, you use the Azure portal to deploy an isolated Docker container and make its application available with a fully qualified domain name (FQDN). After configuring a few settings and deploying the container, you can browse to the running application:

Container groups in Azure Container Instances. This article describes what container groups are and the types of scenarios they enable.

Learn more with self-paced training
Run container images in Azure Container Instances. Learn how Azure Container Instances can help you quickly deploy containers, how to set environment variables, and specify container restart policies.

Implement Azure Container Apps. Learn how Azure Container Apps can help you deploy and manage microservices and containerized apps on a serverless platform that runs on top of Azure Kubernetes Service.

Introduction to Docker containers. Learn the benefits of using Docker containers as a containerization platform. Discuss the infrastructure provided by the Docker platform.





Point 6: Manage virtual machines with the Azure CLI

Learn how to use the cross-platform Azure CLI to create, start, stop, and perform other management tasks related to virtual machines in Azure.

Learning objectives
In this module, you will:

Create a virtual machine with the Azure CLI.
Resize virtual machines with the Azure CLI.
Perform basic management tasks using the Azure CLI.
Connect to a running VM with SSH and the Azure CLI.


1- What is the Azure CLI?

It's quite common for IT departments to manage a large set of Azure resources, ranging from Azure Virtual Machines to managed websites.

While the Azure portal is easy to use for one-off tasks, navigating through the various panes adds time when you have to create, change, or delete multiple things. This is where the command line shines; you can issue commands quickly and efficiently, or even use scripts to run repetitive tasks. With Azure, you have two different command-line tools you can work with: Azure PowerShell and the Azure CLI.

With either of these tools, you can write scripts to check the cloud-server status, deploy new configurations, open ports in the firewall, or connect to a virtual machine to change a setting. Windows admins tend to prefer Azure PowerShell, while developers and Linux admins often use the Azure CLI.

This module focuses on using the Azure CLI to create and manage virtual machines hosted in Azure. If you'd like to get an overview of the Azure CLI, how to install it, and how to work with your Azure subscriptions, make sure to check out the Control Azure services with the CLI training module.

What is the Azure CLI?
The Azure CLI is Microsoft's cross-platform command-line tool for managing Azure resources. It's available for macOS, Linux, and Windows, or in the browser using Azure Cloud Shell.

The Azure CLI can help you manage Azure resources such as virtual machines and disks from the command line or in scripts. Let's get started and see what it can do with Azure Virtual Machines.

Learning objectives
In this module, you will:

Create a virtual machine with the Azure CLI.
Resize virtual machines with the Azure CLI.
Perform basic management tasks using the Azure CLI.
Connect to a running VM with SSH and the Azure CLI.
Prerequisites
Basic understanding of the Azure CLI tool from the Control Azure services with the CLI module.


Next unit: Exercise - Create a virtual machine


2- Exercise - Create a virtual machine

Let's start with the most obvious task: creating an Azure Virtual Machine.

Logins, subscriptions, and resource groups
You'll be working in the Azure Cloud Shell on the right. Once you activate the sandbox, you'll be logged into Azure with a free subscription that Microsoft Learn manages. You don't have to log in to Azure on your own or select a subscription; this is done for you. You'd also normally create a resource group to hold new resources. In this module, the Azure sandbox creates a resource group for you, which you'll use to execute all the commands.

Create a Linux VM with the Azure CLI
The Azure CLI includes the vm command to work with virtual machines in Azure. We can supply several subcommands to do specific tasks. The most common include:

Sub-command	Description
create	Create a new virtual machine
deallocate	Deallocate a virtual machine
delete	Delete a virtual machine
list	List the created virtual machines in your subscription
open-port	Open a specific network port for inbound traffic
restart	Restart a virtual machine
show	Get the details for a virtual machine
start	Start a stopped virtual machine
stop	Stop a running virtual machine
update	Update a property of a virtual machine
 Note

For a complete list of commands, you can check the Azure CLI reference documentation.

Let's start with the first one: az vm create. You can use this command to create a virtual machine in a resource group. There are several parameters you can pass to configure all the aspects of the new VM. The four parameters that must be supplied are:

Parameter	Description
--resource-group	The resource group that will own the virtual machine; use [sandbox Resource Group].
--name	The name of the virtual machine; must be unique within the resource group.
--image	The operating system image to use to create the VM.
--location	The region in which to place the VM. Typically, this would be close to the VM's consumer.
In addition, it's helpful to add the --verbose flag to see progress while the VM is being created.

Create a Linux virtual machine
Let's create a new Linux virtual machine. Execute the following command in Azure Cloud Shell to create an Ubuntu VM in the "West US" location.

Azure CLI

Copy
az vm create \
  --resource-group "[sandbox resource group name]" \
  --location westus \
  --name SampleVM \
  --image Ubuntu2204 \
  --admin-username azureuser \
  --generate-ssh-keys \
  --verbose 
 Tip

You can use the Copy button to copy commands to the clipboard. To paste, right-click on a new line in the Cloud Shell terminal and select Paste, or use the Shift+Insert keyboard shortcut (⌘+V on macOS).

This command creates a new Ubuntu Linux virtual machine with the name SampleVM. Notice that the Azure CLI tool waits while the VM is being created. You can add the --no-wait option to tell the Azure CLI tool to return immediately and have Azure continue creating the VM in the background. This is useful if you're executing the command in a script.

We're specifying the administrator account name through the --admin-username flag to be azureuser. If you omit this, the az vm create command will use your current user name. Because the rules for account names are different for each OS, it's safer to specify a specific name.

 Note

Common names such as "root" and "admin" aren't allowed for most images.

We're also using the generate-ssh-keys flag. Linux distributions use this parameter, and it creates a pair of security keys so we can use the ssh tool to access the virtual machine remotely. The two files are placed into the .ssh folder on your machine and in the VM. If you already have an SSH key named id_rsa in the target folder, then that SSH key will be used rather than generating a new key.

Once Azure CLI finishes creating the VM, you'll get a JSON response which includes the current state of the virtual machine and its public and private IP addresses assigned by Azure:

JSON

Copy
{
  "fqdns": "",
  "id": "/subscriptions/20f4b944-fc7a-4d38-b02c-900c8223c3a0/resourceGroups/Learn-2568d0d0-efe3-4d04-a08f-df7f009f822a/providers/Microsoft.Compute/virtualMachines/SampleVM",
  "location": "westus",
  "macAddress": "00-0D-3A-58-F8-45",
  "powerState": "VM running",
  "privateIpAddress": "10.0.0.4",
  "publicIpAddress": "40.83.165.85",
  "resourceGroup": "2568d0d0-efe3-4d04-a08f-df7f009f822a",
  "zones": ""
}




Next unit: Exercise - Test your new virtual machine


3- Exercise - Test your new virtual machine

When you create a virtual machine, it's assigned a public IP address that's reachable over the Internet and a private IP address used within the Azure data center. Both of these values appear in the JSON block the create command returns, like the following:

JSON

Copy
{
   ...
  "privateIpAddress": "10.0.0.4",
  "publicIpAddress": "40.83.165.85",
   ...
}
Connecting to the VM with SSH
We can quickly test that the Linux VM is up and running by using the public IP address in the Secure Shell (ssh) tool. Remember that we set our admin name to azureuser, so we need specify that. Make sure to use the public IP address from your running instance.

Azure CLI

Copy
ssh azureuser@<public-ip-address>
 Note

We don't need a password because we generated an SSH key pair as part of the VM creation. The first time you shell into the VM, you will receive a prompt regarding the authenticity of the host.

This is because we are attempting to access an IP address directly instead of through a host name. Answering "yes" will save the IP address as a valid host for connection and allow the connection to proceed.

Output

Copy
The authenticity of host '40.83.165.85 (40.83.165.85)' can't be established.
RSA key fingerprint is SHA256:hlFnTCAzgWVFiMxHK194I2ap6+5hZoj9ex8+/hoM7rE.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '40.83.165.85' (RSA) to the list of known hosts.
Then, you'll be presented with a remote shell where you can enter Linux commands.

Output

Copy
Welcome to Ubuntu 18.04.3 LTS (GNU/Linux 5.0.0-1014-azure x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Wed Aug 21 20:32:04 UTC 2019

  System load:  0.0               Processes:           108
  Usage of /:   4.2% of 28.90GB   Users logged in:     0
  Memory usage: 9%                IP address for eth0: 10.0.0.5
  Swap usage:   0%

0 packages can be updated.
0 updates are security updates.

The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.

To run a command as administrator (user "root"), use "sudo <command>".
See "man sudo_root" for details.

azureuser@SampleVM:~$
Try a few commands, such as ps or ls as practice. When you're finished, sign out of the virtual machine by typing exit or logout.

Next unit: Exercise - Explore other VM images


4- Exercise - Explore other VM images

We used UbuntuLTS for the image to create the virtual machine. Azure has several standard VM images you can use to create a virtual machine.

Listing images
You can get a list of the available VM images using the following command:

Azure CLI

Copy
az vm image list --output table
 Note

If you get the error az: command not found, type exit into the shell and try again.

This outputs the most popular images that are part of an offline list built into the Azure CLI. However, there are hundreds of image options available in the Azure Marketplace.

Getting all images
You can get a full list by adding the --all flag to the command. Because the list of images in the Marketplace is very large, it's helpful to filter the list with the --publisher, --sku or –-offer options.

For example, try the following command to see all Wordpress images available:

Azure CLI

Copy
az vm image list --sku Wordpress --output table --all
Or this command to see all images provided by Microsoft:

Azure CLI

Copy
az vm image list --publisher Microsoft --output table --all
These commands can take a few moments to complete.

Location-specific images
Some images are only available in certain locations. Try adding the --location [location] flag to the command to scope the results to ones available in the region where you want to create the virtual machine. For example, type the following into Azure Cloud Shell to get a list of images available in the eastus region.

Azure CLI

Copy
az vm image list --location eastus --output table
Try checking some of the images in the other Azure sandbox available locations:

westus2
southcentralus
centralus
eastus
westeurope
southeastasia
japaneast
brazilsouth
australiasoutheast
centralindia
 Tip

These are the standard images that are provided by Azure. Keep in mind that you can also create and upload your own custom images to create VMs based on unique configurations or less common versions or distributions of an operating system.

Your command window is probably full now. If you like, you can type clear to clear the screen.

Next unit: Exercise - Sizing VMs properly


5- Exercise - Sizing VMs properly

Virtual machines must be sized appropriately for the expected work. A VM without the correct amount of memory or CPU will fail under load or run too slowly to be effective.

Predefined VM sizes
When you create a virtual machine, you can supply a VM size value that determines the amount of compute resources devoted to the VM, including CPU, GPU, and memory made available to the virtual machine from Azure.

Azure defines a set of predefined VM sizes for Linux and Windows from which to choose based on the expected usage.

Type	Sizes	Description
General purpose	Dsv3, Dv3, DSv2, Dv2, DS, D, Av2, A0-7	Balanced CPU-to-memory. Ideal for dev/test and small to medium applications and data solutions.
Compute optimized	Fs, F	High CPU-to-memory. Good for medium-traffic applications, network appliances, and batch processes.
Memory optimized	Esv3, Ev3, M, GS, G, DSv2, DS, Dv2, D	High memory-to-core. Great for relational databases, medium to large caches, and in-memory analytics.
Storage optimized	Ls	High disk throughput and IO. Ideal for big data, SQL, and NoSQL databases.
GPU optimized	NV, NC	Specialized VMs targeted for heavy graphic rendering and video editing.
High performance	H, A8-11	Our most powerful CPU VMs with optional high-throughput network interfaces (RDMA).
The available sizes change based on the region in which you're creating the VM. You can get a list of the available sizes using the vm list-sizes command. Try typing the following command into Azure Cloud Shell:

Azure CLI

Copy
az vm list-sizes --location eastus --output table
Here's an abbreviated response for eastus:

Output

Copy
  MaxDataDiskCount    MemoryInMb  Name                      NumberOfCores    OsDiskSizeInMb    ResourceDiskSizeInMb
------------------  ------------  ----------------------  ---------------  ----------------  ----------------------
                 2          2048  Standard_B1ms                         1           1047552                    4096
                 2          1024  Standard_B1s                          1           1047552                    2048
                 4          8192  Standard_B2ms                         2           1047552                   16384
                 4          4096  Standard_B2s                          2           1047552                    8192
                 8         16384  Standard_B4ms                         4           1047552                   32768
                16         32768  Standard_B8ms                         8           1047552                   65536
                 4          3584  Standard_DS1_v2                       1           1047552                    7168
                 8          7168  Standard_DS2_v2                       2           1047552                   14336
                16         14336  Standard_DS3_v2                       4           1047552                   28672
                32         28672  Standard_DS4_v2                       8           1047552                   57344
                64         57344  Standard_DS5_v2                      16           1047552                  114688
        ....
                64       3891200  Standard_M128-32ms                  128           1047552                 4096000
                64       3891200  Standard_M128-64ms                  128           1047552                 4096000
                64       3891200  Standard_M128ms                     128           1047552                 4096000
                64       2048000  Standard_M128s                      128           1047552                 4096000
                64       1024000  Standard_M64                         64           1047552                 8192000
                64       1792000  Standard_M64m                        64           1047552                 8192000
                64       2048000  Standard_M128                       128           1047552                16384000
                64       3891200  Standard_M128m                      128           1047552                16384000
Specify a size during VM creation
We didn't specify a size when we created our VM, so Azure selected a default general-purpose size for us. However, we can specify the size as part of the vm create command using the --size parameter. For example, you could use the following command to create a two-core virtual machine:

Azure CLI

Copy
az vm create \
    --resource-group "[sandbox resource group name]" \
    --name SampleVM2 \
    --image Ubuntu2204 \
    --admin-username azureuser \
    --generate-ssh-keys \
    --verbose \
    --size "Standard_DS2_v2"
 Warning

Your subscription tier enforces limits on how many resources you can create, as well as the total size of those resources. Quota limits depend upon your subscription type and region. The Azure CLI lets you know when you exceed this limit with a Quota Exceeded error. If you hit this error in your own paid subscription, you can request to raise the limits associated with your paid subscription (up to 10,000 vCPUs) through a free online request.

Resize an existing VM
We can also resize an existing VM if the workload changes or if it was incorrectly sized at creation. Let's use the first VM we created, SampleVM. Before requesting a resize, we must check to see if the desired size is available in the cluster our VM is part of. We can use the vm list-vm-resize-options command:

Azure CLI

Copy
az vm list-vm-resize-options \
    --resource-group "[sandbox resource group name]" \
    --name SampleVM \
    --output table
This command returns a list of all the possible size configurations available in the resource group. If the size we want isn't available in our cluster but is available in the region, we can deallocate the VM. This command stops the running VM and removes it from the current cluster without losing any resources. We can then resize it, which re-creates the VM in a new cluster where the size configuration is available.

 Note

The Microsoft Learn sandbox is limited to a few VM sizes.

To resize a VM, we'll use the vm resize command. For example, perhaps we find our VM is underpowered for the task we want it to perform. We could bump it up to a D2s_v3, where it has 2 vCores and 8 GB of memory. Type this command in Cloud Shell:

Azure CLI

Copy
az vm resize \
    --resource-group "[sandbox resource group name]" \
    --name SampleVM \
    --size Standard_D2s_v3
This command takes a few minutes to reduce the resources of the VM, and once it's done, it returns a new JSON configuration.


Next unit: Exercise - Query system and runtime information about the VM


6- Exercise - Query system and runtime information about the VM

Now that we've created a virtual machine, we can get information about it through other commands.

Let's start by running vm list.

Azure CLI

Copy
az vm list
This command will return all virtual machines defined in this subscription. You can filter the output to a specific resource group through the --resource-group parameter.

Output types
Notice that the default response type for all the commands we've done so far is JSON. This is great for scripting, but most people find it harder to read. You can change the output style for any response through the --output flag. For example, run the following command in Azure Cloud Shell to see the different output style.

Azure CLI

Copy
az vm list --output table
Along with table, you can specify json (the default), jsonc (colorized JSON), or tsv (Tab-Separated Values). Try a few variations with the preceding command to see the difference.

Get the IP address
Another useful command is vm list-ip-addresses, which lists the public and private IP addresses for a VM. If they change, or you didn't capture them during creation, you can retrieve them at any time.

Azure CLI

Copy
az vm list-ip-addresses -n SampleVM -o table
This returns output like:

Output

Copy
VirtualMachine    PublicIPAddresses    PrivateIPAddresses
----------------  -------------------  --------------------
SampleVM          168.61.54.62         10.0.0.4
 Tip

Notice that we're using a shorthand syntax for the --output flag as -o. You can shorten most parameters to Azure CLI commands to a single dash and letter. For example, you can shorten --name to -n and --resource-group to -g. This is handy for entering keyboard characters, but we recommend using the full option name in scripts for clarity. Check the documentation for details about each command.

Get VM details
We can get more detailed information about a specific virtual machine by name or ID running the vm show command.

Azure CLI

Copy
az vm show --resource-group "[sandbox resource group name]" --name SampleVM
This returns a fairly large JSON block with all sorts of information about the VM, including attached storage devices, network interfaces, and all of the object IDs for resources that the VM is connected to. Again, we could change to a table format, but that omits almost all of the interesting data. Instead, we can turn to a built-in query language for JSON called JMESPath.

Add filters to queries with JMESPath
JMESPath is an industry-standard query language built around JSON objects. The simplest query is to specify an identifier that selects a key in the JSON object.

For example, given the object:

JSON

Copy
{
  "people": [
    {
      "name": "Fred",
      "age": 28
    },
    {
      "name": "Barney",
      "age": 25
    },
    {
      "name": "Wilma",
      "age": 27
    }
  ]
}
We can use the query people to return the array of values for the people array. If we just want one of the people, we can use an indexer. For example, people[1] would return:

JSON

Copy
{
    "name": "Barney",
    "age": 25
}
We can also add specific qualifiers that would return a subset of the objects based on some criteria. For example, adding the qualifier people[?age > '25'] would return:

JSON

Copy
[
  {
    "name": "Fred",
    "age": 28
  },
  {
    "name": "Wilma",
    "age": 27
  }
]
Finally, we can constrain the results by adding a select: people[?age > '25'].[name] that returns just the names:

JSON

Copy
[
  [
    "Fred"
  ],
  [
    "Wilma"
  ]
]
JMESQuery has several other interesting query features. When you have time, check out the online tutorial available on the JMESPath.org site.

Filter your Azure CLI queries
With a basic understanding of JMES queries, we can add filters to the data returned by queries like the vm show command. For example, we can retrieve the admin username:

Azure CLI

Copy
az vm show \
    --resource-group "[sandbox resource group name]" \
    --name SampleVM \
    --query "osProfile.adminUsername"
We can get the size assigned to our VM:

Azure CLI

Copy
az vm show \
    --resource-group "[sandbox resource group name]" \
    --name SampleVM \
    --query hardwareProfile.vmSize
Or, to retrieve all the IDs for your network interfaces, we can run the query:

Azure CLI

Copy
az vm show \
    --resource-group "[sandbox resource group name]" \
    --name SampleVM \
    --query "networkProfile.networkInterfaces[].id"
This query technique works with any Azure CLI command, and you can use it to pull specific bits of data out on the command line. It's useful for scripting, as well. For example, you can pull a value out of your Azure account and store it in an environment or script variable. If you decide to use it this way, it's useful to add the --output tsv parameter (which you can shorten to -o tsv). This will return results that only include the actual data values with tab separators.

For example:

Azure CLI

Copy
az vm show \
    --resource-group "[sandbox resource group name]" \
    --name SampleVM \
    --query "networkProfile.networkInterfaces[].id" -o tsv
returns the text: /subscriptions/20f4b944-fc7a-4d38-b02c-900c8223c3a0/resourceGroups/2568d0d0-efe3-4d04-a08f-df7f009f822a/providers/Microsoft.Network/networkInterfaces/SampleVMVMNic


Next unit: Exercise - Start and stop your VM with the Azure CLI


7- Exercise - Start and stop your VM with the Azure CLI

One of the main tasks you'll want to do while running virtual machines is to start and stop them.

Stop a VM
We can stop a running VM with the vm stop command. You must pass the name and resource group, or the unique ID for the VM:

Azure CLI

Copy
az vm stop \
    --name SampleVM \
    --resource-group "[sandbox resource group name]"
We can verify the VM has stopped by attempting to ping the public IP address, using ssh, or through the vm get-instance-view command. This final approach returns the same basic data as vm show, but includes details about the instance itself. Try entering the following command into Azure Cloud Shell to see the current running state of your VM:

Azure CLI

Copy
az vm get-instance-view \
    --name SampleVM \
    --resource-group "[sandbox resource group name]" \
    --query "instanceView.statuses[?starts_with(code, 'PowerState/')].displayStatus" -o tsv
This command should return VM stopped as the result.

Start a VM
We can do the reverse through the vm start command.

Azure CLI

Copy
az vm start \
    --name SampleVM \
    --resource-group "[sandbox resource group name]"
This command will start a stopped VM. We can verify it through the vm get-instance-view query we used in the last section, which should now return VM running.

Restart a VM
Finally, we can restart a VM if we've made changes that require a reboot by running the vm restart command. You can add the --no-wait flag if you want the Azure CLI to return immediately without waiting for the VM to reboot.

Next unit: Exercise - Install software on your VM


8- Exercise - Install software on your VM

The last thing we want to try on our VM is to install a web server. One of the easiest packages to install is nginx.

Install NGINX web server
Locate the public IP address of your SampleVM Linux virtual machine.

Azure CLI

Copy
az vm list-ip-addresses --name SampleVM --output table
Next, open an ssh connection to SampleVM using the Public IP address from the preceding step.

Bash

Copy
ssh azureuser@<PublicIPAddress>
After you're logged in to the virtual machine, run the following command to install the nginx web server. The command will take a few moments to complete.

Bash

Copy
sudo apt-get -y update && sudo apt-get -y install nginx
Exit the Secure Shell:

Bash

Copy
exit
Retrieve your default page
In Azure Cloud Shell, use curl to read the default page from your Linux web server by running the following command, replacing <PublicIPAddress> with the public IP you found previously. You can also open a new browser tab and try to browse to the public IP address.

Bash

Copy
curl -m 80 <PublicIPAddress>
This command will fail, because the Linux virtual machine doesn't expose port 80 (http) through the network security group that secures the network connectivity to the virtual machine. We can fix the failure by running the Azure CLI command vm open-port.

Enter the following command into Cloud Shell to open port 80:

Azure CLI

Copy
az vm open-port \
    --port 80 \
    --resource-group "[sandbox resource group name]" \
    --name SampleVM
It will take a moment to add the network rule and open the port through the firewall.

Run the curl command again.

Bash

Copy
curl -m 80 <PublicIPAddress>
This time, it should return data like the following. You can see the page in a browser as well.

HTML

Copy
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
body {
    width: 35em;
    margin: 0 auto;
    font-family: Tahoma, Verdana, Arial, sans-serif;
}
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support, refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>


Next unit: Summary and cleanup


9- Summary and cleanup

You've created a new Linux virtual machine, changed its size, stopped and started it, and updated the configuration with the Azure CLI.

Clean up
The sandbox automatically cleans up your resources when you're finished with this module.

When you're working in your own subscription, it's a good idea at the end of a project to identify whether you still need the resources you created. Resources that you leave running can cost you money. You can delete resources individually or delete the resource group to delete the entire set of resources.

Additional resources
Azure CLI overview
Azure CLI command reference
Check your knowledge

1. Suppose you're an administrator of several Azure virtual machines. You get a text message indicating some problems with your VMs. You are at a friend’s house and only have your tablet with you. True or false: you'll still be able to access the Azure CLI using the tablet, even though you can't install the CLI on it. 

True

False

2. Suppose you have a script that creates several VMs with different images. When the script issues the command to create the first VM you don't want to block the script while the VM is created, instead you want the script to immediately move on to the next command. What is the best way to do this? 

Add the '--async' argument to your create command.

Use the ampersand (&) to run the process in the background.

Add the '--no-wait' argument to your create command.

3. Most Azure commands return JSON by default. Sometimes this dataset can be large, which makes it difficult to read and tricky to use the result of one command as input to another command. What can you use with Azure CLI to filter the results to get only the data that you need? 

You can use the '--query' argument.

You can use the '--filter' argument.

You can pipe the results to a JSON parsing utility and use filtering capability there.






Point 7: Create a Windows virtual machine in Azure

Azure virtual machines (VMs) enable you to create dedicated compute resources in minutes that can be used just like a physical desktop or server machine.


Learning objectives
In this module, you will:

Create a Windows virtual machine using the Azure portal.
Connect to a running Windows virtual machine using Remote Desktop.
Install software and change the network configuration on a VM using the Azure portal.


1- Introduction

Imagine that you work for a company that does video-data processing and pattern analysis. You're building a new prototype platform to process the video from traffic cameras, analyze trends, and provide actionable data for traffic and road improvements.

To improve your algorithms, you've made arrangements with several new cities to collect their traffic-camera data. However, not all of the video data is in the same format, and many of the formats only have Windows codecs to decode the data. Therefore, you've decided to use Virtual Machines (VMs) to do the initial processing and then push the data onto Azure Functions, which will process a standard format. This approach allows you to bring on new data formats dynamically without stopping the entire system.

Azure provides a robust virtual machine hosting solution that can meet your needs. Let's explore how to create and work with Windows virtual machines in Azure.

Learning objectives
In this module, you'll:

Understand the options that are available for virtual machines in Azure.
Create a Windows virtual machine using the Azure portal.
Connect to a running Windows virtual machine using Remote Desktop.
Install software and change the network configuration on a VM using the Azure portal.
Prerequisites
Basic understanding of Azure Virtual Machines from Introduction to Azure Virtual Machines
Remote Desktop client



Next unit: Create a Windows virtual machine in Azure

2- Create a Windows virtual machine in Azure

Your company has decided to manage the video data from their traffic cameras in Azure using VMs. In order to run the multiple codecs, we first need to create the VMs. We also need to connect and interact with the VMs. In this unit, you'll learn how to create a VM using the Azure portal. You'll configure the VM for remote access, select a VM image, and choose the proper storage option.

Introduction to Windows virtual machines in Azure
Azure VMs are an on-demand, scalable cloud-computing resource. They're similar to virtual machines that are hosted in Windows Hyper-V. They include processor, memory, storage, and networking resources. You can start and stop virtual machines at will, just like with Hyper-V, and manage them from the Azure portal or with the Azure CLI. You can also use a Remote Desktop Protocol (RDP) client to connect directly to the Windows desktop user interface (UI) and use the VM as if you were signed in to a local Windows computer.

Create an Azure VM
You can define and deploy VMs on Azure in several ways: the Azure portal, a script (using the Azure CLI or Azure PowerShell), or through an Azure Resource Manager template. In all cases, you'll need to supply several pieces of information, which we'll cover shortly.

The Azure Marketplace also provides preconfigured images that include both an OS and popular software tools installed for specific scenarios.

Screenshot showing the Azure Marketplace list of Virtual Machines.

Resources used in a Windows VM
When creating a Windows VM in Azure, you also create resources to host the VM. These resources work together to virtualize a computer and run the Windows operating system. These must either exist (and be selected during VM creation), or they'll be created with the VM.

A virtual machine that provides CPU and memory resources
An Azure Storage account to hold the virtual hard disks
Virtual disks to hold the OS, applications, and data
A virtual network (VNet) to connect the VM to other Azure services or your own on-premises hardware
A network interface to communicate with the VNet
A public IP address so you can access the VM (this is optional)
Like other Azure services, you'll need a resource group to contain the VM (and optionally group these resources together for administration). When you create a new VM, you can either use an existing resource group or create a new one.

Choose the VM image
Selecting an image is one of the first and most important decisions you'll make when creating a VM. An image is a template that's used to create a VM. These templates include an OS and often other software, such as development tools or web-hosting environments.

You can include any application that the computer can support in the VM image. You can create a VM from an image that's preconfigured to exactly match your requirements, such as hosting an ASP.NET Core app.

 Tip

You can also create and upload your own images. Check the documentation for more information.

Size your VM
Just as a physical machine has a certain amount of memory and CPU power, so does a virtual machine. Azure offers a range of VMs of differing sizes at different price points. The size that you choose will determine the VM's processing power, memory, and max storage capacity.

 Warning

There are quota limits on each subscription that can impact VM creation. In the classic deployment model, you can't have more than 20 virtual cores across all VMs within a region. You can either split up VMs across regions or file an online request to increase your limits.

VM sizes are grouped into categories, starting with the B-series for basic testing, and running up to the H-series for massive computing tasks. You should select the VM's size based on the workload you want to perform. It's possible to change a VM's size after it's been created, but the VM must be stopped first, so it's best to size it appropriately from the start if possible.

Here are some guidelines based on the scenario you're targeting:
What are you doing?	Consider these sizes
General use computing/web: Testing and development, small to medium databases, or low to medium traffic web servers	B, Dsv3, Dv3, DSv2, Dv2
Heavy computational tasks: Medium traffic web servers, network appliances, batch processes, and application servers	Fsv2, Fs, F
Large memory usage: Relational database servers, medium to large caches, and in-memory analytics.	Esv3, Ev3, M, GS, G, DSv2, Dv2
Data storage and processing: Big Data, SQL, and NoSQL databases, which need high disk throughput and IO	Ls
Heavy graphics rendering or video editing, as well as model training and inferencing (ND) with deep learning	NV, NC, NCv2, NCv3, ND
High-performance computing (HPC): If you need the fastest and most powerful CPU virtual machines with optional high-throughput network interfaces	H
Choose storage options
The next set of decisions revolves around storage. First, you can choose the disk technology. Options include a traditional platter-based hard disk drive (HDD) or a more modern solid-state drive (SSD). Just like the hardware you purchase, SSD storage costs more, but provides better performance.

 Tip

There are two levels of SSD storage available: Standard and Premium. Choose Standard SSD disks if you have normal workloads but want better performance. Choose Premium SSD disks if you have I/O intensive workloads or mission-critical systems that need to process data very quickly.

Map storage to disks
Azure uses virtual hard disks (VHDs) to represent physical disks for the VM. VHDs replicate the logical format and data of a disk drive, but are stored as page blobs in an Azure Storage account. You can choose on a per-disk basis what type of storage it should use (SSD or HDD). This allows you to control each disk's performance, likely based on the I/O you plan to perform on it.

By default, two virtual hard disks (VHDs) will be created for your Windows VM:

The Operating System disk: This is your primary or C: drive and has a maximum capacity of 2048 GB.

A Temporary disk: This provides temporary storage for the OS or any apps. It's configured as the D: drive by default and is sized based on the VM size, making it an ideal location for the Windows paging file.

 Warning

The temporary disk is not persistent. You should only write data to this disk that you're willing to lose at any time.

What about data?
You can store data on the C: drive along with the OS, but a better approach is to create dedicated data disks. You can create and attach additional disks to the VM. Each data disk can hold up to 32,767 gibibytes (GiB) of data, with the maximum amount of storage determined by the VM size you select.

 Note

An interesting capability is to create a VHD image from a real disk. This allows you to easily migrate existing information from an on-premises computer to the cloud.

Unmanaged vs. Managed disks
The final storage choice you'll make is whether to use unmanaged or managed disks.

With unmanaged disks, you're responsible for the storage accounts that are used to hold the VHDs corresponding to your VM disks. You pay the storage account rates for the amount of space you use. A single storage account has a fixed rate limit of 20,000 I/O operations/sec. This means that a single storage account is capable of supporting 40 standard virtual hard disks at full throttle. If you need to scale out, then you need more than one storage account, which can get complicated.

Managed disks are the newer (and recommended) disk-storage model. They elegantly solve the complexity of unmanaged disks by putting the burden of managing the storage accounts onto Azure. You specify the disk type (Premium or Standard) and the disk size, and Azure creates and manages both the disk and the storage it uses. You don't have to worry about storage account limits, which makes them easier to scale out. They also offer several other benefits:

Increased reliability: Azure ensures that VHDs associated with high-reliability VMs will be placed in different parts of Azure storage to provide similar levels of resilience.
Better security: Managed disks are truly managed resources in the resource group. This means they can use role-based access control (RBAC) to restrict who can work with the VHD data.
Snapshot support: You can use snapshots to create a read-only copy of a VHD. You have to shut down the owning VM, but creating the snapshot only takes a few seconds. Once it's done, you can power on the VM and use the snapshot to create a duplicate VM to troubleshoot a production issue or roll back the VM to the point in time that the snapshot was taken.
Backup support: You can automatically back up managed disks to different regions for disaster recovery with Azure Backup, all without affecting the service of the VM.
Network communication
Virtual machines communicate with external resources using a virtual network (VNet). The VNet represents a private network in a single region on which your resources communicate. A virtual network is just like the networks you manage on-premises. You can divide them up with subnets to isolate resources, connect them to other networks (including your on-premises networks), and apply traffic rules to govern inbound and outbound connections.

Plan your network
When you create a new VM, you'll have the option of creating a new virtual network or using an existing VNet in your region.

Having Azure create the network together with the VM is simple, but it's likely not ideal for most scenarios. It's better to plan your network requirements up front for all the components in your architecture and create the VNet structure you'll need separately and then create the VMs and place them into the already-created VNets.

We'll look more at virtual networks a bit later in this module. Let's apply some of this knowledge and create a VM in Azure.

Next unit: Exercise - Create a Windows virtual machine


3- Exercise - Create a Windows virtual machine

Your company processes video content on Windows VMs. A new city has contracted with your company to process their traffic cameras, but it's a model with which you haven't worked before. You need to create a new Windows VM and install some proprietary codecs in order to process and analyze the new video content.

Create a new Windows virtual machine
You can create Windows VMs with the Azure portal, Azure CLI, or Azure PowerShell. The best approach is to use the portal, because the Create a virtual machine wizard collects all the required information and provides hints and validation messages throughout the process.

Sign in to the Azure portal using the same account you used to activate the sandbox.

On the Azure portal, under Azure services, select Create a resource. The Create a resource pane appears.

In Search services and marketplace search box, search for Windows Server and press Enter. Select Windows Server by Microsoft. The Windows Server pane appears.

There are several Windows Server options to choose from to create your VM. In the Plan dropdown list, scroll down, and select [smalldisk] Windows Server 2019 Datacenter.

Select Create. The Create a virtual machine pane appears.

Configure the VM settings
Azure presents a wizard as a series of tabs to walk you through all the configuration details for creating the VM. The first tab is Basics. You can select Next or Previous to move from one tab to another, or you can select any tab in the horizontal menu to move to a customizable configuration section.

Screenshot showing **Basics** tab of the **Create a virtual machine** pane.

Configure basic VM settings
 Note

As you add or change settings in the wizard, Azure validates each value and places a green check mark next to a validated field, or red error indicator below the field. You can hover over an error indicator to get more information about a validation issue.

 Note

It's a best practice to use a standard naming convention for resource names so you can easily identify their purpose. Windows VM names are a bit limited; they must be between 1 and 15 characters, cannot contain non-ASCII or special characters, and must be unique in the current resource group.

On the Basics tab, enter the following values for each setting.

Setting	Value
Project details	
Subscription	Concierge Subscription (the subscription that should be billed for VM hours).
Resource Group	Select [sandbox resource group name].
Instance details	
Virtual machine name	Enter a name for your VM, such as test-vp-vm2 (for Test Video Processor VM #2).
Region	Select a region close to you from the global regions listed in the following table.
Availability options	Accept default No infrastructure redundancy required. This option is used to ensure the VM is highly available by grouping multiple VMs together to deal with planned or unplanned maintenance events or outages.
Security type	Standard
Image	Select [smalldisk] Windows Server 2019 Datacenter - x64 Gen2 from the dropdown list.
VM architecture	Accept default (x64)
Run with Azure Spot discount	Accept default (unchecked).
Size	The Size field isn't directly editable. Select or accept the default Standard DS1 v2, which will give the VM 1 CPU and 3.5 GB of memory. Optionally, select the field to view recommended or recently chosen sizes; select See all sizes to explore filters for sizes based on vCPUs, RAM, Data disks, operations per second, and cost. Select the X in the top right of the pane to close the pane.
Administrator account	
Username	Enter a username you'll use to sign in to the VM.
Password	Enter a password that's at least 12 characters long and has at least three of the following four characteristics: one lower case character, one uppercase character, one number, and one special character that isn't '\' or '-'. Use something you'll remember or write it down, as you'll need it later.
Confirm password	Confirm your password.
Inbound port rules	
Public inbound ports	Select Allow selected ports. We want to be able to access the desktop for this Windows VM using RDP.
Select inbound ports	Select RDP (3389) from the dropdown list. As the note in the UI indicates, we can also adjust the network ports after we create the VM.
Licensing	
Would you like to use an existing Windows Server License	Leave unchecked
The free sandbox allows you to create resources in a subset of the Azure global regions. Select a region from the following list when you create resources:

West US 2
South Central US
Central US
East US
West Europe
Southeast Asia
Japan East
Brazil South
Australia Southeast
Central India
Select Next : Disks.

 Tip

You can use the horizonal scroll bar to slide the view to the left to get back to the VM settings, which had opened a new pane to the right.

Configure disks for the VM
On the Disks tab, enter or select the following values for each setting.

Setting	Value
Disk options	
Encryption at host	Accept the default (unchecked)
OS disk size	Accept the default Image default (30 GiB).
OS disk type	Accept the default Premium SSD (locally redundant storage).
Delete with VM	Accept the default (checked)
Key management	Accept the default.
Enable Ultra Disk compatibility	Accept the default (unchecked)
Data disks	
Select Create and attach a new disk link. The Create a new disk pane appears.	Accept all the default values for the following settings: Name; Source type; Size; Key management; and Enable shared disk. This is where you could use a snapshot, or Storage Blob, to create a VHD.
Select OK to save the settings and close the pane.

Screenshot showing the configure disks section for the VM.

On the Create a virtual machine pane Disks tab, under Data disks, there should now be a new row showing the newly configured disk.

Screenshot showing the newly added disk in the VM.

Configure the network
Select Next : Networking.

In a production system where other components are already in use, it would be important to use an existing virtual network so that the VM can communicate with the other cloud services in the production solution. If no virtual network has been defined in this location, create it here and configure the:

Subnet: First subnet to subdivide the address space; it must fit within the defined address space. After the VNet is created, you can add more subnets.
Public IP: Overall IPV4 space available to this network.
On the Networking tab, let's change some of the settings. Under the input field for Virtual network, select Create new. The Create virtual network pane appears.

On the Create virtual network pane, enter the following values for each setting.

Setting	Value
Name	Accept the default name.
Address space	
Address range	In the row below the heading, enter 172.16.0.0/16 to give the address space a full range of addresses, then check the box next to the address you just entered. If another address range row exists, select the Delete icon to delete it.
Subnets	
Subnet name	Enter default in the first input field, then select the checkbox next to the name you just entered. If another row exists, select it to delete it.
Address range	In the empty input field, enter 172.16.1.0/24 to give the subnet 256 IP addresses of space.
Select OK to save your settings and return to the Create a virtual machine pane.

 Note

By default, Azure will create a virtual network, network interface, and public IP for your VM. It's not trivial to change networking options after the VM is created, so always double-check the network assignments for services you create in Azure.

Finish configuring the VM and create the image
On the Create a virtual machine pane, the rest of the tabs have reasonable defaults and there's no need to change any of them. You can explore the other tabs if you like. Each field has an (i) icon next to it which, if selected, will show a detailed definition of that configuration setting. Reviewing field descriptions is a great way to learn about the settings you can use to configure the VM.

Select Review + create. The system will validate your options and display details about the VM being created.

Select Create to deploy the VM. The Azure dashboard will show the name of the VM that's being deployed and details about your deployment. Deployment may take several minutes.

After deployment completes, select Go to resource. Your virtual machine pane appears.

Now, let's look at what we can do with this VM.

Next unit: Use RDP to connect to Windows Azure virtual machines



4- Use RDP to connect to Windows Azure virtual machines

Now that you have a Windows VM in Azure, the next thing you’ll do is put your applications and data on those VMs to process our traffic videos.

However, unless you’ve set up a site-to-site VPN to Azure, your Azure VMs won’t be accessible from your local network. If you’re just getting started with Azure, it’s unlikely that you have a working site-to-site VPN, so how can you transfer files to Azure VMs? One easy way is to use Azure’s Remote Desktop Connections feature to share your local drives with your new Azure VMs.

Now that we have a new Windows virtual machine, we need to install our custom software onto it. There are several options to choose from:

Remote Desktop Protocol (RDP)
Custom scripts
Custom VM images (with the software preinstalled)
Let's look at the simplest approach for Windows VMs: Remote Desktop.

What is the Remote Desktop Protocol?
Remote Desktop Protocol (RDP) provides remote connectivity to the UI of Windows-based computers. RDP lets you sign in to a remote physical or virtual Windows computer and control that computer as if you were seated at the console. An RDP connection allows you to carry out most operations that you can do from a physical computer's console, except for some power and hardware-related functions.

An RDP connection requires an RDP client. Microsoft provides RDP clients for the following operating systems:

Windows (built-in)
macOS
iOS
Android
The following screenshot displays the Remote Desktop Protocol client in Windows 10.

Screenshot of the user interface of the Remote Desktop Protocol client.

There are also open-source Linux clients, such as Remmina, that allow you to connect to a Windows PC from an Ubuntu distribution.

Connecting to an Azure VM
As we learned a moment ago, Azure VMs communicate on a virtual network. They can also have an optional public IP address assigned to them. With a public IP, we can communicate with the VM over the Internet. Alternatively, we can set up a virtual private network (VPN) that connects our on-premises network to Azure, letting us securely connect to the VM without exposing a public IP. This approach is covered in another module, and is fully documented if you're interested in exploring that option.

One thing to be aware of with public IP addresses in Azure is they're often dynamically allocated. That means the IP address can change over time; for VMs, this happens when the VM is restarted. You can pay more to assign static addresses if you want to connect directly to an IP address instead of a name and need to ensure that the IP address won't change.

How do you connect to a VM in Azure using RDP?
Connecting to a VM in Azure using RDP is a simple process. In the Azure portal, you'll go to your VM's properties, and at the top, select Connect. This shows you the IP addresses assigned to the VM and give you the option to download a preconfigured.rdp file that Windows then opens in the RDP client. You can choose to connect over the public IP address of the VM in the RDP file. Instead, if you're connecting over VPN or ExpressRoute, you can select the internal IP address. You can also select the port number for the connection.

If you're using a static public IP address for the VM, you can save the .rdp file to your desktop. If you're using dynamic IP addressing, the .rdp file only remains valid while the VM is running. If you stop and restart the VM, you must download another .rdp file.

 Tip

You can also enter the public IP address of the VM into the Windows RDP client and select Connect.

When you connect, you'll typically receive two warnings. These are:

Publisher warning: caused by the .rdp file not being publicly signed
Certificate warning: caused by the machine certificate not being trusted
In test environments, you can ignore these warnings. In production environments, the .rdp file can be signed using RDPSIGN.EXE and the machine certificate placed in the client's Trusted Root Certification Authorities store.

Let's try using RDP to connect to our VM.

Next unit: Exercise - Connect to a Windows virtual machine using RDP



5- Exercise - Connect to a Windows virtual machine using RDP

We have our Windows VM deployed and running, but it's not configured to do any work.

Recall that our scenario is a video-processing system. Our platform receives files through FTP. The traffic cameras upload video clips to a known URL, which is mapped to a folder on the server. The custom software on each Windows VM runs as a service and watches the folder and processes each uploaded clip. It then passes the normalized video to our algorithms running on other Azure services.

There are a few things we'd need to configure to support this scenario:

Install FTP and open the ports it needs to communicate
Install the proprietary video codec unique to the city's camera system
Install our transcoding service that processes uploaded videos
Many of these are typical administrative tasks we won't actually cover here, and we don't have software to install. Instead, we'll walk through the steps and show you how you could install custom or third-party software using Remote Desktop. Let's start by getting the connection information.

Connect to the VM with Remote Desktop Protocol
To connect to an Azure VM with an RDP client, you'll need:

Public IP address of the VM (or private if the VM is configured to connect to your network)
Port number
You can enter this information into the RDP client, or download a pre-configured RDP file.

 Note

An RDP file is a text file that contains a set of name/value pairs that define the connection parameters for an RDP client to connect to a remote computer using the Remote Desktop Protocol.

Download the RDP file
In the Azure portal, ensure the Overview pane for the virtual machine that you created earlier is open. You can also find the VM on the Azure Home page under All Resources if you need to open it. The Overview pane has a lot of information about the VM. You can:

Determine whether the VM is running
Stop or restart it
Get the public IP address to connect to the VM
Get the activity of the CPU, disk, and network
In the top menu bar, select Connect, then select Connect in the drop-down.

Note the IP address and Port number settings, then select Download RDP File and save it to your computer.

Before we connect, let's adjust a few settings. On Windows, find the file using Explorer, right-click it, and select Edit (you might need to select Show more options to find the Edit option). On macOS, you'll need to open the file first with the RDP client, then right-click on the item in the displayed list and select Edit.

You can adjust a variety of settings to control the experience in connecting to the Azure VM. The settings you'll want to examine are:

Display: By default, it'll be full screen. You can change this to a lower resolution, or use all your monitors if you have more than one.
Local Resources: You can share local drives with the VM, allowing you to copy files from your PC to the VM. Select the More button under Local devices and resources to select what is shared.
Experience: Adjust the visual experience based on your network quality.
Share your Local C: drive so it will be visible to the VM.

Switch back to the General tab and select Save to save the changes. You can always come back and edit this file later to try other settings.

Connect to the Windows VM
Select Connect.

On the Remote Desktop Connection dialog box, note the security warning and the remote computer IP address, then select Connect to start the connection to the VM.

In the Windows Security dialog box, enter your username and password that you created in the previous exercise.

 Note

If you're using a Windows client to connect to the VM, it will default to known identities on your machine. Select the More choices option, and then select Use a different account that lets you enter a different username/password combination.

In the second Remote Desktop Connection dialog box, note the certificate errors, then select Yes.

Install worker roles
The first time you connect to a Windows server VM, it launches Server Manager. This allows you to assign a worker role for common web or data tasks. You can also launch the Server Manager through the Start menu.

This is where we'd add the Web Server role to the server. This installs IIS, and as part of the configuration, you'd turn off HTTP requests and enable the FTP server. We could also ignore IIS and install a third-party FTP server. We'd then configure the FTP server to allow access to a folder on our big data drive we added to the VM.

Because we aren't going to actually configure that here, just close Server Manager.

Install custom software
We have two approaches we can use to install software. First, this VM is connected to the internet. If the software you need has a downloadable installer, you can open a web browser in the RDP session, download the software, and install it. Second, if your software is custom, like our custom service, you can copy it from your local machine over to the VM to install it. Let's look at this latter approach.

Open File Explorer. In the sidebar, select This PC. You should see several drives:

Windows (C:) drive representing the OS
Temporary Storage (D:) drive
Your local C: drive (it will have a different name than the following screenshot)
Screenshot showing the local drive shared with the Azure VM.

With access to your local drive, you can copy the files for the custom software onto the VM and install the software. We won't actually do that because it's just a simulated scenario, but you can imagine how it would work.

The more interesting thing to observe in the list of drives is what is missing. Notice that our Data drive is not present. Azure added a VHD, but didn't initialize it.

Initialize data disks
Any additional drives you create from scratch will need to be initialized and formatted. The process for doing this is identical to a physical drive.

Launch the Disk Management tool from the Start menu. You might have to go to the Computer Management tool first, then Disk Management, or try searching for Disk Management in the Start Menu.

The Disk Management tool will display a warning that it has detected an uninitialized disk.

Screenshot showing the Disk Management tool warning about an uninitialized data disk in the VM.

Select OK to initialize the disk. It'll then appear in the list of volumes, where you can format it and assign a drive letter.

Open File Explorer, and you should now have your data drive.

Go ahead and close the RDP client to disconnect from the VM. The server will continue to run.

RDP allows you to work with the Azure VM just like a local computer. With Desktop UI access, you can administer this VM as you would any Windows computer; installing software, configuring roles, adjusting features and other common tasks. However, it's a manual process. If you always need to install some software, you might consider automating the process using scripting.


Next unit: Configure Azure virtual machine network settings


6- Configure Azure virtual machine network settings

We've installed our custom software, set up an FTP server, and configured the VM to receive our video files. However, if we try to connect to our public IP address with FTP, we'll find that it's blocked.

Making adjustments to server configuration is commonly performed with equipment in your on-premises environment. In this sense, you can consider Azure VMs to be an extension of that environment. You can make configuration changes, manage networks, open or block traffic, and more through the Azure portal, Azure CLI, or Azure PowerShell tools.

You've already seen some of the basic information and management options in the Overview panel for the virtual machine. Let's explore network configuration a bit more.

Open ports in Azure VMs
By default, new VMs are locked down.

Apps can make outgoing requests, but the only inbound traffic allowed is from the virtual network (for example, other resources on the same local network), and from Azure's Load Balancer (probe checks).

There are two steps to adjusting the configuration to support FTP. When you create a new VM, you have an opportunity to open a few common ports (RDP, HTTP, HTTPS, and SSH). However, if you require other changes to the firewall, you will need to make them yourself.

The process for this involves two steps:

Create a Network Security Group.
Create an inbound rule allowing traffic on port 20 and 21 for active FTP support.
What is a Network Security Group?
Virtual networks (VNets) are the foundation of the Azure networking model, and provide isolation and protection. Network Security Groups (NSGs) are the main tool you use to enforce and control network traffic rules at the networking level. NSGs are an optional security layer that provides a software firewall by filtering inbound and outbound traffic on the VNet.

Security groups can be associated to a network interface (for per-host rules), a subnet in the virtual network (to apply to multiple resources), or both levels.

Security group rules
NSGs use rules to allow or deny traffic moving through the network. Each rule identifies the source and destination address (or range), protocol, port (or range), direction (inbound or outbound), a numeric priority, and whether to allow or deny the traffic that matches the rule. The following illustration shows NSG rules applied at the subnet and network-interface levels.

Illustration showing the architecture of network security groups in two different subnets. In one subnet, there are two virtual machines, each with their own network interface rules. The subnet itself has a set of rules that applies to both the virtual machines.

Each security group has a set of default security rules to apply the default network rules described in the preceding passage. You can't modify these default rules, but you can override them.

How Azure uses network rules
For inbound traffic, Azure processes the security group associated to the subnet, then the security group applied to the network interface. Outbound traffic is processed in the opposite order (the network interface first, followed by the subnet).

 Warning

Keep in mind that security groups are optional at both levels. If no security group is applied, then all traffic is allowed by Azure. If the VM has a public IP, this could be a serious risk, particularly if the OS doesn't provide some sort of firewall.

The rules are evaluated in priority order, starting with the lowest priority rule. Deny rules always stop the evaluation. For example, if an outbound request is blocked by a network interface rule, any rules applied to the subnet will not be checked. In order for traffic to be allowed through the security group, it must pass through all applied groups.

The last rule is always a Deny All rule. This is a default rule added to every security group for both inbound and outbound traffic with a priority of 65500. That means to have traffic pass through the security group, you must have an allow rule or the default final rule will block it. Learn more about security rules.

 Note

SMTP (port 25) is a special case. Depending on your subscription level and when your account was created, outbound SMTP traffic might be blocked. You can make a request to remove this restriction with business justification.

Next unit: Summary


Summary

In this module, you learned how to create a Windows VM using the Azure portal. You then connected to the VM's public IP address and managed it over RDP. You discovered how RDP in Azure provides a similar experience to logging on interactively to a physical computer.

You learned that while RDP allows us to interact with the operating system and software of the virtual machine, the portal allows us to configure the virtual hardware and connectivity. We also could have used PowerShell or the Azure CLI, if we preferred a command-line or scriptable environment.

Clean up
The sandbox automatically cleans up your resources when you're finished with this module.

When you're working in your own subscription, it's a good idea at the end of a project to identify whether you still need the resources you created. Resources that you leave running can cost you money. You can delete resources individually or delete the resource group to delete the entire set of resources.

Knowledge check

1. When you create a Windows virtual machine in Azure, which port would you open using the INBOUND PORT RULES in order to allow remote-desktop access? 

HTTPS

SSH (22)

RDP (3389)

2. Suppose you have an application running on a Windows virtual machine in Azure. What is the best-practice guidance on where the app should store data files? 

OS disk (C:)

Temporary disk (D:)

Attached data disk

3. What is the final rule that is applied in every Network Security Group? 

Allow All

Deny All

You configure the final rule to your needs.



Point 8: Host a web application with Azure App Service

Azure App Service enables you to build and host web applications in the programming language of your choice without managing infrastructure. Learn how to create a website through the hosted web app platform in Azure App Service.


Learning objectives
In this module, you will:

Use the Azure portal to create an Azure App Service web app.
Use developer tools to create the code for a starter web application.
Deploy your code to Azure App Service.


1- Introduction

Imagine you're building a website for a new business, or you're running an existing web app on an aging on-premises server. Setting up a new server can be challenging. You need appropriate hardware, likely a server-level operating system, and a web-hosting stack.

Once it's running, you need to maintain the server. And what happens if your website traffic increases? You might need to invest in additional hardware.

Hosting your web application using Azure App Service makes deploying and managing a web app much easier when compared to managing a physical server. In this module, we'll implement and deploy a web app to App Service.

Learning objectives
In this module, you'll:

Use the Azure portal to create an Azure App Service web app.
Use developer tools to create the code for a starter web application.
Deploy your code to App Service.
Prerequisites
Ability to navigate the Azure portal
Ability to use a command-line interface



Next unit: Create a web app in the Azure portal


2- Create a web app in the Azure portal

In this unit, you'll learn how to create an Azure App Service web app using the Azure portal.

Why use the Azure portal?
The first step in hosting your web application is to create a web app (an Azure App Service app) inside your Azure subscription.

There are several ways you can create a web app. You can use the Azure portal, the Azure Command Line Interface (CLI), a script, or an integrated development environment (IDE) like Visual Studio.

The information in this unit discusses how to use the Azure portal to create a web app, and you'll use this information to create a web app in the next exercise. For this module, we'll demonstrate using the Azure portal because it's a graphical experience, which makes it a great learning tool. The portal helps you discover available features, add other resources, and customize existing resources.

What is Azure App Service?
Azure App Service is a fully managed web application hosting platform. This platform as a service (PaaS) offered by Azure allows you to focus on designing and building your app while Azure takes care of the infrastructure to run and scale your applications.

Deployment slots
Using the Azure portal, you can easily add deployment slots to an App Service web app. For instance, you can create a staging deployment slot where you can push your code to test on Azure. Once you're happy with your code, you can easily swap the staging deployment slot with the production slot. You do all this with a few mouse clicks in the Azure portal.

Screenshot of the staging deployment slot to test the deployments.

Continuous integration/deployment support
The Azure portal provides out-of-the-box continuous integration and deployment with Azure Repos, GitHub, Bitbucket, FTP, or a local Git repository on your development machine. Connect your web app with any of the preceding sources, and App Service will do the rest for you by automatically syncing your code and any future changes on the code into the web app. Furthermore, with Azure Repos, you can define your own build and release process that compiles your source code, runs the tests, builds a release, and finally deploys the release into your web app every time you commit the code. All that happens implicitly, without any need for you to intervene.

Screenshot of setting up deployment options and choosing source for the deployment source code.

Integrated Visual Studio publishing and FTP publishing
In addition to being able to set up continuous integration/deployment for your web app, you can always benefit from the tight integration with Visual Studio to publish your web app to Azure via Web Deploy technology. App Service also supports FTP-based publishing for more traditional workflows.

Built-in autoscale support (automatic scale-out based on real-world load)
Scaling up/down or scaling out is baked into the web app. Depending on the web app's usage, you can scale your app up/down by increasing/decreasing the resources of the underlying machine that's hosting your web app. Resources can be the number of cores or the amount of RAM available.

Scaling out, on the other hand, is the ability to increase the number of machine instances that are running your web app.

Creating a web app
When you're ready to run a web app on Azure, you can visit the Azure portal and create a Web App resource. Creating a web app allocates a set of hosting resources in App Service, which you can use to host any web-based application Azure supports, whether it's ASP.NET Core, Node.js, Java, Python, and so on.

The Azure portal provides a wizard to create a web app. This wizard requires the following fields:

Field	Description
Subscription	A valid and active Azure subscription.
Resource group	A valid resource group.
Name	The name of the web app. This name becomes part of the app's URL, so it must be unique among all Azure App Service web apps.
Publish	You can deploy your application to App Service as code or as a ready-to-run Docker Container. Selecting Container will activate the wizard's Container tab, where you'll provide information about the Docker registry from which App Service will retrieve your image.
Runtime stack	If you choose to deploy your application as code, App Service needs to know what runtime your application uses (examples include Node.js, Python, Java, and .NET). If you deploy your application as a container, you won't need to choose a runtime stack, because your image includes it.
Operating system	App Service can host applications on Windows or Linux servers. For more information, see the Operating systems section in this unit.
Region	The Azure region from which your application will be served.
Pricing Plans	See the Pricing Plans section in this unit for information about App Service plans.
Operating systems
If you're deploying your app as code, many of the available runtime stacks are limited to one operating system or the other. After you choose a runtime stack, the toggle will indicate whether or not you have a choice of operating system. If your target runtime stack is available on both operating systems, select the one that you use to develop and test your application.

If your application is packaged as a container, specify the operating system in your container.

App Service plans
An App Service plan is a set of virtual server resources that run App Service apps. A plan's size (sometimes referred to as its sku or pricing tier) determines the performance characteristics of the virtual servers that run the apps assigned to the plan, and the App Service features to which those apps have access. Every App Service web app you create must be assigned to a single App Service plan that runs it.

A single App Service plan can host multiple App Service web apps. In most cases, the number of apps you can run on a single plan is limited by the apps' performance characteristics and the plan's resource limitations.

App Service plans App Service's unit of billing. The size of each App Service plan in your subscription, in addition to the bandwidth resources the apps deployed to those plans use, determines the price you pay. The number of web apps deployed to your App Service plans has no effect on your bill.

You can use any of the available Azure management tools to create an App Service plan. When you create a web app via the Azure portal, the wizard helps you to create a new plan at the same time if you don't already have one.

Next unit: Exercise - Create a web app in the Azure portal


3- Exercise - Create a web app in the Azure portal

In this unit, you use the Azure portal to create a web app.

Create a web app
Sign in to the Azure portal using the same account you used to activate the sandbox.

On the Azure portal menu, or from the Home page, select Create a resource. Everything you create on Azure is a resource. The Create a resource pane appears.

Here, you can search for the resource you want to create, or select one of the popular resources that people create in the Azure portal.

In the Create a resource menu, select Web.

Select Web App. If you don't see it, in the search box, search for and select Web App. The Create Web App resource pane appears.

On the Basics tab, enter the following values for each setting.

Setting	Value	Details
Project Details		
Subscription	Concierge Subscription	The web app you're creating must belong to a resource group. Here, you select the Azure subscription to which the resource group belongs (or is to belong, if you're creating it within the wizard).
Resource Group	From the dropdown list, select [Sandbox resource group]	The resource group to which the web app belongs. All Azure resources must belong to a resource group.
Instance Details		
Name	Enter a unique name	The name of your web app. This name becomes part of the app's URL: appname.azurewebsites.net. The name you choose must be unique among all Azure web apps.
Publish	Code	The method you want to use to publish your application. When publishing an application as code, you also must configure Runtime stack to prepare App Service resources to run your app.
Runtime stack	Python 3.12	The platform on which you want your application to run. Your choice might affect whether you have a choice of operating system - for some runtime stacks, App Service supports only one operating system.
Operating System	Linux	The operating system used on the virtual servers to run your app.
Region	East US	The geographical region from which your app is hosted.
Pricing plans		
Linux Plan	Accept default	The name of the App Service plan that powers your app. By default, the wizard creates a new plan in the same region as the web app.
Pricing plan	Standard S1	The pricing tier of the service plan being created. The pricing plan determines the performance characteristics of the virtual servers that power your app and the features to which it has access. Select Standard S1 in the drop-down.
Screenshot showing web app creation details.

Leave any other settings as default. Select Review + Create to go to the review pane, and then select Create. The portal shows the deployment pane, where you can view the status of your deployment.

 Note

It can take a moment for deployment to complete.

Preview your web app
When deployment is complete, select Go to resource. The portal shows the App Service Overview pane for your web app.

Screenshot showing the App Service pane with the URL link of the overview section highlighted.

To preview your web app's default content, select the URL under Default domain at the top right. The placeholder page that loads indicates that your web app is up and running and is ready to receive deployment of your app's code.

Screenshot showing the newly created App Service in a browser.

Leave the browser tab with the new app's placeholder page open. You'll come back to it after your app is deployed.

Next unit: Prepare the web application code


4- Prepare the web application code

In this unit, you'll learn how to create the code for your web application and integrate it into a source-control repository.

Bootstrap a web application
Now that you've created the resources for deploying your web application, you have to prepare the code you want to deploy. There are many ways to bootstrap a new web application, so what we'll learn here might be different to what you're used to. The goal is to quickly provide you a starting point to complete a full cycle up to the deployment.

 Note

All the code and commands shown on this page are only for explanation purposes; you do not need to execute any of them. We'll use them in a subsequent exercise.

To create a new web application starter using a few lines of code, you can use Flask, which is a commonly used web-application framework. You can install Flask using the following command:

Bash

Copy
pip install flask
After Flask is available in your environment, you can create a minimal web application using this code:

Python

Copy
from flask import Flask
app = Flask(__name__)

@app.route("/")
def hello():
    return "Hello World!\n"
This example code creates a server that answers every request with a "Hello World!" message.

Adding your code to source control
After your web application code is ready, the next step is usually to put the code into a source-control repository such as Git. If you have Git installed on your machine, running these commands in your source-code folder will initialize the repository.

Bash

Copy
git init
git add .
git commit -m "Initial commit"
These commands allow you to initialize a local Git repository and create a first commit with your code. You immediately gain the benefit of keeping a history of your changes with commits. Later on, you'll also be able to synchronize your local repository with a remote repository; for example, hosted on GitHub. This allows you to set up continuous integration and continuous deployment (CI/CD). While we recommend using a source-control repository for production applications, it's not a requirement to be able to deploy an application to Azure App Service.

 Note

Using CI/CD enables more frequent code deployment in a reliable manner by automating builds, tests, and deployments for every code change. It enables delivering new features and bug fixes for your application faster and more effectively.

Next unit: Exercise - Write code to implement a web application



5- Exercise - Write code to implement a web application

In this unit, you'll use developer tools to create the code for a starter web application.

Create a new web project
To create a starter web application, we'll use the Flask web-application framework.

Run the following commands in Azure Cloud Shell to set up a virtual environment and install Flask in your profile:

Bash

Copy
python3 -m venv venv
source venv/bin/activate
pip install flask
Run these commands to create and switch to your new web app directory:

Bash

Copy
mkdir ~/BestBikeApp
cd ~/BestBikeApp
Create a new file for your web app by opening application.py in the python interactive editor:

Bash

Copy
code application.py
Copy and paste the following Python code to create the main web app functionality:

Python

Copy
from flask import Flask
app = Flask(__name__)

@app.route("/")
def hello():
    return "<html><body><h1>Hello Best Bike App!</h1></body></html>\n"
Save your file and exit the editor by selecting the ... menu on the top right, and then selecting Save > Close Editor, or by pressing Ctrl+S and Ctrl+Q on Windows and Linux.

To deploy your application to Azure, you'll need to save the list of application requirements you made for it in a requirements.txt file. To do so, run the following command:

Bash

Copy
pip freeze > requirements.txt
Optionally test your web app
You can test your application locally in Azure while it's running.

Open a second command shell session in a new browser tab https://shell.azure.com/.

From your primary command shell session (to the right), run the following commands to activate the virtual environment:

Bash

Copy
cd ..
source venv/bin/activate
From your primary command shell session (to the right), run the following commands to start your web application:

Bash

Copy
cd ~/BestBikeApp
export FLASK_APP=application.py
flask run
From your second command shell session, run the following command to browse to your web application:

Bash

Copy
curl http://127.0.0.1:5000/
You should get the following HTML output:

HTML

Copy
<html><body><h1>Hello Best Bike App!</h1></body></html>
From your primary command shell session, press Ctrl+C to quit your web app, then close the secondary Azure Cloud Shell.

Next unit: Deploy code to App Service


6- Deploy code to App Service

Now, let's see how we can deploy our application to App Service.

Automated deployment
Automated deployment, or continuous integration, is a process used to push out new features and bug fixes in a fast and repetitive pattern with minimal impact on end users.

Azure supports automated deployment directly from several sources. The following options are available:

Azure Repos: You can push your code to Azure Repos, build your code in the cloud, run the tests, generate a release from the code, and finally push your code to an Azure Web App.
GitHub: Azure supports automated deployment directly from GitHub. When you connect your GitHub repository to Azure for automated deployment, any changes you push to your production branch on GitHub will be automatically deployed for you.
Bitbucket: Due to its similarities to GitHub, you can configure an automated deployment with Bitbucket.
Manual deployment
There are a few options that you can use to manually push your code to Azure:

Git: App Service web apps feature a Git URL that you can add as a remote repository. Pushing to the remote repository will deploy your app.
az webapp up: webapp up is a feature of the az command-line interface that packages your app and deploys it. Unlike other deployment methods, az webapp up can create a new App Service web app for you if you haven't already created one.
Deploy application packages: You can use az webapp deploy to deploy a ZIP, WAR, EAR, or JAR to App Service. You can also deploy scripts and static files with the same method.
Visual Studio: Visual Studio features an App Service deployment wizard that walks you through the deployment process.
FTP/S: FTP or FTPS is a traditional way of pushing your code to many hosting environments, including App Service.



Next unit: Exercise - Deploy your code to App Service


7- Exercise - Deploy your code to App Service

In this unit, you deploy your web application to App Service.

Deploy with az webapp up
Let's deploy our Python application with az webapp up. This command packages up our application and sends it to our App Service instance, where the app is built and deployed.

First, we need to gather some information about our web app resource. Run these commands to set shell variables that contain our app's name, resource group name, plan name, sku, and location. These use different az commands to request the information from Azure; az webapp up needs these values to target our existing web app.

Bash

Copy
export APPNAME=$(az webapp list --query [0].name --output tsv)
export APPRG=$(az webapp list --query [0].resourceGroup --output tsv)
export APPPLAN=$(az appservice plan list --query [0].name --output tsv)
export APPSKU=$(az appservice plan list --query [0].sku.name --output tsv)
export APPLOCATION=$(az appservice plan list --query [0].location --output tsv)
Now, run az webapp up with the appropriate values. Make sure you're in the BestBikeApp directory before running this command.

Bash

Copy
cd ~/BestBikeApp
az webapp up --name $APPNAME --resource-group $APPRG --plan $APPPLAN --sku $APPSKU --location "$APPLOCATION"
The deployment takes a few minutes, during which time you get status output.

Verify the deployment
Let's browse to your application. In the output, just before the JSON code block, there's a line with a URL. Select that link to open your app in a new browser tab. The page might take a moment to load because the App Service is initializing your app for the first time.

Once your program loads, you get the greeting message from your app. You deployed successfully!

Screenshot of Python's welcome page showing Hello Best Bike App!

Next unit: Summary

Summary

You've successfully created and deployed a web application to Azure App Service.

App Service simplifies managing and controlling your web app in comparison to traditional hosting options. Your App Service Plan can help you reduce the time and effort spent running and managing your web app, and provides advanced cloud features such as autoscaling and Azure DevOps integration.

Clean up
The sandbox automatically cleans up your resources when you're finished with this module.

When you're working in your own subscription, it's a good idea at the end of a project to identify whether you still need the resources you created. Resources that you leave running can cost you money. You can delete resources individually or delete the resource group to delete the entire set of resources.

Learn More
Continuous deployment to Azure App Service
Set up staging environments in Azure App Service
Deployment FAQs for Web Apps in Azure
Azure App Service and Azure Functions on Azure Stack Hub overview
Configure deployment sources for App Services on Azure Stack Hub
Check your knowledge

1. True or false: Azure App service can automatically scale your web application to meet traffic demand. 

True

False

2. Which of the following isn't a valid automated deployment source? 

GitHub

Azure Repos

SharePoint
Azure Administrator Associate

Chapter 6: Monitor and back up Azure resources



Modules in this learning path


Introduction to Azure Backup




Configure virtual machine backups

Learn how to configure virtual machine backups including restore operations.



Configure Azure Monitor

Learn how to configure Azure Monitor, including querying the Azure Monitor activity log.



Configure Log Analytics

You learn how to configure Log Analytics including structuring queries.



Configure Network Watcher

You learn how to configure Network Watcher and troubleshoot common networking problems.


Improve incident response with Azure Monitor alerts

Respond to incidents and activities in your infrastructure through alerting capabilities in Azure Monitor.



Analyze your Azure infrastructure by using Azure Monitor logs

Use Azure Monitor logs to extract valuable information about your infrastructure from log data.



Monitor your Azure virtual machines with Azure Monitor

Learn how to monitor your Azure VMs by using Azure Monitor to collect and analyze VM host and client metrics and logs.






Point 1: Introduction to Azure Backup

Learning objectives
By the end of this module, you'll be able to:

Evaluate whether Azure Backup is appropriate to use for your backup needs.
Describe how the features of Azure Backup work to provide backup solutions for your needs.


1- Introduction

Information technology workers understand the importance of data to the organization. The need to protect that data drives decisions around storage, backups, and security. Many companies implement policies that dictate backup specifications for frequency, duration of storage for the backup, and restore policies.

For on-premises scenarios, backup solutions might have included local redundant storage solutions or off-site storage. Scenarios using backup to tape drives and storing offsite come with the resulting delay in restoring the data because of the need to transport the tapes back to the server rooms, and from performing the restore operation. It can result in significant downtime.

These backup solutions may not always address some of the most important considerations such as security of the backups, the potential for the company to be impacted by a ransomware attack, or human error in the backup and restore operations. An ideal solution would be cost-effective, simple to use, and secure. This is where Azure Backup comes in.

Diagram of a backup scenario with a company's servers and workstations on the left, with files and folders, using the Backup Agent to back up the data to Microsoft Azure storage.

Azure Backup can also address scenarios for your Azure environments, with support for:

Azure VMs
Azure Managed Disks
Azure Files
SQL Server in Azure VMs
SAP HANA databases in Azure VMs
Azure Database for PostgreSQL servers
Azure Blobs
Azure Database for PostSQL - Flexible servers
Azure Database for MySQL - Flexible servers
Azure Kubernetes cluster
Example scenario
You're running an application powered by SQL Server. The database is running in an always-on availability group across three Azure VMs. You want to back up the databases using an Azure native backup service. You're looking to store the backup for 10 years in a cheaper storage for your audit and compliance needs. You'd like to monitor the backup jobs daily for all such databases.

Diagram of an application using a SQL Server backend database and Azure Backup for data backup scenarios.

What will we be doing?
We'll evaluate the features and capabilities of Azure Backup to help decide if:

Azure Backup can offer a solution for your backup needs
You can back up and restore the data you need for your organization
Azure Backup offers secure storage of your data
What is the main goal?
By the end of this session, you'll be able to decide if Azure Backup is the right solution to consider for your data protection.

Next unit: What is Azure Backup?


2- What is Azure Backup?

Let's start with a definition of Azure Backup and take a quick tour of the core features. This overview should help you see whether Azure Backup might be a good fit for your data protection needs.

What is Azure Backup?
The Azure Backup service provides simple, secure, and cost-effective solutions to back up your data and recover it from the Microsoft Azure cloud.

Diagram of the Azure Backup service implementing backup agents in the on-premises environment to the cloud. Middle section displays the components of Azure Backup for security and scalability with an underlying bar indicating central management.

Azure Backup definition
Azure Backup is an Azure service that provides cost effective, secure, and zero-infrastructure backup solutions for all Azure-managed data assets.

The centralized management interface makes it easy to define backup policies and protect a wide range of enterprise workloads. Including, Azure Virtual Machines, Azure Disks, SQL and SAP databases, Azure file shares, and blobs.

Diagram of Azure Backup architecture displaying workloads at the bottom, feeding upwards into the data plane, and tying into the management plane. Management contains backup policies, Azure policies, Azure Monitor, and Azure Lighthouse services.

When to think of Azure Backup?
As the IT admin of your organization, you're responsible for meeting the compliance needs for all the data assets of the firm; backup is a critical aspect of it. There are also various application admins in your company who need to do self-service backup and restore to take care of issues like data corruption or rogue admin scenarios. You're looking for an enterprise-class backup solution to protect all your workloads and manage them from a central place.

Azure Backup can provide backup services for the following data assets:

On-premises files, folders, and system state
Azure Virtual Machines (VMs)
Azure Managed Disks
Azure Files Shares
SQL Server in Azure VMs
SAP HANA (High-performance Analytic Appliance) databases in Azure VMs
Azure Database for PostgreSQL servers
Azure Blobs
Azure Database for PostSQL - Flexible servers
Azure Database for MySQL - Flexible servers
Azure Kubernetes cluster
Screenshot of Azure Backup center displaying a list of backup jobs. The list displays the backup instance, data source, operation type, and status.

Key features
Let's look at some key features of Azure Backup.

Feature	Description	Usage
Zero-infrastructure backup solution	Unlike conventional backup solutions, no backup server or infrastructure is needed. Similarly, no backup storage needs to be deployed, because Azure Backup automatically manages and scales it.	Zero-infrastructure solution eliminates capital expenses and reduces operational expenses. It increases ease of use by automating storage management.
At-scale management	Natively manage your entire backup estate from a central console called Backup Center. Use APIs, PowerShell, and Azure CLI to automate Backup policy and security configurations.	Backup center simplifies data protection management at-scale by enabling you to discover, govern, monitor, operate, and optimize backup management, all from one unified console, helping you to drive operational efficiency with Azure.
Security	Azure Backup provides built-in security to your backup environment, both when your data is in transit and at rest by using capabilities encryption, private endpoints, alerts, and so on.	Your backups are automatically secured against ransomware, malicious admins, and accidental deletions.
How do Recovery Time Objective and Recovery Point Objective work?
Recovery Time Objective (RTO) is the target time within which a business process must be restored after a disaster occurs to avoid unacceptable consequences. For instance, if a critical application goes down due to a server failure and the business can only tolerate a maximum of four hours of downtime, then the RTO is four hours.

Recovery Point Objective (RPO) is the maximum amount of data loss, measured in time, that your organization can sustain during an event.

The following example scenario describes both the RPO and RTO concepts:

Your organization has an RPO of one hour for your customer database, which means you perform backups every hour. If a data loss incident occurs, you lose not more than one hour of data. When you set RTO to three hours, then if a system failure occurs, you aim to restore access to the database within three hours to minimize the impact on operations.

Next unit: How Azure Backup works


3- How Azure Backup works

Let's take a look at how Azure Backup works to provide the data protection you need. You'll learn how the different aspects of the backup service make it easy to back up various types of data and how it offers security for your backups as well. We'll discover these aspects of the Azure Backup Service:

Workload integration layer - Backup Extension. Integration with the actual workload (such as Azure VM or Azure Blobs) happens at this layer.
Data Plane - Access Tiers. Three access tiers where the backups could be stored:
Snapshot tier
Standard tier
Archive tier
Data Plane - Availability and Security. The backup data is replicated across zones or regions (based on the redundancy specified by the user).
Management Plane – Recovery Services vault/Backup vault and Backup center. Vault provides an interface for the user to interact with the backup service.
What data is backed up and how?
The simplest explanation for Azure Backup is that it backs up data, machine state, and workloads, running on on-premises machines and VM instances to the Azure cloud. Azure Backup stores the backed-up data in Recovery Services vaults and Backup vaults.

For on-premises Windows machines, you can back up directly to Azure with the Azure Backup Microsoft Azure Recovery Services (MARS) agent. Alternatively, you can back up these Windows machines to a backup server, perhaps a System Center Data Protection Manager (DPM), or Microsoft Azure Backup Server (MABS). You can then back up that server to a Recovery Services vault in Azure.

If you're using Azure VMs, you can back them up directly. Azure Backup installs a backup extension to the Azure VM agent that's running on the VM, which allows backing up the entire VM. If you only want to back up the files and folders on the VM, you can do so by running the MARS agent.

Azure Backup stores backed-up data in vaults: Recovery Services vaults and Backup vaults. A vault is an online-storage entity in Azure that's used to hold data such as backup copies, recovery points, and backup policies.

Supported backup types
Azure Backup supports full backups and incremental backups. Your initial backup will be a full backup type. The incremental backup is used by DPM/MABS for disk backups, and used in all backups to Azure. As the name suggests, incremental backups only focus on blocks of data that have changed since the previous backup.

Azure Backup also supports SQL Server backup types. The following table outlines the support for SQL Server type backups:

Type	Description	Usage
Full	A full database backup backs up the entire database. It contains all the data in a specific database or in a set of filegroups or files. A full backup also contains enough logs to recover that data.	At most, you can trigger one full backup per day. You can choose to make a full backup on a daily or weekly interval.
Differential	A differential backup is based on the most recent, previous full-data backup. It captures only the data that's changed since the full backup.	At most, you can trigger one differential backup per day. You can't configure a full backup and a differential backup on the same day.
Multiple backups per day	Back up Azure VMs hourly with a minimum recovery point objective (RPO) of 4 hours and a maximum of 24 hours.	You can use Enhanced backup policy to set the backup schedule to 4, 6, 8, 12, and 24 hours, respectively for new Azure offerings, such as Trusted Launch VM.
Selective disk backup	Selectively back up a subset of the data disks that are attached to your VM, and then restore a subset of the disks that are available in a recovery point, both from instant restore and vault tier. It helps you manage critical data in a subset of the VM disks and use database backup solutions when you want to back up only their OS disk to reduce cost.	Azure Backup provides Selective Disk backup and restore capability using Enhanced backup policy.
Transaction Log	A log backup enables point-in-time restoration up to a specific second.	At most, you can configure transactional log backups every 15 minutes.
Workload integration layer - Backup Extension
A backup extension specific to each workload is installed on the source VM or a worker VM. At the time of backup (as defined by the user in the Backup Policy), the backup extension generates the backup, which could be:

Storage: snapshots when using an Azure VM or Azure Files.

Stream backup for databases like SQL or HANA running in VMs.

The backup data is eventually transferred to the data plane (Azure Backup managed storage) via secure Azure networks (Network Security Groups (NSG), Firewalls, or more sophisticated private endpoints).

Data Plane - Access Tiers
There are three access tiers where the backups could be stored:

Snapshot tier: (Workload-specific term) In the first phase of VM backup, the snapshot taken is stored along with the disk. This form of storage is referred to as snapshot tier. Snapshot-tier restores are faster (than restoring from a vault) because they eliminate the wait time for snapshots to get copied to from the vault before triggering the restore operation. The snapshots of the VM/Azure Files/Azure Blobs/and so on are retained in the customer’s subscription itself in a specified resource group. This ensure restores are quick, because the backup/snapshot is available locally to the customer.

Vault-Standard tier: Backup data for all workloads supported by Azure Backup is stored in vaults, which hold backup storage, an autoscaling set of storage accounts managed by Azure Backup. The Vault-Standard tier is an online storage tier that allows you to store an isolated copy of backup data in a Microsoft-managed tenant, thus creating an extra layer of protection. For workloads where snapshot tier is supported, there's a copy of the backup data in both the snapshot tier and the vault-standard tier. The vault-standard tier ensures that backup data is available even if the data source being backed up is deleted or compromised.

Archive tier: Customers rely on Azure Backup for storing backup data, including their Long-Term Retention (LTR) backup data with retention needs being defined by the organization's compliance rules. In most cases, the older backup data is rarely accessed and is only stored for compliance needs.

Azure Backup supports backup of long-term retention points in the archive tier.

All tiers offer different recovery time objectives (RTO) and are priced differently.

Diagram of the various workloads such as on-premises server, Azure VMs, Azure files, etc. feeding into the data plane where the access tiers are located.

Data Plane - Availability and Security
The backup data is replicated across zones or regions (based on the redundancy specified by the user). You can choose from locally redundant storage (LRS), Geo-redundant storage (GRS), or zone-redundant storage (ZRS). These options provide you with highly available data storage capabilities.

The data is kept safe by encrypting it and implementing Azure role-based access control (Azure RBAC). You choose who can perform backup and restore operations. Azure Backup also provides protection against malicious deletion of your backup by using soft-delete operations. A deleted backup is stored for 14 days, free of charge, which allows you to recover the backup if needed.

Azure Backup also supports a backup data lifecycle management scenario that allows you to comply with retention policies.

Graphic displaying the three security options of Azure RBAC, encryption, and soft delete as icons.

Management Plane – Recovery Services vault/Backup vault and Backup center
Azure Backup uses vaults (Recovery Services vaults and Backup vaults) to orchestrate and manage backups. It also uses vaults to store backed-up data. The vault provides an interface for the user to interact with the backup service. Azure Backup Policies within each vault define when the backups should get triggered and how long they need to be retained.

You can use a single vault or multiple vaults to organize and manage your backup. If your workloads are all managed by a single subscription and single resource, you can use a single vault to monitor and manage your backup estate. If your workloads are spread across multiple subscriptions, you can create multiple vaults with one or more vaults per subscription.

Diagram of recovery service vault graphics showing option for backup policies and management with the portal, SDKs, or the Command-line interface (CLI).

Backup center allows you to have a single pane of glass to manage all tasks related to backups. Backup center is designed to function well across a large and distributed Azure environment. You can use Backup center to efficiently manage backups spanning multiple workload types, vaults, subscriptions, regions, and Azure Lighthouse tenants.

Screenshot of the Backup center user interface in the Azure portal. This image is displaying backup information for Azure Virtual machines related to jobs and backup instances.

Next unit: When to use Azure Backup


4- When to use Azure Backup

Here, we'll discuss how you can decide if Azure Backup is the right choice for your data protection needs. We'll highlight common backup scenarios where Azure Backup provides benefits, such as:

Ensuring availability of your data.
Protecting your Azure workloads.
Securing your data.
Decision criteria
Azure Backup is an Azure service that provides secure and zero-infrastructure backup solutions for all Azure-managed data assets. It protects a wide range of enterprise workloads, including Azure Virtual Machines, Azure Disks, SQL and SAP databases, Azure file shares and blobs.

The main criteria that we're evaluating are outlined in the following table. The table contains some key areas where Azure Backup can provide services to you for data protection.

Criteria	Consideration
Azure workloads	Azure VM, Azure Disks, SQL Server, or SAP HANA database running in Azure VM, Azure Blobs, Azure Disks, Azure Database for PostgreSQL.
Compliance	Customer-defined backup policy with long-term retention across multiple zones or regions.
Operational recoveries	With self-service backup and restores, the application administrator can take care of issues that might arise such as accidental deletion or data corruption.
Apply the criteria
In the introduction, we presented a scenario where your organization might have an application that's relying on data from a back-end SQL Server installation. SQL Server is running on three Azure VMs. The data in the backup must be retained for up to 10 years to meet compliance requirements. You also want to be able to monitor the backups.

Before we dive into how Azure Backup can help meet these needs, it's important to understand what's not currently supported. If your three Azure VMs are deployed across multiple subscriptions or regions, you should be aware that Azure Backup doesn’t support cross-region backup for most workloads. However, it does support cross-region restore in a paired secondary region.

Can Azure Backup protect the Azure VMs hosting the SQL Server instances?
Azure Backup is able to back up entire Windows and Linux VMs using backup extensions. As a result, you can back up the entire VM that is hosting SQL Server. If you only want to back up the files, folders, and system state on the Azure VMs, you can use the Microsoft Azure Recovery Services (MARS) agent.

If your main concern is to only back up the SQL Server data, Azure Backup provides support for that as well. Azure Backup offers a stream-based, specialized solution to back up SQL Servers running in Azure VMs. This solution aligns with Azure Backup's benefits of zero-infrastructure backup, long-term retention, and central management.

Additionally, Azure Backup provides the following advantages specifically for SQL Server:

Workload aware backups that support all backup types: full, differential, and log
15-minute recovery point objective (RPO) with frequent log backups
Point-in-time recovery up to a second
Individual database-level backup and restore
Diagram of SQL Server hosted on an Azure VM and being backed up to a Recovery Services Vaults in Azure Backup. Displayed are also a data path and controls arrow depicting two-way flow for the data path and control path flow from Azure Backup to the backup extension on the VM.

Does Azure Backup help with compliance?
You can implement required access control mechanisms for your backups. Vaults (Recovery Services and Backup vaults) provide the management capabilities and are accessible via the Azure portal, Backup Center, Vault dashboards, SDK, CLI, and even REST APIs. It's also an Azure role-based access control (Azure RBAC) boundary, providing you with the option to restrict access to backups only to authorized Backup Admins.

Short-term retention can be "minutes" or "daily." Retention for "Weekly," "monthly," or "yearly" backup points is referred to as Long-term retention.

Long-term retention can be:

Planned (compliance requirements): If you know in advance that data is required years from the current time, use Long-term retention.
Unplanned (on-demand requirement): If you don't know in advance, then you can use on-demand backup with specific custom retention settings (these custom retention settings aren't impacted by policy settings).
On-demand backup with custom retention: If you need to take a backup not scheduled via backup policy, then you can use an on-demand backup. It can be useful for taking backups that don’t fit your scheduled backup or for taking granular backup (for example, multiple IaaS VM backups per day since scheduled backup permits only one backup per day). It's important to note that the retention policy defined in scheduled policy doesn't apply to on-demand backups.
You can also implement policy management to help with compliance. Azure Backup Policies within each vault define when the backups should be triggered and how long they need to be retained. You can also manage these policies and apply them across multiple items.

Does Azure Backup simplify monitoring and administration?
Monitoring and Reporting: Azure Backup integrates with Log Analytics and provides the ability to see reports via Workbooks as well.

Azure Backup provides in-built job monitoring for operations such as configuring backup, backing up, restore, delete backup, and so on. It's scoped to the vault and ideal for monitoring a single vault.

If you need to monitor operational activities at scale, Backup Explorer provides an aggregated view of your entire backup estate, enabling detailed drill-down analysis and troubleshooting. It's a built-in Azure Monitor workbook that gives a single, central location to help you monitor operational activities across the entire backup estate on Azure, spanning tenants, locations, subscriptions, resource groups, and vaults.

Next unit: Knowledge check


5- Knowledge check


1. Which tier allows for quick restore operations on a backup? 

Snapshot Tier

Standard Tier

Archive Tier

2. What tool can you use to manage backups spanning multiple workload types, vaults, subscriptions, regions, and Azure Lighthouse tenants? 

Azure Monitor

System Center Data Protection Manager (DPM)

Backup center

3. What must be present before you can back up an entire VM or content on an Azure VM? 

Backup extensions.

Microsoft Azure Backup Server (MABS)

Recovery point objective policies in an Azure vault.



Summary

Our goal was to help you evaluate whether Azure Backup would offer the features and capabilities to help you protect your data. During the module, we explored how Azure Backup might address:

Ensuring availability of your data.
Protecting your Azure workloads.
Securing your data.
We applied the criteria to a scenario where your company was hosting an application that used a SQL Server database instance running on multiple Azure VMs. We noted how Azure Backup could provide data protection by backing up our Azure VMs or the files, folders, and system state on those VMs.

We also saw how Azure Backup helps with compliance by offering retention options for the data and security with encryption and RBAC. Using Backup center, we showed how easy it is to manage these backups.

Backup center simplifies data protection management at-scale by allowing you to discover, govern, monitor, operate, and optimize backup management, all from one unified console. This helps you to drive operational efficiency with Azure. Your backups are automatically secured against ransomware, malicious admins, and accidental deletions.







Point 2: Configure virtual machine backups

Learn how to configure virtual machine backups including restore operations.


Learning objectives
In this module, you learn how to:

Identify features and usage cases for different Azure backup methods.

Configure virtual machine snapshots and backup options.

Implement virtual machine backup and restore, including soft delete.

Perform site-to-site recovery by using Azure Site Recovery.


1- Introduction

Azure Backup provides independent and isolated backups to guard against unintended destruction of the data on your virtual machines. Administrators can implement Azure services to support their backup requirements, including the Microsoft Azure Recovery Services (MARS) agent for Azure Backup, the Microsoft Azure Backup Server (MABS), Azure managed disks snapshots, and Azure Site Recovery.

Your company has several critical virtual machine workloads running on Azure. You're responsible for ensuring the company can recover these virtual machines if there's data loss or corruption. You're using the built-in capabilities of Azure Backup to help protect your virtual machines. Your configuration uses Azure Backup for both Azure and on-premises workloads.

In this module, you learn about different virtual machine backup strategies. These strategies work for both Azure and on-premises virtual machines. You review how Azure Backup provides many options for backup and recovery. You also learn about other strategies like snapshots, soft delete, and Azure Site Recovery.

The goal of this module is to equip you with the knowledge and skills to effectively use Azure Backup and Recovery Services.

Learning objectives
In this module, you learn how to:

Identify features and usage cases for different Azure virtual machine backups.

Configure virtual machine snapshots and backup options.

Implement virtual machine backup and restore, including soft delete.

Perform site-to-site recovery by using Azure Site Recovery.

Compare different virtual machine backup options.

Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Familiarity with Azure and on-premises virtual machines and their components.

Working knowledge of how-to backup and store data in an enterprise infrastructure.

Basic understanding of data protection and disaster recovery tasks.

Next unit: Explore options to protect virtual machine data



2- Explore options to protect virtual machine data

You can protect your data by taking backups at regular intervals. Azure provides several backup options for virtual machines to support different scenarios and configuration requirements.

Things to know about backup options for virtual machines
Let's examine four options for backing up your virtual machines: Azure Backup, Azure Site Recovery, and Azure managed disks snapshots and images. The following table summarizes these options and provides scenarios for using the different methods. As you review these options, think about which method can support the requirements for the business scenario presented in this module.

Azure backup option	Configuration scenarios	Description
Azure Backup	Back up Azure virtual machines running production workloads

Create application-consistent backups for both Windows and Linux virtual machines	Azure Backup takes a snapshot of your virtual machine and stores the data as recovery points in geo-redundant recovery vaults. When you restore from a recovery point, you can restore your entire virtual machine or specific files only.
Azure Site Recovery	Quickly and easily recover specific applications

Replicate to the Azure region of your choice	Azure Site Recovery protects your virtual machines from a major disaster scenario when a whole region experiences an outage due to a major natural disaster or widespread service interruption.
Azure managed disks - snapshot	Quickly and easily back up your virtual machines that use Azure managed disks at any point in time

Support development and test environments	An Azure managed disks snapshot is a read-only full copy of a managed disk stored as a standard managed disk by default. A snapshot exists independent of the source disk and can be used to create new managed disks. Each snapshot is billed based on the actual size used. If you create a snapshot of a managed disk with a capacity of 64 GB used only 10 GB, Azure bills you for 10 GB.
Azure managed disks - image	Create an image from your custom virtual hard disk (VHD) in an Azure storage account or directly from a generalized (via Sysprep) virtual machine

Create hundreds of virtual machines by using your custom image without copying or managing any storage account	Azure managed disks also support creating a managed custom image. This process captures a single image that contains all managed disks associated with a virtual machine, including both the operating system and data disks.
Things to consider when creating images versus snapshots
It's important to understand the differences and benefits of creating an image and a snapshot backup of an Azure managed disk.

Consider images. With Azure managed disks, you can take an image of a generalized, deallocated virtual machine. The image includes all of the disks attached to the virtual machine. You can use the image to create a virtual machine that includes all of the disks.

Consider snapshots. A snapshot is a copy of a disk at the point in time the snapshot is taken. The snapshot applies to one disk only, and doesn't have awareness of any disk other than the one it contains. Snapshot backups are problematic for configurations that require the coordination of multiple disks, such as striping. In this case, the snapshots need to coordinate with each other, but this functionality isn't currently supported.

Consider operating disk backups. If you have a virtual machine with only one disk (the operating system disk), you can take a snapshot or an image of the disk. You can create a virtual machine from either a snapshot or an image.

Next unit: Create virtual machine snapshots in Azure Backup



3- Create virtual machine snapshots in Azure Backup

An Azure Backup job creates a snapshot for your virtual machine in two phases:

Phase 1: Take a snapshot of the virtual machine data

Phase 2: Transfer the snapshot to an Azure Recovery Services vault

The following diagram highlights this process.

Illustration that shows the Azure Backup job process for a virtual machine as described in the text.

After the Azure Backup job completes, you can use recovery points for the snapshot to restore your virtual machine or specific files.

Things to know about snapshots and recovery points
Let's take a closer look at the characteristics of snapshots and recovery points in Azure Backup.

By default, Azure Backup keeps snapshots for two days to reduce backup and restore times. The local retention reduces the time required to transform and copy data back from an Azure Recovery Services vault.

You can set the default snapshot retention value from one to five days.

Incremental snapshots are stored as Azure page blobs (Azure Disks).

Recovery points for a virtual machine snapshot are available only after both phases of the Azure Backup job are complete.

Recovery points are listed for the virtual machine snapshot in the Azure portal and are labeled with a recovery point type.

After a snapshot is first taken, the recovery points are identified with the snapshot recovery point type.

After the snapshot is transferred to an Azure Recovery Services vault, the recovery point type changes to snapshot and vault.

Things to consider when using snapshots and recovery points
Here are some important benefits and considerations about using snapshots and recovery points.

Consider recovery after Phase 1. To restore your virtual machine from the snapshot, use the snapshot captured in Phase 1 of the Azure Backup job. Phase 2 transfers the snapshot to the Recovery Services vault, so recovery points can be created. You don't have to wait for Phase 2 to complete before attempting a full restore from the snapshot.

Consider disk type, sizing, pricing. Back up Standard SSD disks, Standard Hard Disk Drive (HDD) disks, and Premium SSD disks. Use disk sizes up to 32 TB. For Premium Azure storage accounts, snapshots taken for instant recovery points count towards the 10-TB limit of allocated space.

 Note

Azure Backup doesn't recommend resizing disks.

Consider snapshot retention and cost savings. Configure how long Azure Backup retains your snapshots based on your restore needs. Depending on your requirements, you might set the snapshot retention value to a minimum of one day. This setting can help reduce costs for snapshot retention, if you don't perform restores frequently.

Next unit: Set up Azure Recovery Services vault backup options


4- Set up Azure Recovery Services vault backup options

An Azure Recovery Services vault is a storage entity in Azure that houses data. The data is typically copies of data, or configuration information for virtual machines, workloads, servers, or workstations. You can use Recovery Services vaults to organize your backup data and minimize your management overhead.

Things to know about Recovery Services vaults
Here are some characteristics of Azure Recovery Services vaults.

A Recovery Services vault stores backup data for various Azure services, such as IaaS virtual machines (Linux or Windows) and Azure SQL databases.

Azure Recovery Services vaults support System Center Data Protection Manager (DPM), Windows Server, Microsoft Azure Backup Server (MABS), and other services.

In the Azure portal, you can use an Azure Recovery Services vault to back up your Azure virtual machines:

Screenshot that shows backup options for an Azure virtual machine to an Azure Recovery Services vault.

A Recovery Services vault can be used to back up your on-premises virtual machines, such as Hyper-V, VMware, System State, and Bare Metal Recovery:

Screenshot that shows backup options for an on-premises Azure virtual machine to an Azure Recovery Services vault.

For details about creating an Azure Recovery Services vault, see Configure Azure Recovery Services vault backup options.

Next unit: Back up your virtual machines



5- Back up your virtual machines

To use Azure Backup to protect your Azure virtual machines, you follow a simple three-step process: create a vault, define your backup options, and trigger the backup job.

Illustration that shows the three basic steps to back up an Azure virtual machine by using Azure Backup.

Step 1: Create a Recovery Services vault
The first step is to create an Azure Recovery Services vault for your virtual machine backups. The vault must be created within your Azure subscription, and in the region where you want to store the data.

You also need to specify how you want your storage replicated, either geo-redundant (default) or locally redundant.

Geo-redundant (GRS): (Default) Use GRS when Azure is your primary backup storage endpoint.

Locally redundant (LRS): If Azure isn't your primary backup storage endpoint, use LRS to reduce your storage costs.

Step 2: Define your backup policy options
After you create your vault, you need to define your backup policy. The policy specifies when to take the data snapshots, and how long to keep the snapshots.

Your virtual machine is protected by taking snapshots of your data at defined intervals. The snapshots produce recovery points that are stored in your Recovery Services vault.

If it becomes necessary to repair or rebuild your virtual machine, you can restore your machine by using your saved recovery points. In your backup policy, you can specify to trigger a backup from one to five times per day.

Step 3: Back up your virtual machine
The last step is to run the Azure Backup job process and create your backups.

To run the backup job, the Azure Backup extension requires the Microsoft Azure Virtual Machine Agent to be present on your Azure virtual machine.

If your virtual machine was created from the Azure gallery, the agent is installed by default on your machine.

If your virtual machine was migrated from an on-premises data center, you need to manually install the agent on your machine.

For details, see Install the Azure Virtual Machine Agent.

Next unit: Restore your virtual machines



6- Restore your virtual machines

After you back up your virtual machine, the backup snapshots and recovery points are stored in your Recovery Services vault. You can recover your machine by accessing the snapshot, or restore data to a specific point-in-time by using recovery points.

Screenshot that shows recovery points in a Recovery Services vault for a virtual machine snapshot in the Azure portal.

Things to know about restoring your virtual machines
Let's review a few points about restoring your virtual machines from your backup snapshots.

You can select recovery points for your virtual machine snapshots in the Azure portal.

When you trigger a restore operation, Azure Backup creates a job to track the restore operation.

Azure Backup creates and temporarily displays notifications about the restore operation.

You can track the restore operation by monitoring the job notifications in the Azure portal.


Next unit: Implement soft delete for your virtual machines



7- Implement soft delete for your virtual machines

Azure Storage now offers the soft delete option for Azure Blob objects. With this feature, you can more easily recover modified or deleted data.

Flowchart that shows how backup items remain in the soft delete state for 14 days until the item is permanently deleted.

Soft delete for virtual machines protects backups of your virtual machines from unintended deletion. Even after the backups are deleted, the soft delete state preserves them for 14 more days.

 Important

Soft delete only protects deleted backup data. If a virtual machine is deleted without a backup, the soft delete feature won't preserve the data. All resources should be protected with Azure Backup to ensure full resilience.

Things to know about soft delete for backups
Review the following details regarding implementing soft delete for your virtual machine backups.

Stop backup job. Before you can delete or retain backup data for your virtual machine, you must stop the active backup job. After you stop the backup job in the Azure portal, you can choose to delete or retain your backup data.

Apply soft delete state. Prevent your virtual machine backup data from being permanently deleted by selecting Delete backup data followed by Stop backup. The soft delete state is applied to your backup data, and the data is retained for 14 days. If you apply the state to a virtual machine, the machine is referred to as soft deleted.

View soft delete data in the vault. During the 14 day retention period, the Recovery Services vault shows your soft deleted virtual machine with a red soft delete icon.

 Note

When a Recovery Services vault contains any soft deleted items, the vault can't be deleted. First delete or undelete all soft deleted items, and then delete the vault.

Undelete backup items. Before you can restore a soft deleted virtual machine, you must undelete the backup data.

Restore items. After you undelete the backup item, you can restore your virtual machine by selecting Restore virtual machine from the chosen recovery point in the backup.

Resume backups. When the undelete process completes, the backup job status returns to Stop backup with retain data, and you can choose Resume backup. The resume operation retrieves the backup item in the active state according to the backup policy selected by the user. The policy defines the backup and retention schedules.


Next unit: Implement Azure Site Recovery


8- Implement Azure Site Recovery

Overview of Azure Site Recovery
Azure Site Recovery is a service that helps ensure business continuity by replicating workloads from a primary site to a secondary location.


Suppose you work for a large e-commerce company that relies heavily on its online platform to generate revenue. One day, a major storm hits the region where your primary data center is located, causing a power outage and rendering your website inaccessible. This outage results in significant financial losses and damages your company's reputation. To prevent such incidents in the future, you decide to implement Azure Site Recovery. By replicating your workloads to a secondary location, you can ensure that your applications remain accessible. You can continue serving your customers and minimize the impact on your business.

The following illustration shows two regions connected by Azure Traffic Manager. Azure Site Recovery is implemented to enable failover from region 1 to region 2.

Illustration that shows an implementation of Azure Site Recovery to enable failover from region 1 to region 2.

Things to know about Azure Site Recovery
Azure Site Recovery supports many configurations and complements various Azure services. You can implement Site Recovery to back up your virtual machines and physical machines in the following scenarios:

Replicate Azure virtual machines from one Azure region to another

Replicate on-premises VMware virtual machines, Hyper-V virtual machines, physical servers (Windows and Linux), and Azure Stack virtual machines to Azure

Replicate AWS Windows instances to Azure

Replicate on-premises VMware virtual machines, Hyper-V virtual machines managed by System Center VMM, and physical servers to a secondary site

Things to consider when using Site Recovery
There are many benefits to implementing Azure Site Recovery. As you review the following features, consider how the service can support your business requirements.

Feature	Description
Consolidated management	Set up and manage replication, failover, and failback from a single location in the Azure portal.
Reduced cost and complexity	Replicate to Azure to eliminate the cost and complexity of maintaining a secondary datacenter.
Replication resilience	Orchestrate replication without intercepting your app data and gain the resilience of Azure Storage. When failover occurs, Azure virtual machines are created, based on the replicated data.
Continuous replication	Access continuous replication for Azure virtual machines and VMware virtual machines, and replication frequency as low as 30 seconds for Hyper-V.
Snapshot recovery points	Replicate by using recovery points with app-consistent snapshots that capture disk data, all data in memory, and all transactions in process.
Failover and easy fall back	Run planned failovers for expected outages with zero-data loss. Run unplanned failovers with minimal data loss depending on replication frequency. Easily fall back to your primary site when it's available again.
Integration	Integrate with Azure for simple application network management, including reserving IP addresses, configuring load-balancers, and integrating Azure Traffic Manager for efficient network switchovers.



Next unit: Interactive lab simulation


9- Interactive lab simulation

Lab scenario
Your organization decided to use Azure Backup and Recovery Services. As the Azure Administrator you need to:

Determine how to back up and restore files hosted on Azure virtual machines and on-premises computers.
Identify methods for protecting data stored in the Recovery Services vault.
Architecture diagram
Architecture diagram as explained in the text.

Objectives
 Note

This interactive lab simulation covers two areas explored in this Learning Path. This module focuses on virtual machine backups, which is covered in Task 2 and Task 3. The simulation is also appropriate for the Configure file and folder backups module.

Task 1: Establish the lab environment.
Review an Azure Resource Manager (ARM) template.
Use the ARM template to deploy two virtual machines. These virtual machines are used to test different backup scenarios.

Task 2: Create a Recovery Services vault.
Create a Recovery Services vault in the same region you deployed the virtual machines.
Configure the Recovery Services vault for Geo-redundant storage and soft delete.

Task 3: Implement Azure virtual machine-level backup.
Configure the Recovery Services vault to back up Azure virtual machines.
Create a backup policy to run daily at 12:00 AM.
Enable backup for one of the virtual machines.

Task 4: Implement file and folder backup.
Connect through remote desktop to a virtual machine and access the Azure portal.
Configure the Recovery Services vault to back up on-premises files and folders.
Install the download agent for Windows Server or Windows Client.
Register the agent with the Recovery Services vault.
Create a backup schedule and back up local files.
Confirm the backed-up files are in the Recovery Services vault.

Task 5: Perform file recovery by using the MARS agent.
Remove files that were backed up in the previous task.
Use the Recover Data Wizard to retrieve the deleted files.

Task 6: Perform file recovery by using Azure virtual machine snapshots (optional).

Task 7: Review the Azure Recovery Services soft delete functionality (optional).
 Note

Select the thumbnail image to start the lab simulation. When you're done, be sure to return to this page so you can continue learning.

Screenshot of the simulation page.

Next unit: Knowledge check


10- Knowledge check

Your company has critical virtual machine workloads running on Azure. You're using Azure Backup and other Azure services to help protect your virtual machines. You must develop a configuration plan to recover virtual machines and backup items. A few teams submitted configuration requirements and questions for your consideration:

The infrastructure team has a mix of Azure virtual machines running production workloads, including Windows Servers and Linux servers.

The Engineering team requested backup support for their development database disks.

You're researching how backups in a soft delete state can be restored to recover from machine failures.

Answer the following questions
Choose the best response for each of the following questions. Then select Check your answers.


1. What's the best backup method for the company's production virtual machines? 

Azure managed disks snapshots

Azure Backup

Azure Site Recovery

2. Which option should you recommend to back up the Engineering database disks? 

Azure virtual machine backup

Azure Site Recovery

Azure managed disks snapshots

3. A recent malware attack deleted several virtual machine backups. How long are backup items available in a soft delete state? 

14 days

Seven days

30 days



Summary and resources

Azure Backup provides independent and isolated backups to guard against unintended destruction of the data on your virtual machines.

In this module, you identified features and usage cases for different Azure backup methods. You learned how to configure Azure managed disks snapshots and Azure Backup options. You explored how to implement Azure virtual machine backup and restore, including soft delete. You discovered how to complete site-to-site recovery by using Azure Site Recovery. You compared the Microsoft Azure Recovery Services (MARS) agent for Azure Backup and the Microsoft Azure Backup Server (MABS).

The main takeaways from this module are:

Azure Site Recovery enables failover and continued access to applications if an outage occurs. This protection is provided by replicating workloads to a secondary location.

Azure Backup provides secure backups for virtual machines, allowing for the restoration of entire virtual machines or specific files.

Both Azure Site Recovery and Azure Backup offer features such as consolidated management, reduced cost and complexity, and replication resilience.

Learn more
An overview of Azure VM backup. This article describes how the Azure Backup service backs up Azure virtual machines.

Back up a virtual machine in Azure. This quickstart enables backup on an existing Azure virtual machine.

Quickstart: Set up disaster recovery to a secondary Azure region for an Azure VM. This quickstart describes how to set up disaster recovery for an Azure VM by replicating it to a secondary Azure region.

Learn more with self-paced training
Introduction to Azure Backup. This training module helps you determine if Azure Backup is appropriate for your backup needs.

Introduction to Azure Site Recovery. This module explains what Azure Site Recovery does, how it works, and when you should choose it.

Protect your virtual machines by using Azure Backup (exercise, subscription required). Learn how Azure Backup works to protect and restore virtual machine data.

Implement hybrid backup and recovery with Windows Server IaaS (exercise,subscription required). Learn about Windows IaaS VM recovery, perform backup and restore of on-premises workloads, and manage Azure VM backups.

Protect your Azure infrastructure with Azure Site Recovery. Learn how to use Azure Site Recovery to manage and orchestrate replication for your on-premises infrastructure.







Point 3: Configure Azure Monitor

Learn how to configure Azure Monitor, including querying the Azure Monitor activity log.


Learning objectives
In this module, you learn how to:

Identify the features and usage cases for Azure Monitor.
Configure and interpret metrics and logs.
Identify the Azure Monitor components and data types.
Configure the Azure Monitor activity log.


1- Introduction

Azure Monitor is a comprehensive solution that collects, analyzes, and responds to telemetry data from both on-premises and cloud environments.

Suppose you work for a large e-commerce company that relies heavily on its online platform to generate revenue. During a major sales event, your website experiences a sudden increase in traffic, causing performance issues and impacting customer experience. As a result, customers are unable to complete their purchases. Poor customer experience led to lost sales and had a negative impact on the company's reputation. To prevent this from happening again, you need a tool that can monitor the availability and performance of your applications and services in real-time. With this tool you can quickly identify and resolve issues. Azure Monitor is the solution that can help you achieve this.

In this module, you will learn about the features and usage cases of Azure Monitor. You learn how to configure and interpret metrics and logs. You review the different components and data types in Azure Monitor. You also learn how to access and query the activity log.

The goal of this module is to equip you with the knowledge and skills to effectively use Azure Monitor.

Learning objectives
In this module, you learn how to:

Identify the features and usage cases for Azure Monitor.
Configure and interpret metrics and logs.
Identify the Azure Monitor components and data types.
Configure the Azure Monitor activity Log.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Familiarity with basic monitoring, evaluation, and reporting concepts.
Knowledge of Azure resources and services that benefit from monitoring activities.
Knowledge of the Azure portal so you can implement monitoring techniques.


Next unit: Describe Azure Monitor key capabilities


2- Describe Azure Monitor key capabilities

Azure Monitor provides you with a comprehensive solution for collecting, analyzing, and responding to telemetry data from your on-premises and cloud environments. The service features help you understand how your applications are performing. You can use Azure Monitor to proactively identify issues that affect your apps and resources, and take action to maximize their availability and performance.


Things to know about Azure Monitor
Azure Monitor provides features and capabilities in three areas:

Monitor and visualize metrics: Azure Monitor gathers numerical metric values from your Azure resources according to your preferences. Azure Monitor offers different methods for viewing your metric data to help you understand the health, operation, and performance of your system.

Query and analyze logs: Azure Monitor Logs (Log Analytics) generates activity logs, diagnostic logs, and telemetry information from your monitoring solutions. The service provides analytics queries that you can use to help with troubleshooting and visualizations of your log data.

Set up alerts and actions: Azure Monitor lets you set up alerts for your gathered data to notify you when critical conditions arise. You can configure actions based on the alert conditions, and take automated corrective steps based on triggers from your metrics or logs.

Next unit: Describe Azure Monitor components


3- Describe Azure Monitor components

Monitoring is the act of collecting and analyzing data. The data can be used to determine the performance, health, and availability of your business applications and the resources they depend on.

An effective monitoring strategy helps you understand the detailed operation of the components of your applications. Monitoring also helps you increase your uptime by proactively notifying you of critical issues. You can then resolve the issues before they become severe.

Azure includes multiple services that individually perform a specific role or task in the monitoring space. Together, these services deliver a comprehensive solution for collecting, analyzing, and acting on data from your applications and the Azure resources that support them. The services also work to monitor critical on-premises resources to provide a hybrid monitoring environment. Understanding the tools and data that are available is the first step in developing a complete monitoring strategy for your application.

Things to know about monitoring with Azure
Let's take a look at the various Azure components that support Azure Monitor capabilities. The following diagram provides a high-level view of how Azure and Azure Monitor work together to provide you with a robust monitoring and diagnostics solution.

Diagram that shows the different monitoring and diagnostic services available in Azure as described in the text.

The monitoring and diagnostic services offered in Azure are divided into broad categories such as Core, Application, Infrastructure, and Shared Capabilities.

Data stores in Azure Monitor hold your metrics and logs. Azure Monitor Metrics and Azure Monitor Logs are the two base types of data used by the service.

Various monitoring sources provide Azure Monitor with the metrics and logs data to analyze. These sources can include your Azure subscription and tenant, your Azure service instances, your Azure resources, data from your applications, and more.

Azure Monitor Insights performs different functions with the collected data, including analysis, alerting, and streaming to external systems.

Get insights: Access the Azure Application Insights extension to Azure Monitor to use the Application Performance Monitoring (APM) features. You can use APM tools to monitor your application performance and gather trace logging data. Application Insights are available for many Azure services, such as Azure Virtual Machines and Azure Virtual Machine Scale Sets, Azure Container Instances, Azure Cosmos DB, and Azure IoT Edge.

Visualize: Utilize the many options in Azure Monitor for viewing and interpreting your gathered metrics and logs. You can use Power BI with the Azure Workbooks feature of Azure Monitor and access configurable dashboards and views.

Analyze: Work with Azure Monitor Logs (Log Analytics) in the Azure portal to write log queries for your data. You can interactively analyze your log data by using Azure Monitor Metrics and the powerful analysis engine.

Respond: Set up log alert rules in Azure Monitor to receive notifications about your application performance. You can configure the service to take automated action when the results of your queries and alerts match certain conditions or results.

Integrate: Ingest and export log query results from the Azure CLI, Azure PowerShell cmdlets, and various APIs. Set up automated export of your log data to your Azure Storage account or Azure Event Hubs. Build workflows to retrieve your log data and copy to external locations with Azure Logic Apps.

Next unit: Define metrics and logs


4- Define metrics and logs

All data collected by Azure Monitor fits into one of two fundamental types, metrics and logs:

Metrics are numerical values that describe some aspect of a system at a particular point in time. Metrics are lightweight and capable of supporting near real-time scenarios.

Logs contain different kinds of data organized into records with different sets of properties for each type. Data like events and traces are stored as logs along with performance data so all the data can be combined for analysis.

Things to know about Azure Monitor metrics
Let's examine how to work with Azure Monitor metrics in the Azure portal.

For many Azure resources, the metrics data collected by Azure Monitor is displayed on the Overview page for the resource in the Azure portal. Consider the overview for an Azure virtual machine that has several charts that show performance metrics.

You can use Azure Monitor metrics explorer to view the metrics for your Azure services and resources.

In the Azure portal, select any graph for a resource to open the associated metrics data in metrics explorer. The tool lets you chart the values of multiple metrics over time. You can work with the charts interactively or pin them to a dashboard to view them with other visualizations.

Illustration that depicts Azure Monitor metrics data graphs providing information to Metric Analytics in the Azure portal.

Things to know about Azure Monitor Logs
You can also work with Azure Monitor Logs (Log Analytics) in the Azure portal. Let's review the details.

In the Azure portal, log data collected by Azure Monitor is stored in Log Analytics.

Log Analytics includes a rich query language to help you quickly retrieve, consolidate, and analyze your collected data.

You can work with Log Analytics to create and test queries. Use the query results to directly analyze the data, save your queries, visualize the data, and create alert rules.

Azure Monitor uses a version of the Data Explorer query language. The language is suitable for simple log queries, but also includes advanced functionality like aggregations, joins, and smart analytics. You can quickly learn the query language by completing several available lessons. Particular guidance is provided for users familiar with SQL and Splunk.

Illustration that depicts an Azure Monitor Logs database providing information to Log Analytics in the Azure portal.

Next unit: Identify monitoring data and tiers


5- Identify monitoring data and tiers

Azure Monitor can collect data from various sources. You can think of the collected data as being categorized by tier. Tiers can include data collected from many sources, such as:

Your application
The operating system
Services and resources used by your application
The platform that supports your application
Things to know about data collection
Review the following details about how Azure Monitor collects different categories of data.

Azure Monitor begins collecting data as soon as you create your Azure subscription and add resources.

When you create or modify resources, this data is stored in Azure Monitor activity logs.

Performance data about resources, along with the amount of resources consumed, is stored as Azure Monitor metrics.

Extend the data you're collecting by enabling diagnostics and adding Azure Monitor Agent to compute resources. By extending your data sources, you can collect data for the internal operation of the resources.

Azure Monitor Agent also lets you configure different data sources to collect logs and metrics from Windows and Linux guest operating systems.

Azure Monitor can collect log data from any REST client by using the Data Collector API. The Data Collector API lets you create custom monitoring scenarios and extend monitoring to resources that don't expose data through other sources.

Monitoring data tiers
The following table summarizes the tiers of monitoring data that are collected by Azure Monitor.

Data tier	Description
Application	The Application tier contains monitoring data about the performance and functionality of your application code. This data is collected regardless of your platform.
Guest OS	Monitoring data about the operating system on which your application is running is organized into the Guest OS tier. Your application can run in Azure, another cloud, or on-premises.
Azure resource	The Azure resource tier holds monitoring data about the operation of any Azure resource you utilize, including consumption details for the resource.
Azure subscription	The Azure subscription tier contains monitoring data about the operation and management of your Azure subscription. The tier also contains data about the health and operation of Azure itself.
Azure tenant	Data about the operation of your tenant-level Azure services, such as Microsoft Entra ID, is organized into the Azure tenant tier.



Next unit: Describe activity log events

6-  Describe activity log events

The Azure Monitor activity log is a subscription log that provides insight into subscription-level events that occur in Azure. Events can include a range of data from Azure Resource Manager operational data to updates on Azure service health events.

How to use the Azure Activity Log

Things to know about activity logs
Let's examine some details about working with activity logs in Azure Monitor.

You can use the information in activity logs to understand the status of resource operations and other relevant properties.

Activity logs can help you determine the "what, who, and when" for any write operation (PUT, POST, DELETE) performed on resources in your subscription.

Activity logs are kept for 90 days.

You can query for any range of dates in an activity log, as long as the starting date isn't more than 90 days in the past.

You can retrieve events from your activity logs by using the Azure portal, the Azure CLI, PowerShell cmdlets, and the Azure Monitor REST API.

Diagram that shows how Azure Monitor activity logs gather information from compute and non-compute resources in Azure.

Business scenarios
Activity logs can help you monitor your configuration and get details for many scenarios, such as:

What operations happened on resources in my subscription?

Who initiated the operations?

When did the operations occur?

What's the current status of the operations?

What are the values of other properties that can help with my analysis of the resources and operations?

Next unit: Query the activity log


7- Query the activity log

In the Azure portal, you can filter your Azure Monitor activity logs so you can view specific information. The filters enable you to review only the activity log data that meets your criteria. You might set filters to review monitoring data about critical events for your primary subscription and production virtual machine during peak business hours.

Screenshot that shows filter options for activity logs in the Azure portal.

Things to know about activity log filters
Let's review some of the filters you can set to control what data to review in your activity log:

Subscription: Show the data for one or more specified Azure subscription names.

Timespan: Show data for a specified time by choosing the start and end time for events, such as a six-hour period.

Event Severity: Show events at the selected severity levels, including Informational, Warning, Error, or Critical.

Resource group: Show data for one or more specified resource groups within your specified subscriptions.

Resource (name): Show data for the specified resources.

Resource type: Show data for resources of a specified type, such as Microsoft.Compute/virtualmachines.

Operation name: Show data for a selected Azure Resource Manager operation, such as Microsoft.SQL/servers/Write.

Event initiated by: Show operation data for a specified user who performed the operation, referred to as the "caller."

After you define a set of filters, you can pin the filter set to the Azure Monitor dashboard. You can also download your activity log search results as a CSV file.

In addition to the filters, you can enter a text string in the Search box. Azure Monitor tries to match your search string against data returned for all fields in all events that corresponds to your filter settings.

Things to know about event categories
The following table summarizes the categories of events that you can review in your activity logs. The information displayed for events is based on your other filter settings.

Event category	Event data	Examples
Administrative	All create, update, delete, and action operations performed through Azure Resource Manager, and any changes to role-based access control (RBAC) in your filtered subscriptions	create virtual machine

delete network security group
Service Health	All service health events for Azure services and resources connected with your filtered subscriptions, including Action Required, Assisted Recovery, Incident, Maintenance, Information, or Security	SQL Azure in East US is experiencing downtime

Azure SQL Data Warehouse Scheduled Maintence Complete
Resource Health	All resource health events for your filtered Azure resources, including Available, Unavailable, Degraded, or Unknown, and identified as Platform Initiated or User Initiate	Virtual Machine health status changed to unavailable

Web App health status changed to available
Alert	All activations of Azure alerts for your filtered subscriptions and resources	CPU % on devVM001 has been over 80 for the past 5 minutes

Disk read LessThan 100000 in the last 5 minutes
Autoscale	All events related to the operation of the autoscale engine based on any autoscale settings defined for your filtered subscriptions	Autoscale scale up action failed
Recommendation	Recommendation events for certain Azure resource types, such as web sites and SQL servers, based on your filtered subscriptions and resources	Recommendations for how to better utilize your resources
Security	All alerts generated by Microsoft Defender for Cloud affecting your filtered subscriptions and resources	Suspicious double extension file executed
Policy	All effect action operations performed by Azure Policy for your filtered subscriptions and resources, where every action taken by Azure Policy is modeled as an operation on a resource	Audit and Deny


Next unit: Interactive lab simulation



8- Interactive lab simulation

Lab scenario
Your organization wants insight into the performance and configuration of Azure resources. As the Azure Administrator you need to:

Explore Azure virtual machine monitoring capabilities, including available metrics.
Explore alerts and notification features.
Review logs by using Azure Monitor Logs (Log Analytics) queries.
Architecture diagram
Architecture diagram as explained in the text.

Objectives
Task 1: Provision the lab environment.
Review an Azure Resource Manager (ARM) template.
Use the ARM template to deploy a virtual machine to use to test monitoring scenarios.
Task 2: Register the Microsoft Insights and Microsoft Alerts Management resource providers.
Create a Log Analytics workspace in the same region as the virtual machines.
Create an Azure Automation Account and associate it with the Azure Monitor Logs (Log Analytics) workspace.
Enable update management.
Task 3: Create and configure an Azure Monitor Logs (Log Analytics) workspace and Azure Automation-based solutions.
Review Azure virtual machine monitoring options.
Review the list of available metrics.
Task 4: Review default monitoring settings of Azure virtual machines.
Task 5: Configure Azure virtual machine diagnostic settings.
Review the Azure virtual machine monitoring settings and enable guest-level monitoring.
Enable Azure Monitor Agent and available metrics.
Task 6: Review Azure Monitor functionality.
Configure Azure Monitor metrics.
Create an alert rule based on average percentage CPU.
Configure notifications for an action group.
Trigger increased CPU utilization and review alert notifications.
Task 7: Review Azure Monitor Logs (Log Analytics) functionality.
Create a log query to chart the virtual machine's available memory over the last hour.
Run the log query and preview the data.
 Note

Select the thumbnail image to start the lab simulation. When you're done, be sure to return to this page so you can continue learning.

Screenshot of the simulation page.

Next unit: Knowledge check


9- Knowledge check

Your company supports large-scale applications in the cloud. They've decided to implement Azure Monitor for a simplified logging strategy that consolidates the log data for improved visibility across services. You're tasked with developing the plan to track the health of the cloud applications. A few teams have submitted configuration requirements and questions for your consideration:

The Engineering team has requested a summary of data that can be collected by Azure Monitor.

The IT team wants to know how long activity logs can be retained.

You need to track changes across different categories, including the deletion of network security groups (NSGs) through Azure Resource Manager.

Answer the following questions
Choose the best response for each of the following questions. Then select Check your answers.


1. Which category includes information to help track NSGs and Azure Resource Manager? 

Service Health

Administrative

Policy

2. What data does Azure Monitor collect? 

Azure billing details

Back up of database transaction logs

Data from many different sources, such as the Application event log

3. How long are Azure Monitor activity logs kept? 

90 days

30 days

120 days



Summary and resources

Azure Monitor helps you maximize the availability and performance of your applications and services.

In this module, you identified the features and usage cases for Azure Monitor. You examined how to configure and interpret metrics and logs. You explored the Azure Monitor components and data types. You learn how to configure Azure Monitor Logs (Log Analytics) activity Log monitoring.

The main takeaways from this module are:

Azure Monitor is a comprehensive solution for monitoring and analyzing telemetry data from both on-premises and cloud environments.

Azure Monitor helps you understand the performance of your applications and allows you to proactively identify and resolve issues.

Azure Monitor offers features in three areas: monitoring and visualizing metrics, querying and analyzing logs, and setting up alerts and actions.

Learn more with Azure documentation
Azure Monitor documentation. Learn about monitoring Azure and on-premises services. Understand how to aggregate and analyze metrics, logs, and traces. Respond to issues by firing alerts that can send notifications or by calling automated solutions.

Azure Monitor Metrics. Azure Monitor Metrics is a feature of Azure Monitor that collects numeric data from monitored resources into a time-series database. Metrics are numerical values that are collected at regular intervals and describe some aspect of a system at a particular time.

Azure Monitor Logs. Azure Monitor Logs is a feature of Azure Monitor that collects and organizes log and performance data from monitored resources. Several features of Azure Monitor store their data in Logs and present this data in various ways to assist you in monitoring the performance and availability of your cloud and hybrid applications and their supporting components.

Learn more with self-paced training
Design a holistic monitoring strategy on Azure. Learn how to select the appropriate monitoring solution based on a usage case.

Monitor and report on security events in Microsoft Entra ID. Learn how to monitor Microsoft Entra security events to prevent unauthorized access and potential data loss.

Monitor, diagnose, and troubleshoot your Azure Storage. Discover the tools available to detect and correct problems with your Azure Storage to enable your cloud storage infrastructure to operate at peak performance.





Point 4: Configure Log Analytics

You learn how to configure Log Analytics including structuring queries.


Learning objectives
After completing this module, you'll be able to:

Identify the features and usage cases for Log Analytics.

Create a Log Analytics workspace.

Structure a Log Analytics query and review results.


1- Introduction

Log Analytics is a tool in Azure Monitor that allows you to edit and run log queries for data collected in Azure Monitor Logs. It offers query features and tools, supports the Kusto Query Language (KQL), and allows for detailed analysis and problem-solving.

Imagine you're an Azure Administrator working for a large e-commerce company. Your company recently experienced a major security breach, and you need to investigate the root cause and prevent future incidents. You have access to logs from various Azure services, but manually analyzing them would be time-consuming and inefficient.

By using Log Analytics, you can easily query and analyze the logs to identify any suspicious activities, track changes, and ensure compliance with security standards. With Log Analytics, you can quickly assess update requirements and time-to-complete, track changes, and identify access issues in your systems. It helps meet strict SLAs for businesses and provides a single interface for analyzing data from multiple sources.

The goal of this module is to provide you with the knowledge and skills to effectively use Log Analytics in Azure Monitor.

Learning objectives
In this module, you learn how to:

Identify the features and usage cases for Log Analytics in Azure Monitor.
Structure and create a Log Analytics workspace in the Azure portal.
Use KQL to query a Log Analytics workspace and review results.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Working knowledge of Azure Monitor including data sources and collected data.
Experience with the Azure portal including navigating and locating resources.
Familiarity with structuring and executing data queries.



Next unit: Determine Log Analytics uses


2- Determine Log Analytics uses

Log Analytics is a tool for Azure Monitor that's available in the Azure portal. You can use Log Analytics to edit and run log queries for the data collected in Azure Monitor Logs. Log queries help you to search for patterns and identify issues.

Screenshot that shows an example of Azure Monitor Logs in the Azure portal.

Things to know about Log Analytics
Let's examine some characteristics of Log Analytics in Azure Monitor.

Log Analytics in Azure Monitor offers query features and tools that help you answer virtually any question about your monitored configuration.

Log Analytics supports the Kusto Query Language (KQL). You can create simple or complex queries with KQL, including:

Search and sort by value, time, property state, and more
Join data from multiple tables
Aggregate large sets of data
Perform intricate operations with minimal code
When your Azure Monitor Logs contain sufficient collected data, and you understand how to construct the appropriate query, you can use Log Analytics to complete detailed analysis and problem solving.

Things to consider when using Log Analytics
Some features in Azure Monitor, such as insights and solutions, process log data without exposing you to the underlying queries. To use other Azure Monitor features, you need to understand how to construct queries and apply them to interactively analyze data in Azure Monitor Logs. The following business scenarios showcase the advantages of querying Azure Monitor Logs with Log Analytics.

Business scenario: Assess update requirements and time-to-complete
An important daily task for IT admins is to assess system update requirements and plan for configuration patches. Accurate scheduling is critical because the patching process relates to SLAs to the business and can negatively affect business functions.

In the past, administrators had to schedule a patch update with only limited knowledge of how long it might take to complete the process. With an Azure subscription, admins can access benefits of the Microsoft Azure platform. Azure collects data from all customers performing patches. Azure uses the gathered data to provide an average patching time for specific updates.

This use of "crowd-sourced" data is unique to cloud systems. It's a great example of how Log Analytics in Azure Monitor can you help meet strict SLAs for your business.

Business scenario: Track changes and identify access issues
Troubleshooting an operational incident is a complex process that requires access to multiple data streams. By monitoring your systems from the Azure platform, you can easily perform analysis from multiple angles. You have access to data from a wide variety of sources through a single interface for correlation of information.

By tracking changes across the Azure environment, Log Analytics in Azure Monitor can help you easily identify common issues, such as:

Abnormal behavior from a specific account
Users installing unapproved software
Unexpected system reboots or shutdowns
Evidence of security breaches
Specific problems in loosely coupled applications



Next unit: Create a Log Analytics workspace


3- Create a Log Analytics workspace

When you capture logs and data in Azure Monitor, Azure stores the collected information in a Log Analytics workspace. Your Log Analytics workspace is the basic management environment for Azure Monitor Logs.

How to define your Log Analytics scope

Things to know about the Log Analytics workspace
To get started with Log Analytics in Azure Monitor, you need to create your workspace. Each workspace has a unique workspace ID and resource ID. After you create your workspace, you configure your data sources and solutions to store their data in your workspace.

Screenshot that shows how to create a Log Analytics workspace in the Azure portal.

To create your Log Analytics workspace, configure the following parameters:

Name: Provide a name for your new Log Analytics workspace. The name for your workspace must be unique within your resource group.

Subscription: Specify the Azure Subscription to associate with your workspace.

Resource Group: Specify the resource group to associate with your workspace. You can choose an existing resource group or create a new one. The resource group must contain at least one Azure Virtual Machines instance.

Region: Select the region where you deploy your virtual machines.

 Note

The region must support Log Analytics. You can review the regions that support Log Analytics. In the Search for a product box, enter "Azure Monitor."

Pricing: The default pricing tier for a new workspace is pay-as-you-go. Charges incur only after you start collecting data.

Each Log Analytics workspace in Azure Monitor can have a different pricing tier. You can change the pricing tier for a workspace and also track the changes.

Next unit: Create Kusto (KQL) queries



4- Create Kusto (KQL) queries

Log Analytics in Azure Monitor supports the Kusto Query Language (KQL). The KQL syntax helps you quickly and easily create simple or complex queries to retrieve and consolidate your monitoring data in the repository.

Write KQL log queries for Azure Monitor
Watch the following video to learn how to write KQL log queries with Log Analytics in Azure Monitor. The video covers the following concepts:

View table data in the Azure Monitor Logs repository
Create simple and complex queries
Filter and summarize search results
Add visualizations for search results

In the next unit, we take a closer look at how to structure a KQL query.

Things to consider when using KQL queries
Here are some of the many things you can accomplish with KQL log queries in Log Analytics:

Create and save searches of your data stored in the Azure Monitor Logs repository.

Use your saved log searches to directly analyze your data in the Azure portal.

Configure your saved log searches to run automatically.

Configure your saved log searches to produce notification alerts.

Add visualizations for your saved log searches to see graphical views of your environment health.

Export your data from the repository into tools like Power BI or Excel to analyze your data outside of Log Analytics.

Next unit: Structure Log Analytics queries



5- Structure Log Analytics queries

Administrators build Log Analytics queries from data stored in dedicated tables in a Log Analytics workspace. Some common dedicated tables include Event, Syslog, Heartbeat, and Alert. When you build a Kusto Query Language (KQL) query, you begin by determining which tables in the Azure Monitor Logs repository have the data you're looking for.

The following illustration highlights how KQL queries use the dedicated table data for your monitored services and resources.

Illustration that shows how to build Log Analytics queries from data in dedicated tables in a Log Analytics workspace.

Things to know about KQL query structure
Let's take a closer look at dedicated table data and how to structure a KQL log query.

Each of your selected data sources and solution stores its data in dedicated tables in your Log Analytics workspace.

Documentation for each data source and solution includes the name of the data type that it creates and a description of each of its properties.

The basic structure of a query is a source table followed by a series of commands (referred to as operators).

A query can have a chain of multiple operators to refine your data and perform advanced functions.

Each operator in a query chain begins with a pipe character |.

Many queries require data from a single table only, but other queries can use various options and include data from multiple tables.

KQL log query examples
Let's review some common KQL log query operators and example syntax.

We can build queries to search for data in the StormEvent table that has five entries:

type	event	severity	start	duration	region
Water	Freezing rain	1	6:00 AM 01-27-2023	3 hours	1, 2
Wind	High winds	1	8:00 AM 01-27-2023	12 hours	1, 2, 4, 5
Temperature	Below freezing	2	11:00 PM 01-26-2023	10 hours	1, 2, 4, 5
Water	Snow	3	4:00 PM 01-26-2023	10 hours	1, 2, 4, 5
Water	Flood warning	2	9:00 AM 01-26-2023	10 hours	3
To find other operators and examples, review: Analyze monitoring data with Kusto Query Language - Training | Microsoft Learn.

Count number of items
Use the count operator to discover the number of records in an input record set.

The following example returns the number of records in the StormEvent table. The query results reveal the StormEvent table has five entries.

Kusto

Copy
StormEvent | count
Query results:

count
5
Return first number of items
Use the top operator to see the first N records of your input record set, sorted by your specified columns. The columns correspond to data properties defined in the dedicated table.

The following example returns the first three data records for StormEvent. The results table shows the storm event name, the severity, and the forecasted duration.

Kusto

Copy
StormEvent | top 3 by event severity duration
Query results:

event	severity	duration
Freezing rain	1	3 hours
High winds	1	12 hours
Below freezing	2	10 hours
Find matching items
Use the where operator to filter your table to the subset of rows that match the supplied predicate value. The predicate value indicates what to search for in the table, as in where=="find-this".

The following example filters the data records for StormEvent to use only records that match "snow."

Kusto

Copy
StormEvent | where event=="snow"
Your query filters to one row in the StormEvent table:

type	event	severity	start	duration	region
Water	Snow	3	4:00 PM 01-26-2023	10 hours	1, 2, 4, 5


Next unit: Knowledge check


6- Knowledge check

Your company operates a large web farm with over 100 virtual machines. They want to use Log Analytics to configure their input data sources. You're developing queries with the Kusto query language (KQL) to filter and evaluate the virtual machine log data. Here are some of the tasks and considerations you need to address:

You need a complex query to monitor the virtual machine logs and view the data in different models.

You're investigating scenarios for using Log Analytics agents.

The website team needs for a summary of options to organize the log data in Azure Monitor.

Answer the following questions
Choose the best response for each of the following questions. Then select Check your answers.


1. How does Azure Monitor organize log data? 

Event queues

Text files

Tables

2. What KQL commands build an aggregation of input data and produce visuals for query results? 

summarize and render

aggregate and visualize

count and project

3. Log Analytics agents can run on which resource? 

Only on cloud computers

On multiple platforms including other cloud providers

Only on physical computers



Summary and resources

Azure Administrators work with Log Analytics in the Azure portal to run log queries on their data in Azure Monitor logs. Administrators can create Kusto Query Language (KQL) queries to consolidate and analyze their data.

In this module, you identified the features and usage cases for Log Analytics in Azure Monitor. You created a Log Analytics workspace in the Azure portal. You reviewed how to use KQL to structure a Log Analytics query and review the results.

The main takeaways from this module are:

Log Analytics in Azure Monitor allows you to edit and run log queries for data collected in Azure Monitor Logs.
It supports the Kusto Query Language (KQL) and provides features for detailed analysis and problem-solving.
Log Analytics helps assess update requirements, track changes, and identify access issues in your systems.
Learn more
Create a Log Analytics workspace. This article shows you how to create a Log Analytics workspace. You need a Log Analytics workspace to collect and analyze data.

Log Analytics tutorial. This tutorial walks you through the Log Analytics interface, gets you started with some basic queries, and shows you how you can work with the results.

Get started with log queries in Azure Monitor. In this tutorial, you learn to write log queries in Azure Monitor. This article includes a link to a demonstration environment.

Learn more with self-paced training
Write your first query with the Kusto Query Language (KQL). Get started by writing simple queries in Kusto Query Language (KQL) to explore and gain insights from your data.

Gain insights from your data by using Kusto Query Language. Write advanced queries in Kusto Query Language to help you gain insights from your data. Communicate these results visually in charts.

Analyze your Azure infrastructure by using Azure Monitor logs (sandbox). Use Azure Monitor logs to extract valuable information about your infrastructure from log data.






Point 5: Configure Network Watcher

You learn how to configure Network Watcher and troubleshoot common networking problems.

Learning objectives
After completing this module, you'll be able to:

Identify the features and usage cases for Azure Network Watcher.
Configure diagnostic capabilities like IP Flow Verify, Next Hop, and Network Topology.


1- Introduction

Azure Network Watcher is a powerful tool that allows you to monitor, diagnose, and manage resources in an Azure virtual network.

Imagine you're an IT administrator for a large e-commerce company. Your company relies heavily on its Azure virtual network to host its website and handle customer transactions. One day, you receive reports from customers that they're unable to access the website or complete their purchases. You need to quickly identify the cause of the connectivity issues and resolve them to minimize the impact on your business.

Your organization plans to use Network Watcher. Network Watcher's monitoring and diagnostic capabilities let you easily pinpoint the root cause of the problem and take appropriate actions. By using features such as IP flow verification, next hop analysis, and connection troubleshooting, you can ensure that your virtual network is functioning optimally.

In this module, you learn about the various features and use cases of Azure Network Watcher. The topics covered include IP flow verification, next hop analysis, and the topology tool. The module guides you on how to diagnose network configuration issues, such as broken security rules.

The goal of this module is to provide you with a comprehensive understanding of Azure Network Watcher and its capabilities. By the end of this module, you effectively monitor, diagnose, and manage your Azure virtual network using Network Watcher.

Learning objectives
In this module, you learn how to:

Identify the features and usage cases for Azure Network Watcher.
Configure diagnostic capabilities like IP flow verify, next hop, and network topology.
Skills measured
The content in the module helps you prepare for Exam AZ-104: Microsoft Azure Administrator.

Prerequisites
Knowledge of Azure networking features such as virtual networks and traffic routes.

Familiarity with how to systematically troubleshoot an issue.

Next unit: Describe Azure Network Watcher features


2- Describe Azure Network Watcher features

Azure Network Watcher provides tools to monitor, diagnose, view metrics, and enable or disable logs for resources in an Azure virtual network. Network Watcher is a regional service that enables you to monitor and diagnose conditions at a network scenario level.

Screenshot of the Network Watcher Get Started page in the Azure portal.

Things to know about Network Watcher
Let's review some of the prominent features of Network Watcher.

Feature	Description	Scenarios
IP flow verify	Quickly diagnose connectivity issues from or to the internet, and from or to your on-premises environment.	Identify if a security rule blocks ingress or egress traffic to or from a virtual machine

Troubleshoot issues to determine if other exploration is required
Next hop	View the next connection point (or next hop) in your network route, and analyze your network routing configuration.	Determine if there's a next hop, and view the next hop target, type, and route table

Confirm traffic reaches an intended target destination
VPN troubleshoot	Diagnose and troubleshoot the health of your virtual network gateway or connection with gathered data. View connection statistics, CPU and memory information, IKE security errors, packet drops, and buffers and events.	View summary diagnostics in the Azure portal

Review detailed diagnostics in generated log files stored in your Azure storage account

Simultaneously troubleshoot multiple gateways or connections
NSG diagnostics	Use flow logs to map IP traffic through a network security group (NSG). A common implementation for NSG flow logs is to meet security compliance regulations and auditing requirements.	Define prescriptive NSG rules for your organization, and conduct periodic compliance audits

Compare your prescriptive NSG rules against the effective rules for each virtual machine in your network
Connection troubleshoot	Azure Network Watcher Connection Troubleshoot is a more recent addition to the Network Watcher suite of networking tools and capabilities. Check a direct TCP or ICMP connection from a virtual machine, application gateway, or Azure Bastion host to a virtual machine.	Troubleshoot your network performance and connectivity issues in Azure

Troubleshoot connection issues for a virtual machine, application gateway, or Azure Bastion host
 Note

To use Network Watcher, you must be an Owner, Contributor, or Network Contributor. If you create a custom role, the role must be able to read, write, and delete the Network Watcher service.

Things to consider when using Network Watcher
Azure Network Watcher supports many Azure monitoring tasks and scenarios. As you review these features, think about how Network Watcher can support your Azure monitoring requirements.

Consider remote monitoring. Automate remote network monitoring with packet capture. You can monitor and diagnose networking issues without logging in to your virtual machines.

Consider alert notifications. Set alerts to trigger packet capture, and access real-time performance information at the packet level. When you observe an issue, you can investigate in detail for better diagnoses.

Consider NSG flow log diagnosis. Use NSG flow logs to gain insight into your network traffic. Build a deeper understanding of your network traffic pattern by using NSG flow logs. Information provided by flow logs helps you gather data for compliance, auditing, and monitoring your network security profile.

Consider log analysis. Diagnose your most common Azure VPN Gateway and connections issues. You can identify issues and use the generated detailed logs to assist your analysis.

Next unit: Review IP flow verify diagnostics



3- Review IP flow verify diagnostics

The IP flow verify feature in Azure Network Watcher checks connectivity from or to the internet, and from or to your on-premises environment. This feature helps you identify if a security rule is blocking traffic to or from your virtual machine or the internet.

Screenshot of the IP flow verify feature in the Azure portal.

Things to know about IP flow verify
Let's examine the configuration details and functionality of the IP flow verify feature in Azure Network Watcher.

You configure the IP flow verify feature with the following properties in the Azure portal:

Virtual machine and network interface
Local (source) port number
Remote (destination) IP address, and remote port number
Communication protocol (TCP or UDP)
Traffic direction (Inbound or Outbound)
The feature tests communication for a target virtual machine with associated network security group (NSG) rules by running inbound and outbound packets to and from the machine.

After the test runs complete, the feature informs you whether communication with the machine succeeds (allows access) or fails (denies access).

If the target machine denies the packet because of an NSG, the feature returns the name of the controlling security rule.

Things to consider when using IP flow verify
The IP flow verify feature is ideal for helping to ensure correct application of your security rules.

When you deploy a virtual machine, Azure applies several default security rules to your machine. The security rules allow or deny traffic to or from your virtual machine. You can override Azure's default rules or create other rules.

At some point, your virtual machine might be unable to communicate with other resources because of a security rule. You can use the IP flow verify feature to troubleshoot your NSG rules.

If test runs fail, but the IP flow verify feature doesn't indicate the issue is related to your NSG rules, you need to explore other areas, such as firewall restrictions.

Next unit: Review next hop diagnostics


4- Review next hop diagnostics

The next hop feature in Azure Network Watcher checks if traffic is being directed to the intended destination. This feature lets you view the next connection point (or next hop) in your network route, and helps you verify a correct network configuration.

Screenshot of the next hop feature in the Azure portal.

Things to know about next hop
Let's review the configuration properties and summary of the next hop feature in Azure Network Watcher.

You configure the next hop feature with the following properties in the Azure portal:

Your subscription and resource group
Virtual machine and network interface
Source IP address
Destination IP address (If you want to confirm a specified target is reachable)
The feature tests the next connection point in your network route configuration.

The next hop test returns three items:

Next hop type
IP address of the next hop (If available)
Route table for the next hop (If available)
Examples of a next hop are Internet, Virtual Network, and Virtual Network Service Endpoint.

If the next hop is a user-defined route (UDR), the process returns the UDR route. Otherwise, next hop returns the system route.

If the next hop is of type None, there might be a valid system route to the destination IP address, but no next hop exists to route the traffic to the target.

Things to consider when using next hop
The next hop feature is ideal for identifying unresponsive virtual machines or broken routes in your network.

When you create a virtual network, Azure creates several default outbound routes for network traffic. Outbound traffic from all resources (such as virtual machines) deployed in the virtual network is routed based on Azure's default routes. You can override Azure's default routes or create other routes.

You might find that a virtual machine can no longer communicate with other resources connected by a specific route. You can use the next hop feature to examine a specific source and destination IP address in your configuration.

Next hop tests the communication between the source and destination, and reports the type of next hop in the traffic route. You can then remove, change, or add a route, to resolve routing issues.

Next unit: Visualize the network topology



5- Visualize the network topology

Administrators sometimes need to troubleshoot virtual networks that they didn't help to create. They might not be fully aware of all the aspects of the infrastructure and configuration.

Azure Network Watcher provides a network monitoring topology tool to help administrators visualize and understand infrastructure. The following image shows an example topology diagram for a virtual network in Network Watcher.

Screenshot of the Network Watcher Topology page in the Azure portal.

Things to know about the topology tool
Review the following characteristics of the network topology capability in Azure Network Watcher.

The Network Watcher Topology tool generates a visual diagram of the resources in a virtual network.

The graphical display shows the resources in the network, their interconnections, and their relationships with each other.

You can view subnets, virtual machines, network interfaces, public IP addresses, network security groups, route tables, and more.

To generate a topology, you need an Azure Network Watcher instance in the same region as the virtual network.

Next unit: Knowledge check



6- Knowledge check

Your company deploys virtual machines in Azure. As an administrator, you're responsible for helping to ensure network connectivity for all resources, and troubleshooting any issues. You're using Azure Network Watcher to support your tasks. You help is needed in several areas.

The infrastructure team thinks it would be helpful to get a visual representation of the company's networking elements.

Users are reporting connectivity errors and timeouts. The help desk thinks a security rule might be blocking traffic to or from one of the virtual machines.

You need to identify which business scenarios to support by using Network Watcher.

Answer the following questions
Choose the best response for each of the following questions. Then select Check your answers.


1. How does Azure Network Watcher support graphical visualizations for networks? 

Next hop

Views

Topology tool

2. What Azure Network Watcher feature can help you quickly troubleshoot the issue reported by the help desk? 

IP Flow Verify

Connection Troubleshoot

Packet capture

3. Which scenario is a good use case for Azure Network Watcher? 

Log activity events

Diagnose network traffic filtering problems to or from a virtual machine

Provide PaaS monitoring and web analytics



Summary and resources

Azure Network Watcher provides the tools you need to monitor, troubleshoot, and optimize your network infrastructure.

In this module, you identified the features and usage cases for Azure Network Watcher. You reviewed how to configure and work with several Network Watcher features, including IP flow verify, next hop, and the topology tool. You discovered how to diagnose your network configuration for several issues, such as broken security rules.

The main takeaways from this module are:

Network Watcher is a powerful tool for monitoring and troubleshooting your network infrastructure in Azure.

Network Watcher has many features including IP flow verify, next hop analysis, and a network topology visualization tool.

The IP Flow Verify feature checks security and admin rules for packet routing to an Azure virtual machine.

The Next Hop feature helps you determine if traffic is being directed to the intended destination.

The network topology feature lets you visualize and understand the infrastructure of virtual networks.

Learn more with documentation
Azure Network Watcher documentation. This collection of articles is your starting point for Network Watcher.

What is Azure Network Watcher?. This article reviews Network Watcher monitoring, network diagnostic tools, and traffic troubleshooting.

Tutorial: Diagnose a virtual machine network routing problem using the Azure portal. In this tutorial, you use Azure Network Watcher next hop tool to troubleshoot and diagnose a VM routing problem.

Tutorial: Diagnose a communication problem between virtual networks using the Azure portal. This tutorial shows you how to use Azure Network Watcher VPN troubleshoot capability to diagnose and troubleshoot a connectivity issue between two virtual networks.

Learn more with self-paced training
Monitor and troubleshoot your end-to-end Azure network infrastructure by using network monitoring tools. Use Network Watcher tools, diagnostics, and logs to help find and fix networking issues in your Azure infrastructure.

Configure monitoring for virtual networks. Understand how to use Azure Network Watcher Connection Monitor, flow logs, NSG diagnostics, and packet capture.






Point 6: Improve incident response with Azure Monitor alerts

Respond to incidents and activities in your infrastructure through alerting capabilities in Azure Monitor.


Learning objectives
In this module, you'll:

Configure alerts on events in your Azure resources based on metrics, log events, and activity log events.
Learn how to use action groups in response to an alert, and how to use alert processing rules to override action groups when necessary.


1- Introduction

Microsoft Azure provides a robust alerting and monitoring solution called Azure Monitor. You can use Azure Monitor to configure notifications and alerts for your key systems and applications. These alerts ensure that the correct team knows when a problem arises.

You work for a large shipping company that recently deployed several web applications to the Azure platform. Due to a configuration error, the customer-facing order tracker was offline. The issue wasn't identified until customers started complaining that they couldn't track their orders. As a consequence, customer satisfaction with your service dropped.

As your company's Azure solution architect, you need to find a solution that detects problems in your environments in real time. The correct team will be notified so it can resolve any problems before your customers notice.

Learning objectives
In this module, you'll:

Explore alerts by using Azure Monitor.
Understand when to use metric, log, and activity log events.
Create and use metric, log, and activity log alerts.
Use action groups to determine what kind of notifications are sent and to whom.
Learn how to use alert-processing rules to override the normal behavior of action groups when needed.
Prerequisites
Knowledge of Azure Monitor


Next unit: Explore the different alert types that Azure Monitor supports


2- Explore the different alert types that Azure Monitor supports

Azure Monitor is a powerful reporting and analytics tool. You can use it for insights into the behavior and running of your environment and applications. You can then respond proactively to faults in your system.

After the downtime that your customers faced, you set up monitoring on your key resources in Azure. With the monitoring in place, you want to make sure the right people are being alerted at the right level.

In this unit, you'll learn how Azure Monitor receives resource data, what makes up an alert, and how and when to use an alert. Finally, you'll learn how to create and manage your own alerts.

Data types in Azure Monitor
Azure Monitor receives data from target resources like applications, operating systems, Azure resources, Azure subscriptions, and Azure tenants. The nature of the resource defines which data types are available. A data type can be a metric, a log, or both a metric and a log:

The focus for metric-based data types is the numerical time-sensitive values that represent some aspect of the target resource.
The focus for log-based data types is the querying of content data held in structured, record-based log files that are relevant to the target resource.
Diagram that represents the target resources feeding into Azure Monitor and the two principal signal types: metrics and logs.

You'll learn about the three signal types that you can use to monitor your environment:

Metric alerts provide an alert trigger when a specified threshold is exceeded. For example, a metric alert can notify you when CPU usage is greater than 95 percent.
Activity log alerts notify you when Azure resources change state. For example, an activity log alert can notify you when a resource is deleted.
Log alerts are based on things written to log files. For example, a log alert can notify you when a web server has returned a number of 404 or 500 responses.
Composition of an alert rule
Every alert or notification available in Azure Monitor is the product of a rule. Some of these rules are built into the Azure platform. You can use alert rules to create custom alerts and notifications. No matter which target resource or data source you use, the composition of an alert rule remains the same.

RESOURCE
The target resource for the alert rule. You can assign multiple target resources to a single alert rule. The type of resource defines the available signal types.
CONDITION
The signal type used to assess the rule. The signal type can be a metric, an activity log, or logs. There are others, but this module doesn't cover them.
The alert logic applied to the data that's supplied via the signal type. The structure of the alert logic changes depending on the signal type.
ACTIONS
The action, like sending an email, sending an SMS message, or using a webhook.
An action group, which typically contains a unique set of recipients for the action.
ALERT DETAILS
An alert name and an alert description that specify the alert's purpose.
The severity of the alert if the criteria or logic test evaluates true. The five severity levels are:
0: Critical
1: Error
2: Warning
3: Informational
4: Verbose
Screenshot of the Create rule page in the Azure Monitor portal.

Scope of alert rules
You can get monitoring data from across most of the Azure services and report on it by using the Azure Monitor pipeline. In the Azure Monitor pipeline, you can create alert rules for these items and more:

Metric values
Log search queries
Activity log events
Health of the underlying Azure platform
Tests for website availability
Manage alert rules
Not every alert rule that you create needs to run forever. With Azure Monitor, you can specify one or more alert rules and enable or disable them, as needed.

As an Azure solution architect, you'd use Azure Monitor to enable tightly focused and specific alerts before any application change. You'd then disable the alerts after a successful deployment.

Alert summary view
The alert page shows a summary of all alerts. You can apply filters to the view by using one or more of the following categories: subscriptions, alert condition, severity, or time ranges. The view includes only alerts that match these criteria.

Screenshot of Azure Monitor alerts page in the Azure Monitor portal.

Alert condition
The system sets the alert condition.

When an alert fires, the alert's monitor condition is set to Fired.
After the underlying condition that caused the alert to fire clears, the monitor condition is set to Resolved.

1. What's the composition of an alert rule? 

Resource, condition, log, alert type

Metrics, logs, application, operating system

Resource, condition, actions, alert details

2. Which of the following is an example of a log data type? 

Percentage of CPU over time

HTTP response records

Database tables

Website requests per hour



3- Use metric alerts for alerts about performance issues in your Azure environment

Azure Monitor can use thresholds to monitor specific resources. In an organization, it's far more useful to be notified when the free disk space on a server is less than five percent instead of being alerted every time a file is saved.

As a solution architect, you want to implement regular threshold monitoring for many of your target resources and instances. Monitoring helps to head off potential issues before they can affect your customers.

In this unit, you'll investigate the different kinds of metric alerts that Azure Monitor supports.

When would you use metric alerts?
In Azure Monitor, you can use metric alerts to achieve regular threshold monitoring of Azure resources. Azure Monitor runs metric alert trigger conditions at regular intervals. When the evaluation is true, Azure Monitor sends a notification. Metric alerts are stateful, and Azure Monitor will send a notification only when the prerequisite conditions are met.

Metric alerts can be useful if, for instance, you need to know when your server CPU utilization is reaching a critical threshold of 90 percent. You can receive alerts when your database storage is getting too low, or when network latency is about to reach unacceptable levels.

Composition of a metric alert
As you learned in the previous unit, all alerts are governed by their rules. For metric alerts, there's another factor to define: the condition type. It can be static or dynamic.

You must define the type of statistical analysis to be used with either static or dynamic metric alerts. Example types are minimum, maximum, average, and total. In this example, you define the period of data to be assessed: the last 10 minutes. Finally, you set the frequency by which the alert conditions are checked: every two minutes.

Use static threshold metric alerts
Static metric alerts are based on simple static conditions and thresholds that you define. With static metrics, you specify the threshold that's used to trigger the alert or notification.

In the previously defined scenario, a static alert with a threshold of 85 percent CPU utilization checks the rule every two minutes. It evaluates the last 10 minutes of CPU utilization data to assess if it rises above the threshold. If the evaluation is true, the alert triggers the actions associated with the action group.

Use dynamic threshold metric alerts
Dynamic metric alerts use machine-learning tools that Azure provides to automatically improve the accuracy of the thresholds defined by the initial rule.

There's no hard threshold in dynamic metrics. However, you'll need to define two more parameters:

The look-back period defines how many previous periods need to be evaluated. For example, if you set the look-back period to 3, then in the example used here, the assessed data range would be 30 minutes (three sets of 10 minutes).

The number of violations expresses how many times the logic condition has to deviate from the expected behavior before the alert rule fires a notification. In this example, if you set the number of violations to two, the alert would be triggered after two deviations from the calculated threshold.

Understand dimensions
Until now, the assessed metric alerts have focused on a single target instance. Azure Monitor supports dimensions, which enable monitoring data to be supplied from multiple target instances.

You can use dimensions to define one metric alert rule and have it applied to multiple related instances. For example, you can monitor CPU utilization across all the servers running your app. You can then receive an individual notification for each server instance when the rule conditions are triggered.

You can define the dimensions by naming each target instance specifically, or you can define the dimensions by using the asterisk (*) wildcard, which uses all available instances.

Scale metric alerts
Azure Monitor supports creating metric alerts that, like dimensions, monitor multiple resources. Scaling is currently limited to Azure virtual machines. However, a single metric alert can monitor resources in one Azure region.

Creating scaling metric alert rules to monitor multiple resources is no different than creating any other metric alert rule; you just select all the resources that you want to monitor.

Like dimensions, a scaling metric alert is individual to the resource that triggered it.



Next unit: Exercise - Use metric alerts to alert on performance issues in your Azure environment


4- Exercise - Use metric alerts to alert on performance issues in your Azure environment

The shipping company you work for wants to avoid any future issues with updates to its applications on the Azure platform. To improve the alert capabilities in Azure, you've chosen to use Azure metric alerts.

In this exercise, you'll create a Linux virtual machine (VM). This VM will run an app that runs the CPU at 100 percent utilization. You'll create monitoring rules in the Azure portal and in the Azure CLI to alert you about high CPU usage.

Create the VM
This VM will run a specific configuration that stresses the CPU and generates the metric monitoring data needed to trigger an alert.

Start by creating the configuration script. To create the cloud-init.txt file with the configuration for the VM, run the following command in Azure Cloud Shell:

Bash

Copy
cat <<EOF > cloud-init.txt
#cloud-config
package_upgrade: true
packages:
- stress
runcmd:
- sudo stress --cpu 1
EOF
To set up an Ubuntu Linux VM, run the following az vm create command. This command uses the cloud-init.txt file that you created in the previous step to configure the VM after it's created.

Azure CLI

Copy
az vm create \
    --resource-group "[sandbox resource group name]" \
    --name vm1 \
    --location eastUS \
    --image Ubuntu2204 \
    --custom-data cloud-init.txt \
    --generate-ssh-keys
Create the metric alert using the Azure portal
 Note

Wait until the VM is successfully created before proceeding with the exercise. The VM creation process is complete when you get the completed JSON output in the Azure Cloud Shell window.

You can use either the Azure portal or the CLI to create a metric alert. In this exercise we'll cover both, starting with the Azure portal.

Sign in to the Azure portal using the same account that you used to activate the sandbox.

On the Azure portal menu, search for and select Monitor. On the Monitor Overview page, select Alerts.

Open the + Create menu, and select Alert rule

On the Select a resource pane, set the scope for your alert rule. You can filter by subscription, resource type, or resource location.

In the Resource type drop-down, start to type "virtual machines", and select Virtual machines.

Check the box next to vm1, then select Apply at the bottom of the pane.

Screenshot that shows the 'Select a resource' pane, with `vm1` selected.

Select Next:Condition at the bottom of the page.

In the Signal name drop-down, select Percentage CPU.

In the Alert logic section, enter (or confirm) the following values for each setting.

Setting	Value
Alert logic	
Threshold	Static
Aggregation type	Maximum
Operator	Greater than
Threshold value	90
When to evaluate	
Check every	1 minute
Lookback period	1 minute
Screenshot that shows the settings for metric condition logic.

Select the Details tab at the top of the page. In the Alert rule details section, enter the following values for each setting.

Setting	Value
Severity	2 - Warning
Alert rule name	Cpu90PercentAlert
Description	Virtual machine is running at or greater than 90% CPU utilization
Expand the Advanced options section and confirm the following values for each setting.

Setting	Value
Enable upon creation	Yes (checked)
Automatically resolve alerts	Yes (checked)
Screenshot that shows the completed settings for the Alert rule details section.

Select Review + create to validate your input, and then select Create.

You've successfully created a metric alert rule that will trigger an alert when the CPU percentage on the VM exceeds 90 percent. The rule will check every minute and review one minute of data. It can take up to 10 minutes for a metric alert rule to become active.

Create the metric alert through the CLI
You can also set up metric alerts by using the CLI. This process can be quicker than using the portal, especially if you're planning to set up more than one alert.

Let's create a new metric alert similar to the one you set up in the Azure portal.

Run the following command in Cloud Shell to obtain the resource ID of the virtual machine you previously created:

Bash

Copy
VMID=$(az vm show \
        --resource-group "[sandbox resource group name]" \
        --name vm1 \
        --query id \
        --output tsv)
Run the following command to create a new metric alert that will be triggered when the VM CPU is greater than 80 percent.

Azure CLI

Copy
az monitor metrics alert create \
    -n "Cpu80PercentAlert" \
    --resource-group "[sandbox resource group name]" \
    --scopes $VMID \
    --condition "max percentage CPU > 80" \
    --description "Virtual machine is running at or greater than 80% CPU utilization" \
    --evaluation-frequency 1m \
    --window-size 1m \
    --severity 3
View your metric alerts in Azure Monitor
In this exercise, you set up an Ubuntu VM and configured it to stress test the CPU. You also created a metric rule to detect when the maximum CPU percentage exceeds 80 percent and 90 percent.

 Note

It might take 10 minutes before you see the alerts show up in the Azure portal.

Return to the Azure portal.

On the Azure portal menu, select Monitor, and then select Alerts in the left menu pane.

This step presents the Alert summary pane, where you can see the count of the number of alerts. If you don't see your alerts listed, wait a few minutes and select Refresh.

Screenshot that shows the alert summary pane.' pane.

You configured your metric alerts with severities of 2 and 3. Select one of the alerts to view the severity level.

Select one of the alerts to show the alert details.



Next unit: Use log alerts to alert on events in your application


5- Use log alerts to alert on events in your application

You can use Azure Monitor to capture important information from log files. Applications, operating systems, other hardware, or Azure services can create these log files.

As a solution architect, you want to explore ways that monitoring log data can detect issues before they become issues for your customers. You know that Azure Monitor supports the use of log data.

In this unit, you'll understand how using log data can improve resilience in your system.

When to use log alerts
Log alerts use log data to assess the rule logic and, if necessary, trigger an alert. This data can come from any Azure resource: server logs, application server logs, or application logs.

By its nature, log data is historical, so usage is focused on analytics and trends.

You can use these types of logs to assess if any of your servers have exceeded their CPU utilization by a given threshold during the last 30 minutes, or you can evaluate response codes issued on your web application server in the last hour.

How log alerts work
Log alerts behave in a slightly different way than other alert mechanisms. The first part of a log alert defines the log search rule. The rule defines how often it should run, the time period under evaluation, and the query to be run.

When a log search evaluates as positive, it creates an alert record and triggers any associated actions.

Composition of log search rules
Every log alert has an associated search rule. The composition of these rules is as follows:

Log query: Query that runs every time the alert rule fires
Time period: Time range for the query
Frequency: How often the query should run
Threshold: Trigger point for an alert to be created
Log search results are one of two types: number of records or metric measurement.

Number of records
Consider using the number-of-records type of log search when you're working with an event or event-driven data. Examples are syslog and web-app responses.

This type of log search returns a single alert when the number of records in a search result reaches or exceeds the value for the number of records (threshold). For example, when the threshold for the search rule is greater or equal to five, the query results have to return five or more rows of data before the alert is triggered.

Metric measurement
Metric measurement logs offer the same basic functionality as metric alert logs.

Unlike number-of-records search logs, metric measurement logs require additional criteria to be set:

Aggregate function: The calculation that will be made against the result data. An example is count or average. The result of the function is called AggregatedValue.
Group field: A field by which the result will be grouped. This criterion is used with the aggregated value. For example, you might specify that you want the average grouped by computer.
Interval: The time interval by which data is aggregated. For example, if you specify 10 minutes, an alert record is created for each aggregated block of 10 minutes.
Threshold: A point defined by an aggregated value and the total number of breaches.
Consider using this type of alert when you need to add a level of tolerance to the results found. One use for this type of alert is to respond if a particular trend or pattern is found. For example, if the number of breaches is five, and any server in your group exceeds 85 percent CPU utilization more than five times within the given time period, an alert fires.

As you can see, metric measurements greatly reduce the volume of alerts that are produced. Still, give careful consideration when you're setting the threshold parameters to avoid missing critical alerts.

Stateless nature of log alerts
One of the primary considerations when you're evaluating the use of log alerts is that they're stateless (stateful log alerts are currently in preview). A stateless log alert will generate new alerts every time the rule criteria are triggered, regardless of whether the alert was previously recorded.

Next unit: Use activity log alerts to alert on events within your Azure infrastructure




6- Use activity log alerts to alert on events within your Azure infrastructure

Activity log alerts allow you to be notified when a specific event happens on some Azure resource. For example, you can be notified when someone creates a new VM in a subscription.

An activity log can also include alerts for Azure service health. A company can get notifications when service issues or planned maintenance happens on the Azure platform.

As an Azure solution architect, you want to explore the capability to monitor selected Azure resources within your subscription. You'll understand how you can use the resources to improve your team's responsiveness and the stability of your systems.

In this unit, you'll explore the two different kinds of activity log alerts. Now that you've seen all the different kinds of alerts you can use in Azure Monitor, you'll see how you can trigger actions for your alerts. Actions might include sending an email or creating an IT Service Management (ITSM) support ticket.

When to use activity log alerts
So far, you've seen two different types of alerts supported in Azure Monitor. Metric alerts are ideally suited to monitoring for threshold breaches or spotting trends; Log alerts allow for greater analytical monitoring of historical data.

Activity log alerts are designed to work with Azure resources. Typically, you'd create this type of log to receive notifications when specific changes occur on a resource within your Azure subscription.

There are two types of activity log alerts:

Specific operations: Applies to resources within your Azure subscription, and often has a scope with specific resources or a resource group. You'll use this type when you need to receive an alert that reports a change to an aspect of your subscription. For example, you can receive an alert if a VM is deleted or new roles are assigned to a user.
Service health events: Include notice of incidents and maintenance of target resources.
Composition of an activity log alert
It's important to note that activity log alerts will monitor events only in the subscription where the log alert was created.

Activity log alerts are based on events. The best approach for defining them is to use Azure Monitor to filter all the events in your subscription until you find the one that you want. To begin the creation process, you'll then select Add activity log alert.

Like the previous alerts, activity log alerts have their own attributes:

Category: Administrative, service health, autoscale, policy, or recommendation
Scope: Resource level, resource group level, or subscription level
Resource group: Where the alert rule is saved
Resource type: Namespace for the target of the alert
Operation name: Operation name
Level: Verbose, informational, warning, error, or critical
Status: Started, failed, or succeeded
Event initiated by: Email address or Microsoft Entra identifier (known as the "caller") for the user
Create a resource-specific log alert
When you create your activity log alert, you'll select Activity Log for the signal type. You'll then see all the available alerts for the resource you select. The following image shows all the administrative alerts for Azure VMs. In this example, an alert is triggered when a VM is powered off.

Changing the monitor service will enable you to reduce the list of options. Selecting Administrative filters all the signals to show only admin-related signals.

Screenshot of the signal logic for activity log alerts related to VMs.

Create a service health alert
Service health alerts aren't like all the other alert types you've seen so far in this module. To create a new alert, search for and select Service Health in the Azure portal. Next, select Health alerts. After you select Create service health alert, the steps to create the alert are similar to the steps you've seen to create other alerts.

Screenshot that shows how to create a new service health alert.

The only difference is that you no longer need to select a resource, because the alert is for a whole region in Azure. What you can select is the kind of health event on which you want to be alerted. You can select service issues, planned maintenance, health advisories, or choose all of the events. The remaining steps of performing actions and naming the alerts are the same.

Next unit: Use action groups and alert processing rules to send notifications when an alert is fired



7-  Use action groups and alert processing rules to send notifications when an alert is fired

When an alert is fired, Azure Monitor, Azure Service Health, and Azure Advisor use action groups to notify users about the alert and take an action. An action group is a collection of notification preferences and actions that are executed when the alert is fired. You can run one or more actions for each triggered alert.

Azure Monitor can perform any of the following actions:

Send an email
Send an SMS message
Create an Azure app push notification
Make a voice call to a number
Call an Azure function
Trigger a logic app
Send a notification to a webhook
Create an ITSM ticket
Use a runbook (to restart a VM or scale a VM up or down)
Once you've created an action group, you can reuse that action group as often as you want. For example, after you've created an action to email your company's operations team, you can add that action group to all service-health events.

While you're creating the alert rule, you can either create a new action group or add an existing action group to the alert rule. You can also edit an existing alert to add an action group.

Alert processing rules
Use alert processing rules to override the normal behavior of a fired alert by adding or suppressing an action group. You can use alert processing rules to add action groups or remove (suppress) action groups from your fired alerts. Alert processing rules are different from alert rules. Alert rules trigger alerts when a condition is met in your monitored resources. Alert processing rules modify the alerts as they're being fired.

You can use alert processing rules to:

Suppress notifications during planned maintenance windows.
Implement management at scale, by specifying commonly used logic in a single rule, instead of having to set it consistently in all your alert rules.
Add an action group to all alert types.
You can apply alert processing rules to different resource scopes, from a single resource, or to an entire subscription. You can also use them to apply various filters or have the rule work on a predefined schedule.

You can control when the alert processing rule applies. By default the rule is always active, but you can select a one-time window for this rule to apply, or you can have set a recurrence such as a weekly recurrence.

Next unit: Exercise -Use an activity log alert and an action group to notify users about events in your Azure infrastructure




8- Exercise -Use an activity log alert and an action group to notify users about events in your Azure infrastructure


The shipping company for which you work wants to avoid any future issues with updates to its applications on the Azure platform. To improve the alerting capabilities within Azure, you can activity log alerts.

Your goal is to set up a Linux VM and create an activity log monitoring rule to detect when a VM is deleted. You'll then delete the VM to trigger this alert.

Create the Azure activity log monitor
Sign in to the Azure portal with the same account you used to activate the sandbox.

On the Azure portal resource menu or under Azure services, select Monitor. The Overview pane for Monitor appears.

In the Monitor menu, select Alerts. The Monitor | Alerts pane appears.

On the command bar, select Create + and select Alert rule. The Create an alert rule pane appears with the Scope section open and the Select a resource pane open on the right.

In the Select a resource pane, the Filter by subscription field should already be populated with Concierge Subscription. In the Filter by resource type dropdown list, search for and select Virtual machines.

You want an alert when any virtual machine in your resource group is deleted. Select the box for the [sandbox resource group name] resource group, then select Apply.

Screenshot that shows the Select a scope pane with the sandbox resource group selected.

The Create an alert rule pane reappears with the Scope target resource showing All Virtual machines. Select the Condition tab. The Select a signal pane appears.

Select the See all signals link, then search for and select Delete Virtual Machine (Virtual Machines). Select Apply

The Create an alert rule pane reappears. You want to receive alerts of all types, so leave Alert logic settings at their default of All selected. Leave the Create an alert rule pane open for the next section.

Add an email alert action
For the previous Azure Monitor alert, you didn't add any actions. You just viewed triggered alerts in the Azure portal. Actions let you send an email for notifications, to trigger an Azure function, or to call a webhook. In this exercise, we're adding an email alert when VMs are deleted.

On the Create an alert rule pane, select the Next: Actions button, and select Create action group. The Create an action group pane appears.

On the Basics tab, enter the following values for each setting.

Setting	Value
Project details	
Subscription	Concierge Subscription
Resource group	From the dropdown list, select your sandbox resource group
Region	Global (default)
Instance details	
Action group name	Alert the operations team
Display name	AlertOpsTeam
Select Next: Notifications, and enter the following values for each setting.

Setting	Value
Notification type	Select Email/SMS message/Push/Voice
Name	VM was deleted
The Email/SMS message/Push/Voice pane appears automatically. If it didn't, select the Edit pencil icon.

Select Email, and in the Email box, enter your email address, and then select OK.

Select Review + create to validate your input.

Select Create.

The Create an alert rule pane reappears. Select the Next: Details button and enter the following values for each setting.

Setting	Value
Alert rule name	VM was deleted
Description	A VM in your resource group was deleted
Expand the Advanced options section and confirm that Enable alert rule upon creation is selected.

Screenshot that shows a completed alert details section.

Select Review + create to validate your input, then select Create.

Recipients added to the configured action group (operations team) receive a notification:

When they're added to the action group
When the alert is activated
When the alert is triggered
It can take up to five minutes for an activity log alert rule to become active. In this exercise, if you delete the virtual machine before the rule deploys, the alert rule might not be triggered. Because of this delay, you might not see the same results in the following steps after you delete the VM.

Delete your virtual machine
To trigger an alert, you need to delete the Linux VM that you created in the previous exercise.

On the Azure portal menu or from the Home page, select Virtual machines.

Check the box for the vm1 virtual machine.

Select Delete from the menu bar.

Type "yes" in the Confirm delete field, then select Delete.

In the title bar, select the Notifications icon and wait until vm1 is successfully deleted.

View your activity log alerts in Azure Monitor
In the exercise, you set up an Ubuntu VM and created an activity log rule to detect when the VM was deleted. You then deleted a VM from your resource group. Let's check whether an alert was triggered.

You should have received a notification email that reads, Important notice: Azure Monitor alert VM was deleted was activated... If not, open your email program and look for an email from azure-noreply@microsoft.com.

Screenshot of alert email.

On the Azure portal resource menu, select Monitor, and then select Alerts in the menu on the left.

You should have three verbose alerts that were generated by deleting vm1.

Screenshot that shows all alerts with Name, Severity, Alert condition, User response and Fired time.

Select the name of one of the alerts (For example, VM was deleted). An Alert details pane appears that shows more details about the event.

Add an alert processing rule to the alert
We're going to schedule a one-time, overnight, planned maintenance. It starts in the evening and continues until the next morning.

In the Azure portal resource menu, select Monitor, select Alerts in the menu on the left, and select Alert processing rules in the menu bar.

Select + Create.

Check the box for your sandbox resource group as the scope of the alert processing rule, then select Apply.

Select Next: Rule settings, then select Suppress notifications.

Select Next: Scheduling.

By default, the rule works all the time, unless you disable it. We're going to define the rule to suppress notifications for a one-time overnight planned maintenance. Enter these settings for the scheduling of the alert processing rule:

Setting	Value
Apply the rule	At a specific time
Start	Enter today's date at 10pm.
End	Enter tomorrow's date at 7am.
Time zone	Select the local timezone.
Screenshot of the scheduling section of an alert processing rule.

Select Next: Details and enter these settings:

Setting	Value
Resource group	Select your sandbox resource group.
Rule name	Planned Maintenance
Description	Suppress notifications during planned maintenance.
Select Review + create to validate your input, then select Create.

Next unit: Summary

Summary

In this module, you learned how Azure Monitor alerts and notifications help you manage your systems and environment. You explored three different types of alerts: metric, log, and activity log.

You learned how metric alerts enable time-series evaluations, which trigger an action group when the alert is fired.

You also learned how log alert rules specify log queries to run at regular time intervals. The alerts trigger an action group when a match is found.

You learned how activity log alerts enable notifications when a named Azure resource meets the specified conditions.

Lastly, you learned how to apply an action group to an alert to send notifications when an alert is fired, and how to use alert processing rules to override the behavior of an action group when necessary.

Clean up
The sandbox automatically cleans up your resources when you're finished with this module.

When you're working in your own subscription, it's a good idea at the end of a project to identify whether you still need the resources you created. Resources that you leave running can cost you money. You can delete resources individually or delete the resource group to delete the entire set of resources.

Learn more
For more info about Azure Monitor and each of the alert types, see:

What are Azure Monitor alerts?
Metric alerts
Log alerts
Activity log alerts






Point 7: Analyze your Azure infrastructure by using Azure Monitor logs

Use Azure Monitor logs to extract valuable information about your infrastructure from log data.


Learning objectives
In this module, you'll:

Identify the features and capabilities of Azure Monitor logs.
Create basic Azure Monitor log queries to extract information from log data.


1- Introduction

Logging and monitoring the health of your services is a vital component of production applications. You need to be able to determine the causes of failures. You also need to identify any problems before they occur.

Azure Monitor is an important tool to help you in this process. It allows you to gather monitoring and diagnostic information about the health of your services. You can use this information to visualize and analyze the causes of problems that might occur in your app.

Suppose that you work for a large organization's operations team. The organization is running large-scale production apps in the cloud. The team wants to consolidate its log data in a single service to improve visibility across services and simplify its logging strategy.

The team began implementing Azure Monitor logs. It wants to fully understand how the logs work. It also wants to know the service's capabilities to query and evaluate the log data that's fed into it.

Learning objectives
In this module, you'll:

Identify the features and capabilities of Azure Monitor logs.
Create basic Azure Monitor log queries to extract information from log data.



Next unit: Features of Azure Monitor logs

2- Features of Azure Monitor logs


Features of Azure Monitor logs

Azure Monitor is a service for collecting and analyzing telemetry. It helps you get maximum performance and availability for your cloud applications and for your on-premises resources and applications. It shows how your applications are performing and identifies any issues with them.

Data collection in Azure Monitor
Azure Monitor collects two fundamental types of data: metrics and logs. Metrics tell you how a resource is performing and the other resources that it's consuming. Logs contain records that show when resources are created or modified.

The following diagram gives a high-level view of Azure Monitor. On the left are the data-monitoring sources: Azure, operating systems, and custom sources. At the center of the diagram are the data stores for metrics and logs. On the right are the functions that Azure Monitor performs with this collected data, such as analysis, alerting, and streaming to external systems.

Diagram of Azure Monitor's architecture, displaying the sources of monitoring data, the data stores, and functions performed on the data.

Azure Monitor collects data automatically from a range of components. For example:

Application data: Data that relates to your custom application code.
Operating-system data: Data from the Windows or Linux virtual machines that host your application.
Azure resource data: Data that relates to the operations of an Azure resource, such as a web app or a load balancer.
Azure subscription data: Data that relates to your subscription. It includes data about Azure health and availability.
Azure tenant data: Data about your Azure organization-level services, such as Microsoft Entra ID.
Because Azure Monitor is an automatic system, it begins to collect data from these sources as soon as you create Azure resources like virtual machines and web apps. You can extend the data that Azure Monitor collects by:

Enabling diagnostics: For some resources, such as Azure SQL Databases, you'll receive full information about a resource only after you've enabled diagnostic logging for it. You can use the Azure portal, the Azure CLI, or PowerShell to enable diagnostics.
Adding an agent: For virtual machines, you can install the Log Analytics agent and configure it to send data to a Log Analytics workspace. This agent increases the amount of information that's sent to Azure Monitor.
Your developers might also want to send data to Azure Monitor from custom code, such as a web app, an Azure function, or a mobile app. They send data by calling the Data Collector API. You can communicate with this REST interface through HTTP. This interface is compatible with various development frameworks, such as .NET Framework, Node.js, and Python. Developers can choose their favorite language and framework to log data in Azure Monitor.

Logs
Logs contain time-stamped information about resource changes. The type of information recorded varies by log source. The log data is organized into records, with different sets of properties for each type of record. The logs can include numeric values like Azure Monitor metrics, but most include text data rather than numeric values.

The most common type of log entry records an event. Events can occur sporadically rather than at fixed intervals or according to a schedule. Events are created by applications and services, which provide the context for the events. You can store metric data in logs to combine them with other monitoring data for analysis.

You can log data from Azure Monitor in a Log Analytics workspace. Azure provides an analysis engine and a rich query language. The logs show the context of any problems, and are useful for identifying root causes.

Screenshot of an example query against Azure logs with the query text on top and a graph displaying the results below.

Metrics
Metrics are numerical values that describe some aspect of a system at a point in time. Azure Monitor can capture metrics in near-real time. The metrics are collected at regular intervals, and are useful for alerting because of their frequent sampling. You can use various algorithms to compare a metric to other metrics and observe trends over time.

Metrics are stored in a time-series database. This data store is most effective for analyzing time-stamped data. Metrics are suited for alerting and fast detection of issues. They can tell you about system performance. If needed, you can combine them with logs to identify the root cause of issues.

Screenshot of an example chart in Azure Metrics displaying average CPU percentage.

Analyzing logs by using Kusto
To retrieve, consolidate, and analyze data, you can specify a query to run in Azure Monitor logs. You can write a log query with the Kusto query language, which Azure Data Explorer also uses.

You can test log queries in the Azure portal so you can work with them interactively. You'll typically start with basic queries, then progress to more advanced functions as your requirements become more complex.

In the Azure portal, you can create custom dashboards, which are targeted displays of resources and data. You can build each dashboard from a set of tiles. Each tile might show a set of resources, a chart, a data table, or some custom text. Azure Monitor provides tiles that you can add to dashboards; for example, you might use a tile to display the results of a Kusto query in a dashboard.

In the example scenario, the operations team can consolidate its monitoring data by visualizing it in charts and tables. These tools are effective for summarizing data and presenting it to different audiences.

By using Azure dashboards, you can combine various kinds of data, including both logs and metrics, into a single pane in the Azure portal. For example, you might want to create a dashboard that combines tiles that show a graph of metrics, a table of activity logs, charts from Azure Monitor, and the output of a log query.

Check your knowledge

1. What data does Azure Monitor collect? 

Data from a variety of sources, such as the application event log, the operating system (Windows and Linux), Azure resources, and custom data sources

Azure billing details

Backups of database transaction logs

2. What two fundamental types of data does Azure Monitor collect? 

Metrics and logs

Username and password

Email notifications and errors



Next unit: Create basic Azure Monitor log queries to extract information from log data


3- Create basic Azure Monitor log queries to extract information from log data

You can use Azure Monitor log queries to extract information from log data. Querying is an important part of examining the log data that Azure Monitor captures.

In the example scenario, the operations team will use Azure Monitor log queries to examine the health of its system.

Write Azure Monitor log queries by using Log Analytics
You can find the Log Analytics tool in the Azure portal and use it to run sample queries or to create your own queries:

In the Azure portal, in the left menu pane, select Monitor.

The Azure Monitor page appears along with more options, including Activity Log, Alerts, Metrics, and Logs.

Select Logs.

Here, you can enter your query and see the output.

Screenshot of Azure Monitor with a new query tab opened.

Write queries by using the Kusto language
You can use the Kusto Query Language to query log information for your services running in Azure. A Kusto query is a read-only request to process data and return results. You'll state the query in plain text by using a data-flow model that's designed to make the syntax easy to read, write, and automate. The query uses schema entities that are organized in a hierarchy similar to that of Azure SQL Database: databases, tables, and columns.

A Kusto query consists of a sequence of query statements, delimited by a semicolon (;). At least one statement is a tabular expression statement. A tabular expression statement formats the data arranged as a table of columns and rows.

A tabular expression statement's syntax has a tabular data flow from one tabular query operator to another, starting with a data source. A data source might be a table in a database or an operator that produces data. The data then flows through a set of data-transformation operators that are bound together with the pipe (|) delimiter.

For example, the following Kusto query has a single tabular expression statement. The statement starts with a reference to a table called Events. The database that hosts this table is implicit here, and is part of the connection information. The data for that table, stored in rows, is filtered by the value of the StartTime column. The data is filtered further by the value of the State column. The query then returns the count of the resulting rows.

Kusto

Copy
Events
| where StartTime >= datetime(2018-11-01) and StartTime < datetime(2018-12-01)
| where State == "FLORIDA"  
| count
 Note

The Kusto query language that Azure Monitor uses is case-sensitive. Language keywords are typically written in lowercase. When you're using names of tables or columns in a query, make sure to use the correct case.

Events captured from the event logs of monitored computers are just one type of data source. Azure Monitor provides many other types of data sources. For example, the Heartbeat data source reports the health of all computers that report to your Log Analytics workspace. You can also capture data from performance counters and update management records.

The following example retrieves the most recent heartbeat record for each computer. The computer is identified by its IP address. In this example, the summarize aggregation with the arg_max function returns the record with the most recent value for each IP address.

Kusto

Copy
Heartbeat
| summarize arg_max(TimeGenerated, *) by ComputerIP



Next unit: Exercise - Create basic Azure Monitor log queries to extract information from log data


4- Exercise - Create basic Azure Monitor log queries to extract information from log data

The operations team doesn't currently have enough information about its system behavior to diagnose and resolve problems effectively. To address this issue, the team has configured an Azure Monitor workspace with the company's Azure services. It runs Kusto queries to get the status of the system and attempts to identify the causes of any problems that might occur.

In particular, the team is interested in monitoring security events to check for possible attempts to break into the system. An attacker might try to manipulate the applications running on the system, so the team also wants to gather application data for further analysis. An attacker might also try to halt the computers that compose the system, so the team wants to examine how and when machines are stopped and restarted.

In this exercise, you'll practice performing Azure log queries against a demo project that contains sample data in tables, logs, and queries.

Create basic Azure Monitor log queries to extract information from log data
Let's use the Azure Demo Logs pane to practice writing queries. The demo project workspace is prepopulated with sample data. Azure offers an optimized SQL-like query with visualization options of its data in a language called KQL (Kusto Query Language.)

Open the Logs demo environment. In the top-left corner, under New Query 1, you'll find Demo, which identifies the workspace, or the scope of the query. The left side of this pane contains several tabs: Tables, Queries, and Functions. The right side has a scratchpad for creating or editing queries.

On the New Query 1 tab, enter a basic query on the first line of the scratchpad. This query retrieves the details of the 10 most recent security events.

Kusto

Copy
SecurityEvent
    | take 10
In the command bar, select Run to execute the query and view the results. You can expand each row in the results pane for more information.

Sort the data by time by adding a filter to your query:

Kusto

Copy
SecurityEvent
    | top 10 by TimeGenerated
Add a filter clause and a time range. Run this query to fetch records that are more than 30 minutes old, and that have a level of 10 or more:

Kusto

Copy
SecurityEvent
    | where TimeGenerated < ago(30m)
    | where toint(Level) >= 10
Run the following query to search the AppEvents table for records of the Clicked Schedule Button event being invoked in the last 24 hours:

Kusto

Copy
AppEvents 
    | where TimeGenerated > ago(24h)
    | where Name == "Clicked Schedule Button"
Run the following query to display the number of different computers that generated heartbeat events each week for the last three weeks. The results appear as a bar chart:

Kusto

Copy
Heartbeat
    | where TimeGenerated >= startofweek(ago(21d))
    | summarize dcount(Computer) by endofweek(TimeGenerated) | render barchart kind=default
Use predefined Azure log queries to extract information from log data
In addition to writing queries from scratch, the operations team can also take advantage of predefined queries in Azure Logs that answer common questions related to their resources' health, availability, usage, and performance.

Use the Time Range parameter in the command bar to set a custom range. Select the month, year, and day to a range from January to today. You can set and apply a custom time to any query.

On the toolbar, select Queries. The Queries pane appears. Here, in the drop-down list in the left menu, you can view a list of the sample queries grouped by Category, Query type, Resource type, Solution, or Topic.

Select Category in the drop-down list, and then select IT & Management Tools.

In the search box, enter Distinct missing updates cross computers. Select the query in the left pane, then select Run. The Logs pane reappears, with the query returning a list of Windows updates missing from virtual machines that are sending logs to the workspace.

 Note

You can also run this same query from the Logs pane. In the left pane, select the Queries tab, then select Category in the Group by dropdown list. Now scroll down the list, expand IT & Management Tools, and double-click Distinct missing updates cross computers. Select Run to run the query. When you select a predefined query in the left pane, the query code is appended to whatever query exists in the scratchpad. Remember to clear the scratchpad before opening or adding a new query to run.

In the left pane, clear the search box. Select Queries, then select Category in the Group by dropdown list. Expand Azure Monitor, and double-click Computers availability today. Select Run. This query creates a time series chart with the number of unique IP addresses sending logs into the workspace each hour for the last day.

Select Topic in the Group by dropdown list, scroll down to expand Function App, and then double-click Show application logs from Function Apps. Select Run. This query returns a list of application logs, sorted by time with the latest logs shown first.

You'll notice that from the Kusto queries you used here, it's easy to target a query to a specific time window, event level, or event log type. The security team can easily examine heartbeats to identify when servers are unavailable, which might indicate a denial-of-service attack. If the team spots the time when a server was unavailable, it can query for events in the security log around that time to diagnose whether an attack caused the interruption. Additionally, predefined queries can also evaluate VM availability, identify missing Windows updates, and review firewall logs to view denied network flows intended for the VMs of interest.

Next unit: Summary


Summary

In this module, you learned how to use Azure Monitor. You looked at Azure Monitor logs to extract valuable information about your infrastructure from log data by using queries, and you performed these queries by using the Kusto query language.

You learned how to:

Explore the types of data that Azure Monitor collects.
Create Azure Monitor log queries to extract information from the log data.
You can now use Azure Monitor to analyze your environment and troubleshoot issues.

Learn more
For more information about Azure Monitor, check out the following articles:

Azure Monitor overview
Get started with log queries in Azure Monitor
Optimize log queries in Azure Monitor
Create and share dashboards of Log Analytics data
Analyze and visualize monitoring data




Point 8: Monitor your Azure virtual machines with Azure Monitor

Learn how to monitor your Azure VMs by using Azure Monitor to collect and analyze VM host and client metrics and logs.


Learning objectives
Understand which monitoring data you need to collect from your VM.
Enable and view recommended alerts and diagnostics.
Use Azure Monitor to collect and analyze VM host metrics data.
Use Azure Monitor Agent to collect VM client performance metrics and event logs.


1- Introduction

Suppose you're the IT administrator for a musical group's website that's hosted on Azure virtual machines (VMs). The website runs mission-critical services for the group, including ticket booking, venue information, and tour updates. The website must respond quickly and remain accessible during frequent updates and spikes in traffic.

You need to maintain sufficient VM size and memory to effectively host the website without incurring unnecessary costs. You also need to proactively prevent and quickly respond to any access, security, and performance issues. To help achieve these objectives, you want to quickly and easily monitor your VMs' traffic, health, performance, and events.

Azure Monitor provides built-in and customizable monitoring abilities that you can use to track the health, performance, and behavior of the VM host and the operating system, workloads, and applications running on your VM. This learning module shows you how to view VM host monitoring data, set up recommended alert rules, and use VM insights and custom data collection rules (DCRs) to collect and analyze monitoring data from inside your VMs.

Prerequisites
To complete this module, you need the following prerequisites:

Familiarity with virtualization, Azure portal navigation, and Azure VMs.
Access to an Azure subscription with at least Contributor role. If you don't have an Azure subscription, create a free account and add a subscription before you begin. If you're a student, you can take advantage of the Azure for students offer.
Learning objectives
Understand which monitoring data you need to collect from your VM.
Enable and view recommended alerts and diagnostics.
Use Azure Monitor to collect and analyze VM host data.
Use Azure Monitor Agent to collect VM client performance metrics and event logs.


Next unit: Monitoring for Azure VMs

2- Monitoring for Azure VMs

In this unit, you explore Azure monitoring capabilities for VMs, and the types of monitoring data you can collect and analyze with Azure Monitor. Azure Monitor is a comprehensive monitoring solution for collecting, analyzing, and responding to monitoring data from Azure and non-Azure resources, including VMs. Azure Monitor has two main monitoring features: Azure Monitor Metrics and Azure Monitor Logs.

Metrics are numerical values collected at predetermined intervals to describe some aspect of a system. Metrics can measure VM performance, resource utilization, error counts, user responses, or any other aspect of the system that you can quantify. Azure Monitor Metrics automatically monitors a predefined set of metrics for every Azure VM, and retains the data for 93 days with some exceptions.

Logs are recorded system events that contain a timestamp and different types of structured or free-form data. Azure automatically records activity logs for all Azure resources. This data is available at the resource level. Azure Monitor doesn't collect logs by default, but you can configure Azure Monitor Logs to collect from any Azure resource. Azure Monitor Logs stores log data in a Log Analytics workspace for querying and analysis.

VM monitoring layers
Azure VMs have several layers that require monitoring. Each of the following layers has a distinct set of telemetry and monitoring requirements.

Host VM
Guest operating system (OS)
Client workloads
Applications that run on the VM
Diagram that shows fundamental VM architecture.

Host VM monitoring
The VM host represents the compute, storage, and network resources that Azure allocates to the VM.

VM host metrics
VM host metrics measure technical aspects of the VM such as processor utilization and whether the machine is running. You can use VM host metrics to:

Trigger an alert when your VM is reaching its disk or CPU limits.
Identify trends or patterns.
Control your operational costs by sizing VMs according to usage and demand.
Azure automatically collects basic metrics for VM hosts. On the VM's Overview page in the Azure portal, you can see built-in graphs for the following important VM host metrics.

VM availability
CPU usage percentage (average)
OS disk usage (total)
Network operations (total)
Disk operations per second (average)
You can use Azure Monitor Metrics Explorer to plot more metrics graphs, investigate changes, and visually correlate metrics trends for your VMs. With Metrics Explorer you can:

Plot multiple metrics on a graph to see how much traffic hits your VM and how the VM performs.
Track the same metric over multiple VMs in a resource group or other scope, and use splitting to show each VM on the graph.
Select flexible time ranges and granularity.
Specify many other settings such as chart type and value ranges.
Send graphs to workbooks or pin them to dashboards for quickly viewing health and performance.
Group metrics by time intervals, geographic regions, server clusters, or application components.
Screenshot showing CPU percentage usage and inbound flow chart.

Recommended alert rules
Alerts proactively notify you of specified occurrences and patterns in your VM host metrics. Recommended alert rules are a predefined set of alert rules based on commonly monitored host metrics. These rules define recommended CPU, memory, disk, and network usage levels to alert on, as well as VM availability, which alerts you when the VM stops running.

You can quickly enable and configure recommended alert rules when you create an Azure VM, or afterwards from the VM's portal page. You can also view, configure, and create custom alerts by using Azure Monitor Alerts.

Activity logs
Azure Monitor automatically records and displays activity logs for Azure VMs. Activity logs include information like VM startup or modifications. You can create diagnostic settings to send activity logs to the following destinations:

Azure Monitor Logs, for more complex querying and alerting and for longer retention up to two years.
Azure Storage, for cheaper, long-term archiving.
Azure Event Hubs, to forward outside of Azure.
Boot diagnostics
Boot diagnostics are host logs you can use to help troubleshoot boot issues with your VMs. You can enable boot diagnostics by default when you create a VM, or afterwards for existing VMs.

Once you enable boot diagnostics, you can see screenshots from the VM's hypervisor for both Windows and Linux machines, and view the serial console log output of the VM boot sequence for Linux machines. Boot diagnostics stores data in a managed storage account.

Guest OS, client workload, and application monitoring
VM client monitoring can include monitoring the operating system (OS), workloads, and applications that run on the VM. To collect metrics and logs from guest OS and client workloads and applications, you need to install Azure Monitor Agent and set up a data collection rule (DCR).

DCRs define what data to collect and where to send that data. You can use a DCR to send Azure Monitor metrics data, or performance counters, to Azure Monitor Logs or Azure Monitor Metrics. Or, you can send event log data to Azure Monitor Logs. In other words, Azure Monitor Metrics can store only metrics data, but Azure Monitor Logs can store both metrics and event logs.

VM insights
VM insights is an Azure Monitor feature that helps get you started monitoring your VM clients. VM insights is especially useful for exploring overall VM usage and performance when you don't yet know the metric of primary interest. VM insights provides:

Simplified Azure Monitor Agent onboarding to enable monitoring a VM's guest OS and workloads.
A preconfigured DCR that monitors and collects the most common performance counters for Windows and Linux.
Predefined trending performance metrics charts and workbooks from the VM's guest OS.
A set of predefined workbooks that show collected VM client metrics over time.
Optionally, collection of processes running on the VM, dependencies with other services, and a dependency map that displays interconnected components with other VMs and external sources.
Predefined VM insights workbooks show performance, connections, active ports, traffic, and other collected data from one or several VMs. You can view VM insights data directly from a single VM, or see a combined view of multiple VMs to view and assess trends and patterns across VMs. You can edit the prebuilt workbook configurations or create your own custom workbooks.

Client event log data
VM insights creates a DCR that collects a specific set of performance counters. To collect other data, such as event logs, you can create a separate DCR that specifies the data you want to collect from the VM and where to send it. Azure Monitor stores collected log data in a Log Analytics workspace, where you can access and analyze the data by using log queries written in Kusto Query Language (KQL).

Check your knowledge

1. What are the two main types of monitoring data that Azure Monitor collects for Azure VMs? 

Metrics and logs.

VM insights and Alerts.

Workbooks and Workspaces.

2. What are the layers of a VM that need to be monitored? 

VM host OS, SKU, and disks.

VM host, guest OS, client workloads, and applications.

Subscription, resource group, and VM.


Next unit: Monitor VM host data


3- Monitor VM host data

You want to monitor the VMs that host your website, so you decide to quickly create a VM in the Azure portal and evaluate its built-in monitoring capabilities. In this unit, you use the Azure portal to create a Linux VM with recommended alerts and boot diagnostics enabled. As soon as the VM starts up, Azure automatically begins collecting basic metrics and activity logs, and you can view built-in metrics graphs, activity logs, and boot diagnostics.

Create a VM and enable recommended alerts
Sign in to the Azure portal, and in the Search field, enter virtual machines.

On the Virtual machines page, select Create, and then select Azure virtual machine.

On the Basics tab of the Create a virtual machine page:

In the Subscription field, select the correct subscription if not already selected.
Under Resource group:
Select Create new.
Under Name, enter learn-monitor-vm-rg.
Select OK.
For Virtual machine name, enter monitored-linux-vm.
For Image, select Ubuntu Server 20.04 LTS - x64 Gen2.
Leave the other settings at their current values, and select the Monitoring tab.

Screenshot that shows the Basics tab of the Create a virtual machine page.

On the Monitoring tab, select the checkbox next to Enable recommended alert rules.

On the Set up recommended alert rules screen:

Select all the listed alert rules if not already selected, and adjust the values if desired.
Under Notify me by, select the checkbox next to Email, and enter an email address to receive alert notifications.
Select Save.
Under Diagnostics, for Boot diagnostics, ensure that Enable with managed storage account (recommended) is selected.

 Note

Don't select Enable guest OS diagnostics. The Linux Diagnostics Agent (LAD) is deprecated, and you can enable guest OS and client monitoring later.

Select Review + create at the bottom of the page, and when validation passes, select Create.

Screenshot that shows the Monitoring tab and alert rule configuration screen of the Create a virtual machine page.

On the Generate new key pair popup dialog box, select Download private key and create resource.

It can take a few minutes to create the VM. When you get the notification that the VM is created, select Go to resource to see basic metrics data.

View built-in metrics graphs
Once your VM is created, Azure starts collecting basic metrics data automatically. Built-in metrics graphs, along with the recommended alerts you enabled, can help you monitor whether and when your VM encounters health or performance issues. You can then use more advanced monitoring and analytics capabilities to investigate issue causes and remediation.

To view basic metrics graphs, on the VM's Overview page, select the Monitoring tab.

Screenshot that shows Monitoring tab on a VM's Overview screen.

Under Performance and utilization > Platform metrics, review the following metrics graphs related to the VM's performance and utilization. Select Show more metrics if all the graphs don't appear immediately.

VM Availability
CPU (average)
Disk bytes (total)
Network (total)
Disk operations/sec (average)
Screenshot that shows the platform metrics graphics on the VM Overview page.

Under Guest OS metrics, notice that guest OS metrics aren't being collected yet. In the next units, you configure VM insights and data collection rules to collect guest OS metrics.

View the activity log
You can view the VM's activity log by selecting Activity log from the VM's left navigation menu. You can also retrieve entries by using PowerShell or the Azure CLI.

Screenshot of the activity log for a VM.

View boot diagnostics
You enabled boot diagnostics when you created the VM. You can view boot diagnostics to view boot data and troubleshoot startup issues.

In the left navigation menu for the VM, select Boot diagnostics under Help.

On the Boot diagnostics page, select Screenshot to see a startup screenshot from the VM's hypervisor. Select Serial log to view log messages created when the VM started.

Screenshot that shows the boot diagnostic image captured.

Check your knowledge

1. What do you need to do to enable recommended alert rules when you create a VM? 

Nothing, they're enabled by default.

Go to Alerts and select Create.

Select Enable recommended alert rules on the Monitoring tab.

2. Which metrics graph isn't available by default on the Monitoring tab when you create a VM? 

VM Availability

Guest OS Available Memory

Percentage CPU (average)


Next unit: Use Metrics Explorer to view detailed host metrics


4- Use Metrics Explorer to view detailed host metrics

You want to investigate how your VM's CPU capability is affected by the traffic flowing into it. If the built-in metrics charts for a VM don't already show the data you need, you can use Metrics Explorer to create customized metrics charts. In this unit, you plot a graph that displays your VM's maximum percentage CPU and average inbound flow data together.

Azure Monitor Metrics Explorer offers a UI for exploring and analyzing VM metrics. You can use Metrics Explorer to view and create custom charts for many VM host metrics in addition to the metrics shown on the built-in graphs.

Understand Metrics Explorer
To open Metrics Explorer, you can:

Select Metrics from the VM's left navigation menu under Monitoring.
Select the See all Metrics link next to Platform metrics on the Monitoring tab of the VM's Overview page.
Select Metrics from the left navigation menu on the Azure Monitor Overview page.
Screenshot that shows Metrics Explorer.

In Metrics Explorer, you can select the following values from the dropdown fields:

Scope: If you open Metrics Explorer from a VM, this field is prepopulated with the VM name. You can add more items with the same resource type (VMs) and location.
Metric Namespace: Most resource types have only one namespace, but for some types, you must pick a namespace. For example, storage accounts have separate namespaces for files, tables, blobs, and queues.
Metric: Each metrics namespace has many metrics available to choose from.
Aggregation: For each metric, Metrics Explorer applies a default aggregation. You can use a different aggregation to get different information about the metric.
You can apply the following aggregation functions to metrics:

Count: Counts the number of data points.
Average (Avg): Calculates the arithmetic mean of values.
Maximum (Max): Identifies the highest value.
Minimum (Min): Identifies the lowest value.
Sum: Adds up all the values.
You can select flexible time ranges for graphs from the past 30 minutes to the last 30 days, or custom ranges. You can specify time interval granularity from one minute to one month.

Create a metrics graph
To create a Metrics Explorer graph that shows host VM maximum percentage CPU and inbound flows together for the past 30 minutes:

Open Metrics Explorer by selecting See all Metrics on the VM's Monitoring tab or selecting Metrics from the VM's left navigation menu.

Scope and Metric Namespace are already populated for the host VM. Select Percentage CPU from the Metrics dropdown list.

Aggregation is automatically populated with Avg, but change it to Max.

Screenshot of the Percentage CPU metrics graph for a VM.

Select Add metric at upper left.

Under Metric, select Inbound Flows. Leave Aggregation at Avg.

At upper right, select Local Time: Last 24 hours (Automatic - 15 minutes), change it to Last 30 minutes, and select Apply.

Your graph should look similar to the following screenshot:

Screenshot that shows a graph of CPU usage and inbound traffic.

Check your knowledge

1. How do you add another metric to an existing Metrics Explorer graph? 

Select the metric from the dropdown list in the Metric field.

Select New chart.

Select Add metric.

2. Which of these parameters isn't included in the dropdown fields when you define a Metrics Explorer graph? 

Metric Namespace

Time range

Aggregation


Next unit: Collect client performance counters by using VM insights


5- Collect client performance counters by using VM insights

Besides monitoring your VM host's health, utilization, and performance, you need to monitor the software and processes running on your VM, which are called the VM guest or client. In this unit, you enable the Azure Monitor VM insights feature, which offers a quick way to start monitoring the VM client.

The VM client includes the operating system and other workloads and applications. To monitor the software running on your VM, you install the Azure Monitor Agent, which collects data from inside the VM. VM insights:

Installs Azure Monitor Agent on your VM.
Creates a data collection rule (DCR) that collects and sends a predefined set of client performance data to a Log Analytics workspace.
Presents the data in curated workbooks.
Although you don't need to use VM insights to install Azure Monitor Agent, create DCRs, or set up workbooks, VM insights makes setting up VM client monitoring easy. VM insights provides you with a basis for monitoring the performance of your VM client and mapping the processes running on your machine.

Enable VM insights
In the Azure portal, on your VM's Overview page, select Insights from the left navigation menu under Monitoring.

On the Insights page, select Enable.

On the Monitoring configuration page, select Azure Monitor Agent (Recommended).

Under Data collection rule, note the properties of the DCR that VM insights creates. In the DCR description, Processes and dependencies (Map) is set to Enabled, and a default Log Analytics workspace is created or assigned.

Select Configure.

Screenshot that shows enabling and configuring VM insights.

Configuration of the workspace and the agent installation typically takes 5 to 10 minutes. It can take another 5 to 10 minutes for data to become available to view in the portal.

When the deployment finishes, confirm that the Azure Monitor Agent and the Dependency Agent are installed by looking on the Properties tab of the VM's Overview page under Extensions + applications.

On the Monitoring tab of the Overview page, under Performance and utilization, you can see that Guest OS metrics are now being collected.

Screenshot that shows Guest OS metrics on the VM's Monitoring tab.

View VM insights
VM insights creates a DCR that sends client VM performance counters to Azure Monitor Logs. Because the DCR sends its metrics to Azure Monitor Logs, you don't use Metrics Explorer to view the metrics data that VM insights collects.

To view the VM insights performance graphs and maps:

Select Insights from the VM's left navigation menu under Monitoring.

Near the top of the Insights page, select the Performance tab. The prebuilt VM insights Performance workbook shows charts and graphs with performance-related data for the current VM.

Screenshot that shows the prebuilt VM insights Performance workbook.

You can customize the view by specifying a different Time range at the top of the page and different aggregations at the top of each graph.

Select View Workbooks to select from other available prebuilt VM insights workbooks. Select Go To Gallery to select from a gallery of other VM insights workbooks and workbook templates, or to edit and create your own workbooks.

Select the Map tab on the Insights page to see the workbook for the Map feature. The map visualizes the VM's dependencies by discovering running process groups and processes that have active network connections over a specified time range.

Screenshot that shows a dependency map on the Map tab of VM insights.

Check your knowledge

1. What capabilities does enabling VM insights provide? 

Prebuilt client performance workbooks and guest OS metrics.

Graphs that show several host metrics on one graph with customizable timeframes.

Azure VM log collection and analytics.

2. What's a quick way to install the Azure Monitor Agent to collect guest OS metrics? 

Install the diagnostics extension under Diagnostics settings.

You don't have to install or enable anything to use the Azure Monitor Agent to collect guest OS metrics.

Select the Azure Monitor Agent when you enable VM insights.


Next unit: Collect VM client event logs


6- Collect VM client event logs

Azure Monitor Metrics and VM insights performance counters help you identify performance anomalies and alert when thresholds are reached. But to analyze the root causes of issues you detect, you need to analyze log data to see which system events caused or contributed to the issues. In this unit, you set up a data collection rule (DCR) to collect Linux VM Syslog data, and view the log data in Azure Monitor Log Analytics by using a simple Kusto Query Language (KQL) query.

VM insights installs the Azure Monitor Agent and creates a DCR that collects predefined performance counters, maps process dependencies, and presents the data in prebuilt workbooks. You can create your own DCRs to collect VM performance counters that the VM insights DCR doesn't collect, or to collect log data.

When you create DCRs in the Azure portal, you can select from a range of performance counters and sampling rates, or add custom performance counters. Or, you can select from a predefined set of log types and severity levels or define custom log schemas. You can associate a single DCR to any or all VMs in your subscription, but you might need multiple DCRs to collect different types of data from different VMs.

Create a DCR to collect log data
In the Azure portal, search for and select monitor to go to the Azure Monitor Overview page.

Screenshot that shows the Azure Monitor Overview page.

Create a Data Collection Endpoint
You must have a data collection endpoint to send log data to. To create an endpoint:

In the Azure Monitor left navigation menu under Settings, select Data Collection Endpoints.
On the Data Collection Endpoints page, select Create.
On the Create data collection endpoint page, for Name, enter linux-logs-endpoint.
Select the same Subscription, Resource group, and Region as your VM uses.
Select Review + create, and when validation passes, select Create.
Create the Data Collection Rule
To create the DCR to collect the event logs:

In the Monitor left navigation menu under Settings, select Data Collection Rules.

On the Data Collection Rules page, you can see the DCR that VM insights created. Select Create to create a new data collection rule.

Screenshot of the Data Collection Rules screen with Create highlighted.

On the Basics tab of the Create Data Collection Rule screen, provide the following information:

Rule name: Enter collect-events-linux.
Subscription, Resource Group, and Region: Select the same as for your VM.
Platform Type: Select Linux.
Select Next: Resources or the Resources tab.

Screenshot of the Basics tab of the Create Data Collection Rule screen.

On the Resources screen, select Add resources.

On the Select a scope screen, select the monitored-linux-vm VM, and then select Apply.

On the Resources screen, select Enable Data Collection Endpoints.

Under Data collection endpoint for the monitored-linux-vm, select the linux-logs-endpoint you created.

Select Next: Collect and deliver, or the Collect and deliver tab.

Screenshot of the Resources tab of the Create Data Collection Rule screen.

On the Collect and deliver tab, select Add data source.

On the Add data source screen, under Data source type, select Linux Syslog.

On the Add data source screen, select Next: Destination or the Destination tab, and make sure the Account or namespace matches the Log Analytics workspace that you want to use. You can use the default Log Analytics workspace that VM insights set up, or create or use another Log Analytics workspace.

On the Add data source screen, select Add data source.

On the Create Data Collection Rule screen, select Review + create, and when validation passes, select Create.

Screenshot of Review + create highlighted on the Create Data Collection Rule screen.

View log data
You can view and analyze the log data collected by your DCR by using KQL log queries. A set of sample KQL queries is available for VMs, but you can write a simple query to look at the events your DCR is collecting.

On your VM's Overview page, select Logs from the left navigation menu under Monitoring. Log Analytics opens an empty query window with the scope set to your VM.

You can also access log data by selecting Logs from the left navigation of the Azure Monitor Overview page. If necessary, select Select scope at the top of the query window to scope the query to the desired Log Analytics workspace and VM.

 Note

The Queries window with sample queries might open when you open Log Analytics. For now, close this window, because you're going to manually create a simple query.

In the empty query window, type Syslog, and then select Run. All the system log events the DCR collected within the Time range are displayed.

You can refine your query to identify events of interest. For example, you can display only the events that had a SeverityLevel of warning.

Screenshot that shows the events returned from the Syslog by the DCR.

Check your knowledge

1. How can you collect event log data from your VMs? 

Create a DCR.

Enable VM insights.

Enable boot diagnostics.

2. How can you view log data collected by a DCR? 

In the Monitoring tab of your VM Overview page.

By selecting Data Collection Rules in Azure Monitor.

By using a KQL query in your Log Analytics workspace.


Summary

Azure Monitor helps you collect, analyze, and alert on various types of host and client monitoring data from your Azure VMs.

Azure Monitor provides a set of VM host logs and performance and usage metrics for all Azure VMs.
You can enable recommended alert rules when you create VMs or afterwards to alert on important VM host metrics.
Azure Monitor Metrics Explorer lets you graph and analyze metrics for Azure VMs and other resources.
VM insights provides a simple way to monitor important VM client performance counters and processes running on your VM.
You can create data collection rules to collect other metrics and logs from your VM client.
You can use Log Analytics to query and analyze log data.
Now that you understand these tools, you're confident that Azure Monitor can effectively monitor your Azure VMs and help you keep your website running effectively.

Clean up resources
In this module, you created a VM in your Azure subscription. So you don't continue to incur charges for this VM, you can delete it or the resource group that contains it.

To delete the resource group that contains the VM and its resources:

Select the Resource group link at the top of the Essentials section on the VM's Overview page.
At the top of the resource group page, select Delete resource group.
On the delete screen, select the checkbox next to Apply force delete for selected virtual machines and virtual machine scale sets. Enter the resource group name in the field, and then select Delete.
Learn more
To learn more about monitoring your VMs with Azure Monitor, see the following resources:

Azure Monitor documentation
Monitor virtual machines with Azure Monitor
Supported metrics with Azure Monitor
Azure Monitor activity log
Supported metrics for Microsoft.Compute/virtualMachines
What is VM insights?
Create interactive reports with VM insights workbooks
View app dependencies with VM insights
Azure Monitor Agent
Collect events and performance counters from virtual machines with Azure Monitor Agent
Tutorial: Collect guest logs and metrics from an Azure virtual machine

Module incomplete

Google Cloud Fundamental

Chapter 1: Digital Transformation with Google Cloud

There's much excitement about cloud technology and digital transformation, but often many unanswered questions.

For example: What is cloud technology? What does digital transformation mean? How can cloud technology help your organization? Where do you even begin?

If you've asked yourself any of these questions, you're in the right place. This course provides an overview of the types of opportunities and challenges that companies often encounter in their digital transformation journey. If you want to learn about cloud technology so you can excel in your role and help build the future of your business, then this introductory course on digital transformation is for you. This course is part of the Cloud Digital Leader learning path.

When you complete this course, you can earn the badge displayed here! View all the badges you have earned by visiting your profile page. Boost your cloud career by showing the world the skills you have developed!

Some people imagine that the technical capacity to understand the use of the cloud is all you need in order to transform a business.

And I think this would be a big mistake.

Hello, my name is Vint Cerf.

I'm vice president and Chief Internet Evangelist at Google.

My primary job is to make sure there is more Internet out there for everyone.

But my big interest is making sure the cloud is useful as well.

I think if you're concerned about being ready for cloud, you need to understand what its capabilities are, what it can deliver.

You don't necessarily have to understand in great detail how it does it, but you have to appreciate what it can

do and how much flexibility it offers in terms of new products and services, or transforming ways in which all businesses operate.

Without that insight, without that understanding, it's very hard to steer yourself into an advantageous place in the cloud world.

If you think a little bit about the scale of the cloud.

You'll appreciate that most companies and certainly most individuals would not be capable of investing in and maintaining the computing capacity and data storage capacity of today's clouds.

That transformation means that companies have access to facilities they otherwise could not get access to.

Failure to at least attend to new technology could be a fatal risk.

And that's why we think everyone should be at least aware of what the new technologies are and whether or not they fit into the corporate structure.

Let's talk a little bit about leadership.

I think most leaders don't need to know in detail how the Internet works, but they have to have

a kind of fundamental appreciation for what it means to get access to cloud based technology through the Internet.

Think of the Internet and the cloud as an enabling infrastructure that can be purposed in infinite different ways.

So it's important that the leaders at least have a conceptual grasp of what these technologies can do for their companies, their products and services.

This is not just about technology.

It's about how technology is used.

And that requires real vision.


There's a lot of excitement about cloud technology and digital transformation.

But you might also have many unanswered questions.

For example: What is cloud technology?

What does digital transformation mean?

How does cloud technology help you or your organization?

Where do you even begin?

If you've asked yourself any of these questions, you're in the right place.

At Google Cloud, we want to provide you with the necessary information and tools for success as you begin your journey to the cloud and digital transformation.

This course, “Digital Transformation with Google Cloud” was designed to help you: Understand why and how the cloud revolutionizes businesses.

Explain general cloud concepts.

And identify the benefits and tradeoffs of using IaaS, or infrastructure as a service; PaaS, or platform as a service; and SaaS, or software as a service.

You’ll begin by defining some important terms that you’ll hear throughout the course, and by describing the benefits of adopting cloud technology to digitally transform a business.

Next, you’ll explore some fundamental cloud concepts.

You’ll learn how migrating to the cloud affects your organization’s flexibility, agility, reliability, and total cost of ownership.

You’ll also explore the different types of infrastructure and explore various use cases for them.

From there, you’ll learn about cloud computing models, and the shared responsibility between an organization and its cloud provider in hardware, software, security.

Throughout the course you’ll be presented with graded knowledge assessments.

You must pass these assessments to receive course credit.

Okay, let's get started!

Section 1: Why Cloud Technology is Transforming Business

To understand how the cloud is transforming businesses, it’s important to learn about the basics of cloud and cloud technology.

In this section of the course, you’ll explore: key terms related to the cloud and digital transformation, the benefits of cloud technology with regard to an organization’s

digital transformation, the differences between on-premises infrastructure, public cloud, private cloud, hybrid cloud, and multicloud, and the drivers and challenges that lead organizations to undergo a digital transformation.

Let’s get started!

Point 1: Innovations, paradigm shifts, and digital transformation

Innovation doesn’t come in a linear pattern.

It comes in waves.

And each of these waves is powered by a breakthrough technology.

There was the age of the printing press, the steam engine, electricity, the transportation age, the first computers and, today, data and cloud infrastructure.

Each of these inventions triggered thousands of innovations, changing what's possible in life and work.

Consider the invention of the printing press.

It was revolutionary because it gave everyone access to books, encyclopedias, and even playing cards in their daily lives.

It also led to a broader recognition of intellectual property through widely distributed patents, which in turn prepared the world for the first industrial revolution.

There was no turning back!

Steam-powered engines brought us cars and trains which then radically transformed the transportation industry; allowing businesses to produce and transport goods at scale.

The entire Industrial Revolution resulted from new technologies that came together and facilitated new ways of working.

In the same way, electricity brought us the light bulb, household appliances and eventually the computer.

What the printing press, the steam engine and electricity all have in common is that they’re examples of

a paradigm shift: a fundamental and irreversible change in the way that humans work and engage with the world.

How do these examples relate to cloud technology?

Well, we’re right in the middle of another paradigm shift: one of digital transformation.

Cloud technology is transforming how organizations create value how people work, and ultimately, how people live.

It’s the catalyst for thousands of innovations that change how we navigate the world, how we interact with media, how we diagnose illness, or how we combat environmental issues.

Digital transformation, as a term, has become prominent over the past few years.

But what are the key components of a digital transformation, how do they relate to the use of cloud technologies, and why so many organizations pursue it?

At Google Cloud, we define digital transformation as when an organization uses new digital technologies, such as public, private, and hybrid

cloud platforms to create or modify business processes, culture, and customer experiences to meet the needs of changing business and market dynamics.

Organizations choose digital transformation frameworks to foster innovation, generate new revenue streams, , and adapt quickly to market changes and customer needs.

Digital transformation helps organizations change how they operate and redefine relationships with their customers, employees, and partners by modernizing their applications, creating new services, and delivering value.

For that reason, rapid advances in digital technology are redefining every industry.

Many vehicles are now software-driven, and they receive regular updates much like a laptop or phone.

In chemistry, big data and artificial intelligence (or AI) facilitates drug discovery.

Financial service institutions use cloud’s vast computing power to provide better insights than ever before.

With smart analytics that are increasingly embedded in everything and devices that generate exponential amounts of data traditional on-premises computing solutions can no longer suffice.

As business innovation becomes more driven by software, the IDC FutureScape report predicts that over 50% of all IT spending will go toward digital transformation and innovation by 2024.

In fact, IDC also predicts that, by 2025, more than 90% of new enterprise apps will have AI embedded within them.

Leading organizations will rely more heavily on AI to launch new business models, create more customized experiences, and optimize operations to reduce costs.

Understanding the scale and power of the cloud is more critical than ever before.

Point 2: What is the cloud?

So what is the cloud and cloud technology, exactly?

And how does it support digital transformation?

The cloud is a metaphor for the network of data centers which store and compute information that’s available through the internet.

Essentially, instead of describing a complex web of software, servers, computers, networks, and security systems, all of that has been combined into one word: “cloud.”

To better understand the cloud, it might help to explore the different ways organizations can implement their information technology (or IT) infrastructure.

The list includes on-premises, private cloud public cloud, hybrid cloud, and multicloud implementations.

On-premises IT infrastructure, which is often abbreviated to “on-prem,” refers to hardware and software applications that

are hosted on-site, located and operated within an organization's data center to serve their unique needs.

This implementation is the traditional way of managing IT infrastructure.

The benefit of on-premises is that it doesn’t require third-party access which gives owners physical

control over the server hardware and software and doesn’t require them to pay for ongoing access.

However, to have the computing power to run their required workloads, organizations must buy physical servers and other infrastructure through procurement processes that can take months.

These systems require physical space, typically a specialized room with sufficient power and cooling.

After configuring and deploying the systems, businesses then need expert personnel to manage them.

This long process is difficult to scale when demand spikes or business expands.

Organizations often acquire more computing resources than they actually need, which results in low utilization and high overhead.

Cloud computing addresses these issues by offering computing resources as scalable, on-demand services.

A private cloud is a type of cloud computing where the infrastructure is dedicated to a single organization instead of the general public.

This type is also known as single-tenant or corporate cloud.

Typically, an organization has to perform the same kind of ongoing maintenance and management for a private cloud as it would for traditional on-premises infrastructure.

A private cloud is hosted within an organization’s own private servers, either at an organization’s own data center, at a third-party colocation facility, or by using a private cloud provider.

Private cloud computing gives businesses many of the benefits of a public cloud—including self-service, scalability, and elasticity—with more customization available from dedicated on-premises infrastructure.

Organizations might use private cloud if they have already made significant investments in their own infrastructure

or if, for regulatory reasons, data must be kept on-premises or hosted in a certain way.

The public cloud is where on-demand computing services and infrastructure are managed by a third-party

provider, such as Google Cloud, and shared with multiple organizations or “tenants” through the public internet.

This sharing is why public cloud is known as multi-tenant cloud infrastructure, but each tenant’s data and applications running in the cloud are hidden from other tenants.

You can think of it like an apartment building that’s maintained by a property management company.

The building has many units and tenants.

Each unit might have a slightly different layout, but still has all the amenities a tenant needs to live there.

And each unit is locked and private to the tenant who pays for that space.

In these lessons, when we refer to “cloud,” unless otherwise stated, we’re talking about the public cloud.

Because public cloud has on-demand availability of computing and infrastructure resources, organizations don't need to acquire, configure, or manage those resources themselves, and they only pay for what they use.

There are typically three types of cloud computing service models available in public cloud: The first is infrastructure as a service (IaaS), which offers compute and storage services.

The second is platform as a service (PaaS), which offers a develop-and-deploy environment to build cloud apps.

And the third is software as a service (SaaS), which delivers apps as services, where users get access to software on a subscription basis.

We’ll explore these three models in detail later.

The final two ways that organizations can implement IT infrastructure is is hybrid cloud or multi-cloud.

Although they’re not the same, these two terms are often used interchangeably, so let's take a moment to define them.

In a hybrid cloud, applications run in a combination of different environments.

The most common hybrid cloud example is combining a public and private cloud environment, like an on-premises data center and a public cloud computing environment, like Google Cloud.

The term multicloud describes architectures that combine at least two public cloud providers.

Organizations might operate a combination of on-premises and multiple public cloud environments, therefore implementing both hybrid and multicloud simultaneously.

So, although hybrid cloud and multicloud are related, they aren’t interchangeable terms.

Today, most organizations embrace a multicloud strategy.

According to the “Flexera 2022 State of the Cloud Report,”, 89% of respondents reported having a

multicloud strategy, and 80% of them take a hybrid approach by combining public and private cloud.

Point 3: The benefits of cloud computing

So, what are the benefits of cloud computing compared to traditional on-premises infrastructure?

It's scalable.

Cloud computing gives organizations access to scalable resources and the latest technologies on-demand, so they don’t need to worry about capital expenditures or limited fixed infrastructure.

This can significantly accelerate infrastructure deployment time.

It’s flexible.

Organizations and their users can access cloud services from anywhere scaling services up or down as needed to meet business requirements.

It’s agile.

Organizations can develop new applications and rapidly get them into production, without worrying about the underlying infrastructure.

It offers strategic value.

Because cloud providers stay updated with the latest innovations and offer them as services to customers, organizations

can get more competitive advantages and a higher return on investment—than if they’d invested in soon-to-be obsolete technologies.

This lets organizations innovate and try new ideas faster.

It’s secure.

Cloud computing security is recognized as stronger than that in enterprise data centers, because of the depth and breadth of the security mechanisms and dedicated teams that cloud providers implement.

Finally, it’s cost-effective.

No matter which cloud computing service model organizations implement, they only pay for the computing resources they use.

They don’t need to overbuild data center capacity to handle sudden spikes in demand or business growth, and they can deploy IT staff to work on more strategic initiatives.

Point 4: Real world example

As the world and business change, keeping technology the same instead of being open to transforming is risky for an organization.

Let’s illustrate this by looking at two examples: one embraces new technology and uses it to their advantage, and the other doesn’t.

First up is Nintendo.

Nintendo has been creating games since 1889!

They started with traditional Japanese playing cards, called Hanafuda, which were made possible by the printing press.

From there, they have consistently used new technology to transform their business, and become a leader in the gaming industry.

They were even among the first to introduce gaming consoles and mobile gaming devices.

Still, they didn’t just stop after these successes.

Instead, they revolutionized mobile gaming when they launched Pokemon Go in 2016, and then the first cloud gaming console—Nintendo Switch—one year later in 2017.

At a time when most of their competitors were failing, Nintendo transformed by using one new technology

after the next, consistently maintaining and even expanding its market share and customer base along the way.

More recently, Nintendo has been using Google Cloud to bring games to smartphones worldwide.

So, what makes Nintendo so successful at transforming?

The answer is that they consistently focus on “why” they exist, not “how” they operate.

They exist because they want people to play, and naturally, they’ll use any new technology as a resource to achieve this mission.

If they focused on liquid crystal displays as the best tool for gaming, then each new technology would have posed a threat to them.

Instead, they utilized liquid crystal displays for a while, and then quickly shifted as the next technology became available to continue motivating people to play.

By contrast, companies that sold encyclopedias, for example, all focused on “how” they operate (how to print and sell a specific set of books).

And this was what they were proud of: a beautiful set of leather-bound books lined up on the shelves of the finest libraries.

And because of their high cost, only a few scholars or the elite could afford them.

For businesses that made and sold encyclopedias, they needed printing presses well-kept warehouses, bookshelf makers, a way to ship and receive heavy containers, and a good door-to-door selling model.

These companies were so focused on manufacturing books that they lost sight of their initial mission: to capture, catalog, and share human knowledge.

When new technology such as CD-ROMs became available, this short-term mindset led many of the original

encyclopedia companies to view the technology as a threat to their business instead of an opportunity.

Ironically, many of the CD-ROM-based encyclopedia providers made the exact same mistake, and were later overtaken

by by cloud-based applications such as Wikipedia or traditional encyclopedia companies, such as Britannica, which moved online.

Nintendo and encyclopedia companies were both born from the printing era; Nintendo began with traditional playing cards, and encyclopedias stemmed from hard copy books.

Because they reacted to technological innovations differently, they experienced different outcomes.

The reality is that digital transformation is an ongoing process, not a one-time effort.

Today, countless industries around the world are disrupted by digitalization: from healthcare to entertainment, from retail to manufacturing.

It's critical that organizations embrace new technology as an opportunity to evolve, serve their customers better, and gain a competitive advantage.

This is where cloud computing plays a significant role.

Point 5: Cloud eras

To understand the cloud computing landscape today and what true digital transformation looks like, we should first understand how we got here.

It started with the VM cloud era.

VM stands for virtual machine.

New organizations, mostly startups, realized that they could forgo ever buying or operating hardware and just start in the cloud.

This was a major catalyst for many of the great cloud-native companies that we all rely on today, such as Twitter, Spotify, and PayPal.

By the end of this first VM cloud era, very few startups operated their own data centers.

Next was the infrastructure cloud era, which is when organizations migrated their IT infrastructure to the cloud.

This migration saved costs because infrastructure could scale up and down more quickly and easily.

Faster development was possible because companies didn’t need long-term infrastructure planning and security was better.

Also, reducing the management load on IT staff let organizations direct more people and resources to focus on building new capabilities.

In this last decade of the infrastructure cloud, companies that ignored this migration were left trailing behind.

Although the return on investment of these early cloud migrations was important, it didn’t provide compelling transformative or disruptive results or fundamentally change how people worked outside of IT.

This is because digital transformation is more than simply migrating and shifting systems to the cloud for cost saving and convenience.

As we look ahead, reinventing the future means changing not only where business is done, but how it is done.

It requires maximizing the benefits of the cloud and building an environment that enables every person, process, and technology to bring the highest level of innovation to the business.

This is what brings us to the transformation cloud era, where organizations are not just making infrastructure decisions, but are truly focusing on transforming.

Digitalization is now fundamental, and this era is about spreading transformation among all teams in an organization.

To facilitate this degree of constant innovation and progress, today’s most ambitious organizations are building transformation clouds.

A transformation cloud is a new approach to digital transformation.

It provides an environment for app and infrastructure modernization, data democratization people connections and trusted transactions.

It’s built on an easy-to-use platform with customized industry solutions that gives organizations the confidence that they are saving money and creating a more sustainable future for everyone.

The result is an organization that benefits from cloud computing to drive innovation, generate new revenue streams, and adapt quickly to market changes and customer needs.

Point 6: Challenges that lead to a digital transformation

A major indicator for organizations that are accelerating their innovation is how they think about transformation.

Instead of asking infrastructure questions about where their apps and services should run, they ask transformation questions about

how to build an environment that helps every person, process, and technology to adapt to changing business needs.

So, what are the types of problems and questions that make organizations undergo a digital transformation?

At Google, when we talk to our customers about their biggest business challenges and what they need to accelerate digital transformation, we consistently hear five themes.

First, they want to be the best at understanding and using data.

Today, organizations must unify data across streams, lakes, warehouses, and databases so that they can quickly and

easily break down data silos, generate real-time insights, and make better business decisions; thus reducing cost and inefficiencies.

Second, they want the best technology infrastructure.

Organizations are looking for a cloud platform that will serve as their foundation for growth and has the flexibility to innovate securely and adapt quickly based on market needs.

Third, they want to create the best hybrid workplace.

The fundamental shift in how and where we work requires new, stronger connections and collaboration, and many interactions that took place in person have been digitized.

This change requires more intentional connections and collaboration.

Fourth, it's critical for organizations to know that their data, systems, and users are secure.

The digital world is seeing more severe security issues, so now companies are rethinking their security posture.

They must find ways to identify and protect everything from people customers customers to data and transactions in a fast-changing environment.

Finally, organizations are prioritizing sustainability as a critical, board-level topic.

They want to create a more sustainable future through products and services that minimize environmental impact.

These are the top drivers for digital transformation that we see, and the challenges that many organizations face as they navigate their journey.

Point 7: Google's Transformation Cloud

There are five primary capabilities that form the basis of the transformation cloud.

They are: Data Open infrastructure Collaboration Trust And sustainable technology and solutions.

Let’s explore each, starting with the data cloud.

Data is the key to unlocking value from AI, making it critical for innovation and differentiation.

Data is the key to unlocking value from AI, making it critical for innovation and differentiation.

Data is the key to unlocking value from AI, making it critical for innovation and differentiation.

But becoming a data-driven company can be difficult if datasets are siloed across operational and analytical data stores.

According to the NewVantage Partners’ Data and AI Executive Survey 2022, only 26.5% of companies

have succeeded in creating a data-driven organization, to realize tangible and measurable value from their data.

A data cloud is a unified solution to manage data across the entire data lifecycle, regardless of whether it sits in Google Cloud or in other clouds.

It lets organizations identify and process data with great scale, speed, security, and reliability.

Leading companies like Ford, Spotify, Wayfair, and UPS use a data cloud to encourage data-driven transformation quickly, securely, and at scale, all with AI built in.

Next up is the open infrastructure.

Organizations choose to modernize their IT systems on Google’s open infrastructure cloud because it gives them freedom

to securely innovate and scale from on-premises, to edge, to cloud on an easy, transformative, and open platform.

Open infrastructure cloud brings Google Cloud services to different physical locations, while leaving the operation, governance, and evolution of the services to Google Cloud.

Instead of relying on a single service provider or closed technology stack, today most organizations want the freedom to

run applications in the place that makes the most sense, using hybrid and multicloud approaches based on open source software.

An open infrastructure cloud facilitates faster innovation and reduces lock-in to a single cloud provider by giving organizations the

choice by giving organizations the choice and flexibility to build, migrate, and manage their applications across on-premises and multiple clouds.

Let’s take a moment to define two terms that are often confused: open standard and open source.

Open standard refers to software that follows particular specifications that are openly accessible and usable by anyone.

They have guidelines for software functionality, which help avoid vendor lock-in and ensure that the products that use these standards perform in an interoperable way.

Examples of open standards are HTTP for requesting content on the web or XML for storing structured data.

Open source refers to software whose source code is publicly accessible and free for anyone to use, modify, and share.

A decentralized community generally develops open source software as a public collaboration, based on a philosophy of transparency and the open exchange of ideas.

Open source plays a critical role in an open cloud to deliver customers the portability they expect.

Google has a long history of sharing technology through open source: from projects like Kubernetes, which is now the industry

standard in container interoperability in the cloud, to TensorFlow, a platform to help everyone develop and train machine learning models.

Another way we provide flexibility is through hybrid and multicloud environments managed by products like Anthos, which is built on open technologies like Kubernetes, Istio, and Knative.

And finally, an open infrastructure embraces a partner ecosystem— —and the breadth of solutions it can offer its customers —instead of competing with it.

Collaboration helps transform how people connect, create, and collaborate.

A transformation cloud isn’t just about technology.

People and culture are just as important.

Organizations have increased both location and time flexibility in work arrangements since the COVID-19 pandemic began, and hybrid work is here to stay.

With the definition of the workplace forever changed, it's essential that information and frontline workers across regions and industries it's

essential that information and frontline workers across regions and industries connect, create, and collaborate securely from anywhere, and on any device.

This new hybrid work environment needs to support a mix of in-person and remote interactions, including immersive digital and mobile experiences.

At Google, for example, we offer a collaboration cloud through Google Workspace.

Workspace brings together communication and collaboration apps including Gmail, Chat, Calendar, Drive, Docs, Sheets, and Meet into a people-first experience powered by Google AI.

A trusted cloud helps organizations protect what's important with advanced security tools.

According to Cybersecurity Ventures, the annual cost of cyber crime is expected to reach $10.5 trillion annually by 2025.

Due to the rise of cybersecurity threats, every company is rethinking its security posture.

This means finding ways to identify and protect everything, from people and customers to data and transactions —in a fast-changing environment.

Organizations see the cloud as more secure than on-premises, and they want to make it simple so that employees, customers, and contractors can safely access their services.

They want to create better visibility to find, analyze, resist, and remediate threats at global scale, and benefit from cloud innovations while maintaining control of their digital assets.

Finally, a transformation cloud is built on a sustainable foundation, using technology and solutions that help organizations build and work more sustainably.

Today, organizations are now encouraged to help create a cleaner, more sustainable world and they need new technologies that help them progress consistently.

According to IDC, cloud computing is estimated to save 1 billion metric tons of CO2 emissions by 2024.

The largest corporations have the opportunity to lead the way in helping the world reduce its emissions and operate on carbon-free energy always.

For that reason, companies are moving to the cloud, and they want a sustainable infrastructure to power their business.

At Google Cloud, for example, we partner with customers to decarbonize their digital apps and infrastructure with our sustainable technology and solutions.

We proudly operate the cleanest cloud in the industry, with the smartest data centers that are 2 times as energy-efficient as a typical enterprise data center.

Moving to Google Cloud can dramatically decrease a customer's IT-related carbon footprint.

Point 8: The Google Cloud Adoption Framework

So how can organizations approach their cloud journey?

Moving to the cloud offers enormous benefits for transforming businesses.

Yet there are also risks.

The challenge is multi-dimensional, with far reaching implications for the solutions that will run in the cloud and also for the technologies that support them.

The people who must implement them and the processes that govern them.

The rubric of people, process, and technology is a familiar one.

It forms the basis of the Google Cloud Adoption Framework, which was created to support customers on their cloud journey.

The value of the Google Cloud adoption Framework is that it serves as a map to help

organizations adopt the cloud quickly and effectively by creating a comprehensive action plan for accelerating cloud adoption.

It does this by structuring and aligning short term tactical, mid-term, strategic, and long term transformational business objectives.

It provides a solid assessment of where an organization is in its cloud journey and actionable programs that get it to where it wants to be.

A cloud maturity assessment helps to establish where an organization is currently regarding the cloud adoption themes recognized by Google Cloud.

It can quickly reveal any areas where an organization might be weaker or underinvested.

This is especially powerful if an organization was previously unaware of this lack of maturity.

The Google Cloud adoption framework is more than just a model.

It's also a map to real, tangible tasks that organizations need to adopt the cloud.

After cloud maturity has been assessed and actions have been recommended, it's easy to scope and structure a cloud adoption program using the framework.

Summary

This brings us to the end of the first section of the Digital Transformation with Google Cloud course.

Let's do a quick review of how cloud technology is transforming businesses.

Digital transformation is more than lifting and shifting old I.T. infrastructure to the cloud for cost saving and convenience.

As we look ahead, reinventing the future means changing not only where business is done, but how it is done.

It requires maximizing the benefits of the cloud and building an environment that lets every person, process and technology bring the highest level of innovation to the business.

We already see tremendous success with leading organizations that embrace the transformation cloud, move their businesses well beyond infrastructure, and rapidly migrate toward the next era of their cloud evolution.


Quiz
Passing score: 75%

1.

Select the definition of digital transformation.

When an organization uses new digital technologies to create or modify on-premises business processes.

When an organization uses new digital technologies to create or modify business processes, culture, and customer experiences.

When an organization uses new digital technologies to create or modify technology infrastructure to focus on cost saving.

When an organization uses new digital technologies to create or modify financial models for how a business is run.

2.

As the world and business changes, organizations have to decide between embracing new technology and transforming, or keeping their technology and approaches the same. What risks might an organization face by not transforming as their market evolves?

Focusing on ‘why’ they operate can lead to inefficient use of resources and disruption.

Organizations risk losing market leadership if they spend too much time on digital transformation.

Focusing on ‘how’ they operate can prevent organizations from seeing transformation opportunities.

Embracing new technology can cause organizations to overspend on innovation.

3.

An organization has a new application, and user subscriptions are growing faster than on-premises infrastructure can handle. What benefit of the cloud might help them in this situation?

It's cost effective, so the organization will no longer have to pay for computing once the app is in the cloud.

It's secure, so the organization won't have to worry about the new subscribers data.

It provides physical access, so the organization can deploy servers faster.

It's scalable, so the organization could shorten their infrastructure deployment time.

4.

What is the benefit of implementing a transformation cloud that is based on open infrastructure?

On-premises software isn't open source, so cloud applications are more portable.

Open source software reduces the chance of vendor lock-in.

Open standards make it easier to hire more developers.

Open source software makes it easier to patent proprietary software.

5.

Which item describes a goal of an organization seeking digital transformation?

Ensure better security by decoupling teams and their data.

Reduce emissions by using faster networks in their on-premises workloads.

Streamline their hardware procurement process to forecast at least a quarter into the future.

Break down data silos and generate real time insights.

6.

What is the cloud?

A metaphor for a network of data centers.

A Google product made up of on-premises IT infrastructure.

A Google product for computing large amounts of data.

A metaphor for the networking capability of internet providers.

7.

Select the two capabilities that form the basis of a transformation cloud? Select two correct answers.

Sustainable cloud ensures the costs of cloud resources are controlled to prevent budget overrun.

A trusted cloud gives control of all resources to the user to ensure high availability at all times.

Open infrastructure gives the freedom to innovate by running applications in the place that makes the most sense.

Data cloud provides a unified solution to manage data across the entire data lifecycle.

Collaboration cloud ensures that the device a user connects with only works on the corporate network.

8.

What is seen as a limitation of on-premises infrastructure, when compared to cloud infrastructure?

The on-premises networking is more complicated.

Scaling processing is too difficult due to power consumption.

The on-premises hardware procurement process can take a long time.

Maintenance workers do not have physical access to the servers.

9.

An organization has made significant investments in their own infrastructure and has regulatory requirements for their data to be hosted on-premises. Which cloud implementation would best suit their needs?

Private Cloud

Software as a service

Platform as a service

Public Cloud



Section 2: Fundamental Cloud Concepts

Introduction

To understand the impact that the cloud can have on a business, it’s important to first recognize some of the fundamental cloud concepts.

By the end of this section, you’ll be able to: Describe the benefits of moving to cloud infrastructure through customer business use cases.

Explain how moving to the cloud shifts an organization's spending from capital expenditure to operational expenditure, and how that affects their total cost of ownership.

Identify when private, hybrid, or multicloud infrastructures best apply to different business use cases.

Define basic network infrastructure terminology.

And explain how Google Cloud supports digital transformation with global infrastructure and data centers connected by a fast, reliable network.

Cloud adoption has made a positive impact on some of the world’s leading companies across various industries.

Let’s start with an example of how the cloud provided flexibility and improved performance for the Canadian food and pharmacy leader Loblaw.

Loblaw is Canada’s largest retailer, with nearly 2,500 corporate, franchised, and associate-owned locations and nearly 200,000 full- and part-time employees.

The Vice President of Technology at Loblaw explained: ”We want our tech talent focused on creating better experiences for our customers, not maintaining infrastructure.

” Loblaw took a lift-and-shift approach to accelerate the initial migration to Google Cloud.

This means they focused on moving their existing virtual machines on-premises to Compute Engine instances in the cloud without needing to redesign them.

However, they designed the cloud architecture to be easily converted to using Google Kubernetes Engine for automated deployment and scaling later.

This shows how an open cloud can grow with an organization’s needs.

With a more responsive ecommerce site and the ability to handle more traffic without affecting the customer experience, Loblaw could run marketing promotions and generate additional revenue.

With these improvements, they expect to reclaim 50% of their Site Reliability Engineers’ time to focus on innovation.

Now let’s shift our focus to another example, this time on how the cloud provided scalability and cost reduction.

HSBC is one of the world’s largest banking and financial services organizations, serving more than 40 million global customers from offices in 64 countries and territories.

Committed to a cloud-first strategy, in January 2021, HSBC embarked on an ambitious project to enhance and future-proof their risk management on the cloud.

Their previous on-premises system was not capable of meeting future regulatory and business demands.

HSBC built a cloud-native risk management solution that boosts calculation speed to be ten times faster while lowering costs.

This equated to three billion calculations per second.

The power of a data cloud is that it has almost unlimited resources to process large volumes of data and reduce time to insights.

A Global Head of Traded Risk Technology at HSBC explains, “We knew that a cloud-native solution gave us the ability to scale and

run at a reduced cost. We did a proof of concept using Google Cloud, and we quickly realized that this could be very successful.”

HSBC built a cost-effective platform that is faster and more efficient while meeting their regulatory and compliance requirements.

And in a final example, let’s look at how the cloud has brought agility and valuable insights, while maintaining trust, to an organization.

The American Cancer Society is a community-based voluntary health organization dedicated to eliminating cancer as a major health problem.

Their mission is to free the world from cancer by funding and conducting research, sharing expert information, supporting patients, and spreading the word about prevention.

Among women, breast cancer is the most commonly diagnosed type of cancer.

Yet, if detected early, it’s also one of the most survivable cancers.

Mia M. Gaudet, PhD, is the Scientific Director of Epidemiology Research at the American Cancer Society.

Through her research, she’s obtained over 1,700 high-resolution tissue images from participants diagnosed with breast cancer.

This valuable data could help them discover factors that could lead to a cancer diagnosis and improve survival rates.

The challenge was to identify novel patterns in digital images of breast cancer tissues to potentially improve patient outcomes.

Their research group partnered with Slalom, a Google Cloud Premier Partner, and sought to combine machine learning–powered insights with cloud computing performance to improve timeliness and accuracy.

By building a machine learning pipeline using Google Cloud AI Platform, now called Vertex AI, they trained models for AI image analysis of tissue scans to find cancer indicators.

The team achieved 12 times faster image analysis with enhanced quality and accuracy by removing human limitations.

Dr. Gaudet said, "The ability to perform image analysis by using deep learning for epidemiologic breast cancer studies

opens a new frontier of research, and Google Cloud makes it easier. We're excited about what we'll find."

The American Cancer Society is now equipped with processes and a cloud infrastructure that will be reusable on similar projects, providing a foundation for future work.


Point 1: Total cost of ownership (TCO)

Organizations often perform a cloud total cost of ownership (or TCO) analysis when they are considering moving to the cloud.

This analysis aims to weigh the cost of cloud adoption against the cost of running their current on-premises systems.

For on-premises, TCO is associated with assessing the cost of static resources throughout their lifetime.

However due to the dynamic nature of the cloud, predicting future costs can be challenging.

A common mistake that organizations make when attempting to calculate cloud TCO is to directly compare the running costs of the cloud against their on-premises system.

These costs are not equivalent.

The cost of on-premises infrastructure is dominated by the initial purchase of hardware and software, but cloud computing costs are based on monthly subscriptions or pay-per-use models.

It's also important to consider all the operational costs of running your own data center, such as power, cooling, maintenance, and other support services.

A data center is a building or facility that houses a large amount of IT infrastructure, computing, and storage resources in one place.

Finally, intangible costs, such as the opportunity cost of not migrating to cloud and the missed benefits, should be considered.

Point 2: Capital Expenditures (CapEx) versus Operating Expenses (OpEx)

One area where cloud differs from traditional IT is in how managing costs changes when you move to the cloud.

With organizations moving from on-premises infrastructure to on-demand cloud services, there’s a major shift in spending from capital expenditures to operating expenses.

But what’s the difference between these two?

Capital expenditures, or CapEx, are upfront business expenses put toward fixed assets.

Organizations buy these items once, and they benefit their business for many years.

For example, in IT, these expenditures might mean buying hardware like servers, printers, or cooling systems.

Maintaining these assets is also considered CapEx because it extends their lifetime and usefulness.

Small businesses can find CapEx spending challenging because large one-time purchases are often high cost.

The more money you put toward CapEx means less free cash flow for the rest of the business.

And then there are operating expenses, or OpEx, which are recurring costs for a more immediate benefit.

This represents the day-to-day expenses to run a business.

In IT, these expenses might be yearly services like website hosting or domain registrations, or the subscription fee for cloud services.

OpEx covers the spendings on pay-as-you-go items, but are not considered major long-term investments like CapEx items.

Understanding the difference between CapEx and OpEx is helpful in recognizing how costs differ between on-premises and the public cloud.

In the on-premises CapEx model, cost management and budgeting are a one-time operational process completed annually.

Data centers require a huge CapEx investment up front as organizations purchase space, equipment, and software and hire a workforce to run and maintain everything.

Forecasting is based on a metric such as historic growth to determine the needs for the next month, quarter, year, or even multiple years.

Moving to cloud’s on-demand OpEx model enables organizations to pay only for what they use and only when they use it.

Budgeting is no longer a one-time operational process completed annually.

Instead, spending must be monitored and controlled on an ongoing basis due to the dynamic nature of cloud use within organizations.

How infrastructure is procured has radically changed, too.

In a more decentralized cloud world, any employee can create resources in seconds on infrastructure owned and managed by a cloud provider.

Organizations save on power, cooling, and floor space; they save on management because they don’t have to install, operate, upgrade, and troubleshoot it themselves.

And they're not depreciating the equipment—the cloud provider is.

Cloud gives organizations the ability to start small and grow organically instead of having to guess at what is needed next week, next month, and next year.

Costs match actual usage and are now operational expenses.

Point 3: Private Cloud, Hybrid cloud, and multi-cloud strategy

It's not always possible, or necessary, for an organization to rely solely on the cloud.

For example, requirements might call for on-premises infrastructure to work with public cloud services provided by companies, like Google Cloud.

With the availability of different cloud options and configurations, it’s important to understand what each means.

Let’s explore the definitions of private, hybrid, and multi-cloud, and when an organization might choose each approach.

Let’s begin with private cloud, which is when an organization has virtualized servers in its own

data centers, or those of a private cloud provider, to create its own private dedicated environment.

On-premises servers are also often referred to as private clouds, but generally the distinction can be made

that on-premises software runs in a local environment, whereas a private cloud is accessed through the internet.

Private cloud computing gives an organization many of the benefits of a public cloud — including self-service, scalability, and elasticity — with more customization available than from dedicated on-premises infrastructure.

This approach is often used when an organization has already made significant infrastructure investments, or if, for regulatory reasons, data must be kept on-premises.

In contrast, a hybrid cloud is one in which applications are running in a combination of different environments.

The most common example is combining a private and public cloud environment, like an on-premises data center, and a public cloud computing environment like Google Cloud.

Finally, there’s multicloud, which describes architectures that combine at least two public cloud providers, such as Google Cloud, Amazon Web Services, Microsoft Azure, or others.

An organization might choose multicloud if they want to take advantage of the key strengths of different public cloud providers.

Organizations may also operate a combination of on-premises and multiple public cloud environments, effectively being both hybrid and multicloud simultaneously.

A hybrid cloud approach is one of the most common infrastructure setups today because organizations can continue to use their on-premises servers while also taking advantage of public cloud.

According to Gartner, 81% of organizations are working with two or more public cloud providers.

Additionally a Flexera state of the cloud report showed 93% of enterprises have a multicloud strategy.

So, what is a hybrid or multicloud strategy used for?

Let's explore some different business requirements, drivers, and use cases that lead an organization to choose this kind of approach.

Access to the latest technologies Running workloads in multiple clouds empowers organizations to leverage the latest innovations and capabilities from

each cloud provider, thus taking a best-in-class approach to cloud features and obtaining the scale, security, and agility to innovate fast.

Cloud can help organizations build out capabilities, such as advanced analytics services, that might be difficult, or impossible, to implement in existing environments Modernize at the right

pace With a hybrid cloud, organizations can migrate applications to the cloud at the pace that makes sense for their business and transform their technical infrastructure over time.

Improved return on investment By adding a public cloud provider to their existing on-premises infrastructure, organizations can expand their cloud computing capacity without increasing their data center expenses.

This can help reduce CapEx or general IT spending, and improve transparency regarding costs and resource consumption.

Flexibility through choice of tools Hybrid and multi-cloud strategies have advantages for organizations as a whole, but specifically

benefit development teams that are working on different projects and tackling unique challenges across different lines of business.

A wider choice of tools and developer talent can be applied to a particular business problem, which means responding better to changing market demands.

It also avoids vendor lock-in concerns.

Improve reliability and resiliency Organizations can distribute core workloads across multiple cloud and on-premises infrastructures to reduce downtime and and concerns about over-dependence on a single source of failure.

This approach can improve the quality and availability of a service.

Maintain regulatory compliance Many industries have rules from governmental or regulatory bodies regarding where their app can operate.

Adopting a hybrid solution is an effective way for an organization to ensure compliance with regional data governance, residency, or digital sovereignty requirements.

Running apps on-premises Organizations may have regulated applications that must remain on-premises or mainframe systems that are difficult to move to the cloud.

A hybrid approach provides the freedom to innovate while still meeting And finally, running apps at remote edge locations Organizations in

industries that run distributed apps at remote locations, such as kiosks in retail or networks in telecom, can benefit from hybrid cloud.

These apps often require improved performance and low latency, and a hybrid approach lets them run select apps at the network edge.

Point 4: How a Network supports digital transformation

Digital transformation has increased the importance of the network.

The ability to connect customers, employees, cloud applications, and devices enables modern organizations to succeed.

With every innovation, the underlying apps and services rely on the network to communicate and connect.

But how does a reliable networking architecture support a digital transformation strategy?

A fast, reliable, and low-latency global network ensures exceptional user experience and high performance.

It also makes it easier to communicate and manage data globally.

With ever more distributed workforces and online businesses, having virtual network services that can easily scale [the building rises up] without adding hardware ensures that organizations can adapt.

So, how does a network operate?

Let's start with the foundation of the modern internet: fiber-optic networks.

Fiber-optic cables contain one or more optical fibers, which are thin strands made of glass or plastic.

These fibers are used to transmit data as pulses of light over long distances.

Subsea fiber-optic cables carry 99% of international network traffic, yet we barely notice they exist.

The first subsea cable was deployed in 1858 for telegraph messages between Europe and North America.

A message took over 17 hours to deliver, at 2 minutes and 5 seconds per letter by Morse code.

Today, a single cable can deliver a whopping 340 Terabits per second.

That's more than 25 million times faster than the average home internet connection!

Every shared video, sent email, and downloaded app depends on data traffic that moves through international network infrastructure.

But how is this content available to people within milliseconds?

A rich ecosystem of companies and local providers build a global infrastructure that provides businesses and people around the world with the best possible internet experience.

These include companies like internet service providers (or ISPs).

ISPs provide access to the internet to both personal and business customers, handling the traffic between the customer and the internet as a whole.

Some examples of ISPs include Verizon, Vodafone, and Softbank.

The infrastructure that makes Google’s global reach possible is our network of fiber-optic cables that run on both land and sea.

This network connects our data centers and points of presence like highways connect major cities.

Google owns and operates data centers all over the world.

In these Google data centers, products like Search, Gmail, YouTube, and Google Cloud are run for people and organizations around the world, 24 hours a day, seven days a week.

Within this vast global network, how do all the different parts recognize and communicate with each other?

There are protocols that make it work.

Let's start with an IP address.

The IP stands for Internet Protocol, and this address is a series of numbers that can identify a network or the location of a particular device on a network.

A domain name is an easy-to-remember name that maps directly to an IP address or set of IP addresses on the internet.

It’s the unique name that appears after the @ sign in email addresses and after www.

in web addresses.

For instance, the domain name example.com might translate to the IP address 192.168.200.8.

Other examples of domain names are google.com and youtube.com.

And then there’s a Domain Name System, or DNS.

A DNS server stores a database of domain names mapped to IP addresses that can be queried and used by computers to communicate with each other.

This system is like the phone book of the web.

Every time you visit a website, your computer performs a DNS lookup.

A phone book translates a name like "Acme Pizza" into the correct phone number to call; similarly,

the DNS translates a web address like "www.google.com" into the IP address of the computer hosting that site.

In this case, it’s the Google homepage.

Point 5: Network Performance: Bandwidth and latency

Now that you’ve been introduced to some of the fundamentals of networking, let’s explore how networks perform and are measured.

Two important terms in networking are bandwidth and latency.

Let’s define them both.

Bandwidth is a measure of measure of how much data a network can transfer in a given amount of time.

This ​​rate of data transfer is typically measured in terms of “megabits per second” (or Mbps) or “gigabits per second” (or Gbps).

Generally speaking, a higher bandwidth allows a computer to download information from the internet more quickly.

One way to think of bandwidth is to picture water flowing through a pipe.

The bandwidth would be the volume of water a pipe can handle flowing through per second.

A wider pipe can handle more water.

An internet service provider may provide a home internet connection with 100 MegaBits per second to over

1 GigaBit per second; a data center may have with bandwidth from 10 to 100 GigaBits per second!

Having a high bandwidth is useful when sending a large amount of data per second, such as streaming high-definition video, but it’s not the only important measure of network performance.

For example, for users playing real-time multiplayer games online, latency will matter much more.

For example, for users playing real-time multiplayer games online, latency will matter much more.

Network latency is the amount of time it takes for data to travel from one point to another.

Often measured in milliseconds, latency, sometimes called lag, describes delays in communication over a network.

Going back to our flowing water analogy, latency is the delay from the moment the water pipe is opened until water starts flowing through.

Ideally, latency should be as close to zero as possible.

However, because it’s a result of the physical distance that data must travel – through wires, fiber optics, routers, and

more – to reach its destination, each “hop” along the way adds a small amount of latency to the communication.

No matter how much data you can send and receive at once, it can only travel as fast as network latency allows.

Imagine an image file took just 10 milliseconds to download with a high-bandwidth connection, but a user had to wait 100 milliseconds before receiving the first byte of data.

In this case, the latency, or how much time it took for data packets to travel from one point to another in the network, accounted for most of the time.

Cloud computing and mobile technologies have made it easier for developers to reach global audiences, but high latency can drag down an application's performance.

Websites run slower for some users depending on their physical location, even if both the user and the server have excellent bandwidth.

So the farther a user is from a server, or the more fragmented the network is, the bigger the latency.

Reducing latency is essential to reaching users faster.

Point 6: Google Cloud regions and zones

Google has invested billions of dollars over the years to build its network, which is one of the largest networks of its kind on Earth.

It’s designed to give customers the highest possible throughput and lowest possible latencies for their applications.

Google Cloud’s infrastructure is based Google Cloud’s infrastructure is based in five major geographic locations: North America, South America, Europe, Asia, and Australia.

Having multiple service locations is important because choosing where to locate applications affects qualities like availability, durability, and latency,

the latter of which measures the time a packet of information takes to travel from its source to its destination.

Each of these locations is divided into several different regions and zones.

Regions represent independent geographic areas and are composed of zones.

For example, London, or europe-west2, is a region that currently comprises three different zones.

A zone is an area where Google Cloud resources are deployed.

For example, if you launch a virtual machine using Compute Engine it will run in the zone that you specify to ensure resource redundancy.

You can run resources in different regions.

This is useful for bringing applications closer to users around the world, and also for protection in case there are issues with an entire region, such as a natural disaster.

Some of Google Cloud’s services support placing resources in what we call a multi-region.

For example, Cloud Storage lets you place data within the Europe multi-region.

That means it's stored redundantly in at least two geographic locations, separated by at least 160 kilometers within Europe like London and Belgium.

You can find the most up-to-date numbers for Google Cloud regions and zones at cloud.google.com/about/locations.


Point 7: Google's edge network

A recommended best practice for organizations is to keep their traffic on Google's private network for most of its journey, using the

same network that powers products like Gmail, Google Search and YouTube allows organizations to take advantage of the performance that global infrastructure provides.

When a user opens a Google app or Web page, Google responds to that request from an edge network location that will provide the lowest latency.

Understanding Google's Edge Network and how it maintains caches that store popular content near its users helps organizations choose when to hand off traffic to Google.

A network's edge is defined as a place where a device or an organization's network connects to the Internet.

It's called "the edge" because it's the entry point to the network.

Google's Edge Network is how we connect with ISPs to get traffic to and from users.

It's made up of network infrastructure that organizations can hand off traffic to based on users needs, performance and cost.

Google aims to deliver its services with high performance, high reliability and low latency for users.

We have invested in network infrastructure that's aligned with this goal and that also allows us to exchange traffic efficiently and cost effectively with network operators.

This is how network infrastructure supports digital transformation.

Summary

This brings us to the end of the second section of the Digital Transformation with Google Cloud course.

Let's do a quick review of some fundamental cloud concepts.

We covered examples of how customers from different industries were able to transform their business through creating new ways of working when moving to the cloud.

Calculating the total cost of ownership differs greatly from the static and long term procurement on premises world to the more dynamic and on demand cloud world.

Moving to cloud means shifting spending from a capital expenditure to operational expenses model, enabling organizations to pay only for what they use and only when they use it.

We defined private, hybrid, and multi-cloud and described the different business drivers that lead an organization to choose these kinds of approaches.

And finally, you learned the importance of a fast, reliable and low latency global network as a foundation to transformation and exceptional user experience.


Quiz
Passing score: 75%

1.

An organization wants to innovate using the latest technologies, but also has compliance needs that specify data must be stored in specific locations. Which cloud approach would best suit their needs?

Multicloud

On-premises infrastructure

Hybrid Cloud

Public Cloud

2.

Which network performance metric describes the amount of data a network can transfer in a given amount of time?

Fiber optics

Domain Name System (DNS)

Bandwidth

Latency

3.

A financial services organization has bank branches in a number of countries, and has built an application that needs to run in different configurations based on the local regulations of each country. How can cloud infrastructure help achieve this goal?

Reliability of the infrastructure availability.

Total cost of ownership of the infrastructure.

Flexibility of infrastructure configuration.

Scalability of infrastructure to needs.

4.

An organization has shifted from a CapEx to OpEx based spending model. Which of these statements is true?

Budgeting will only happen on an annual basis.

They will only pay for what they forecast.

Hardware procurement is done by a centralized team.

They will only pay for what they use.

5.

An organization wants to ensure they have redundancy of their resources so their application remains available in the event of a disaster. How can they ensure this happens?

By putting resources in different zones.

By putting resources in the Domain Name System (DNS).

By assigning a different IP address to each resource.

Using the edge network to cache the whole application image in a backup.


Session 3: Cloud Computing Models and Shared Responsibiliy

Introduction

When moving to the cloud, there are decisions to make about how to manage and operate different cloud services.

One of those decisions is around the type of cloud computing service model to use.

Organizations typically choose service model types based on their specific business requirements.

In this section of the course, you’ll explore three main cloud computing service models: IaaS, or

Infrastructure as a Service PaaS, or Platform as a Service and SaaS, or Software as a Service

Because the levels of responsibility between an organization and their cloud service provider vary depending on

which model is used, you’ll also examine the shared responsibility model between Google Cloud and our customers.

By the end of this section, you’ll be able to: Define IaaS, PaaS, and SaaS.

Compare and contrast the benefits and tradeoffs of IaaS, PaaS, and SaaS.

Determine which computing model applies to various business scenarios and use cases.

Describe the cloud shared responsibility model.

And identify which responsibilities are the cloud provider’s or the customer’s for on-premises and cloud computing models.

Point 1: Cloud computing service models

The world of cloud computing has a diverse set of computing service models to choose from, depending on customer requirements.

You might have heard of terms like IaaS, PaaS and SaaS.

These terms represent the different cloud computing models provided “as a service” by cloud providers.

”As a service” refers to the way IT resources are consumed in these models, and is a key difference between cloud computing and traditional IT.

In traditional IT, an organization consumes resources,such as hardware, software, and development tools, by purchasing, installing, managing, and maintaining them in its own on-premises or self-managed data center.

Organizations are responsible for all of their IT infrastructure when it's completely on-premises.

In cloud computing, the cloud service provider owns, manages, and maintains the resources.

The customer consumes those resources, which are provided on a subscription or pay-as-you-go basis.

All you need is an internet connection.

Cloud computing allows for a third party to be responsible for some part of the infrastructure.

This means that organizations then have more time to focus on their core business.

Coming up, we're going to explore three different cloud computing service models: Infrastructure as a service, or IaaS, which offers infrastructure resources such as compute and storage.

Platform as a service, or PaaS, which offers a develop-and-deploy environment to build cloud apps.

And software as a service, or SaaS, which delivers complete applications as services.

Each model offers distinct features and functionalities, and knowing the differences between them helps organizations choose one to best fit their business’ needs.

It’s important to remember that most organizations that use cloud often use a combination of cloud computing models to solve for different needs.

You can visualize these cloud computing models in layers.

As you move up the layers from one model to another, each model requires less knowledge and management of the underlying infrastructure.

This concept is called abstraction.

In cloud architecture, as the level of abstraction increases, less is known about the underlying implementation.

The goal of "abstracting away” infrastructure is to reduce complexity by removing unnecessary information and simplifying operations.

Think about abstraction in the way that you operate a car.

When you turn on the ignition, press the brake, put the car into gear, and accelerate, you’re not thinking about how the engine is physically operating under the hood, right?

That complexity is abstracted away from you, so you can focus on driving safely to your destination.

Abstraction is one of the core features of cloud computing.

When choosing between cloud computing service models, organizations must decide the level of control and management

they’ll require, or how much they want to hide technical details and focus on business needs.

Let’s use a transportation analogy to see how on-premises, IaaS, PaaS, and SaaS compare with each other.

On-premises IT infrastructure is like owning a car.

When you buy a car, you’re responsible for its usage and maintenance.

Upgrading means buying a new car, which takes time and can be costly.

IaaS is like leasing a car.

When you lease a car, you choose a car and drive it wherever you want, but the car isn’t yours.

Upgrading is easier though, as you can just lease a new car.

PaaS is like taking a taxi.

You provide specific directions, like the code, but the driver does the actual driving.

And SaaS is like going by bus.

You still get access to transport, but it's less customizable.

Buses have designated routes, and you share the space with other passengers.

Point 2: IaaS (Infrastructure as a Service)

Now let's look at each of these computing models in more detail.

We’ll start with infrastructure as a service, or IaaS.

IaaS is a computing model that offers the on-demand availability of almost infinitely scalable infrastructure resources, such as compute, networking, storage, and databases as services over the internet.

IaaS allows organizations to lease the resources they need instead of having to buy hardware outright, and they only pay for what they use.

It provides the same technologies and capabilities as a traditional data center without having to physically maintain or manage all of it.

One of the main reasons businesses choose IaaS is to reduce their capital expenditures and transform them into operational expenses.

IaaS is appealing because acquiring computing resources to run applications or store data the traditional way requires time and capital.

Organizations must purchase equipment through procurement processes that can take months.

They must also invest in physical spaces, which are typically specialized rooms with power and cooling.

And after deploying the systems, they need IT professionals to manage them.

This traditional way is challenging to scale when demand spikes or business grows.

Organizations risk running out of capacity, or overbuilding and ending up with underutilized infrastructure.

In contrast, IaaS resources are offered as individual services, so organizations can choose what they need.

The cloud provider manages the infrastructure, and businesses can concentrate on installing, configuring, and managing software and keeping their data secure.

Compute Engine and Cloud Storage are examples of Google Cloud IaaS products.

You can create and run virtual machines with Compute Engine, and you can store any type of data with Cloud Storage.

So, what are the benefits of IaaS?

It’s economical.

Because IaaS resources are used on demand and you only pay for what you use, IaaS costs are fairly predictable and easy to budget for.

It’s efficient.

IaaS resources are regularly available when you need them.

As a result, there are fewer delays when infrastructure is expanded resources aren’t wasted by overbuilding capacity.

This efficiency leads to faster development lifecycles and ultimately a faster time to market.

It boosts productivity.

Because the cloud provider is responsible for setting up and maintaining the physical infrastructure, IT departments save time and money.

They can then redirect resources to more strategic activities.

It’s reliable.

IaaS has no single point of failure.

Even if one component of the hardware resources fails, the service usually remains available.

And it’s scalable.

One of the biggest advantages of IaaS in cloud computing is the capability to scale the resources up and down rapidly rapidly according to business needs.

So, what scenarios would IaaS be good for?

The flexibility and scalability of IaaS is useful for organizations that: Have unpredictable workload volumes or need to move quickly in response to business fluctuations.

Require more infrastructure scalability and agility than traditional data centers can provide.

Have high business growth that outpaces infrastructure capabilities.

Experience unpredictable spikes in demand for infrastructure services.

And see low utilization of existing infrastructure resources.

Point 3: PaaS (Platform as a Service)

Platform as a Service, or PaaS, is a computing model that offers a cloud-based platform for developing, running, and managing applications.

PaaS provides a framework for developers that they can build upon and use to create customized applications.

PaaS is appealing because it provides a platform for developers to develop, run, and manage their own apps without having to build and maintain the associated infrastructure.

They can also use built-in software components to build their applications, which reduces the amount of code they have to write.

Cloud Run and BigQuery are examples of Google Cloud PaaS products.

Cloud Run is a fully managed, serverless platform for developing and hosting applications at scale, which takes care of provisioning servers and scaling app instances based on demand.

BigQuery is a fully managed enterprise data warehouse that manages and analyzes data, and can be queried to answer big data questions with zero infrastructure management.

So, what are the benefits of PaaS?

It reduces development time.

Developers can go straight to coding instead of spending time setting up and maintaining a development environment, which leads to faster time to market.

which leads to faster time to market.

It's scalable.

With PaaS, organizations can purchase additional capacity for building, testing, staging, and running applications whenever they need it.

It also allows for applications to be designed to take advantage of the inherent scalability of cloud infrastructure.

It reduces management.

By abstracting the management of underlying resources even further than IaaS, PaaS offloads infrastructure management, patches, updates, and other administrative tasks to the cloud service provider.

This provides a cost-effective way to focus on new functionality.

And it's flexible.

With support for different programming languages and easy collaboration for distributed teams, PaaS provides developers with the flexibility to deliver various projects—from prototypes to enterprise solutions—on the same platform.

So, what scenarios would PaaS be good for?

PaaS is suitable for organizations that: Want to create unique and custom applications without investing a lot in owning and managing infrastructure.

Want to rapidly test and deploy applications.

Have many legacy applications and want to reduce the cost of operations.

Have a new app project that they want to deploy quickly by growing and updating the app as fast as possible.

Want to only pay for resources while they’re being used.

And want to offload time-consuming tasks such as setting up and maintaining application servers and development and testing environments.

Point 4: Saas (Software as a service)

Software as a service, or SaaS, is a computing model that offers an entire application, managed by a cloud provider, through a web browser.

The cloud provider hosts the application software in the cloud and delivers it through a browser.

With this model, you don’t need to download or install any of it.

SaaS is appealing because it abstracts technology completely from the consumer; SaaS is appealing because it abstracts technology completely

from the consumer; the end user doesn’t need to care about the underlying infrastructure, which is the cloud provider's responsibility.

Organizations simply pay a subscription fee for access to a ready-to-use software product.

Google Workspace, which includes tools such as Gmail, Google Drive, Google Docs, and Google Meet, is a Google Cloud SaaS product.

So, what are the benefits of SaaS?

It's low maintenance.

SaaS eliminates the need to have IT staff download and install applications on each individual computer.

With SaaS, vendors manage all potential technical issues, such as data, servers, storage, and updates in the cloud.

This helps to streamline maintenance and support for an organization.

It's cost-effective.

SaaS is based on a subscription model with a fixed, inclusive, monthly or annual account fee.

Predictable costs and per-user budgeting allows for clear financial governance.

It's flexible.

Everything is available over the internet when a user signs in to their personalized account online.

They can access the software from anywhere, any device, anytime.

And what scenarios would SaaS be good for?

Well, SaaS is suitable for organizations that: Well, SaaS is suitable for organizations that: Want to use standard software solutions that require minimal customization.

Don’t want to invest time or internal expertise in maintaining applications or infrastructure.

Need more time for IT teams to focus on strategic projects.

And need to access apps from various devices and locations.

Point 5: Choosing a cloud computing model

So, how does an organization decide which cloud computing model is the best option for them?

The answer depends on their business needs, required functionality, and available expertise.

If they are looking for a highly flexible, scalable service— while maintaining control of their infrastructure— then IaaS is the right choice.

This model offers the most control and customization, but also requires the most management responsibilities and technical expertise.

If they need a platform designed for building software products, then PaaS would help their business immediately.

This provides a cost-effective way to build applications, but still requires some technical expertise and less management.

If they want features that are ready to use, without the hassle of installations, then SaaS might be the best option.

This represents the least management responsibilities and technical expertise, but also offers the least control and customization.

These computing models are not mutually exclusive, though.

Depending on the use case, most organizations will use combinations of all three to solve for different business needs.

They’ll need to compare their options based on variables such as management level, control, responsibility, flexibility, and expertise needed.

For example, imagine a large organization needs to implement a new inventory management system.

If they had the in-house expertise to develop it and the willingness to manage the infrastructure, they could build this with IaaS resources.

The organization's IT team would have complete control over server configurations, but also bear the burden of managing and maintaining them.

They could choose a PaaS solution and build a custom CRM application while offloading management of

infrastructure to the cloud service provider; retaining complete control over application features, but reducing the management load.

Finally they could choose to buy a ready-made SaaS solution; having no daily management of infrastructure, , but also giving up all control over features and functionality in the software.

Each of these options is a viable solution, so organizations must compare the benefits and tradeoffs for each use case.

These cloud computing service models give organizations choices, flexibility, and options that on-premises hosting simply can’t provide.

Point 6: The shared responsibility model

One area of responsibility where each of the cloud computing models differ is security.

When an organization manages its data in its own data centers, that organization is responsible for all aspects of its security.

However, as infrastructure is moved to the cloud, some aspects of the responsibility shift to the cloud provider.

This concept is called the shared responsibility model.

Security in the cloud is a shared responsibility between the cloud provider and the customer.

Although direct responsibilities change based on the cloud computing service model, organizations are always in

control of securing their data, and the cloud provider is always responsible for securing the infrastructure.

At Google Cloud, we defend organizations’ data against threats and fraudulent activity with the same infrastructure and security services we use for our own operations.

However, security of the cloud and security in the cloud are two different things.

Simply put, the cloud provider is responsible for the security of the cloud, while the customer is responsible for security in the cloud.

It's important for organizations to understand how the specific customer responsibilities vary according to the type of cloud computing model used.

This is especially important because, according to a Gartner report, 99% of all cloud security failures will result from user error through the year 2025.

Organizations must understand their roles and responsibilities in cloud security to guarantee it.

Point 7: How the shared responsibility model works

If you look at the various cloud computing models together, you can see where the cloud provider’s responsibility ends and where the customer's responsibility begins.

A general guideline for shared responsibility is that "if you configure or store it, you're responsible for securing it."

This generally means that a cloud provider is responsible for securing the parts of the cloud that it directly controls, such as hardware, networks, and physical security.

At the same time, the customer is responsible for securing anything that they create within the cloud, such as the configurations, access policies, and user data.

No matter which cloud provider you use, there is shared responsibility.

Let’s examine the ratios of responsibility between Google Cloud as a service provider and our customers.

The blue squares represent the parts of the infrastructure security that the customer is responsible for, while the yellow squares represent the elements that Google Cloud is responsible for.

Let's begin with on-premises.

When an organization runs its own on-premises data centers, security for the infrastructure is solely the responsibility of the organization's internal teams.

They are responsible for securing servers and the data stored on them.

Next is infrastructure as a service.

When an organization transitions to an IaaS computing model, it assigns some IT security responsibilities to Google Cloud.

This includes being responsible for the physical resources and sharing responsibility with the customer for the security of the infrastructure and network.

The rest, such as the security of the operating system, software stack required to run their applications, and their data, is the responsibility of the customer.

This allows customers the most freedom and control, but also places most of the responsibility in their hands.

When an organization uses the platform as a service model, more of the responsibility is passed over to Google Cloud.

This includes full responsibility for the physical infrastructure, the access and authentication, network security, and guest operating systems.

The customer is still responsible for the security of any content, such as code or data, produced on the platform.

Lastly, with the software as a service model, Google Cloud is responsible for almost every aspect of security—from the underlying infrastructure to the actual application.

Customers still have some security responsibilities, such as application usage, access policies like authentication settings to prevent phishing attacks, and the user content.

One important aspect of the shared responsibility model is that customers are always responsible for the security of

their data, whether they have on-premises data centers or only pay a monthly subscription for a single user license.

The customer controls who or what has access to their data.

Google Cloud is committed to keeping customers’ data secure, but security is a shared responsibility, and requires collaboration.

Summary

This brings us to the end of the third section of the Digital Transformation with Google Cloud course.

Let’s do a quick review of cloud computing models and shared responsibility.

You learned about the three main cloud computing service models: IaaS, or infrastructure as a service, PaaS, or platform as a service, and SaaS, or software as a service.

Each model brings a different level of service and set of products to suit an organization's needs.

You also learned about how the main benefits and trade-offs of each cloud computing model can help organizations choose the one that best fits their business.

Organizations will be better equipped to make the right resourcing and budget decisions knowing exactly what is involved in each computing model.

Finally, you learned about the shared responsibility between the cloud provider and its customers.

While the cloud provider will keep the cloud infrastructure safe and secure, it's the responsibility of the customer to keep its data secure.

Quiz
Passing score: 75%

1.

Which option best describes a benefit of Infrastructure as a Service (IaaS)?

It has low management overhead, as all administration and management tasks for data, servers, storage, and updates are handled by the cloud vendor.

It reduces development time, as developers can go straight to coding instead of spending time setting up and maintaining a development environment.

It's cost-effective, as all infrastructure costs are handled under a single monthly or annual subscription fee.

It’s efficient, as IaaS resources are available when needed and resources aren’t wasted by overbuilding capacity.

2.

Which cloud computing service model offers a develop-and-deploy environment to build cloud applications?

Function as a Service (FaaS)

Platform as a Service (PaaS)

Software as a Service (SaaS)

Infrastructure as a Service (IaaS)

3.

An organization wants to move their collaboration software to the cloud, but due to limited IT staff one of their main drivers is having low maintenance needs. Which cloud computing model would best suit their requirements?

Infrastructure as a Service (IaaS)

Platform as a Service (PaaS)

IT as a service (ITaaS)

Software as a Service (SaaS)

4.

In the cloud computing shared responsibility model, what types of content are customers always responsible for, regardless of the computing model chosen?

The customer is responsible for securing anything that they create within the cloud, such as the configurations, access policies, and user data.

The customer is not responsible for any of the data in the cloud, as data management is the responsibility of the cloud provider who is hosting the data.

The customer is responsible for security of the operating system, software stack required to run their applications and any hardware, networks, and physical security.

The customer is responsible for all infrastructure decisions, server configurations and database monitoring.


Chapter 1's (Digital Transformation with Google Cloud) Summary: 

This concludes the “Digital Transformation with Google Cloud” course where you learned about the foundations of cloud technology and saw how this technology is changing business in a digital era.

In the first section of the course, Why Cloud Technology is Transforming Business, you learned:

key terms related to the cloud and digital transformation, the benefits of cloud technology with regard

to an organization’s digital transformation, the differences between on-premises infrastructure, public cloud, private cloud, hybrid

cloud, and multicloud, and the drivers and challenges that lead organizations to undergo a digital transformation.

In the second section of the course, Fundamental cloud concepts, you learned: the benefits of moving to cloud infrastructure through customer business use

cases, the difference between a solution and a product in Google Cloud, how moving to the cloud shifts an organization's spending from capital expenditure

to operational expenditure, and how that affects their total cost of ownership, when private, hybrid, or multicloud infrastructures best apply to different business use

cases, basic network infrastructure terminology, and how Google Cloud supports digital transformation with global infrastructure and data centers connected by a fast, reliable network.

In the third section of the course, Cloud Computing Models and Shared Responsibility, you learned: the definitions, benefits, and tradeoffs of IaaS, PaaS, and SaaS, which computing

model applies to various business scenarios and use cases, what the cloud shared responsibility model is, and which responsibilities are the cloud provider’s or the customer’s for on-premises

and cloud computing models Now you’ve had a comprehensive introduction to digital transformation, move on to the next course in the series, Exploring Data Transformation with Google

Cloud, where you’ll learn about: the value of data, the cloud data transformation journey, Google Cloud data management solutions, and Google Cloud smart analytics and business intelligence solutions.


Google Cloud Fundamental

Chapter 2: Exploring Data Transformation with Google Cloud

Cloud technology can bring great value to an organization, and combining the power of cloud technology with data has the potential to unlock even more value and create new customer experiences.

“Exploring Data Transformation with Google Cloud” explores the value data can bring to an organization and ways Google Cloud can make data useful and accessible.

Part of the Cloud Digital Leader learning path, this course aims to help individuals grow in their role and build the future of their business.

When you complete this course, you can earn the badge displayed here! View all the badges you have earned by visiting your profile page. Boost your cloud career by showing the world the skills you have developed!

Introduction

Business data is not a new term because organizations have long applied information about performance and operations to make decisions.

With traditional methods though, data analysis can take days or months and is often incomplete.

In addition, specialized teams are often required to produce complex reports.

With cloud technology, this doesn't need to be the case.

Data can now be consumed, analyzed and used at speed and scale never before possible.

In fact, organizations can now benefit from Cloud technology to ingest data in real time, to train machine learning models, and to act in ways that benefit their business.

You no longer need to be a data scientist or technical expert to perform data analysis.

With that in mind, this course, exploring data transformation with Google Cloud is designed to help you understand the value of data and how it affects customer experiences.

Learn about the different Google Cloud data management solutions that are available and explore the ways that Google Cloud products have made data more useful and accessible to a workforce.

Throughout the course, you'll be presented with graded knowledge assessments.

You must pass these assessments to receive course credit.

Let's get started.


Section 1: The Value of Data

Introduction

The word data is used a lot in today's business world.

There's a good reason for that, because capturing, managing, and using data is central to redefining customer experiences and creating new value in almost every industry.

In this section of the course, you will explore how data generates business insights and drives decision making.

Basic data management concepts like databases, data warehouses, and data lakes.

How organizations can create value by using their current data, collecting new data, and sourcing data externally.

How the Cloud unlocks business value from all types of data, including structured data and previously untapped unstructured data.

The data value chain from the initial creation of data through data activation and the importance that data governance plays in a successful data journey.


Point 1: How data creates value

Data is an essential ingredient for driving innovation and differentiation and is the key to unlocking value from artificial intelligence.

Data powers AI-driven business insights, helps companies make better real-time decisions, and is the basis for how companies build and run their applications.

We're generating more data every day, and the complexity and speed of data arrival are changing the business environment.

However, the most valuable insights no longer come just from sales, inventory, and personnel data.

They are often hidden across unstructured data points from a myriad of sources and systems.

Extracting those insights requires the right blend of tools, skills, and strategy.

Some data is easy to capture, like financial data, because it can be found in databases and spreadsheets.

But other data might not be as easy.

For example, how your customers engage with you across social media platforms.

And after you capture data, how do you store it so that you can gain insights from it?

With machine learning, or ML, and artificial intelligence, or AI, organizations can generate insights from data both past and present.

And also perceive, predict, recommend, and categorize data in new ways.

For example, ML lets online retailers who use smart analytics tools to ingest real-time customer behavior data while they service the best suggestions for particular users.

So with every click that the user makes, their website experience becomes increasingly personalized.

However, some organizations struggle to remove the barriers that sit between them and their data.

According to a report by Accenture titled Closing the Data Value Gap, 68% of organizations say that they are still unable to realize tangible and measurable value from data.

Organizations that want to adapt must determine how to close the gap and support value generation.

An intelligent data Cloud is the key to unlocking more business value.

Point 2: Unlocking business value from data

Unlocking the value of data is central to digital transformation.

To generate insights, you might need to combine different types of data.

However, not all data is created and organized the same way.

Data can be categorized into three main types, structured, semi-structured, and unstructured.

Structured data is highly organized and well-defined.

It's typically stored in a table with relationships between the different rows and columns, like in a spreadsheet or database.

Because structured data is organized this way, it's easy to analyze.

For example, it's common for organizations to use structured data and customer relationship management tools, or CRMs, as they follow customer behavior patterns and trends.

Semi-structured data falls somewhere in between structured and unstructured data.

It's organized into a hierarchy, but without full differentiation or any particular ordering.

Examples include emails, HTML, JSON, and XML files.

Although this data type doesn't have a formal structure, it contains tags or other markers that make it easier to analyze than unstructured data.

Unstructured data is information that either doesn't have a predefined data model or isn't organized in a predefined manner.

Categories include text, which is the most common and is often generated and collected from sources like documents, presentations, or even social media posts.

Data files like images, audio files, and videos and infrastructure activity and performance data like

log files from servers, networks, and applications or output data from Internet of things IoT sensors.

Organizations can use unstructured data in many ways.

For example, a marketing team might analyze social media posts to identify sentiment toward a brand.

Or customer service teams might train automated chatbots to augment support staff by analyzing language in customer communications, and providing interactive responses.

But in general, unstructured data has historically been difficult to analyze.

According to the Harvard Business Review, on average, less than 1% of an organization's unstructured data is analyzed or used at all.

Until recently, tools to tap the potential of unstructured data were either unavailable or prohibitively expensive and complex.

What makes this statistic even more concerning is that, according to Gartner research, unstructured data represents 80% to 90% of all new enterprise data.

This reveals a staggering gap between the data being generated and the value that it's providing.

But Cloud technology has changed that.

With the right Cloud tools, businesses can extract value from unstructured data by using machine learning

to discover trends or even use application programming interfaces or APIs to extract structure from the data.

An example of an API is Google Cloud's Vision API, which uses machine learning to

detect products within a picture and can then even label the picture to describe its contents.

Understanding the different types of data available can help organizations define what's possible with the data solutions they have.

One of the transform powers of the Cloud is how it can unlock value from structured and the previously untapped unstructured data.

Point 3: Data management concepts

Organizations need a modern approach to enterprise data to manage the vast volumes that are produced.

The list of options often includes databases, data warehouses, and data lakes.

Let's explore each of these options, starting with databases.

A database is an organized collection of data stored in tables and accessed electronically from a computer system.

Let's examine two types of databases, relational and nonrelational.

A relational database stores and provides access to data points that are related to one another.

This means storing information in tables, rows, and columns that have a clearly defined schema that represents the structure or logical configuration of the database.

A relational database can establish links or relationships between information by joining tables, and structured query language or SQL can be used to query and manipulate data.

Relational databases are highly consistent, reliable, and best suited for dealing with large amounts of structured data.

They're designed for business data processing and storing the online transactional data needed to support the daily operations of a company.

A nonrelational database, sometimes known as a NoSQL database, is less structured in format and doesn't use a tabular format of rows and columns like relational databases.

Instead, nonrelational databases follow a flexible data model, which makes them ideal for storing data that changes its organization frequently, or for applications that handle diverse types of data.

This includes when large quantities of complex and diverse data need to be organized, or when the data regularly evolves to meet new business requirements.

Choosing the right database depends on the use case.

Google Cloud relational database products include Cloud SQL and Cloud Spanner, while Bigtable is a non relational database product.

We'll look at these products in more detail later.

Let's explore another data management concept the data warehouse.

Like a database, a data warehouse is a place to store data.

However, while a database is designed to capture data for storage, retrieval, and use, a data warehouse is designed to analyze data.

A data warehouse is an enterprise system used for the analysis and reporting of structured and semi-structured data from multiple sources.

Think of the data warehouse as the central hub for all business data.

Business data might include, point of sale transactions, marketing automation, or even customer relationship management data.

Suited for both ad hoc analysis and custom reporting, a data warehouse can help analyze

sales and identify trends because it can store both current and historical data in one place.

This capability can provide a long range view of data over time, which makes a data warehouse a primary component of business intelligence.

BigQuery is Google Cloud's data warehouse offering.

We'll explore BigQuery in more detail later.

Although data warehouses handle structured and semi structured data, they're not typically the answer for how to handle large amounts of available unstructured data like images, videos, and documents.

Unstructured data which doesn't conform to a well-defined schema is often disregarded in traditional analytics.

A data lake is a repository designed to ingest, store, explore, process, and analyze any type or volume of

raw data, regardless of the source, like operational systems, web sources, social media or Internet of things or IoT.

It can store different types of data in its original format, ignoring size limits and without much preprocessing or adding structure.

Having this unprocessed raw data available for analysis prevents unintentionally contaminating the data or adding bias.

It also means that raw data can be enriched by merging it with other data at the same time.

This differs from a data warehouse that contains structured data that has been cleaned and processed ready for the strategic analysis based on predefined business needs.

Data lakes often consist of many different products depending on the nature of the data that is ingested.

For example, the best Google Cloud products for storing structured data are Cloud SQL, Cloud Spanner, or BigQuery.

For semi-structured data, the options include Datastore and Bigtable.

And for storing unstructured data, Cloud storage is an option.

Data warehouses and data lakes should be considered complementary instead of competing tools.

Although both store data in some capacity, each is optimized for different uses.

Traditional data warehouse users are business intelligence analysts who are closer to the business and focus on driving insights from data.

These users traditionally use the data to answer questions.

Data lake users and also analysts include data engineers and data scientists.

They're closer to the raw data with the tools and capabilities to explore, mine, and experiment with the data.

These users find answers in the data, but they also find questions.

As enterprises are increasingly focus on data driven decision making, data warehouses and data lakes play a critical role in an organization's data transformation journey.

Democratization of data lets users gain a deeper understanding of business situations because they have more context than ever before.

Today, organizations need a 360 degree real time view of their businesses to gain a competitive edge.

Point 4: The role of data in digital transformation

Organizations have access to data like never before.

This includes both internal information called first-party data and external information, which is usually data about customers and industry, often called second or third-party data.

As organizations have digitized their operations, many types of business data have become available, including information about their customers.

First-party data is the proprietary customer datasets that a business collects from customer or audience transactions and interactions.

These datasets might include information about digital interactions, like the length of time a user spends at a web page.

Second-party data often describes first-party data from another organization, such as a partner or other

business in their supply chain that can be easily deployed to augment a company's internal datasets.

The organization does not directly own this data, but it is relevant to their business.

Finally, there is third-party data, which are datasets collected and managed by organizations that do not directly interact with an organization's customers or business.

These datasets might come from government, nonprofit, or academic sources like weather or public demographic data, or from industry specific sources like analysts reports, or industry benchmarking.

Third-party data is often shared or purchased on data marketplaces or exchanges such as the Google Cloud Marketplace.

Using external data can greatly increase the value of data by providing new context and insights.

Let's explore an example of how an airline transform their business through data.

Budget airlines don't provide food as part of their service, instead they charge customers for meals if they want them.

The solution might seem cost effective, but it can be difficult to estimate the number of meals required on board.

If the airline overestimates the number of meals needed, they risk wasting food and losing revenue.

But if they underestimate, they risk selling out of food, providing poor customer service, and losing potential revenue.

One budget airline in Asia reimagined how they could solve this problem by using data.

They began by identifying factors to help estimate stock, such as the size of the plane and the number of passengers.

But they soon discovered that estimates based on these factors were inaccurate.

This meant having to think about their data differently by analyzing information such as destination, time of flight, and flight connections before and after each journey.

Using this information, they uncovered actionable insights.

For example, they learned that flights to and from India required 73% more vegetarian meals.

With these new insights, the airline was able to predict the number of meals required more accurately,

which in turn provided a more positive customer experience and improve the profitability of their food service.

This is just one example of how Cloud technology can unlock new value by reimagining data.

No matter where you are in your company, you too can use data to solve challenges.

Point 5: The Data value chain

When you think about data processing, it's important to place it within the broader context of the data value chain.

Imagine data traveling along an assembly line, like a car in a factory.

The assembly line progressively adds parts and value to an object that moves along it.

Raw data at the beginning of the line is eventually transformed into actions that humans or machines take.

Let's examine the steps in this data value chain.

Data genesis is the initial creation of a unit of data.

This could be a click on a website, the swipe of a card, a sensor recording from an IoT device, or countless other examples.

It's the raw material that will eventually be turned into an insight ready for action.

Data collection brings that initial unit of data to the assembly line through ingestion.

The basic function of ingestion is to extract data from the system in which it's hosted and bring it to a new system.

It could have dramatically different requirements based on the volume, velocity, and variety of the raw

data that's required for a given analysis and how fast the data needs to be analyzed.

Data processing is where the collected raw data is transformed into a form that's ready to derive insights from.

The data will likely need to be adjusted for example, by merging different datasets together.

It can be a single-stage operation or can be a complex tree of cascading procedures.

In our manufacturing process analogy, this phase is where raw materials take the shape of the pre-assembly parts of a manufactured product.

Data storage is where the data lands can be found and is ready for analysis and action.

As with real-world manufacturing, where storage options vary depending on the type of product that is processed, different types of data can be stored in different ways.

For example, NoSQL is available for fast reads and writes, data warehousing for fast access to analysis, and object storage for unstructured data.

There are also customized options of these standard stores.

Data analysis provides direction for business-oriented action.

To continue with our manufacturing line analogy in this stage, inputs from the data processing stage are assembled into a final product.

Finally, the last step in the data value chain is data activation.

When an analysis is produced, it needs to be pushed to the relevant business procedures and decision-makers so that action can be taken and the value chain completed.

The most common points of activation are applications that make automated decisions, and business intelligence dashboards that guide humans toward better, more informed decisions.

In our manufacturing line example, this is the step where a fully produced product is put to its intended use.

There's no one way to assemble a data value chain, as there's no one way to create a real-world manufacturing line.

Similarly, as technologies progress, new inputs become available, your workforce evolves, or the desired output changes.

The optimal value chain will also change.

However, at its core, the value chain principles hold.

We want to use raw data to perform actions that benefit the business.

Point 6: Data governance

In the last decade, the amount of data produced has increased exponentially, and the Cloud has made it easier to collect, store, and analyze it at a lower cost.

Organizations are now challenged to democratize and embed data in every decision, while they also ensure that it's secure and protected from unauthorized use.

An effective data governance program can help implement data directives to achieve this.

But what exactly is data governance?

Data governance means setting internal standards, data policies that apply to how data is gathered, stored, processed, and disposed of.

It governs who can access certain data, what data is under governance.

It also involves complying with external standards set by industry associations, government agencies, and other stakeholders.

Data governance focuses on making the data available to all stakeholders across the full life cycle of the data in a form that they can readily access and

use, in a manner that generates the desired business outcomes through insights and analysis, and if relevant, in a way that conforms to regulatory standards and compliance needs.

Data governance brings several benefits.

It makes data more valuable.

Data Governance implements processes to ensure high quality data and provides a platform that makes it easier to share data securely with stakeholders across the organization.

It helps users make better, more timely decisions, through data governance.

Users throughout an organization get the data they need to reach and service customers, design and improve products and services and seize opportunities for new revenues.

By democratizing data, organizations can embed data in all decision making.

It improves cost controls.

Data helps organizations manage resources and operate more efficiently.

Because they can eliminate data duplication caused by information silos, they don't overbuy and have to maintain expensive hardware.

It enhances regulatory compliance.

An increasingly complex regulatory climate has made it more important for organizations to establish rigorous data governance practices.

They avoid risks associated with noncompliance and proactively anticipate new regulations.

It helps earn greater trust from customers and suppliers.

By being in auditable compliance with both internal and external data policies, organizations gain the trust of customers and partners.

It helps manage risk.

With robust data governance, organizations can reduce concerns about exposure of sensitive data to individuals or systems who lack proper authorization.

Security breaches from malicious, outsiders, or even insiders who access data they don't have the right to see.

It allows more personnel access to more data.

Strong data governance provides confidence that the right personnel get access to the right data and that this democratization of data does not negatively impact the organization.

It's possible that organizations without an effective data governance program will suffer from compliance violations.

This can lead to finds poor data quality which generates lower quality insights that impact business decisions.

Challenges in finding data which results in delayed analysis and missed business opportunities, and poorly trained data models for AI, which reduces the model accuracy and benefits of using AI.

Every organization needs data governance.

As businesses throughout all industries progress on their digital transformation journeys, data has quickly become the most valuable asset they possess.

Quiz
Passing score: 80%

1.

A solar energy company wants to analyze weather data to better understand the seasonal impact on their business. On which platform could they find free-to-use weather datasets?

Google Cloud console

Google Play

App Engine

Google Cloud Marketplace

2.

New cloud tools make it possible to harness the potential of unstructured data. Which of these use cases best demonstrates this?

Analyzing historical sales figures to predict future trends

Analyzing social media posts to identify sentiment toward a brand

Creating visualizations from seasonal weather data

Using GPS coordinates to power a ride-sharing app

3.

Which is a repository designed to ingest, store, explore, process, and analyze any type or volume of raw data, regardless of the source?

Data archive

Data warehouse

Database

Data lake

4.

What is Google Cloud’s modern and serverless data warehousing solution?

Vertex AI

BigQuery

Compute Engine

Cloud Storage

5.

An online retailer uses a smart analytics tool to ingest real-time customer behavior data to surface the best suggestions for particular users. How can machine learning guide this activity?

Through machine learning, a user’s credit card transactions can be analyzed to determine regular purchases.

Through machine learning, with every click that the user makes, their website experience becomes increasingly personalized.

Machine learning can be used to make all users see the same product recommendations, regardless of their preferences or behavior.

Machine learning can help identify user behavior in real time, but cannot make personalized suggestions based on the data.

6.

Which data type is highly organized and well-defined?

A hybrid of structured, semi-structured, and unstructured data

Unstructured data

Structured data

Semi-structured data

7.

What is data governance?

The process of deleting unnecessary data to save storage space

The process of setting internal data policies and ensuring compliance with external standards

The process of analyzing data to gain insights and make informed decisions

The process of collecting and storing data for future use

8.

Which step in the data value chain is where collected raw data is transformed into a form that’s ready to derive insights from?

Data analysis

Data storage

Data genesis

Data processing

9.

A car insurance company has a large database that stores customer details, including the vehicles they own and past claims. The structure of the database means that information is stored in tables, rows, and columns. What type of database is this?

An object database

A non-relational database

An XML database

A relational database

1

0.

Which represents the proprietary customer datasets that a business collects from customer or audience transactions and interactions?

Second-party data

First-party data

Third-party data


Section 2: Google cloud data Management Solutions

In this section of the course, you'll explore Google Cloud data management products and solutions, and learn how they apply to different business use cases.

Introduction

Data plays such an integral role in an organization's operations.

For this reason, it's crucial to have an effective way of storing and managing it.

Google Cloud offers a wide range of data management products and solutions each applicable to different business use cases.

In this section of the course, you'll explore Google Cloud data management options and the differences between them.

The different storage classes available with Cloud storage, how to choose the right storage product to meet the needs of your organization.

Ways an organization can migrate and or modernize their current database in the Cloud.

Point 1: Unstructured data storage

Every application needs to store data like media to be streamed, or even sensor data from devices, and different applications and workloads require different storage solutions.

Google Cloud offers several core storage products.

This list includes: Cloud Storage, Cloud SQL, Cloud Spanner, Big Query, Fire Store and Cloud Bigtable.

Depending on your use case, you might use one or several of these services to do the job.

Let's begin with Cloud Storage, which is a service that offers developers and IT organizations durable and highly available object storage.

But what is object storage?

Object storage is a computer data storage architecture that manages data as objects instead of as file

storage, which is a file and folder hierarchy, or as block storage, which is chunks of a disc.

These objects are stored in a packaged format that contains the binary form of the actual data

and relevant associated metadata such as creation date, author, resource type and permissions and a globally unique identifier.

These unique keys are in the form of URLs, which means object storage interacts well with web technologies.

Data commonly stored as objects include video, pictures, and audio recordings.

This type of data is referred to as unstructured, which means that it doesn't have a predefined

data model or isn't organized in a predefined manner as you might find in a structured database format.

Cloud storage lets customers store any amount of data and retrieve it as often as needed.

It's a fully managed, scalable service that has a wide variety of uses, such as serving website content,

storing data for archival and disaster recovery and distributing large data objects to end users through direct download.

There are four primary storage classes in Cloud storage.

The first is standard storage.

Standard storage is considered best for frequently accessed or hot data.

It's also great for data that's stored for only brief periods of time.

The second storage class is nearline storage.

This option is best for storing infrequently accessed data, like reading or modifying data on average once a month or less.

Examples might include data backups, long tail multimedia content, or data archiving.

The third storage class is coldline storage.

This is also a low cost option for storing infrequently accessed data.

However, as compared to nearline storage, coldline storage is meant for reading or modifying data at most, once every 90 days.

The fourth storage class is archive storage.

This is the lowest cost option used, ideally, for data archiving, online backup and disaster recovery.

It's the best choice for data that you plan to access less than once a year

because it has higher costs for data access and operations and a 365 day minimum storage duration.

Although each of these four classes have differences, it's worth noting there are several characteristics that apply across all of these storage classes which include: unlimited storage

with no minimum object size requirement, worldwide accessibility and locations, low latency and high durability, a uniform experience which extends to security tools and API's and Geo redundancy.

If data is stored in a multi region or dual region, this means placing physical servers in geographically

diverse data centers to protect against catastrophic events and natural disasters, and load balancing traffic for optimal performance.

Cloud storage also provides a feature called auto class which automatically transitions objects to appropriate storage classes based on each object's access pattern.

The feature moves data that's not access to colder storage classes to reduce storage cost and moves data that is access to standard storage to optimize future accesses.

Auto class simplifies and automates cost saving for your Cloud storage data.

Point 2: Structured data storage

In the previous lesson, you saw how Cloud storage is used to store unstructured data.

Now let's explore some Google Cloud data storage products that are suited for storing structured data.

Structured data consists of numbers and values that are organized in a predefined format that's easily searchable in a relational database.

Earlier in the course, we mentioned that a relational database stores information in tables, rows and

columns, that have a clearly defined schema that represents the structure or logical configuration of the database.

Cloud SQL offers fully managed relational databases, including MySQL, PostgreSQL and SQL server, as a service.

It's designed to transfer mundane, but necessary and often time-consuming tasks to Google, like applying

patches and updates, managing backups, and configuring replications so you can focus on building great applications.

Trusted by thousands of the largest enterprises around the world, organizations that use Cloud SQL obtain various benefits.

It doesn't require any software installation or maintenance, it supports managed backups, so backed up data is securely stored and accessible if a restore is required, it encrypts customer

data when on Google's internal networks and when stored in database tables, temporary files and backups, and it includes a network firewall which controls network access to each database instance.

Cloud Spanner is a fully managed, mission, critical relational database service that scales horizontally to handle unexpected business spikes.

Battle tested by Google's own mission, critical applications and services, Spanner is a service that powers Google's multi-billion dollar business.

Cloud Spanner is especially suited for applications that require a SQL relational database management system with joins and secondary indexes built in high availability,

which provides data redundancy to reduce downtime when a zone or instance becomes unavailable, the goal is to prevent a single point of failure.

Strong global consistency, which ensures that all locations where data is stored are updated to the most recent data version quickly,

and high numbers of input and output operations per second, tens of thousands of reads and writes per second or more.

Both Cloud SQL and Cloud Spanner are fully managed database services, but how do they differ?

Cloud SQL is a fully managed relational database service for MySQL, PostgreSQL, and SQL server with greater than 9

9.95% availability.

Database migration service, DMS, makes it easy to migrate your production databases to Cloud SQL with minimal downtime.

Then there's Cloud Spanner, which is a fully managed relational database with unlimited scale, strong consistency, and up to 9

9.999% availability with zero downtime for planned maintenance and schema changes.

This globally distributed acid compliant Cloud database automatically handles replicas, sharding, and transaction processing so you can quickly scale to meet any usage pattern and ensure success of products.

When considering which option is best for your business, consider this.

If you've outgrown any relational database, are sharding your databases for throughput, high performance, need transactional

consistency, global data and strong consistency, or just want to consolidate your database, consider using Cloud Spanner.

If you don't need horizontal scaling or a globally available system, Cloud SQL is a cost-effective solution.

The final structured data storage solution that we'll explore is BigQuery.

BigQuery is a fully managed data warehouse.

As we've already learned, the data warehouse is a large store that contains petabytes of data

gathered from a wide range of sources within an organization and is used to guide management decisions.

Because it's fully managed, BigQuery takes care of the underlying infrastructure, so users can focus on

using SQL queries to answer business questions without having to worry about deployment, scalability, and security.

BigQuery provides two services in one; storage and analytics.

It's a place to store petabytes of data.

For reference, one petabyte is equivalent to 11,000 movies at 4K quality.

BigQuery is also a place to analyze data with built-in features like machine learning, geospatial analysis, and business intelligence.

Data in BigQuery is encrypted at rest by default, without any action required from a user.

Encryption at rest is encryption used to protect data that's stored on a disc, including solid state drives or backup media?

BigQuery provides seamless integration with the existing partner ecosystem.

Businesses can tap into our ecosystem of system integrators and data integration partners to help enhance analytics and reporting.

These integrations mean that BigQuery lets organizations make the most of existing investments in business intelligence, data ingestion, and data integration tools.

Industry research shows that 90% of organizations have a multi Cloud strategy, which adds complexity to data integration, orchestration, and governance.

BigQuery works in a multi Cloud environment, which lets data teams eradicate data silos by using BigQuery to securely and cost effectively analyze data across multiple Cloud providers.

BigQuery also has built in machine learning features so that ML models can be written directly in BigQuery by using SQL.

If other professional tools such as Vertex AI from Google Cloud are used to train ML models, datasets can

be exported from BigQuery directly into Vertex AI for a seamless integration across the data to AI life cycle.

Point 3: Semi-structured data storage

Semi structured data contains elements of both structured and unstructured data.

It does have some defining or consistent characteristics, but generally doesn't follow a structure as rigid as a relational database.

Semi structured data is easier to organize because it usually contains some organizational properties such as tags or meta data.

An example of unstructured data is an email message.

While the actual content of the email is unstructured, it does contain structured data such as

the name and email address of the sender and recipient, the time sent, and so on.

Google Cloud offers two semi structured data storage products, Firestore and Cloud Bigtable.

Firestore is a flexible, horizontally scalable NoSQL Cloud database for storing and syncing data in real time.

Firestore can be directly accessed by mobile and web applications.

Firestore performs data storage in the form of documents, with the documents being stored in collections.

Documents support a wide variety of data types, such as nested objects, numbers, and strings.

One of Firestore's main features is automatic scaling.

It's been designed to scale automatically depending on user demand, but retains the same level of performance irrespective of database size.

Firestore also provides offline usage through a comprehensive database on users devices.

Offline data access ensures that applications run without interruption, even if the user gets disconnected from the Internet.

Then there's Cloud Bigtable, Google's NoSQL, big data database service.

It's the same database that powers many core Google services including Search Analytics, Maps, and Gmail.

Bigtable is designed to handle large workloads at consistent low latency, which means Bigtable responds to

requests quickly and high throughput, which means it can send and receive large amounts of data.

For this reason, it's a great choice for both operational and analytical applications, including Internet of Things, user analytics, and financial data analysis.

When deciding on a storage option, you might choose Bigtable.

If you're working with more than one terabyte of semi structured or structured data, data

is fast with high throughput or it's rapidly changing, you're working with NoSQL data, data is

a time series or has natural ordering, you're working with big data and running batch

or real time processing on the data, or you're running machine learning algorithms on the data.

Point 4: Choosing the right storage product

You've learned about the different storage options that Google Cloud offers, but in what scenarios should you use each one?

Ultimately, it's a combination of the data type that needs to be stored and the business need.

If data is unstructured then Cloud storage is the most appropriate option.

You have to decide a storage class, standard near line, code line or archive, or whether to let the auto class feature decide that for you.

If data is structured or semi structured choosing a storage product will depend on whether work loads are transactional or analytical.

Transactional workloads stem from online transaction processing or OLTP systems, which are used when fast data inserts and updates are required to build row based records.

An example of this is point of sale transaction records.

Then there are analytical workloads which stem from online analytical processing or OLAP systems, which are used when entire datasets need to be read.

They often require complex queries, for example, aggregations.

An example here would be analyzing sales history to see trends and aggregated views.

After you determine if the workloads are transactional or analytical, you must determine whether the data will be accessed by using SQL.

If your data is transactional and you need to access it by using SQL, then Cloud SQL and Cloud Spanner are two options.

Cloud SQL works best for local to regional scalability and Cloud Spanner is best to scale a database globally.

If the transactional data will be accessed without SQL Firestore might be the best option.

Firestore is a transactional no SQL document oriented database.

If you have analytical workloads that require SQL commands, BigQuery might be the best option.

BigQuery Google's data warehouse solution lets you analyze petabyte scale datasets.

Alternatively, Cloud Bigtable provides a scalable no SQL solution for analytical workloads.

It's best for real time high through put applications that require only millisecond latency.

Point 5: Data migration and modernization

Running modern applications on legacy on-premises databases requires overcoming expensive, time-consuming challenges around latency, throughput, availability, and scaling.

With database modernization, organizations can move data from traditional databases to fully managed, or modern databases with relative ease.

There are different ways that an organization can migrate or modernize their current database in the Cloud.

The most straightforward method is a lift and shift platform migration.

This is where databases are migrated from on-premises and private Cloud environments to the same type of database hosted by a public cloud provider such as Google Cloud.

Although this solution makes the database more difficult to modernize, it does bring with it the benefits of minimal upheaval, and having data and infrastructure managed by the Cloud provider.

Alternatively, a managed database migration allows the migration of databases from SQL server, MySQL, post grace equal, and others to a fully managed Google Cloud database.

Although this migration requires careful planning and might call slight upheaval, a fully managed solution lets you focus on higher-priority work that really adds value to your organization.

Google Cloud's database migration service, DMS can easily migrate your databases to Google Cloud, or data stream can be used to synchronize data across databases, storage systems, and applications.

Let's look at a real-life use case, with 18 fulfillment centers, 38 delivery centers, and a catalog of more than 22 million items.

The online retailer Wayfair needed a way to quickly move from their on-premises data centers, which ran on SQL server, to Google Cloud.

This had to be achieved without inconveniencing their team of over 3,000 engineers, their tens of millions of customers, or their 16,000 supplier partners.

The goal was to lift and shift their workloads as quickly as possible with minimal changes, and then use Cloud databases to modernize those workloads.

Wayfair chose Google Cloud, because of the clear path for shifting workloads to the Cloud.

By using Cloud SQL server, Google Cloud provided the flexibility to be deliberate about which engine and product to run Wayfair systems on going forward.

They liked how they could run SQL server on virtual machines, VMs for example, but could also benefit from database offerings like Cloud SQL and Cloud Spanner.

Now that migration is complete, they also use Google Cuponet's Engine GKE and Compute Engine VMs to host the services built by the Google Cloud Team.

They also use Pub-Sub and data flow for sending operational data, to their analytical store in big query.

Quiz
Passing score: 80%

1.

Which Google Cloud product can be used to synchronize data across databases, storage systems, and applications?

Dataprep

Pub/Sub

Dataproc

Datastream

2.

A data analyst for an online retailer must produce a sales report at the end of each quarter. Which Cloud Storage class should the retailer use for data accessed every 90 days?

Archive

Nearline

Standard

Coldline

3.

Which is the best SQL-based storage option for a transactional workload that requires local or regional scalability?

Cloud Storage

Cloud SQL

Cloud Bigtable

Cloud Spanner

4.

Which strategy describes when databases are migrated from on-premises and private cloud environments to the same type of database hosted by a public cloud provider?

Refactoring

Managed database migration

Remain on-premises

Lift and shift

5.

Which would be the best SQL-based storage option for a transactional workload that requires global scalability?

Cloud SQL

Cloud Spanner

Firestore

Cloud Bigtable

6.

What are the two services that BigQuery provides?

Compute and analytics

Migration and analytics

Networking and storage

Storage and analytics

7.

What is Google's big data database service that powers many core Google services, including Google Search, Google Analytics, Google Maps Platform, and Gmail?

Cloud Bigtable

Cloud SQL

Cloud Storage

Cloud Spanner

8.

Data in the form of video, pictures, and audio recordings is well suited to object storage. Which product is best for storing this kind of data?

Cloud SQL

Firestore

BigQuery

Cloud Storage

9.

Which characteristic is true for all Cloud Storage classes?

Geo-redundancy if data is stored in a multi-region or dual-region

Accessibility only within one region

Maximum storage limits

High latency and low durability

10.

BigQuery works in a multicloud environment. How do organizations benefit from this feature?

Security is more effective when BigQuery is run in on-premises environments.

Multicloud support in BigQuery is only intended for use in disaster recovery scenarios.

BigQuery lets organizations save costs by limiting the number of cloud providers they use.

Data teams can eradicate data silos by analyzing data across multiple cloud providers.


Section 3: Making Data Useful and accessible

In this section of the course, you'll examine how smart analytics, business intelligence tools, and streaming analytics can add value in different business use cases.

Introduction 

It's not always easy for organizations to make smart business decisions based on the data they've collected or produced.

And too often there can be blockers in place that make analyzing it difficult for part or all of a workforce.

With Google Cloud, that doesn't need to be the case.

In the final section of this course, you'll explore how Looker makes it easy for a workforce to access the data they need, when they need it.

How streaming analytics in real time can make data more useful, and two Google Cloud products that modernize data pipelines: Pub/Sub and Dataflow.

Point 1: Business Intelligence and insights using Looker

When data is in a database, a fair amount of effort and expertise might still be required to uncover insights.

This goal can be achieved by using a business intelligence solution.

However, the challenge that organizations often face is identifying the right business intelligence solution.

Some solutions are too complex and not accessible by those outside the data engineering or data analysis teams.

The this means other teams have to put in requests and wait for answers which defeats the purpose of gaining real-time insights.

Other solutions let everyone in the business perform their own data analysis, but they can only perform their analysis with a selection of the available data.

This means that only a few people or possibly no one, has a full view of the organization's business data.

Looker is a Google Cloud business intelligence platform designed to help individuals and teams analyze, visualize and share data.

This includes creating interactive dashboards and reports that are easy to understand and share.

By having a reliable authority for business data, anyone on a team can explore it, ask and answer their own questions, and create visualizations.

This approach empowers organizations to not just uncover insights, but also act on them.

Looker supports BigQuery, along with more than 60 different SQL databases.

Together, BigQuery and Looker provide rich interactive dashboards and reports without compromising performance, scale, security or data freshness.

Looker is also 100% web based, which makes it easy to integrate into existing workflows and share with multiple teams at an organization.

So how can Looker be used?

Let's explore an example.

Diamond Resorts, a global leader in hospitality offers destinations, events and experiences to help people recharge, connect and enjoy.

They had previously used a mixture of complex Excel workbooks and legacy BI tools to track important metrics.

Each business unit operated and ran their own silo data initiatives.

As a result, there were no common view of business or single authority for common metrics, redundant data engineering efforts.

Because work was never shared or used across the organization, and inconsistent project prioritization because decisions were driven primarily on intuition as opposed to actual data.

Also, infrastructure did not meet business requirements with executive reporting efforts that took months to complete.

Data that was duplicative across multiple business units without proper governance, multiple reporting tools and data warehouses throughout the business.

And infrastructure that didn't support advanced analytics aspirations.

Diamond Resorts wanted to create a single common Cloud-based architecture that was fully managed.

Establishing data governance and enabling the business to be more data-driven while they set the foundation for advanced analytics efforts.

They migrated to the Cloud and began using Looker to help improve business agility.

This decision let them gain access to real-time insights in less than three months.

It helped them to navigate COVID changes with important operational metrics such as daily booking and cancellations, while it also provided a 360 degree customer view.

And in addition to this manual, reporting for the yield management team was decreased by hours each day.

The Chief Information Officer said projects that we anticipated coming in future years were suddenly ready to be tackled within weeks.

This is just one example of how an effective business intelligence solution can let businesses transform to better serve their customers.

Point 2: Streaming analytics

Data traditionally is moved in batches.

Batch processing often processes large volumes of data at the same time with long periods of latency.

An example is payroll and billing systems that have to be processed on either a weekly or monthly basis.

Although this approach can be efficient to handle large volumes of data, it doesn't work with time

sensitive data that's meant to be streamed because that data can be staled by the time it's processed.

Streaming analytics is the processing and analyzing of data records continuously instead of in batches.

Generally, streaming analytics is useful for all types of data sources that send data in small sizes, often in kilobytes, in a continuous flow as the data is generated.

This results in the analysis and reporting of events as they happen.

Sources of streaming data include equipment sensors, clickstreams, social media feeds, stock market quotes, app activity, and more.

Companies use streaming analytics to analyze data in real time and provide insights into a wide range of activities such as metering, server activity, geolocation of devices or website clicks.

Use cases include e-commerce.

User clickstreams can be analyzed to optimize the shopping experience with real time pricing, promotions, and inventory management.

Financial services.

Account activity can be analyzed to detect abnormal behavior in the data stream and generate a security alert.

Investment services.

Market changes can be tracked in settings adjusted to customer portfolios based on configure constraints such as selling when a certain stock value is reached.

News media.

User click records can be streamed from various news source platforms, and the data can

be enriched with demographic information to better serve articles that are relevant to the targeted audience.

Utilities.

Throughput across a power grid can be monitored and alerts generated, or workflows initiated when established thresholds are reached.

Google Cloud offers two main streaming analytics products to ingest, process, and analyze event streams in real time, which makes data more useful and accessible from the instant it's generated.

Pub/Sub ingests hundreds of millions of events per second, but data flow unifies streaming in batch data analysis and builds cohesive data pipelines.

A data pipeline represents a series of actions or stages that ingest raw data from different sources and then move that data to a destination for storage and analysis.

You'll explore these products in more detail in the next section.

Point 3: Pub/Sub and dataflow

One of the early stages in a Data pipeline is Data Ingestion, which is where large amounts of streaming data are received.

Data, however, might not always come from a single structured database.

Instead, the data might stream from a thousand or even a million different events that are all happening asynchronously.

A common example of this data is from IoT, or Internet of Things applications.

These can include sensors on taxis that send out location data every 30 seconds or temperature sensors around a data center to help optimize heating and cooling.

Pub/Sub is a distributed messaging service that can receive messages from various device streams, such as gaming events, IoT devices, and application streams.

The name is short for Publisher/Subscriber or Publish messages to subscribers.

After messages have been captured from the streaming input sources, you need a way to pipe that data into a data warehouse for analysis.

This is where Dataflow comes in.

Dataflow creates a pipeline to process both streaming data and Batch Data.

Process, in this case, refers to the steps to extract, transform, and load data, sometimes referred to as ETL.

A popular solution for pipeline design is Apache Beam.

It's an open source, unified programming model to define and execute data processing pipelines, including ETL, batch, and stream processing.

Dataflow handles much of the complexity for infrastructure setup and maintenance and is built on Google's infrastructure.

This product allows for reliable auto scaling to meet data pipeline demands.

Dataflow is serverless and fully managed.

Serverless computing means that software developers can build and run applications without having to provision or manage the backend infrastructure.

For example, Google Cloud manages infrastructure tasks on behalf of the users, like resource provisioning, performance tuning, and ensuring pipeline reliability.

And a fully managed environment is one where software can be deployed, monitored, and managed without needing an operations team.

You can create this environment by using automation tools and technologies.

Using a serverless and fully managed solution like Dataflow means that you can spend more time analyzing the

insights from your datasets and less time provisioning resources to ensure your pipeline will successfully complete its next cycles.

Quiz
Passing score: 80%

1.

Which statement is true about Dataflow?

It allows easy data cleaning and transformation through visual tools and machine learning-based suggestions.

It handles infrastructure setup and maintenance for processing pipelines.

It’s a cloud-based data warehouse for storing and analyzing streaming and batch data.

It’s a messaging service for receiving messages from various device streams.

2.

What Google Cloud business intelligence platform is designed to help individuals and teams analyze, visualize, and share data?

Looker

Dataflow

Cloud Storage

Dataplex

3.

Streaming analytics is the processing and analyzing of data records continuously instead of in batches. Which option is a source of streaming data?

Temperature sensors

Medical test results

Payroll records

Customer email addresses

4.

What is Google Cloud’s distributed messaging service that can receive messages from various device streams such as gaming events, Internet of Things (IoT) devices, and application streams?

Looker

Dataplex

Dataproc

Pub/Sub

5.

What feature of Looker makes it easy to integrate into existing workflows and share with multiple teams at an organization?

It supports over 60 different SQL databases.

It’s 100% web based.

It creates easy to understand visualizations.

It’s cost effective.

6.

What does ETL stand for in the context of data processing?

Extract, transform, and load

Enhanced transaction logic

Event-time logic

Enrichment, tagging, and labeling

Chapter 2 Summary

This brings us to the end of the Exploring data transformation with Google Cloud course.

Let's do a quick recap.

In the first section of the course, the value of data, you learned how data generates business insights and drives decision making.

Basic data management concepts like databases, data warehouses and data lakes.

How organizations can create value by using the current data, collecting new data, and sourcing data externally.

How the Cloud unlocks business value from all types of data, including structured data and previously untapped, unstructured data.

About the data value chain from the initial creation of data to data activation and the importance that data governance plays in a successful data journey.

In the second section of the course, Google Cloud Data Management Solutions, you learned about Google Cloud data management options and the differences between them.

About the different storage classes available with Cloud storage.

How to choose the right storage product to meet the needs of your organization, and ways an organization can migrate and or modernize their current database in the Cloud.

Finally, in the third section of the course, Making Data Useful and Accessible, you learned how

looker makes it easy for a workforce to access the data they need when they need it.

How streaming analytics in real time can make data more useful, and about two Google Cloud products that modernize data pipelines, Pub/Sub and Dataflow.

Now that you've had a comprehensive introduction to data transformation, move on to the next course in the series.

Innovating with Google Cloud Artificial Intelligence, where you'll learn about the fundamentals of artificial intelligence and machine learning.

Selecting Google Cloud AI solutions and building and using Google Cloud AI solutions.

We'll see you next time.
Google Cloud Fundamental

Chapter 3: Innovating with Google Cloud Artificial Intelligence

Artificial intelligence (AI) and machine learning (ML) represent an important evolution in information technologies that are quickly transforming a wide range of industries. “Innovating with Google Cloud Artificial Intelligence” explores how organizations can use AI and ML to transform their business processes.

Part of the Cloud Digital Leader learning path, this course aims to help individuals grow in their role and build the future of their business.

Introduction 

Artificial intelligence and machine learning represent an important evolution in information technologies that are quickly transforming a wide range of industries.

As organizations digitally transform, they can find themselves with lots of data.

As time progresses, the amount of data they have only grows.

Although that data is really valuable, it can be very laborious to collect, process, and analyze.

New tools and methodologies are needed to manage what's being collected, analyze it for insights, and then act on those insights.

What do these organizations do?

This is where artificial intelligence and machine learning come in.

This course; Innovating with Google Cloud Artificial Intelligence, is designed to help you explore important

AI and machine learning or ML concepts, and understand how they can bring value to your

business, learn about the AI and ML solutions that Google Cloud offers, and understand how

Google Cloud's pre-trained APIs, AutoML, and custom AI and ML products can help transform your business.

Throughout the course, you'll be presented with graded knowledge assessments.

You must pass these assessments to receive course credit.

Let's get started.

Section 1: AI and ML Fundamentals

Artificial intelligence and machine learning can provide many benefits to a business, but it’s important to understand the fundamentals before starting any AI or ML initiative. In this section of the course, you'll explore many of those fundamental concepts.

Introduction

Google has nine products with over 1 billion users.

Android, Google Chrome, Gmail, Google Drive, Google Maps, Google Search, the Google Play Store, Youtube and Google photos.

Artificial intelligence and machine learning were integrated into these products to make the user experience of each even more efficient and productive.

This includes features like search in photos, recommendations in Youtube, smart composing in Gmail, and traffic predictions in Google Maps.

Google continues to innovate products powered by new technologies such as generative AI, which can produce content for you.

As you consider how AI and ML could provide a benefit to your business, understanding the basics is important.

In this section of the course, you'll explore the difference between artificial intelligence and machine learning, how machine learning differs from data analytics and business intelligence.

Different types of problems that AI solutions are suited to solve.

The importance of using quality data for machine learning, and the importance of responsible and explainable AI.

Point 1: AI and ML defined

People commonly use the terms artificial intelligence, AI, and machine learning, ML interchangeably.

The confusion is understandable because artificial intelligence and machine learning are closely related.

However, these trending technologies differ in several ways, including scope and application.

Before we advance, let's define each of the terms.

Artificial intelligence is a broad field which refers to the use of technologies to build machines and computers that can mimic cognitive functions associated with human intelligence.

These functions include, being able to see, understand, and respond to spoken or written language, analyze data, make recommendations and more.

Although artificial intelligence is often thought of as a system in itself, it's a set of

technologies implemented in a system to let it reason, learn, and act to solve a complex problem.

Machine learning is a subset of AI that lets a machine learn from data without being explicitly programmed.

It relies on various models to analyze large amounts of data, learn from the insights, and then make predictions and informed decisions.

Machine learning algorithms improve performance over time as they are trained or exposed to more data.

Machine learning models at the output, or what the program learns from running an algorithm on training data.

When more data is used, the model improves.

One helpful way to remember the difference between the two is to imagine them as umbrella categories.

Artificial intelligence is the overarching term that covers a variety of specific approaches and algorithms.

Machine learning sits beneath that umbrella, but so do other major sub fields such as deep learning, robotics, expert systems, and natural language processing.

Another area of AI you may be hearing a lot about is generative AI.

This is a type of artificial intelligence that can produce new content, including text, images, audio, and synthetic data.

Google applies generative AI to products like Google Workspace to help users easily automate different types of tasks, like generating summaries of long documents.

Google also provides development tool kits, such as generative AI APIs to developers to help them create customized products and services.

Generative AI can be used in a variety of applications, such as conversational bots, content generation, document synthesis, and product discovery.

Point 2: How AI and ML differ from data analytics and business intelligence

 Within your organization, perhaps you're familiar with a specific dashboard that analysts view every day.

Or maybe managers review a particular report each month.

Both the dashboard and the report are examples of backward looking data.

They look at what happened in the past.

Most data analysis and business intelligence is based on historical data, used to calculate metrics or identify trends.

But to create value in your business, you need to use that data to make decisions for future business.

This is where artificial intelligence and machine learning come in.

They're the key to unlocking these capabilities.

Let's consider an example to emphasize this point.

Maya leads the business strategy and operations team for an international airline to establish a trend in customer purchasing patterns, she's looking at historical annual reports.

She can use this data to generate dashboards that present information such as customer demographic distribution and sales in recent years.

But there's nothing new or transformational about this decision making process.

Maya is simply using data analytics to illustrate what's happened in the past.

But what if Maya could predict the satisfaction rate of each flight, or predict customer complaints and get ahead of them?

To do this effectively, she needs access to a lot more data and use ML models to make predictions for future business.

The data she needs might include the number of passengers per flight, the duration of

each flight, the customer satisfaction ratings per flight, and the number of customer complaints per flight.

She also needs to understand factors that contributed to customer complaints, weather reports, seasonal indicators, and the time to resolution data for customer complaints.

With all of these various data points, Maya might predict the quality of a single flight and its customer complaints.

But there are hundreds of flights each day.

The real value for Maya would come from being able to make predictive insights for all flights all year round.

More importantly, it would be far more valuable if she could dynamically adjust pricing or staff assignments, or even catering based on the predictions.

Remember, ML provides a method to teach a computer how to solve problems by feeding examples of the correct answers.

With access to the right data, Maya can use machine learning to uncover these types of predictive insights to benefit the airline and its customers.

Point 3: Problems that ML is suited to solve

Machine learning lets computer systems continuously adjust and enhance themselves as they accrue more experiences.

For this reason, when more data is put into them, the results are more accurate.

With this in mind, ML is suited to solve four common business problems.

The first is replacing or simplifying rule based systems.

Let's use Google Search as an example.

Suppose you want to search for the Giants, a US sports team.

If you type in Giants, should the search results show you the San Francisco Giants or the New York Giants?

One's a baseball team based in California and the other is an American football team based in New York.

In years gone by, the search engine used hand coded rules to decide which sports team to show user.

If the query is Giants and the user is in the Bay Area, show them results about San Francisco Giants.

If the user is in the New York area, show them results about NY Giants.

If the user is anywhere else, show them results about tall people.

This was for just one query.

If you multiply this process by millions of different queries and users each day, you can probably imagine how complex the whole code base became.

This is a perfect problem for ML to solve.

If all the data that's available shows which search results users clicked on per query, a machine learning model can be trained to predict the rank for search results.

A second business problem ML can help solve relates to automating processes.

ML is designed to make predictions and repeated decisions at scale.

Let's explore another example, this time from a property developer headquartered in Thailand called Ananda Development.

For every sale, both an Ananda Development inspector and the buyer have to conduct a detailed check of the property.

This was a manual, time-consuming process that was prone to much human error.

Inspectors would visually check hundreds of items a day for problems, list any issues on paper and then photograph the findings.

Multiplied across several projects, this workload adds up.

Ananda Development decided to create a mobile application to make this process more efficient.

Inspectors would verbally describe defects and critical issues to the application that ran on their smartphones.

The application would then track and document the inspection results.

In planning the application, the business realized it would need to recognize and convert to text, Thai language, speech and a version of English spoken by many Thai people.

They decided to automate this process using Google's speech-to-text API.

Furthermore, Ananda Development wanted to establish a pathway to use machine learning to complete condominium inspections by using remotely piloted drones.

They decided to automate that process by using the Cloud Vision API to capture images of defects and automatically classify information about each one.

Within three months of implementation, Ananda Development had saved around 130 hours of inspection time and over 100,000 US dollars in manpower costs.

The inspection process is now more efficient and accurate.

And as another benefit, buyers also receive copies of electronic inspection reports and updated status notes as defects are repaired.

So far, you heard about ML problems that use structured data to make predictions at scale.

A third type of business problem that ML can help solve is understanding unstructured data like images, videos, and audio.

This example comes from Ocado, one of the world's largest online only grocery supermarkets.

Previously, when Ocado received emails, they would all go to a central mailbox for sorting and forwarding by a human.

This process was time-consuming and led to a poor customer experience.

To improve and scale this process, Ocado used ML's ability to process natural language to identify the customer's

sentiment and the topic of each message, so that they could route it immediately to the relevant department.

This eliminated multiple rounds of reading and triaging, and ultimately improved customer satisfaction, and retention.

And finally, there's personalization.

Many businesses use ML to personalize user experiences and YouTube is a great example of personalization in action.

When you watch a video on YouTube, you've probably noticed there's a list of recommended videos that are up next.

When your video finishes, a new video will play and YouTube wants it to be interesting and useful for you.

By using ML to provide personalized recommendations, YouTube can deliver a better customer experience.

Many businesses use this same approach to surface product recommendations on their websites that are personalized to individual users.

Other businesses use personalization to surface new content like music recommendations or films to stream.

It's important to remember that ML models aren't standalone solutions and that solving complex business challenges requires combinations of models.

There are of course, many more applications of machine learning for businesses and you can learn even more about them in our machine learning courses.

Point 4: Why ML requires high-quality data

Data is used by machine learning models to derive predictive insights and make repeated decisions.

However, the accuracy of those predictions relies on large volumes of data that is correct and free of errors.

Data is considered low quality if it's not aligned to the problem, or is biased in some way.

If you feed an ML model low quality data, it's like teaching a child with incorrect information.

An ML model can't make accurate predictions by learning from incorrect data.

So, how can you ensure that you have quality data when training an ML model?

To assess it's quality, data is evaluated against six dimensions.

Completeness, uniqueness, timeliness, validity, accuracy, and consistency.

Let's explore what each of these mean in more detail.

The completeness of data refers to whether all the required information is present.

If the data is incomplete, then the model will not learn all the patterns that are necessary to make accurate predictions.

Take, for example, the training of an ML model that's reliant on a data set of customer transactions.

If some transactions are missing critical information, such as the date of the transaction, the accurate training of the model will be affected.

Data should be unique.

If a model is trained on a data set with a high number of duplicates, the ML model may not be able to learn accurately.

This is because it will be confused by the duplicate records and won't be able to accurately identify patterns.

For example, if you're training a model to identify a breed of dog from a photo, it's important to have images of many different unique breeds.

If the data set contains many thousands of images, but most of them are just

photos of Labradors, the model will find it difficult to correctly identify most other breeds accurately.

The timeliness of the data refers to whether the data is up-to-date and reflects the current state of the phenomenon that's being modeled.

If the data is not timely, then the model might be making predictions based on outdated or irrelevant information.

Training an ML model to predict stock market fluctuations might rely on a data set of stock prices.

If the data is several months old, it's untimely for making current predictions.

Validity means the data conforms to a set of predefined standards and definitions, such as type and format.

Validity also ensures that data is in an acceptable range.

An example of invalid data is a date of 08-12-2019, when the standard format is defined as year, month, and date.

Accuracy reflects the correctness of the data, such as the correct birth date or the accurate number of units sold.

For example, in a data set of images, some images might be labeled as dogs when they actually show cats.

Note how accuracy is different from validity.

Whereas validity focuses on type, format, and range, accuracy is focused on form and content.

Finally, the consistency of the data refers to whether the data is uniform and doesn't contain any contradictory information.

If data is inconsistent, then an ML model might be unable to make accurate predictions.

If the same entity appears with different names or values across different parts of the data, it would lead to inconsistent data.

For example, in a dataset of customer information, the same customer might appear as John Smith in one place, and J.Smith in another.

Remember, data is the only lens through which a model views the world.

Anything the model can't see, it assumes doesn't exist.

So it's your responsibility to provide the model with complete and correct data.

The good news is that most of these problems can be solved simply by getting more high quality data, but you have to be purposeful in collecting that data.

Point 5: The importance of responsible and explainable AI

AI has significant potential to help solve challenging problems, including advancing medicine, understanding language, and fueling scientific discovery.

To realize that potential, it's critical that AI is used responsibly.

To that end, Google has established principles that guide Google AI applications, best practices to share our work with communities outside of Google and programs to operate rationalize our efforts.

The principles state that AI should be socially beneficial, avoid creating or reinforcing unfair bias, be built

and tested for safety, be accountable to people, incorporate privacy design principles, uphold high standards of scientific excellence.

And be made available for uses that accord with these principles.

In addition to these principles, Google will not design or deploy AI in the following application areas.

Technologies that cause or are likely to cause overall harm.

Weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people.

Technologies that gather or use information for surveillance, violating internationally accepted norms.

And technologies whose purpose contravenes widely accepted principles of international law and human rights.

Although these are Google's own guiding AI principles, we urge other organizations to develop their own set of principles that encourage responsible AI development.

It's also important for organizations to debug and improve ML model performance and help others understand their model's behavior.

Organizations building ML models also need help with detecting and resolving bias, drift, and other gaps in their data and models.

In addition, having human interpretable explanations of your ML models will help grow end-user trust and improve transparency.

Explainable AI is Google Cloud's set of tools and frameworks to help you understand and interpret predictions made by your machine learning models.

These tools are natively integrated with several Google products and services to ensure transparent AI development.

Quiz
Your score: 100% Passing score: 75%
Congratulations! You passed this assessment.
check

1.

How do data analytics and business intelligence differ from AI and ML?

Data analytics and business intelligence use automated decision-making processes, whereas AI and ML require human intervention and interpretation of data.

Data analytics and business intelligence are used only in small businesses, whereas AI and ML are used exclusively by large corporations.
check
Data analytics and business intelligence identify trends from historical data, whereas AI and ML use data to make decisions for future business.

Data analytics and business intelligence involve advanced algorithms for predicting future trends, whereas AI and ML focus on processing historical data.
That is the correct answer!
check

2.

Which use case demonstrates ML’s ability to process natural language?

Identifying the artist, title, or genre of a song to create playlists based on the user's listening habits.

Segmenting images into different parts or regions to extract information, such as the text on a sign.
check
Identifying the topic and sentiment of customer email messages so that they can be routed to the relevant department.

Detecting people and objects in surveillance footage to use as evidence in criminal cases.
That is the correct answer!
check

3.

Which technology relies on models to analyze large amounts of data, learn from the insights, and then make predictions and informed decisions?

Expert systems
check
Machine learning

Robotics

Natural language processing
That is the correct answer!
check

4.

Which option refers to the use of technologies to build machines and computers that can mimic cognitive functions associated with human intelligence?

Natural language processing

Deep learning
check
Artificial intelligence

Machine learning
That is the correct answer!
check

5.

Artificial intelligence is best suited for replacing or simplifying rule-based systems. Which is an example of this in action?

Using a reinforcement learning algorithm to train autonomous drones for package delivery.
check
Training a machine learning model to predict a search result ranking.

Implementing AI to develop a new product or service that has never been seen before.

Using AI to replace a human decision-maker in complex situations, such as those involving life-or-death choices.
That is the correct answer!
check

6.

Which dimension for measuring data quality means that the data conforms to a set of predefined standards and definitions such as type and format?

Accuracy

Uniqueness
check
Validity

Consistency
That is the correct answer!
check

7.

What does the consistency dimension refer to when data quality is being measured?

Whether a dataset is free from duplicate values that could prevent an ML model from learning accurately.
check
Whether the data is uniform and doesn’t contain any contradictory information.

Whether all the required information is present.

Whether the data is up-to-date and reflects the current state of the phenomenon that is being modeled.
That is the correct answer!
check

8.

You’re watching a video on YouTube and are shown a list of videos that YouTube thinks you are interested in. What ML solution powers this feature?

Content moderation
check
Personalized recommendations

Clickbait detection

Video transcription
That is the correct answer!
check

9.

Google applies generative AI to products like Google Workspace, but what is generative AI?

A type of artificial intelligence that can create and sustain its own consciousness.

A type of artificial intelligence that can make decisions and take actions.
check
A type of artificial intelligence that can produce new content, including text, images, audio, and synthetic data.

A type of artificial intelligence that can understand and respond to human emotions.
That is the correct answer!
check

10.

Google's AI principles are a set of guiding values that help develop and use artificial intelligence responsibly. Which of these is one of Google’s AI principles?

AI should be accountable to other machines.
check
AI should be socially beneficial.

AI should create or reinforce unfair bias.

AI should be made available for any use.


Section 2: Google cloud's AI and ML Solutions

In this section of the course, you'll explore four options to build ML models with Google Cloud: BigQuery ML, pre-trained APIs, AutoML, and custom training.

Introduction

Historically, artificial intelligence and machine learning were not accessible to ordinary people.

Most of the people capable of developing AI and ML solutions were specialty engineers, who were scarce in number and expensive to hire.

The reality is that ML is more accessible now than ever before, which allows more people to build, even those without the technical expertise.

Google Cloud offers four options for building machine learning models.

The first option is BigQuery ML.

This is a tool for using SQL queries to create and execute machine learning models in BigQuery.

If you already have your data in BigQuery and your problems fit the predefined ML models, this could be your choice.

The second option is to use pre trained APIs, or application programming interfaces.

This option lets you use machine learning models that were built and trained by Google, so you don't have

to build your own ML models if you don't have enough training data or sufficient machine learning expertise in house.

The third option is AutoML, which is a no code solution, letting you build your own machine learning models on Vertex AI through a point and click interface.

And finally, there's custom training through which you can code your very own machine learning environment,

the training, and the deployment, which gives you flexibility and provides control over the ML pipeline.

In this second section of the course, you'll learn more about these four options for building machine learning models, and you'll also learn about some of Google's other AI solutions.

Point 1: BigQuery ML

Machine learning on large data sets requires extensive programming and knowledge of ML frameworks.

These requirements restrict solution development to a very small set of people within each company, and

they exclude data analysts who understand the data but have limited machine learning knowledge and programming expertise.

Although BigQuery started solely as a data warehouse, over time it has evolved to provide additional features that support the data to AI life cycle.

BigQuery ML democratizes the use of machine learning by empowering data analysts, but primary data warehouse users, to build and run models by using existing business intelligence tools and spreadsheets.

Predictive analytics can guide business decision making across the organization.

Using Python or Java to program an ML solution isn't necessary.

Models are trained and access directly in BigQuery by using SQL, which is a language familiar to data analysts.

BigQuery ML brings machine learning to the data.

It reduces complexity because fewer tools are required.

It also increases speed of production because moving and formatting large amounts of data for Python-based ML frameworks is not required for model training in BigQuery.

BigQuery ML also integrates with Vertex AI, Google Cloud's end to end AI and ML platform.

When BigQuery ML models are registered to the Vertex AI model registry, they can be deployed to endpoints for online prediction.

Point 2: Pre-trained APIs

Google Cloud's pre trained API's are a great option If you don't have your own training data.

These are ideal in situations where an organization doesn't have specialized data scientists, but it does have business analysts and developers.

This is the fastest and lowest effort of the machine learning approaches, but is less customizable than the others.

Google Cloud's pre trained API's can help developers build smart apps quickly by providing access to ML models for common tasks like analyzing images, videos, and text.

API's can be deployed in a virtual private cloud, on premises, or in Google's public cloud regardless of the level of ML experience.

Let's imagine a developer building a mobile app that users will submit photos to the developer needs

the app to recognize what the images are and filter out any that aren't safe for work.

The developer might choose Vision API.

This offers powerful, pre trained machine learning models, which use Google data to automatically detect faces, objects, text, and even sentiment in images.

The developer can use Vision API to assign labels to images and quickly classify them into millions of predefined categories.

The natural language API is another out of the box, pre trained API.

If a business has a contact form on its website that receives many messages every day.

This data can be difficult and time intensive to manually handle, categorize an action.

Natural language API discovers syntax, entities and sentiment in text and classifies texts into a predefined set of categories.

In this case, it can decide if comments represent complaints, Praise, and attempt to learn more about your business and more.

Google also offers several other pre trained API's.

The Cloud Translation API converts texts from one language to another.

The speech to text API converts audio to text for data processing.

The text to speech.

API converts text into high quality voice audio.

And the video intelligence API recognizes motion and action in video.

How well a machine learning model is trained depends on how much data is available to train it.

As you might expect, Google has lots of images, texts and ML researchers to train its pre trained models.

This means less work for you and a faster return on your investment.

Point 3: AutoML

Another more custom way to use machine learning to solve problems is to train models by using your own data.

This is where Vertex AI comes in.

Vertex AI brings together Google Cloud services for building ML under one unified user interface.

You can use your own training data with Vertex AI to manage and build ML projects.

AutoML and Vertex AI lets you build and train machine learning models from end to end by using graphical user interfaces.

Often referred to as GUIs without writing a line of code.

This means that after your data is ingested into Vertex AI, AutoML chooses the best machine learning model for you by comparing different models and tuning parameters.

What once required, a lot of manual work is done automatically and quickly, which results in a trained model that is both accurate and customized to your data.

This lets machine learning practitioners focus on the problems that they are trying to solve.

Instead of the details of machine learning.

AutoML is a great option for businesses that want to produce a customized ML model,

but are not willing to spend too much time coding and experimenting with thousands of models.

Let's go back to our image recognition example, which used Vision API, a pre-existing model trained with Google data.

Imagine you work for a car manufacturing company.

Vision API can tell you the difference between generic images found in Google databases, like the difference between a wheel and a door.

But it can't help a car manufacturer distinguish between good or defective parts.

In this case, a developer could use an AutoML vision instance and train it with your specialized data.

This automates the training of machine learning models, which means that you could upload a batch

of images and train an image classification model through the easy to use graphical interface of AutoML.

Models can be further optimized and deployed directly from the cloud.

Now let's focus on another feature of AutoML.

Earlier you saw how the natural language API could be used for processing entries into an online contact form.

But if your text examples don't fit neatly into the natural language API, sentiment based or vertical topic based

classification scheme, and you want to use your own specialized data instead, you need to use AutoML natural language.

AutoML natural language lets you build and deploy custom machine learning models.

The analyzed documents, categorize them, identify entities within them, or assess attitudes within them.

You can use the AutoML user interface to upload your training data and test your custom model without a single line of code.

Vertex AI makes this customization possible.

Those examples are just a few of the many Google Cloud ML offerings.

You can also find APIs that categorize videos, convert audio to text, or text to audio, understand natural language, translate from one language to another, and more.

In fact, in many of the most innovative applications for machine learning, several of these applications are combined.

Point 4: Custom Models

Vertex AI is also the essential platform for creating custom end to end machine learning models.

This means not only are models trained with your own data, but the models are custom built as well.

Vertex AI provides a suite of products to help at each stage of the ML

workflow, from gathering data to future engineering, building models, and finally, deploying and monitoring those models.

As this approach is fully custom built, end to end, its process takes the longest and requires a specialized team of data scientists and engineers.

However, these fully custom ML models are the most specialized to your needs, and give your business the most differentiation and innovative results.

Vertex AI contains tools that assist programmers with virtual machine imaging in data labeling, training, and predictions.

It also provides pre-built algorithms.

It's important to remember that although these tools are the building blocks to using your data at every stage, there is no one size fits all approach.

Every use case requires a different combination of tools and products.

Point 5: TensorFlow

All Machine Learning models are built on top of Google Cloud's AI foundational infrastructure.

A part of this foundation is TensorFlow, which is an end to end open source platform for machine learning.

TensorFlow has a flexible ecosystem of tools, libraries, and community resources that enable researchers to innovate in ML and developers to build and deploy ML powered applications.

First developed for Google's Internal use, TensorFlow is now open source so that everyone can benefit.

TensorFlow takes advantage of the Tensor Processing Unit, or TPU, which is Google's custom developed application specific integrated circuit used to accelerate machine learning workloads.

TPUs act as domain specific hardware as opposed to general purpose hardware with CPUs and GPUs.

With TPUs, the computing speed increases more than 200 times.

This means that instead of waiting 26 hours for results with a state of the art GPU, you

only need to wait for 7.9 minutes for a full cloud TPU pod to deliver the same results.

Cloud TPUs have been integrated across Google products, and this state of the art hardware and supercomputing technology is available with Google Cloud products and services.

Point 6: AI Solutions

Beyond the customizable options, Google Cloud has also created a set of full AI solutions aimed to solve specific business needs.

Contact Center AI provides models for speaking with customers and assisting human agents, increasing operational efficiency, and personalizing customer care to transform your contact center.

Document AI unlocks insights by extracting and classifying information from unstructured documents such as invoices, receipts, forms, letters, and reports.

The extracted data can then be saved in a database or exported to another application for further analysis.

Discovery AI for retail uses machine learning to select the optimal ordering of products on a retailer's e-commerce site when shoppers choose a category like winter jackets or kitchen ware.

Over time, the AI learns the ideal product ordering for each page on the site by using

historical data, optimizing how and what products are shown for accuracy, relevance, and likelihood of making a sale.

And Cloud Talent Solution uses AI with job search and talent acquisition capabilities, matches candidates to ideal jobs faster, and allows employers to attract and convert higher quality candidates.

These are just some of the fully built AI solutions offered by Google Cloud.

Point 7: Considerations when selecting Google Cloud AI/ML solutions and Products

Google Cloud offers a range of AI and ML solutions and products, but there are several decisions and trade-offs to consider when selecting which to employ.

The first consideration is speed.

How quickly do you need to get your model to production?

AI projects can typically take anywhere 3-36 months to plan and implement, depending on the scope and complexity of the use case.

But business decision makers often underestimate the time it will take.

Pre-trained API's require no model training, because that time-consuming task has already been carried out.

Custom training usually takes the longest time because it builds the ML model from the beginning, unlike autoML and Big query ML.

The next consideration is differentiation.

How unique is your model, or how unique does it need to be?

Google Cloud offers a range of outs of the box solutions for organizations that want to quickly use ML models in their day to day business operations.

These include image recognition solutions and chatbots, which are quick to deploy and can be applied in various use cases.

Alternatively, Vertex AI, which is Google Cloud's unified platform for building, deploying, and managing AI solutions, can give ML engineers and data scientists full control of the ML workflow.

Vertex AI custom training lets you train and serve custom models with code on vertex workbench, which results in highly bespoke ML models.

Another consideration is the expertise required when embarking on an AI or ML project.

Infusing AI into business processes requires roles such as data engineers, data scientists, and machine learning engineers among others.

Organizations should consider their current team and then determine a people strategy, which could include reusing or

repurposing existing resources, upskilling and training current staff, or hiring or working with outside consultants or contractors.

Google Cloud's AI and ML products vary from those that can be employed by data analysts

and business intelligence teams, right up to those more suited to ML engineers and data scientists.

The final consideration is the effort required to build an AI solution.

This depends on several factors, including the complexity of the problem, the amount of data available, and the experience of the team.

Google Cloud can help provide solutions for projects at both ends of the scale.

However, any AI undertaking will generally require much time, effort, and expertise to have a worthwhile impact on business operations.

Quiz
Your score: 100% Passing score: 75%
Congratulations! You passed this assessment.
check

1.

A large media company wants to improve how they moderate online content. Currently, they have a team of human moderators that review content for appropriateness, but are looking to leverage artificial intelligence to improve efficiency. Which of Google’s pre-trained APIs could they use to identify and remove inappropriate content from the media company's website and social media platforms.
check
Natural Language API

Video Intelligence API

Vision API

Speech-to-Text API
That is the correct answer!
check

2.

Which Google Cloud AI solution is designed to help businesses automate document processing?

Contact Center AI
check
Document AI

Cloud Talent Solution

Discovery AI for Retail
That is the correct answer!
check

3.

BigQuery ML is a machine learning service that lets users:

Build and evaluate machine learning models in BigQuery by using Python and Java.

Seamlessly connect with a data science team to create an ML model.

Export small amounts of data to spreadsheets or other applications.
check
Build and evaluate machine learning models in BigQuery by using SQL.
That is the correct answer!
check

4.

Which Google Cloud AI solution is designed to help businesses improve their customer service?
check
Contact Center AI

Discovery AI for Retail

Cloud Talent Solution

Document AI
That is the correct answer!
check

5.

What’s the name of Google’s application-specific integrated circuit (ASIC) that is used to accelerate machine learning workloads?

Graphic Processing Unit (GPU)

Vertex Processing Unit (VPU)

Central Processing Unit (CPU)
check
Tensor Processing Unit (TPU)
That is the correct answer!
check

6.

Which feature of Vertex AI lets users build and train end-to-end machine learning models by using a GUI (graphical user interface), without writing a line of code.

Custom training
check
AutoML

Managed ML environment

MLOps
That is the correct answer!
check

7.

An online retailer wants to help users find specific products faster on their website. One idea is to allow shoppers to upload an image of the product they’re looking to purchase. Which of Google’s pre-trained APIs could the retailer use to expand this functionality?

Natural Language API

Speech-to-Text API
check
Vision API

Video Intelligence API
That is the correct answer!
check

8.

Google Cloud offers four options for building machine learning models. Which is best when a business wants to code their own machine learning environment, the training, and the deployment?

BigQuery ML
check
Custom training

AutoML

Pre-trained APIs


Chapter 3 summary

This brings us to the end of the Innovating with Google Cloud Artificial Intelligence course, let's do a quick recap.

In the first section of the course titled AI and ML fundamentals, you explored the difference between artificial intelligence and machine learning.

How machine learning differs from data analytics and business intelligence, different types of problems that AI solutions are suited to solve.

The importance of using high quality data for machine learning, and the importance of responsible and explainable AI.

And in the second section of the course titled Google Cloud's AI and ML solutions.

You learnt about BigQuery ML, Pre-trained APIs, AutoML and custom models, which are both part of Vertex AI.

Tensorflow, existing AI solutions and what you should consider when choosing a Google Cloud AI or ML solution.

Now that you've had a comprehensive introduction to artificial intelligence and machine learning on Google Cloud.

You can move on to the next course in the series, Modernize Infrastructure and Applications with Google Cloud.

Where you'll learn about, why modernization and migration to the cloud is an important step in an organization transformation journey.

Options for and advantages of running compute workloads in the cloud.

Using containers and serverless computing in application modernization, the business value of application programming interfaces, APIs.

And the business reasons for choosing hybrid or multi-cloud strategies, we'll see you next time.

Google Cloud Fundamental

Chapter 4: Modernize Infrastructure and Applications with Google Cloud

Many traditional enterprises use legacy systems and applications that can't stay up-to-date with modern customer expectations. Business leaders often have to choose between maintaining their aging IT systems or investing in new products and services. 'Modernize Infrastructure and Applications with Google Cloud' explores these challenges and offers solutions to overcome them by using cloud technology.

Part of the Cloud Digital Leader learning path, this course aims to help individuals grow in their role and build the future of their business.

Introduction

New organizations born in the cloud are challenging old business models.

Scale is no longer a competitive advantage it's the norm.

But what does this mean for organizations founded before the cloud era?

They want to know how to not only survive, but thrive.

The answer lies in how organizations structure and use their It resources.

Like moving away from investing their own resources to run and maintain existing IT infrastructure.

With the cloud, organizations can shift their focus to creating new, higher value products and services.

And it's not just about infrastructure.

With the cloud, organizations can develop and build new applications to drive better engagement with customers and employees faster, securely and at scale.

Enterprises are also seeing significant financial benefits from adopting the cloud.

As their approach to IT moves from buying fixed capacity to paying only for what they use, they are changing the economics of technology investment.

For many organizations, infrastructure and application modernization are the foundation for digital transformation.

So, considering that, let's explore the goals of this course.

Modernized infrastructure and applications with Google Cloud was designed to help introduce you to common terminology related to infrastructure and application modernization.

Explore options available to run compute workloads in the cloud, including virtual machines, containers and server less architecture.

And examine options to modernize application development through rehosting and APIs.

Throughout the course, you're presented with graded knowledge assessments.

You must pass these assessments to receive course credit.

Okay, let's get started.

Important cloud migration terms

You'll hear some common terminology when learning about modernizing infrastructure and applications in the Cloud.

Let's introduce or remind you of some of these terms.

The first is workload.

In Cloud computing, a workload is a specific application, service, or capability that can be run in the Cloud or on premises.

Workloads include containers, databases, and virtual machines.

Sometimes workloads get retired.

Retiring a workload means removing it from a platform.

A workload might be retired because it's unnecessary, not cost effective, secure, or compatible with a specific platform.

Alternatively, workloads are often retained.

Retaining a workload means that it's intentionally kept.

When a workload is retained, it's typically kept on premises or in a hybrid Cloud environment.

This means that the workload will continue to be managed by the business and will not be subject to the same level of Cloud provider control.

Many workloads are rehosted in Cloud computing.

Rehost refers to the migration of a workload to the Cloud without changing anything in the workload's code or architecture.

This is often done as a first step in Cloud migration because it's the simplest and quickest way to run a workload in the Cloud.

This process is often referred to as lift and shift.

However, rehosting also has some drawbacks, including it doesn't use all the benefits of Cloud computing.

Managing workloads that were rehosted without making any changes can be difficult.

Scaling workloads that were re hosted without making any changes can also be difficult.

Then there's replatform.

In Cloud computing, replatform refers to the process of migrating a workload to the Cloud while making some changes to the workloads code or architecture.

This process is often called move and improve.

Replatforming lets organizations benefit from the Cloud's scalability, reliability and cost effectiveness, improve the performance of their workloads, and reduce the cost of their workloads.

However, replatforming also has some drawbacks, including it can be a complex and time consuming process.

Making the necessary changes to the workload's code or architecture can be difficult and testing the changes to the workload's code or architecture can also be difficult.

Sometimes workloads are refactored, which refers to the process of changing the code of a workload.

For example, an organization might refactor a workload to use either a Cloud-based microservices architecture or a Cloud-based server-less architecture.

We'll explore what those concepts mean later in this course.

Refactoring has some benefits.

It means that workloads can become more efficient, scalable, or secure, and a valuable investment for organizations that want to use all Cloud capabilities.

That being said, a possible drawback for organizations is that refactoring a workload can be a complex and time consuming process.

Finally, Cloud modernization can inspire and incentivize organizations to reimagine.

In Cloud computing, reimagine refers to the process of rethinking how an organization uses technology to achieve its business goals.

This can involve reconsidering the organization's current Cloud strategy and its use of other technologies such as artificial intelligence and machine learning.

Reimagining Cloud computing can help organizations to improve their efficiency, reduce costs, and increase agility.

It can also help organizations better meet the needs of their customers and partners.

Section 1: Modernizing Infrastructure in the Cloud
 
In this section of the course, you'll explore the options for, and advantages of, running compute workloads in the cloud. You'll also examine containers and the business value of serverless computing.

Introduction

In the context of the Cloud, compute refers to a machine's ability to process information.

Associated tasks include storing, retrieving, comparing, and analyzing the information.

Instead of relying on local servers and storage devices, Cloud computing uses a network of

remote servers to provide on-demand access to various computing resources, including applications, storage, and processing power.

This technology has become increasingly popular in recent years due to its flexibility, scalability, and cost-effectiveness.

In this section of the course, you'll learn about the benefits that Cloud computing can bring to an organization and explore three Cloud computing options, virtual machines, containers, and serverless.

Point 1:  The benefits of running compute workloads in the cloud

Why should an organization consider running compute workloads in the Cloud?

Let's explore some benefits that running compute workloads in the Cloud can bring to an organization.

We'll begin with total cost of ownership, or TCO, which is a measure of the total cost of a system or solution over its lifetime.

It includes the cost of the initial purchase, maintenance and operation, along with any other associated costs.

Cloud computing can help businesses save money on IT costs by eliminating the need to purchase and maintain physical infrastructure.

Cloud providers offer a pay-as-you-go model, which means that organizations only pay for the resources used.

They also offer discounts for long term commitments, which can further reduce TCO for businesses that are planning to use Cloud services for a long period.

Next, there is scalability, which refers to the ability to increase or decrease the number of resources

such as servers, storage, and bandwidth that are available to a Cloud-based application to meet changing demand.

Scalability is important because it provides a means to meet changing demand without having to make large upfront investments in infrastructure.

If a business experiences a sudden spike in demand, it can easily scale up its Cloud resources to meet the demand.

Conversely, if they experience reduced demand, infrastructure can quickly scale down its Cloud resources to save money.

Another benefit to Cloud computing is reliability.

Cloud providers offer a high degree of reliability and up-time, which gives businesses confidence that their data and applications will be available when they need them.

Cloud providers have many ways to ensure the reliability of their services.

Google Cloud for example has multiple data centers located in different parts of the world.

This helps to ensure that if one data center goes down, the others can continue to operate.

Cloud providers also use various technologies to monitor their services and automatically detect and fix problems.

Next is security.

Cloud computing providers offer a high level of security for data and applications.

Organizations need to be sure that their data is being kept safe.

In addition to physical data center security, Cloud security features include data encryption, identity and access management, network

security, virtual private Clouds, and monitoring services that can detect and respond to security threats in real time.

These security features can also help to ensure compliance with government or industry regulations.

Running compute workloads in the Cloud offers a high degree of flexibility for organizations.

Organizations can choose the Cloud services that best meet their needs at any point in time, and then change or adapt those services when necessary.

For example, a business that needs to increase the amount of storage space that it uses can easily add more storage space to its Cloud storage service.

Finally, another benefit of running compute workloads in the Cloud is abstraction.

Abstraction refers to how Cloud providers remove the need for customers to understand the finer details of

the infrastructure implementation by providing management of the hardware, software, and certain aspects of security and networking.

For example, a Cloud storage provider might provide a way for customers to store files so that they

don't have to worry about the finer details of how the files are stored on the Cloud providers' infrastructure.

Abstraction also lets Cloud providers offer many services.

For example, Google Workspace lets customers run productivity applications so that they don't have to

worry about the details of how the applications are actually run or maintained on Google's infrastructure.

Running compute workloads in the Cloud can help organizations get their products and services to market faster by eliminating the need to develop and maintain their own infrastructure.

At the same time, it provides a platform for innovation by providing access to the latest technologies and tools as and when they are released.

Point 2: Virtual machines

Traditionally, various technological pressures compelled many organizations to tightly bind specific computing hardware resources to specific applications.

Virtualization, technology relieved these pressures.

Virtualization is a form of resource optimization that lets multiple systems run on the same hardware.

These systems are called Virtual Machines or VMs.

This means that they share the same pool of processing, storage, and networking resources.

VMs enable organizations to run multiple applications at the same time on a server in a way that is efficient and manageable.

Compute engine is Google Cloud's infrastructure as a service product, that lets users create and run virtual machines on Google infrastructure.

There are no upfront investments, and thousands of virtual CPUs can run on a system that's designed to be fast and to offer consistent performance.

Each virtual machine contains the power and functionality of a full fledged operating system.

This means a virtual machine can be configured much like a physical server by specifying the amount

of CPU power and memory needed, the amount and type of storage needed, and the operating system.

A virtual machine instance can be created through the Google Cloud Console, which is a web based tool to manage Google Cloud

projects, resources and Google Cloud CLI command line interface by using infrastructure automation tools such as Terraform or the Compute Engine API.

An API or application programming interface, is a set of instructions that allows different software programs to communicate with each other.

We'll learn about API's in more detail later in this course.

When you use virtual machines, compute engine bills by the second with a one minute minimum and sustained use discounts start

to apply automatically to virtual machines the longer they run, for each VM that runs for more than 25% of a month.

Compute engine automatically applies a discount for every incremental hour of use.

Compute engine also offers committed use discounts.

This means that when committing to use resources for either a one year or three year

period, discounts are offered over the on demand prices and then there are preemptable and spot VMs.

Let's say that a workload doesn't require a human to sit and wait for it to finish, such as a batch job analyzing a large dataset.

Costs can be reduced in some cases by up to 90% by choosing preemptable or spot VMs to run the job.

A preemptable or spot VM is different from an ordinary compute engine VM in only one respect.

Compute engine has permission to terminate a VM if its resources are needed elsewhere.

Although savings are possible with preemptable or spot VMs, it needs to be ensured that a job can be stopped and restarted without impact.

Spot VMs differ from preemptible VMs by offering more features.

For example, preemptable VMs can only run for up to 24 hours at a time, but spot VMs don't have a maximum run time.

However, the pricing is currently the same for both.

Finally, Compute Engine lets users choose the machine properties of their instances, like the number of virtual CPUs, the operating

system, and the amount of memory by using a set of predefined machine types, or by creating custom machine types.

Point 3: Containers 

Infrastructure as a service, or IS, lets users share compute resources with other developers by using virtual machines to virtualize the hardware.

This lets each developer deploy their own operating system, access the hardware, and build their applications in a self-contained environment with access to the necessary system resources.

Containers follow the same principle as virtual machines.

They provide isolated environments to run software services and optimize resources from one piece of hardware.

However, they're even more efficient.

The key difference between virtual machines and containers is that virtual machines virtualize an entire machine down to the hardware layers.

Whereas containers only virtualize software layers above the operating system level.

Containers start faster and use a fraction of the memory compared to booting an entire operating system.

A container is packaged with your application and all of its dependencies, so it has everything it needs to run.

Containers can be independently developed, tested, and deployed, and are well suited for a microservices based architecture.

This architecture is made up of smaller individual services that run containerized applications, that communicate with each other through APIs or other lightweight communication methods, such as REST or gRPC.

Containers let developers create predictable environments isolated from other system resources.

So if a customer asks for a new feature or a change in the application, developers

can easily make an update to that particular part of the application without affecting the REST.

Containers can run virtually and anywhere, which makes development and deployment easy.

Point 4: Managing containers

Containers improve agility, enhance security, optimize resources and simplify managing applications in the cloud.

Many organizations have a mix of virtual machines and containers.

However, as their It infrastructure setup becomes more complex, they often need a way to manage their services and machines.

For example, an organization can have millions and millions of containers.

This require as keeping them secure and ensuring that they operate efficiently can require significant oversight and management.

Kubernetes, originally developed by Google, is an open-source platform for managing containerized workloads and services.

It makes it easy to orchestrate many containers on many hosts, scale them, and easily deploy rollouts and rollbacks.

This improves application reliability and reduces the time and resources needed to spend on management and operations.

Google Kubernetes Engine or GKE is a Google hosted, managed Kubernetes service in the Cloud.

The GKE environment consists of multiple machines, specifically compute engine instances grouped to form a cluster.

GKE clusters can be customized, and they support different machine types, numbers of nodes, and network settings.

GKE makes it easy to deploy applications by providing an API and a Web based console.

Applications can be deployed in minutes and can be scaled up or down as needed.

GKE also provides many features that can help monitor applications, manage resources, and troubleshoot problems.

Let's explore how Ubie, a Japan based healthcare technology startup, reduced their infrastructure costs and maintenance requirements with Google Kubernetes Engine.

Founded in 2017, Ubie's goal is to get people the right medical care when they need it, and it does this with products designed for hospitals and individuals.

Ubie for hospital, their flagship product, is AI powered questionnaire software that lets patients provide medical details before an appointment.

Ubie initially relied on an alternative, Cloud, to make Ubie for Hospital available in Japan.

As the business added new customers, they needed an infrastructure that could support daily deployments and

provide a secure gateway to connect Ubie to a wide range of customer networks and settings.

Ubie evaluated available options and decided to use Kubernetes in Google Kubernetes Engine.

Google Kubernetes Engine Autopilot, a mode that enables full management of an entire cluster's infrastructure and provides per-pod

billing, presented a compelling option for the business to run Ubie for Hospital more efficiently and cost effectively.

With GKE Autopilot, Ubie could eliminate the need to configure and monitor clusters while only paying for running pods.

The shift reduced Ubie's infrastructure costs by 20%, and GKE Autopilot has helped the business eliminate

Ubie for Hospital infrastructure maintenance and upgrade tasks that could take hours and days to complete.

Another popular option for running containerized applications on Google Cloud is Cloud Run.

Cloud Run is a fully managed serverless platform to deploy and run containerized applications without needing to worry about the underlying infrastructure.

After your application code is containerized and deployed to Cloud Run, Google Cloud takes care of scaling and managing the infrastructure automatically.

Cloud Run is ideal for running stateless applications that need to scale up and down quickly in response to traffic.

This makes cloud run most suitable for simple and lightweight applications such as web applications.

In summary, GKE is ideal when lots of control is required over a Kubernetes Environment and there are complex applications to run.

Alternatively, Cloud Run is ideal for when a simple, fully managed serverless platform that can scale up and down quickly is required.

Point 5: Serverless Computing

Another option for modernizing Cloud applications is serverless computing.

Serverless computing doesn't mean there's no server, it means that resources like compute power are automatically provisioned in the background as needed.

The advantage here is that organizations won't pay for compute power unless they're running a query or application.

At its simplest definition, serverless means that businesses provide the code for whatever function they want and the public Cloud provider does everything else.

Imagine you provide software to businesses that help employees manage their corporate expenses.

You want to add a feature that lets users upload an image with their expense receipt.

In this case, the ability to upload an image is called a function.

You as the software development company write the code for that function directly into your public Cloud platform.

From there, the public Cloud provider manages everything else.

One type of serverless computing solution is called function as a service.

Some functions are a response to specific events, like file uploads to Cloud storage, or changes to database records.

You write the code that defines the response to those events and the Cloud provider does everything else.

Google Cloud offers many serverless computing products.

The first is Cloud Run, which is a fully managed environment for running containerized applications.

With this product, you don't have to worry about the underlying infrastructure.

Then there is Cloud functions, which is the platform for hosting simple single purpose functions that are attached to events emitted from your Cloud infrastructure and services.

For example, sending a notification to a mobile device when a new order is placed on a website.

There is also App Engine, which is a service to build and deploy web applications.

Serverless computing has many benefits, reduced operational costs.

The Cloud provider is responsible for the infrastructure and its maintenance.

Therefore, the application owner does not need to invest in the infrastructure or the human resources required to manage it.

Scalability.

Serverless computing provides automatic scaling of computing resources based on the applications demand.

The Cloud provider manages the scaling process and the application owner only pays for the resources they use.

Faster time to market, the need for infrastructure, setup and configuration is eliminated, which reduces the time required to deploy applications.

This feature lets the application owner focus on writing code and quickly deploying new features.

Reduce development costs.

The development process is simplified because developers can focus on the application's logic and not on the underlying infrastructure.

Improved resilience.

Serverless computing offers improved resilience and availability as the Cloud provider automatically manages the infrastructure's failover and disaster recovery capabilities.

Pay per use pricing model, The application owner only pays for the computing resources they use.

This reduces the cost of unused resources and helps optimize costs.

How might an organization benefit from Cloud computing infrastructure technology?

Let's explore an example specializing in educational technology.

Mashme.io provides video collaboration experiences for over 3 million users in 73 countries.

Connecting 250 full HD live video streams in real time is a major technical challenge.

Latencies need to be kept very low to achieve the face to face experience, and continuous integration in deployment is vital to avoid disruptive downtime for global clients.

Meanwhile, costs have to be kept to a minimum to keep the solution affordable for a growing start up.

To meet those needs, Mashme.io chose to use Google Kubernetes Engine.

Every teacher we speak to tells us that latency is the most important thing for educational video conferencing.

Says Mashme.io meet founder Victor Sanchez Belmar.

Low latency means having servers close to every student that connects to Mashme.io.

With students connecting from around the world, Google Cloud has the global network to make that happen.

The view was that setting up data centers around the world with your own hardware is a good way for a start up to never start.

Instead, Mashme.io started using Google's global network with App Engine before moving to Google Cloud with their own docker containers, and finally, to Google Kubernetes Engine.

This allowed them to update their nodes and services in an almost continuous way without disruption.

Students didn't lose an hour or even a second of class.

Quiz
Your score: 100% Passing score: 75%
Congratulations! You passed this assessment.
check

1.

What open source platform, originally developed by Google, manages containerized workloads and services?
check
Kubernetes

Go

Angular

TensorFlow
That is the correct answer!
check

2.

What portion of a machine does a container virtualize?

Software layers above the firmware level

Hardware layers above the electrical level
check
Software layers above the operating system level

The entire machine
That is the correct answer!
check

3.

A travel company is in the early stages of developing a new application and wants to test it on a variety of configurations: different operating systems, processors, and storage options. What cloud computing option should they use?

Colocation

Containers

A local development environment
check
Virtual machine instances
That is the correct answer!
check

4.

A manufacturing company is considering shifting their on-premises infrastructure to the cloud, but are concerned that access to their data and applications won’t be available when they need them. They want to ensure that if one data center goes down, another will be available to prevent any disruption of service. What does this refer to?

Total cost of ownership

Security
check
Reliability

Flexibility
That is the correct answer!
check

5.

What computing option automatically provisions resources, like compute power, in the background as needed?

Traditional on-premises computing

IaaS (infrastructure as a service)
check
Serverless computing

PaaS (platform as a service)
That is the correct answer!
check

6.

What phrase refers to when a workload is rehosted without changing anything in the workload's code or architecture.

Move and improve

Refactor and reshape

Reimagine and plan
check
Lift and shift


Section 2: Modernizing Applications in the Cloud

In this section of the course, you'll focus on application modernization and the business value of application programming interfaces (APIs). You'll also explore the business reasons for choosing hybrid or multi-cloud strategies.

Introduction

In the previous section of the course, you explored the benefits and options for modernizing It infrastructure with the cloud.

Now let's focus on application modernization.

But first, what exactly is an application?

In its basic form, an application is a computer, program or software that helps users do something.

And in this digital age, applications are everywhere.

Consider how many application patients you interact with each day, from checking email to tracking your fitness with wearable technology that links to an app on your phone.

Customers expect intuitive, well functioning applications that can help them do things faster.

Applications have been developed on premises for years and often still are.

However, on premises application development often slows organizations down.

Deploying an application on premises can be time consuming and can also require specialized IT teams.

Changes can often take six months or more to implement, which can create friction within different parts of an organization.

With cloud technology, businesses can modernize, develop, and manage applications in new ways, which makes them more agile and responsive to user needs.

In this section of the course, you'll compare traditional and modern cloud application development methods.

Explore considerations and tools for rehosting legacy applications in the cloud.

Define application programming interfaces, or APIs.

Examine the benefits of maintaining and managing APIs with Apigee API management, and consider scenarios when a hybrid or multi cloud strategy might be beneficial.

Point 1: The benefits of modern cloud application development

Thanks to advances in cloud technology, the way that software applications are developed has drastically changed.

With modern cloud application development, software development is flexible, scalable, and uses the latest cloud computing technologies to build and deploy applications.

In the past, the traditional software development approach, often referred to as monolithic applications, required all the components of

an application to be developed and deployed as a single, tightly coupled unit, typically using a single programming language.

There are many benefits to the modern cloud application development approach.

Let's explore a few.

We'll begin with architecture.

Modern cloud applications are typically built as a collection of microservices.

Microservices are independently deployable, scalable and maintainable components that can be used to build a wide range of applications.

This can help organizations bring business value to market faster because features can be released as they're completed without waiting for the rest of the application to be complete.

Regarding deployment, modern applications are typically deployed to the cloud and can use managed or partially managed services.

Managed services take care of the day-to-day management of cloud-based infrastructure, such as patching, upgrades, and monitoring.

This can free up staff to focus on other tasks, such as developing new applications.

Partially managed services offer a hybrid approach, where businesses manage some aspects of their cloud-based applications themselves and the cloud provider manages others.

In terms of cost, modern cloud applications use a pay as you go pricing model, which can make them extremely cost effective when configured efficiently.

That means that organizations don't always need to pay for resources they aren't fully utilizing.

Developers can also use prebuilt APIs, which we'll explore later in this section of the

course, and other tools offered by the cloud provider to build and deploy their applications quicker.

And then there's scalability.

Modern cloud-based applications can easily be scaled up or down to meet user demands.

Modern cloud applications are designed to be highly available and resilient with built in features like load

balancing, which is the process of distributing network traffic evenly across multiple servers that support an application.

And automatic failover, which is a process that allows a cloud-based application to automatically switch to a backup server if a failure occurs.

Additionally, cloud service providers typically offer robust monitoring and management tools that allow developers to quickly identify and respond to issues, which can further improve the reliability of cloud applications.

Point 2: Rehosting legacy applications in the cloud

When a business decides to modernize  and move its operations to the cloud,  it might be running several  specialized legacy applications  that aren’t compatible with  cloud-native applications.

  In these situations, a business  might take a rehost migration path,  commonly referred to as lift and shift, where

an application is moved from an  on-premises environment to a cloud environment without making  any changes to the application itself.

Rehosting applications brings with it   the many benefits of cloud computing  that we explored earlier, such as  cost savings, scalability,  reliability, and security.

  However, there are also some potential  drawbacks to choosing a rehost migration   path for legacy applications, including: Complexity: rehosting can be a complex   process.

Businesses need to carefully  plan the migration process and ensure   that they have the right resources in place.

Risk: migrating applications to the cloud   always involves some risk.

Businesses  need to carefully assess and identify   potential risks and ensure that they have  a plan in place in case of any problems.

  Vendor lock-in: by moving applications to the  cloud, businesses might become locked into a   particular cloud provider.

This can potentially  make it difficult to switch providers later.

  Google Cloud offers many solutions for  rehosting specialized legacy applications.

  The first is Google Cloud VMware Engine, which  helps migrate existing VMware workloads to   the cloud without having to rearchitect  the applications or retool operations.

  With Google Cloud VMware Engine, organizations  can maintain their existing VMware   environments and operational processes, while benefiting from the scalability,  security, reliability of Google Cloud.

  By doing this, organizations can also access  a range of Google Cloud services such as  BigQuery, AI/ML, 

and Google Kubernetes Engine, which lets them modernize their   application environment and use new  capabilities and technologies as needed.

  And for organizations with  legacy applications on Oracle,  Google Cloud offers Bare Metal Solution.

  This is a fully managed cloud infrastructure  solution that lets organizations run their   Oracle workloads on dedicated,  bare metal servers in the cloud.

Point 3: Application programming interfaces (API)

Implementing a software service can be complex and changeable.

And if each software service that an organization uses has to be coded for each implementation, the result can be fragile and error-prone.

One way to make things easier is to use APIs or application programming interfaces.

Earlier in this course, you saw how cloud providers offer a variety of resources and services for running applications and performing computational tasks in the cloud.

However, to fully use these resources and services, applications need to be able to interact with them in a standardized and efficient way.

This is where APIs come in.

An API is a set of instructions that lets different software programs communicate with each other.

Think of it as an intermediary between two different programs, which provides a standardized and predictable way for them to exchange data and interact.

An API is like a waiter in a restaurant.

The waiter takes orders from customers, communicates with the kitchen, and then brings the food back to the customers.

Similarly, an API takes requests from one software program, the customer, communicates with another program,

the kitchen, and then returns a response, the food, back to the requesting program, the customer.

APIs can be used in many different applications, from social media platforms to mobile apps and web services.

They let developers access functionality and data from other programs without having to write all the code themselves, saving time and effort.

Google itself provides many APIs that let developers access its products and services.

These include APIs that use the power of Google to search across a website or collection of websites, APIs that let developers

access Google Maps data such as maps, directions and traffic information, and APIs that let developers translate text from one language to another.

In fact, many Google Cloud products and services have documented APIs.

Using APIs can create new business opportunities for organizations and improve online experiences for users.

For example, an organization could expose an API that allows customers to track their shipments or check their account balances from within a third party app.

There's also an opportunity for organizations to create new products that let other companies access their data or services through an API.

Let's explore why an organization might consider this business opportunity.

APIs can be used to create new products and services.

An organization could create an API that allows developers to access data from its database.

This data could then be used to create new products and services.

APIs can be used to generate new revenue streams.

An organization could charge developers to access its APIs.

This could generate new revenue streams for the organization and help to offset the cost of developing and maintaining the APIs.

APIs can create partnerships.

By exposing APIs, organizations can create partnerships with other companies or developers which can lead to new business opportunities and collaborations.

By carefully considering the needs of their customers and partners, organizations can develop APIs that provide value and help to grow their businesses.

Point 4: Apigee API Management

When an organization has implemented API's, it's important to maintain and manage them effectively.

This can be done using a platform such as Apigee API management, Google Cloud's API management service to operate API's with enhanced scale security and automation.

Apigee is a popular choice for organizations that need to manage their API's because it offers many benefits.

It helps organizations secure their API's by providing features such as authentication, authorization and data encryption.

It tracks and analyzes API usage with real time analytics and historical reporting.

It helps with developing and deploying API's through a visual API editor and a test sandbox.

It offers API versioning, API documentation, and even API throttling, which is the process of limiting the number of API requests a user can make in a certain period.

AccuWeather has enjoyed great success, sharing its world class weather data through APIs with a range of

global partners who have built applications for connected cars, smart homes, wearables, smart TV's, mobile devices, and more.

But the company wanted to get its data into the hands of a new customer, individual developers.

It needed a way to engage this audience and tailor its offerings to the varying needs of developers and monetize those different offering levels accordingly.

To implement a simple and fast way for developers to start building with an appropriate level

of API calls and features for their needs, AccuWeather realized it required a sophisticated API management platform.

One that enabled different tiers of offerings by bundling API's into different products, each with their own rate limits and pricing.

With Apigee managing API's for AccuWeather, their users can customize API consumption to their specific needs.

While Apigee helps attract and build that traffic.

With the customizable Apigee developer portal, developers can sign up quickly, learn about the AccuWeather API's and test them out.

With built in analytics, AccuWeather can keep close tabs on who's signing up, what traffic they're producing and from where, and also observe unexpected patterns in traffic activity.

Point 5: Hybrid and multi-cloud

As you've seen throughout this course, organizations can thrive with the help of cloud.

But the reality is that most of the world's enterprise computing still happens on premises.

The path to the cloud can be complex and full of difficult decisions and sometimes workloads remain on premises due to compliance or operational concerns.

How can organizations modernize their IT infrastructure without completely migrating to the cloud?

How can they maintain flexibility and avoid lock in?

Two options are hybrid and multi cloud solutions.

A hybrid cloud environment comprises some combination of on premises or private cloud infrastructure and public cloud services.

This is the situation many organizations are currently in, where some of their data and applications have been migrated to the cloud, while others remain on premises.

Interconnects between the private and public clouds allow interoperability.

A multi-cloud environment is where an organization uses multiple public cloud providers as part of its architecture.

This is ideal for organizations that need flexibility and secure connectivity between the different networks.

An organization might choose to use hybrid cloud multi-cloud or a combination of both if they want

to incorporate specific elements of a public cloud to benefit from the main strengths of that provider.

This lets organizations keep parts of the system's infrastructure on premises while they move other parts to the cloud.

This way they create an environment that is uniquely suited to the organization's needs.

Move only specific workloads to the cloud because a full scale migration is not required for it to

work, benefit from the flexibility, scalability, and lower computing costs offered by Cloud services for running specific workloads.

Add specialized services such as machine learning, content caching, data analysis, long term storage, and IOT or Internet of Things.

To the organization's computing resources toolkit.

How can Google Cloud help in this context?

Google's answer to modern hybrid and Multi-cloud distributed systems and services management is called GKE Enterprise.

GKE Enterprise is a managed production ready platform for running Kubernetes applications across multiple cloud environments.

It provides a consistent way to manage Kubernetes, clusters, applications and services regardless of where they are running.

Some of the benefits of GKE enterprise include Multi-cloud and hybrid-cloud support.

GKE enterprise can run Kubernetes clusters on Google Cloud, AWS, Azure, and other public clouds.

Centralized management GKE Enterprise provides a single centralized console for managing Kubernetes clusters and applications, security and compliance.

GKE Enterprise includes many features that help secure Kubernetes clusters and applications and comply with industry regulations, networking and load balancing.

GKE Enterprise includes a number of features that help network and load balance Kubernetes applications, monitoring and logging GKE Enterprise provides a rich

set of tools for monitoring and maintaining application consistency across an entire network, whether on premises in the cloud or in multiple clouds.

Quiz
Your score: 80% Passing score: 75%
Congratulations! You passed this assessment.
close

1.

In modern application development, which is responsible for the day-to-day management of cloud-based infrastructure, such as patching, upgrades, and monitoring?

Cloud security

Containers

Managed services
close
DevOps
That’s incorrect. Review the module content for the correct answer.
check

2.

What name is given to an environment where an organization uses more than one public cloud provider as part of its architecture?

Community cloud
check
Multicloud

Edge cloud

Hybrid cloud
That is the correct answer!
close

3.

In modern cloud application development, what name is given to independently deployable, scalable, and maintainable components that can be used to build a wide range of applications?

Microservices

Monoliths
close
Containers

DevOps
That’s incorrect. Review the module content for the correct answer.
check

4.

What term is commonly used to describe a rehost migration strategy for an organization that runs specialized legacy applications that aren’t compatible with cloud-native applications?

Move and improve

Install and fall
check
Lift and shift

Build and deploy
That is the correct answer!
check

5.

What’s the name of Google Cloud’s production-ready platform for running Kuberenetes applications across multiple cloud environments?

Google Kubernetes Engine

Container Registry
check
GKE Enterprise

Knative
That is the correct answer!
check

6.

What name is given to an environment that comprises some combination of on-premises or private cloud infrastructure and public cloud services?

Smart cloud
check
Hybrid cloud

Multicloud

Secure cloud
That is the correct answer!
check

7.

What is one way that organizations can create new revenue streams through APIs?
check
By charging developers to access their APIs

By allowing developers to access their data for free

By developing new products and services internally

By using APIs to track customer shipments
That is the correct answer!
check

8.

What term describes a set of instructions that lets different software programs communicate with each other?

Programming communication link

Communication link interface
check
Application programming interface

Network programming interface
That is the correct answer!
check

9.

Which is a fully managed cloud infrastructure solution that lets organizations run their Oracle workloads on dedicated servers in the cloud?
check
Bare metal solution

Google Cloud VMware Engine

SQL Server on Google Cloud

App Engine
That is the correct answer!
check

10.

What is the name of Google Cloud's API management service that can operate APIs with enhanced scale, security, and automation?

AppSheet

Cloud API Manager
check
Apigee

App Engine

 Chapter 4 summary

This brings us to the end of the modernized infrastructure and applications with Google Cloud course.

Let's do a quick recap.

In the first section of the course titled course introduction, you explored some important cloud migration terminology.

In the second section, titled modernizing infrastructure in the cloud, you were introduced to, the benefits of running compute workloads in the cloud.

Virtual machines, containers and how to manage them, and serverless computing.

And in the final section of the course, modernizing applications in the cloud, you learned about the benefits of modern cloud application development.

Rehosting legacy applications in the cloud, APIs and API management with Apigee, and using hybrid and multi-cloud solutions.

Now that you have a comprehensive introduction to modernizing infrastructure and applications on Google Cloud, you can move on to the next course in the series.

Trust and Security with Google Cloud, where you'll learn about fundamental cloud security concepts, google's multi layered

approach to infrastructure security, and how Google Cloud strives to earn and maintain customer trust in the cloud.

We'll see you next time.

Google Cloud Fundamental

Chapter 5: Trust and Security with Google Cloud

As organizations move their data and applications to the cloud, they must address new security challenges. The Trust and Security with Google Cloud course explores the basics of cloud security, the value of Google Cloud's multilayered approach to infrastructure security, and how Google earns and maintains customer trust in the cloud.

Part of the Cloud Digital Leader learning path, this course aims to help individuals grow in their role and build the future of their business.

When you complete this course, you can earn the badge displayed here! View all the badges you have earned by visiting your profile page. Boost your cloud career by showing the world the skills you have developed!

Introduction

At Google Cloud, we understand the responsibility that comes with hosting, serving and safeguarding our customers' valuable data.

As organizations increasingly migrate their data and applications to the cloud, it becomes crucial to address the emerging security challenges.

Trust and security lie at the heart of our product design and development philosophy.

We firmly believe that customers own their data and have complete control over its usage.

Although we've implemented robust security measures to defend against potential breaches, we also acknowledge that security is a dynamic and ongoing process that demands constant attention and investment.

To support you in this journey, we provide a range of security products and services that enable

you to detect, investigate and mitigate cyber threats while aligning with your policy, regulatory and business objectives.

The objective of this course, trust and security with Google Cloud, is to equip you with the knowledge and

skills necessary to discuss fundamental cloud security concepts, explain the business value of Google's multi layered approach to infrastructure security.

And describe how Google Cloud earns and maintains customer trust in the cloud.

Throughout the course you're presented with in knowledge assessments.

You must pass these assessments to receive course credit.

Okay, let's get started.

Section 1: Trust and security in the cloud

In this section of the course, you explore today's top cybersecurity threats and how they impact businesses, differences between cloud security and traditional on-premises security, and key security terms and concepts.

Introduction

In recent years, the rise of cloud computing has transformed the way that organizations store, process and manage their data.

However, with this increased reliance on the cloud, the need for robust security measures has become essential.

Securing data, applications, and infrastructure in the cloud, a complex and ever evolving challenge.

As new threats and vulnerabilities emerge, organizations must stay ahead of the curve and adapt their security strategies to mitigate risks effectively.

In this section of the course, you'll define key security terms and concepts.

Describe the importance of confidentiality, integrity, availability control, and compliance in a cloud security model.

Differentiate between cloud security and traditional on premises security and describe today's top cybersecurity threats and business implications.

Part 1: Key security terms and concepts

In the field of Cloud security.

Understanding the terminology is crucial to navigating the landscape effectively.

In this lesson, we introduce essential security terms and concepts that are commonly encountered when discussing Cloud security.

Let's explore these terms and their significance.

The first three concepts relate to reducing the risk of unauthorized access to sensitive data.

The privileged access security model grants specific users access to a broader set of resources than ordinary users.

For example, a system administrator may have privileged access to perform tasks such as troubleshooting and data restoration.

However, the misuse of privileged access can pose risks.

It's essential to manage and monitor such access carefully.

The least privileged security principle advocates granting users only the access they need to perform their job responsibilities.

By providing the minimum required access, organizations can reduce the risk of unauthorized access to sensitive data.

For instance, a sales representative might only need access to a customer relationship management CRM system without requiring access to other systems like payroll or finance.

The zero-trust architecture security model assumes that no user or device can be trusted by default.

Every user and device must be authenticated and authorized before accessing resources.

Zero-trust architecture helps ensure robust security by implementing strict access controls and continuously verifying user identities.

The next three concepts relate to how an organization can protect itself from cyber threats.

Security by default is a principle that emphasizes integrating security measures into systems and applications from the initial stages of development.

By prioritizing security throughout the entire process, organizations can establish a strong security foundation in their Cloud environments.

Security posture refers to the overall security status of a Cloud environment.

It indicates how well an organization is prepared to defend against cyber attacks by evaluating their security controls, policies, and practices.

Cyber resilience refers to an organization's ability to withstand and recover quickly from cyber attacks.

It involves identifying, assessing and mitigating risks, responding to incidents effectively, and recovering from disruptions quickly.

Finally, let's explore essential security measures to protect Cloud resources from unauthorized access.

A firewall is a network device that regulates traffic based on predefined security rules.

You can think of a firewall like a security guard for a network.

It follows certain rules to decide which traffic is allowed to enter or leave a network.

These rules help keep unauthorized people or harmful things away from important Cloud resources, such as servers, databases, and applications.

Following our previous analogy, a security guard checks everyone who wants to enter and only lets in those who have permission.

Similarly, a firewall checks the incoming and outgoing traffic in a network and only allows the ones that are safe and authorized.

Encryption is the process of converting data into an unreadable format by using an encryption algorithm.

Decryption, however, is the reverse process that uses an encryption key to restore encrypted data back to its original form.

Safeguarding the encryption key is crucial because it holds the secret algorithm necessary for decrypting the data.

Another way to think about encryption and decryption is writing a message in a secret language that only you and the person you want to send it to can understand.

This way, even if someone intercepts the message, they won't be able to read it because they don't know the secret language.

Part 2: Cloud security components

In this Lesson, we learn about the components that make up a cloud security model and discover how they contribute to a robust security posture in today's digital landscape.

We'll first explore three essential aspects of security; confidentiality, integrity, and availability.

These three principles form the foundation of the CIA Triad, a widely used model for developing effective security systems.

The CIA Triad emphasizes the importance of protecting sensitive information, ensuring data accuracy and trustworthiness, and maintaining uninterrupted access to resources and services.

By understanding and implementing measures to address these aspects, organizations can establish a strong security framework to safeguard their digital assets.

Confidentiality is about keeping important information safe and secret.

It ensures that only authorized people can access sensitive data, no matter where it's stored or sent.

Confidentiality is of utmost importance in the Cloud, as sensitive information stored and transmitted across Cloud environments must be protected from unauthorized access or disclosure.

Encryption plays a crucial role in ensuring confidentiality in the Cloud.

By using encryption techniques and safeguarding encryption keys, organizations can ensure that only authorized individuals

can access and decrypt sensitive data, effectively mitigating the risk of data breaches in the Cloud.

Integrity means keeping data accurate and trustworthy.

It ensures that information doesn't get changed or corrupted no matter where it's stored or how it's moved around.

You can think of it like making sure a message doesn't get altered during delivery.

Integrity in the Cloud involves ensuring the accuracy and trustworthiness of data throughout its life cycle.

Implementing data integrity controls such as checksums or digital signatures, enables organizations to verify the authenticity and reliability of their data in the Cloud.

This helps prevent unauthorized modifications or tampering ensuring the integrity of critical information stored and processed in Cloud environments.

Availability is all about making sure that Cloud systems and the services are always accessible and ready for use by the right people when needed.

It's like having a reliable electricity supply that never goes out.

Cloud environments must be designed with redundancy fail-over mechanisms and disaster recovery plans to maximize availability and minimize downtime.

By implementing these measures, organizations can ensure that their systems and applications in the Cloud remain accessible whenever needed, promoting business continuity even in the face of potential disruptions.

Control refers to the measures and processes implemented to manage and mitigate security risks.

It involves establishing policies, procedures, and technical safeguards to protect against unauthorized access, misuse, and potential threats.

Control measures in the Cloud include implementing robust authentication mechanisms, access restrictions, and security awareness training.

These measures help organizations manage and mitigate security risks associated with Cloud based systems.

By ensuring that only authorized individuals have access to sensitive data and systems in the Cloud, organizations can reduce the risk of data breaches and unauthorized activities.

Finally, compliance relates to adhering to industry regulations, legal requirements, and organizational policies.

It involves ensuring that security practices and measures align with established standards and guidelines.

Meeting compliance standards in the Cloud demonstrates an organization's commitment to data privacy and security building trust with stakeholders and minimizing legal and financial risks.

Cloud providers often offer compliance frameworks and certifications that organizations can leverage to meet their regulatory obligations.

By integrating these principles into a comprehensive Cloud security model, organizations can establish a strong foundation for protecting their data, maintaining data integrity, and ensuring continuous access to critical resources.

Point 3: Cloud security versus traditional on-premises security 

In the past, businesses heavily relied on their own infrastructure and local data centers, to manage and protect their digital assets.

They had complete control over their hardware, software, and network components, fostering a sense of trust within their premises.

However, as organizations now connect digitally with customers, partners, and employees, worldwide new risks have emerged that require enhanced security measures.

This is where Cloud service comes into play, by offering a different approach compared to traditional on premises security.

Let's explore these important differences.

The first is, location.

Cloud security involves hosting and managing data and applications in off site data centers operated by cloud service providers.

The responsibility for securing the infrastructure and underlying hardware lies with the cloud provider.

Conversely, traditional on premises security involves hosting and managing data and applications locally on an organization's

own servers and infrastructure, granting direct control and responsibility for securing the physical and virtual environment.

Next is responsibility.

In a Cloud model, the cloud service provider is responsible for securing the infrastructure, network, and physical facilities.

The customer is typically responsible for securing their data, applications, user access, and configurations.

On the other hand, in an on premises set up, the organization is responsible for securing the entire infrastructure including hardware, network, operating systems, applications, and data.

The next difference is scalability.

Cloud security offers scalability and elasticity, which allows organizations to easily scale their resources up or down based on demand.

This flexibility is suitable for dynamic workloads and rapid growth.

In contrast, on premises security requires organizations to provision and maintain their own infrastructure, which can be more time consuming and costly when they scale up or down.

Next is maintenance and updates.

Cloud service providers handle infrastructure maintenance, including security updates, patching, and software upgrades.

Customers can focus on managing their applications and data without worrying about the underlying infrastructure.

On premises environments require organizations to maintain and update their own infrastructure involving regular tasks such as patching, software updates, and hardware upgrades.

The final difference is capital expenditure.

Cloud security follows an operational expenditure OpEx model, where organizations pay for the services they consume on a subscription basis.

This eliminates the need for large, upfront capital investments in physical security infrastructure.

Traditional on premises security models involve significant capital expenditure CapEx, because organizations must purchase and maintain their own security infrastructure.

Understanding these differences between Cloud service and traditional on premises security helps organizations make informed decisions about the most suitable approach for their specific needs.

Cloud service offers benefits such as offloading infrastructure management, scalability, and cost flexibility.

However, traditional on premises security provides direct control over the entire infrastructure.

Organizations must carefully evaluate their requirements and consider factors such as data sensitivity, compliance regulations, and scalability to determine the most effective security strategy for their business.

Point 4: Cybersecurity threats

In today's fast-paced digital world, we're bombarded with attention-grabbing headlines, COEs Beware: The Perils of Career Ending Cyberattacks.

Retailer Pays a Hefty $179 Million Due to Data Breach Fallout.

Credit Agency Settles US Data Breach, Facing up to $700 Million in Penalties.

The realm of cyberattacks is evolving rapidly, and these threats can emerge from unexpected sources, even disguised as government entities.

So what are some common cybersecurity threats faced by organizations?

First is deceptive social engineering, imagine that a skilled manipulator is seeking to extract confidential system information from unsuspecting individuals.

These cybercriminals employ phishing attacks, which collect personal details about you, your employees, or your students.

They skillfully craft tailored emails and mimic authenticity to deceive their targets.

Therefore, anyone within your organization can be tricked into inadvertently downloading malicious attachments, divulging passwords, or compromising sensitive data.

Next is physical damage, whether it be damage to hardware components, power disruptions, or natural disasters such

as floods, fires, and earthquakes, organizations are responsible for safeguarding data even in the face of physical adversity.

You can think of this as protecting precious assets amidst nature's unforgiving forces.

Another threat is malware, viruses, and ransomware, these digital adversaries architect chaos within the cyber domain.

Employing malicious software, they aim to disrupt operations, inflict damage, or gain unauthorized access to computer systems.

The most insidious of these is ransomware, where crucial files are held hostage until a considerable ransom is paid, it's like witnessing the digital equivalent of a calculated extortion scheme.

The next cybersecurity threat is vulnerable third-party systems, imagine inviting a trusted ally into your domain only to discover that they inadvertently compromise your security.

Many organizations rely on third party-systems for essential functions such as finance, inventory management, or account operations.

However, without adequate security measures and regular evaluations, these systems can transform into potential threats, leaving data security vulnerable.

It's like using a tool that unwittingly introduces risks to your own treasured possessions.

The final threat is configuration mishaps.

Even the most seasoned experts make mistakes, misconfiguration occurs when errors arise during the setup or configuration of resources, which inadvertently exposes sensitive data and systems to unauthorized access.

Surveys consistently identify misconfiguration as the most prominent threat to cloud security.

In turn, adopting principles of least privilege and privileged access are imperative because they allow resource access only when explicitly required and authorized.

This is like granting access only to those who have earned your trust.

As technology continues to advance at an astonishing pace, organizations must invest in the right expertise to assess, develop, implement and maintain robust data security plans.


Quiz
Passing score: 75%

1.

Which is a benefit of cloud security over traditional on-premises security?

Only having to install security updates on a weekly basis.

Having physical access to hardware.

Increased scalability.

Large upfront capital investment.

2.

Which security principle advocates granting users only the access they need to perform their job responsibilities?

Zero-trust architecture

Security by default

Privileged access

Least privilege

3.

Which is the responsibility of the cloud provider in a cloud security model?

Managing the customer's user access.

Securing the customer's data.

Configuring the customer's applications.

Maintaining the customer's infrastructure.

4.

What common cybersecurity threat involves tricking users into revealing sensitive information or performing actions that compromise security?

Phishing

Ransomware

Configuration mishap

Malware

5.

Which definition best describes a firewall?

A security model that assumes no user or device can be trusted by default

A software program that encrypts data to make it unreadable to unauthorized users

A set of security measures designed to protect a computer system or network from cyber attacks

A network security device that monitors and controls incoming and outgoing network traffic based on predefined security rules

6.

Which three essential aspects of cloud security form the foundation of the CIA triad?

Containers, infrastructure, and architecture

Certificates, intelligence, and authentication

Confidentiality, integrity, and availability

Compliance, identity, and access management

7.

Which cybersecurity threat occurs when errors arise during the setup of resources, inadvertently exposing sensitive data and systems to unauthorized access?

Virus

Malware

Phishing

Configuration mishaps

8.

Which cybersecurity threat demands a ransom payment from a victim to regain access to their files and systems.

Trojan

Spyware

Virus

Ransomware

9.

Which cloud security principle relates to keeping data accurate and trustworthy?

Integrity

Control

Compliance

Confidentiality

10.

Which cloud security principle ensures that security practices and measures align with established standards and guidelines?

Control

Confidentiality

Integrity

Compliance


Section 2: Google’s Trusted Infrastructure

In this section of the course, you focus on the security benefits of Google's data centers, the role encryption plays in securing data, the difference between authentication, authorization, and auditing, and how Google products can help protect against network attacks.

Introduction

At Google Cloud, we believe in going beyond reliance on a single technology for security.

Our multi-layered strategy builds progressive security layers.

Combining global data centers, purpose-built servers, custom security hardware and software, and two-step authentication.

This approach provides true defense in depth.

In this section of the course, you'll learn about how Google designs and builds its own data centers by using purpose built servers, networking and custom security hardware and software,

the role that encryption plays in securing an organization's data in the ways that it

can protect data exposed to risks in different states, the differences between authentication, authorization, and auditing.

The benefits of using two-step verification and Cloud Identity and Access Management or IAM.

How an organization can protect against network attacks by using Google products and security operations in the Cloud and its related business benefits.

Point 1: Data centers

Data centers are more than just facilities filled with computers.

They're the backbone of round the clock operations for Google services, including search, Gmail, and YouTube.

Moreover, they play a crucial role in storing and processing data for all the services provided on Google Cloud.

At present, Google operates over 30 state of the art data centers worldwide, with some still under construction.

These advanced facilities are meticulously designed to deliver exceptional reliability, top notch security, and outstanding efficiency.

And they ensure that Google services are always available when you need them.

But it doesn't stop there.

Google is committed to minimizing the environmental impact of data centers.

By using cutting edge technologies and renewable energy sources, we strive to reduce our ecological footprint.

Let's explore the benefits of Google designing and building its own data centers using purpose built servers, advanced networking solutions, and custom security hardware and software.

One of the greatest advantages of Google's data centers is the implementation of a zero-trust architecture, which ensures enhanced security at every level.

Our custom hardware and software are purpose built with features like tamper, evident hardware.

Secure boot, and hardware based encryption, which establish a strong security posture within the data center environment.

Physical security is paramount as well, with robust access control measures and biometric authentication in place.

By adopting the principle of least privilege, only authorized personnel have access to the data centers, which minimizes the risk of physical breaches and maintains a privilege access framework.

Furthermore, our data centers embody the concept of security by default.

From the moment you step into a Google data center, you can trust that every aspect has been designed and implemented with your security in mind.

With cyber resilience as a core principle, our data centers are equipped to withstand and recover from potential security incidences and ensure the continuity and integrity of your data.

Efficiency is another important aspect of our data centered design.

Purpose-built servers are optimized for specific tasks.

Which allows them to perform at great speed and with exceptional efficiency.

This reduces energy consumption, cuts down on operating costs, and saves resources in the environment.

In fact, we measure our success through the power usage effectiveness, or PUE score.

By continually striving for the lowest PUE scores, we ensure maximum efficiency in our data centers, leading to significant cost savings and a reduced carbon footprint.

For instance, our data center in Hamina, Finland, stands out as one of the most advanced and efficient facilities in our fleet.

Its innovative cooling system, which uses seawater from the Bay of Finland, sets a new standard for energy efficiency worldwide.

Scalability is another benefit.

Our data centers can quickly and seamlessly accommodate new hardware and servers, which allows us to scale up computing resources on demand.

This flexibility is critical for Google to handle massive data volumes and traffic without any disruptions to services.

Furthermore, managing our own servers and network provides us with unparalleled customization capabilities.

This level of flexibility empowers us to deliver unique services and capabilities that are not available from other providers, giving you access to exclusive features and innovations.

Although designing and building data centers require significant upfront investment, the long-term benefits are substantial.

By optimizing resource for efficiency and scalability, Google can significantly reduce energy consumption and operating costs, which results in remarkable savings over time.

Point 2: Secure storage

Previously, we learned that encryption is like a secret code that transforms data into an unreadable format using special algorithms.

This process ensures that only those with the right key or password can make sense of the data.

It's like using a secret language to protect your information.

By encrypting your data, you can protect it from various risks such as unauthorized access, loss, or damage.

Imagine your data is locked away in a safe.

Without the right key, no one can steal it, tamper with it, or even understand the information inside.

Let's take a closer look at how encryption protects your data in different states.

When data is at rest, it's stored on physical devices like computers or servers.

By encrypting data at rest, even if someone gains physical access to the device, they won't be able to decipher the data without the encryption key.

At Google Cloud, we automatically encrypt all customer content at rest, without any effort required from you.

It's a free and built in feature that adds an extra layer of protection to your valuable data.

If you prefer to manage your encryption keys yourself, you can use our Cloud key management service, Cloud KMS, for added control.

When data is in transit, it's moving over networks or the Internet.

Encryption plays a crucial role here by shielding your data from interception by cyber criminals or unauthorized parties.

It's like sending your information in a locked box that only the intended recipient can open.

At Google Cloud, we employ robust security measures to ensure the authenticity, integrity, and privacy of your data during transit.

We encrypt and authenticate data at multiple network layers, especially when it travels outside the physical boundaries we control.

This way, your information remains safe and secure as it journeys through the digital world.

Data in use refers to data being actively processed by a computer.

Encrypting data in use adds another layer of protection, especially against unauthorized users who might physically access the computer.

We use a technique called memory encryption, which locks your data inside the computer's memory, making it nearly impossible for unauthorized users to gain access to it.

When it comes to encryption algorithms, the advanced encryption standard AES takes center stage.

AES is a powerful encryption algorithm trusted by governments and businesses worldwide.

It's like having a top secret encryption method that keeps your data safe from prying eyes.

Whether your data is resting, traveling, or actively in use, encryption acts as your loyal guardian because it ensures its confidentiality and protection.

At Google Cloud we take encryption seriously to provide you with a secure storage solution you can trust.

Point 3: Identity

Often referred to as the 3As, authentication, authorization, and auditing, are important aspects of cloud identity management used to ensure secure access, manage user privileges, and monitor resource usage.

Let's begin with the first A, authentication.

It serves as a gatekeeper because it verifies the identity of users or systems that seek access.

Authentication involves presenting unique credentials, such as passwords, physical tokens, or biometric data, like fingerprints or voice recognition.

Think of it as presenting your identification card before entering a restricted area.

By validating the credentials provided, the server confirms that you are who you claim to be.

Two-step verification, which you may also hear being referred to as two-factor authentication or multi-factor authentication, is a security feature that adds an extra layer of protection to Cloud-based systems.

With two SV enabled, users need to provide two different pieces of information to log in.

For example, it could be a combination of a password and a code sent to their phone through text message, voice call, or an app like Google Authenticator.

This powerful feature makes unauthorized access more difficult, even if someone manages to obtain your password The second A is authorization.

After a user's identity is authenticated, authorization steps in to determine what that user or system is allowed to do within the system.

Think of it as the access control mechanism.

Different permissions are assigned to individuals or groups based on their roles, responsibilities, and organizational hierarchy.

For example, a system administrator might have the authority to create and remove user accounts, whereas a standard user might only be able to view a list of other users.

This fine-grain control ensures that each user has the appropriate level of access to perform their tasks while preventing unauthorized actions.

The third A, auditing, sometimes referred to as accounting, plays a critical role in monitoring and tracking user activities within a system.

By collecting and analyzing logs of user activity, system events, and other data, auditing helps organizations detect anomalies, security breaches, and policy violations.

It provides a comprehensive record of actions taken on a system or resource which proves invaluable during security incident investigations, compliance tracking, and system performance evaluation.

Just like the surveillance cameras in a shopping mall, auditing keeps a watchful eye on activities happening within your system.

To provide granular control over who has access to Google Cloud resources and what they can do with those resources, organizations can use identity and access management, or IAM.

With IAM, you can create and manage user accounts, assign roles to users, grant and revoke permissions to resources, audit user activity, and monitor your security position.

It provides a centralized and efficient approach to managing access control within your Google Cloud environment.

Imagine IAM as your organization's security headquarters, equipped with robust tools to manage and safeguard your digital assets.

By integrating IAM into your Google Cloud security strategy, you can ensure fine-grained access control, enhanced visibility, and centralized resource management.

This empowers you to protect your organization's sensitive data and resources effectively.

Point 4: Network security

When you expand your network to include Cloud environments, security considerations take on a whole new dimension.

Unlike traditional on-premises setups with clear perimeters, the Cloud brings new possibilities and challenges.

Let's explore some strategies to secure your organization's network and ensure the safety of your valuable data and workloads in Google Cloud.

Embrace the power of zero trust networks.

In the world of security, trust shouldn't be given freely, with Google Cloud's BeyondCorp enterprise, you can implement a zero trust security model.

It means that every access request is thoroughly verified and both the user's identity and context are considered.

This way you maintain strict control over who can access your network and resources, both inside and outside your organization.

Secure your connections to on-premises and multi-Cloud environments.

Many organizations have a mix of Cloud and on-premises workloads or they use multiple Cloud providers for resiliency.

Ensuring secure connectivity across these environments is crucial.

Google Cloud provides private access methods through services like Cloud VPN and Cloud Interconnect, which let you establish secure connections between your on-premises networks and Google Cloud resources.

Protect your perimeter with Google Cloud's powerful tools.

Google Cloud offers various methods to help secure your perimeter, including firewalls and virtual private Cloud

VPC service controls, which help you divide your Cloud into different sections and keep them secure.

You can also utilize shared VPC, which is like having a large fence that separates each Google Cloud project so they can work independently and safely.

With these tools, you can keep your Cloud environment protected and give different teams their own space to work in.

Stay ahead with a web application firewall, External web applications and services are often targeted by cyber threats, including DDoS attacks.

DDoS, which stands for a Distributed Denial-of-Service, is a cyber attack that uses multiple compromised computer systems to

flood a target with more traffic than it can handle, which causes a denial of service to legitimate users.

Google Cloud Armor comes to the rescue by providing robust DDoS protection.

It's like a force field that stops harmful attacks and keeps your website or application safe from things that can make it stop working properly.

Automate infrastructure provisioning for enhanced security.

By adopting automation tools, you can create immutable infrastructure, which means that it can't be changed after provisioning.

Think of infrastructure provisioning tools as your personal assistance for setting up and maintaining your Cloud environment.

When you use tools like Terraform, Jenkins, and Cloud Build, they handle all the behind the scenes work to create a secure and reliable Cloud environment.

It's like having a team of efficient workers who build and organize everything you need to run your environment smoothly.

With these tools, your Cloud environment becomes a well designed workspace where everything has its place and functions perfectly.

The best part is when it's set up, it stays that way, no unexpected changes or disruptions.

If anything does go wrong, these tools are there to quickly identify and fix any issue and ensure that your Cloud environment keeps running smoothly.

These examples illustrate just a few of the ways organizations use Google Cloud to fortify their networks against attacks.

Your specific network set up and security measures will depend on your unique business requirements and risk tolerance.

Point 5: Security operations

SecOps, short for security operations, is all about protecting your organization's data and systems in the Cloud.

It involves a combination of processes and technologies that help reduce the risk of data breaches, system outages, and other security incidents.

Think of it as your secret weapon for keeping your valuable data safe.

Let's explore some of the essential activities involved in SecOps.

Vulnerability management, is the process of identifying and fixing security vulnerabilities in Cloud infrastructure and applications.

It's like regularly checking your castle walls for weak spots.

Google Cloud's security command center, or SCC, provides a centralized view of your security posture.

It helps to identify and fix vulnerabilities, and it ensures that your infrastructure remains solid and protected.

Another crucial activity is log management.

It's like having a watchful eye on your castle grounds, looking out for any suspicious activity.

Google Cloud offers a Cloud logging a service to collect and analyze security logs from your entire Google Cloud environment.

It helps you detect and respond to any signs of trouble.

It ensures you anticipate any potential threats.

Of course, being prepared for security incidents is equally important.

This is where incident response comes in.

Imagine having a team of knights ready to defend your castle at a moment's notice.

Google Cloud has expert incident responders across various domains who are equipped with the knowledge and tools to tackle any security incident swiftly and effectively.

Another crucial aspect of SecOps is educating your employees on security best practices.

Just like teaching everyone in the castle to be vigilant and lock the gates, security awareness

training helps prevent incidents by raising awareness and empowering employees to protect themselves and the organization.

Now you might be wondering, why should your organization implement SecOps?

Well, here are the benefits, reduced risk of data breaches.

SecOps helps identify and fix vulnerabilities, which significantly reduces the risk of data breaches.

Increased up time, a swift and effective incident response minimizes the impact of outages on your business operations, which ensures smoother and uninterrupted services.

Improved compliance, SecOps helps with meeting security regulations such as the General Data Protection Regulation or GDPR, and keeps your organization in good standing.

Enhanced employee productivity, by educating employees on security best practices, SecOps minimizes the risk of human error and promotes a more secure and productive work environment.

SecOps is an integral part of your organization's security strategy.

By implementing SecOps practices, you can fortify your defenses, reduce security risks, and protect your data in the ever changing landscape of Cloud security.


Quiz
Passing score: 75%

1.

What security feature adds an extra layer of protection to cloud-based systems?

Two-step verification (2SV)

Security information and event management (SIEM)

Firewall as a service (FaaS)

Data loss prevention (DLP)

2.

Google Cloud encrypts data at various states. Which state refers to when data is being actively processed by a computer?

Data at rest

Data in use

Data in transit

Data lake

3.

Which practice involves a combination of processes and technologies that help reduce the risk of data breaches, system outages, and other security incidents in the cloud?

Zero trust security

Site reliability engineering (SRE)

Cloud security posture management (CSPM)

Security operations (SecOps)

4.

What Google Cloud product provides robust protection from harmful distributed denial-of-service (DDoS) attacks?

Cloud Load Balancing

Cloud IAM

Google Cloud Armor

Cloud Monitoring

5.

What metric does Google Cloud use to measure the efficiency of its data centers to achieve cost savings and a reduced carbon footprint?

Total cost of ownership (TCO)

Power Usage Effectiveness (PUE)

Energy Efficiency Ratio (EER)

Data Center Infrastructure Efficiency (DCiE)

6.

Which aspect of cloud identity management verifies the identity of users or systems?

Auditing

Authorization

Accounting

Authentication

7.

Which is a powerful encryption algorithm trusted by governments and businesses worldwide?

Lattice-Based Cryptography (LBC)

Isomorphic encryption (IE)

Advanced Encryption Standard (AES)

Post-quantum cryptography (PQC)

8.

Select the correct statement about Identity and Access Management (IAM).

IAM is a cloud security information and event management solution that collects and analyzes log data from cloud security devices and applications.

IAM provides granular control over who has access to Google Cloud resources and what they can do with those resources.

IAM is a system that detects and prevents malicious traffic from entering a cloud network.

IAM is a cloud service that encrypts cloud-based data at rest and in transit.


Section 3: Google cloud's Trust Principles and Compliance

In this section of the course, you learn how Google Cloud's trust principles, transparency reports, and independent third-party audits support customer trust. You also explore the importance of data sovereignty and data residency, and how the Google Cloud compliance resource center and Compliance Reports Manager supports industry and regional compliance needs.


Introduction

At Google we know that privacy plays a critical role in earning and maintaining trust.

Customers need to be sure that their data and applications are safe and secure and so Google Cloud has a strong set of trust principles and

compliance programs in place, which are designed to protect customer data and meet the needs of a wide range of customers, from small businesses to large enterprises.

In this final section of the course, you learn about Google seven Trust principles, data residency and data sovereignty options

with Google Cloud, and how the Google Cloud Compliance Resource Center and Compliance reports manager support industry and regional compliance needs.


Point 1: The google cloud trust principles and transparency

At Google, we believe in transparency and want you to have complete confidence in our services.

Google Cloud's trust principles are designed to empower you and ensure that the security and control of your business data isn't compromised.

Let's review these principles.

One, you own your data, not Google.

We prioritize your control and let you access, export, delete, and manage data permissions within Google Cloud.

Two, Google does not sell customer data to third parties.

We safeguard your data from being used for Google's marketing or advertising purposes.

Three, Google Cloud does not use customer data for advertising.

Your data remains confidential because Google Cloud ensures that it's never utilized to target ads.

Four, all customer data is encrypted by default.

Your data is protected with robust encryption because Google Cloud safeguards it even in the unlikely event of unauthorized access.

Five, we guard against insider access to your data.

We implement stringent security measures to prevent unauthorized employee access to customer data.

Six, we never give any government entity backdoor access.

Your data remains secure and no government entity can access it without proper authorization.

Seven, our privacy practices are audited against international standards.

We undergo regular audits to ensure compliance with rigorous privacy standards.

Transparency reports and independent audits transparency are a core element of our commitment to trust.

We provide valuable insights and accountability through our transparency reports, which shed light on government and corporate actions that affect privacy, security and access to information.

These reports let you stay informed and maintain trust in our services.

Additionally, Google Cloud undergoes independent third party audits and certifications.

This verification process ensures that our data protection practices align with our commitments and industry standards.

Our participation in initiatives like the EU Cloud Code of Conduct further reinforces our dedication to accountability, compliance support, and robust data protection principles.

Point 2: Data residency and data sovereignty

When it comes to storing data and keeping it secure, data sovereignty and data residency are two important concepts to understand.

Data sovereignty refers to the legal concept that data is subject to the laws and regulations of the country where it resides.

For example, the General Data Protection Regulation GDPR in the European Union requires companies to comply

with data protection laws when processing or storing personal data of eu citizens, regardless of their location.

This ensures that individuals have control over their personal data and its usage.

In contrast, data residency refers to the physical location where data is stored or processed.

Some countries or regions have laws or regulations that require data to be stored within their borders.

For instance, some countries mandate that personal data of its citizens must be stored on servers within the country.

This ensures data remains within the jurisdiction of local laws.

Now let's explore how Google Cloud addresses data residency requirements.

We offer a range of options to control the physical location of your data through regions.

Each region consists of one or more data centers, which lets you choose where your data resides.

For example, within the European Union, you can select regions located in various countries like the UK, Belgium, Germany, Finland, Switzerland, and the Netherlands.

By configuring your resources in specific regions, Google ensures that your data is stored only within the selected region, as stated in our service specific terms.

Additionally, Google Cloud provides organization policy constraints coupled with IAM configuration to prevent accidental data storage in the wrong region.

These controls offer peace of mind and reinforce your data residency requirements.

Furthermore, Google Cloud offers features like VPC service controls, which let you restrict network.

Access to data based on defined perimeters.

You can limit user access through IP address filtering, even if they have authorization.

Google Cloud Armor lets you restrict traffic locations for your external load balancer by adding an extra layer of protection.

By using these capabilities, organizations can adhere to data residency and data sovereignty requirements.

Ensure compliance, and maintain control over their valuable data within the Google cloud ecosystem.

Point 3: Industry and regional compliance

As organizations migrate to Cloud, it becomes essential to protect sensitive workloads while ensuring compliance with diverse regulatory requirements and guidelines.

Compliance is a critical aspect of the Cloud journey, because not meeting regulatory obligations can have far reaching consequences.

To assist you in achieving compliance, Google Cloud offers robust resources and tools tailored to support your specific needs.

First, let's explore the Google Cloud Compliance Resource center.

This comprehensive hub provides detailed information on the certifications and compliance standards we satisfy.

You can find mappings of our security, privacy and compliance controls to global standards.

This transparency lets you validate our adherence to industry leading practices.

The resource center also offers valuable documentation on regional and sector specific regulations, and empowers you to navigate complex compliance landscapes.

Imagine you're a healthcare organization subject to HIPAA regulations, which protect sensitive patient health information from being disclosed without the patient's

consent or knowledge, the Resource Center equips you with the necessary insights and documentation to align your compliance efforts with HIPAA requirements.

Similarly, if you operate within the financial sector, you'll find guidance on meeting regulations like PCI DSS, which stands for payment card industry data security standard.

Google Cloud's Compliance Resource Center is your go to source for actionable information and support.

In addition to the resource center, we provide the compliance reports manager; a powerful tool at your disposal.

This intuitive platform offers easy on demand access to critical compliance resources at no extra cost.

Within the compliance reports manager, you'll discover our latest ISO/IEC certificates, SOC reports, and self-assessments.

These resources provide evidence of our adherence to rigorous compliance standards, and help streamline your own reporting and compliance efforts.

Imagine you're enterprise seeking ISO/IEC 27001 certification.

The compliance reports manager lets you access the necessary documentation efficiently, and it saves you time and effort in the certification process.

With this tool, we aim to simplify your compliance journey and empower you to meet your regulatory obligations effectively.

By using the Google Cloud Compliance Resource Center and the Compliance Reports Manager, you can navigate the complex realm of industry and regional compliance with confidence.

Our dedicated teams of engineers and compliance experts work hand in hand with you to address your specific regulatory needs.

Together, we create an integrated controls and governance framework, while we ensure a robust compliance posture.

You can visit the Compliance Resource Center at cloud.google.com/security/compliance, and explore the Compliance Reports Manager at cloud.google.com/ security/compliance/compliance-reports-manager.


Quiz
Passing score: 75%

1.

Which term describes the concept that data is subject to the laws and regulations of the country where it resides?

Data consistency

Data redundancy

Data sovereignty

Data residency

2.

Which Google Cloud feature allows users to control their data's physical location?

Areas

Regions

Districts

Places

3.

Which report provides a way for Google Cloud to share data about how the policies and actions of governments and corporations affect privacy, security, and access to information?

Transparency reports

Security reports

Compliance reports

Billing reports

4.

Where can you find details about certifications and compliance standards met by Google Cloud?

Google Cloud console

Marketplace

Cloud Storage client libraries

Compliance resource center

5.

Which is one of Google Cloud’s seven trust principles?

Google sells customer data to third parties.

Google Cloud uses customer data for advertising.

We give "backdoor" access to government entities when requested.

All customer data is encrypted by default.


Chapter 5 summary

This brings us to the end of the trust and security with Google Cloud course.

Let's do a quick recap.

In the first section of the course titled trust and security in the Cloud, you explored important security terms and concepts, the difference between Cloud security

and traditional on premises security, today's top cybersecurity threats and business implications, and the importance of control, compliance, confidentiality, integrity, and availability in a cloud security model.

In the second section of the course titled Google's trusted infrastructure, you learned about how Google

designs and builds data centers, the role encryption plays in securing an organization's data, the differences between

authentication, authorization, and auditing, the benefits of using two step verification and IAM, ways to protect against

network attacks by using Google products, and security operations in the Cloud and its related business benefits.

Finally, in the third section of the course titled, Google Cloud's trust principles and compliance, you examined Google Seven trust principles, data residency and data sovereignty options with Google Cloud.

How the Google Cloud compliance resource center and Compliance Reports Manager support industry and regional compliance needs.

Now that you had a comprehensive introduction to trust and security on Google Cloud, you can move on to

the final course in the Cloud digital leader series, scaling with Google Cloud operations, where you'll learn about how

Google Cloud supports an organization's financial governance and ability to control their cloud costs, the basic concepts of modern

operations, reliability and resilience in the Cloud, and how Google Cloud helps organizations meet sustainability goals and reduce environmental impact.

We'll see you next time.

Google Cloud Fundamental

Chapter 6: Scaling with Google Cloud Operations

Organizations of all sizes are embracing the power and flexibility of the cloud to transform how they operate. However, managing and scaling cloud resources effectively can be a complex task.

Scaling with Google Cloud Operations explores the fundamental concepts of modern operations, reliability, and resilience in the cloud, and how Google Cloud can help support these efforts.

Part of the Cloud Digital Leader learning path, this course aims to help individuals grow in their role and build the future of their business.

When you complete this course, you can earn the badge displayed here! View all the badges you have earned by visiting your profile page. Boost your cloud career by showing the world the skills you have developed!


Introduction

In today's digital landscape, organizations of all sizes are embracing the power and flexibility of the cloud to transform how they operate.

However, managing and scaling cloud resources effectively can be a complex task.

That's where cloud operations come in.

Cloud operations refer to the set of practices and strategies employed to ensure the smooth functioning, optimization, and scalability of cloud-based systems.

It involves managing and monitoring the infrastructure, applications, and services that run in the cloud, while adhering to best practices for reliability, performance, security, and cost optimization.

Cloud operations play a pivotal role in enabling organizations to achieve digital transformation goals, because they ensure the availability, efficiency, and resilience of critical systems.

So, with this in mind, let’s explore the goals of this course.

“Scaling with Google Cloud Operations” was designed to help you learn how Google Cloud supports an organization's ability to control their cloud costs through financial governance, understand the

fundamental concepts of modern operations, reliability, and resilience in the cloud, and explore how Google Cloud works to reduce our environmental impact and help organizations meet sustainability goals.

Throughout the course, you’re presented with graded knowledge assessments.

You must pass these assessments to receive course credit.

OK, let's get started!


Section 1: Financial Governance and Managing  Cloud Costs

In this section of the course, you learn about the essentials of cloud cost management, including cloud financial governance best practices, resource hierarchy access control, and cloud consumption control.

Introduction

Using cloud technology, either for business improvements or for large-scale transformation, can be challenging.

In fact, one of the common pain points many organizations face, regardless of which cloud provider they use, is managing cloud costs.

For large organizations especially, the transition from predictable capital expenditures for building and maintaining their IT infrastructure to agile operating expenditures using cloud resources requires process and organizational changes.

Managing cloud costs requires vigilance and real-time monitoring in parallel.

In fact, because almost anyone can now access cloud resources on demand, managing IT infrastructure costs no longer sits mainly with the finance team.

Instead, it involves more people across multiple teams.

So you might even be the person responsible for IT budgeting.

Whatever your role, understanding how using cloud technology affects the business from a cost perspective will help you maximize the value your organization gains from using the cloud.

In this first section of the course, you’ll explore the fundamentals of cloud cost management, cloud financial

governance best practices, ways to control access by using the resource hierarchy, and ways to control cloud consumption.


Point 1: Fundamentals of cloud financial governance

Easy access to cloud resources presents a need for precise, real-time control of what’s being consumed.

Having cloud financial governance, which is in part a set of processes and controls that organizations use to

manage cloud spend, can mean the difference between peace of mind and spiraling costs that lead to budget overruns.

As an organization adapts, it'll need a core team across technology, finance, and business functions

to work together to stay on top of cloud costs and make decisions in real time.

The variable nature of cloud costs impacts people, process, and technology.

Let’s explore these three areas, starting with people.

People refers to the different roles involved in managing cloud costs.

For small organizations, one person might fulfill multiple roles and be responsible for managing all aspects of a cloud infrastructure and associated finance.

From budgeting to procurement, tracking optimization, and more.

Large organizations, however, will likely look to a finance team to take on a financial planning and advisory role.

Using business priorities, a finance team is expected to make data-driven decisions on cloud spending, but

they might struggle to understand or monitor cloud spend on a daily, weekly, or monthly basis.

Then there are members of technology and line of business teams.

They can advise on how cloud resources are being used to meet the organization's overall business strategy and what additional resources might be needed throughout the upcoming year.

However, they don’t necessarily factor costs into their decision making.

To manage cloud costs effectively, a partnership across finance, technology, and business functions is required.

This partnership might already exist, or it may take the form of a centralized hub, such as a cloud center of excellence.

The central team would consist of several experts who ensure that best practices are in place across the organization and that there's visibility into the ongoing cloud spend.

The centralized group would also be able to make real-time decisions and discuss trade-offs when spending is higher than planned.

Now let’s transition from people to process.

On a daily or weekly basis, organizations should monitor and analyze their cloud usage and costs.

Then, on a weekly or monthly basis, the finance team should analyze the results, charge back the costs through

the appropriate teams, and determine whether any changes are needed to ensure that the organization's cloud spend is optimized.

Having a culture of accountability in place across teams helps organizations recognize waste, quickly act to eliminate it, and ensure they're maximizing their cloud investment.

It will also help drive cross-group collaboration across technology, finance, and business teams to ensure that their cloud spend aligns with broader business objectives.

And finally, there’s technology.

Google Cloud provides built-in tools to help organizations monitor and manage costs.

These tools help organizations gain greater visibility, drive a culture of accountability for cloud spending across the

organization, control costs to reduce risks of overspending, and provide intelligent recommendations to optimize cost and usage.

You’ll explore some of these tools later in this section.


Point 2: Cloud financial governance best practices

Let’s explore some cloud financial governance best practices that organizations can adopt to increase the predictability and control of their cloud resources.

The first best practice is to identify who manages cloud costs.

If it's a team, it should ideally be a mix of IT managers and financial controllers.

Because Cloud spending is decentralized and variable, it's important to establish a culture of accountability for costs across the organization.

Defining clear ownership for projects and sharing cost views with the departments and teams that are using cloud resources helps establish this accountability culture and more responsible spending.

As well as making teams accountable for their spending, Google Cloud financial governance policies and permissions make it easy to control who can spend and view costs across your organization.

In addition, Google Cloud offers flexible options to organize resources and allocate costs to individual departments and teams.

For example, budgets notify key stakeholders based on your actual or forecasted cloud costs.

Creating multiple budgets with meaningful alerts is an important practice for staying on top of your cloud costs.

The second best practice is to understand what kind of information can be found in an invoice versus cost management tools.

They’re not the same concept.

An invoice is a document that is sent by a cloud service provider to a customer to request payment for the services that were used.

However, a cost management tool is software to help track, analyze, and optimize cloud spend.

An organization is rarely only interested in how much they spend.

They also want to know why they spent that much.

Cost management tools, like those built into the Google Cloud console, are effective for answering the why.

They can provide granular data, uncover trends, and identify actions to take to control or optimize costs.

And this brings us to the third best practice for increasing the predictability and control of cloud resources: use Google Cloud’s cost management tools.

Google Cloud believes in supporting organizations by providing strong financial governance tools that make it easier for customers to align their strategic priorities with their cloud usage.

Before organizations can optimize their cloud costs, they first need to understand what they're spending, whether there are any trends, and what their forecasted costs are.

So, how can this be done?

Start by capturing what cloud resources are being used, by whom, for what purpose, and at what cost.

From there, determine who will be responsible for monitoring that information, who will be involved in managing costs, and how the spending information will be reported on an ongoing basis.

It's also important to set up the cadence and format for ongoing communication with main cloud stakeholders.

Having this plan outlined up front helps ensure that managing costs isn't an afterthought.

And how can you monitor current cost trends and identify areas of waste that could be improved?

Google Cloud provides built-in reporting capabilities, which can help your team gain visibility into costs.

Ideally, reports should be reviewed weekly, at a minimum.

One powerful tool is the Google Cloud Pricing Calculator.

The Pricing Calculator lets you estimate how changes to cloud usage will affect costs.

The calculator is available at cloud.google.com/products/calculator.

Now that you’ve had a chance to explore some cloud financial governance best practices, the next step is to implement them.

If this doesn’t fall into your scope of responsibility, be sure to pass on those best practices to the relevant stakeholders within your organization.


Point 3: Using the resource hierarchy to control access

One important cloud computing consideration involves controlling access to resources.

With on-premises infrastructure, physical access controls were used.

This method, however, is not as effective with resources stored in the cloud.

The Google Cloud resource hierarchy is a powerful tool that can be used to control access to cloud resources.

Much like the folder structure you use to organize and control access to your own files, this resource hierarchy is a tree-like structure that organizes resources into logical groups.

This makes it easier to manage resources and control.

Google Cloud’s resource hierarchy contains four levels, and starting from the bottom up they are: resources, projects, folders, and an organization node.

The first level, resources, represent virtual machines, Cloud Storage buckets, tables in BigQuery, or anything else in Google Cloud.

Resources are organized into projects, which sit on the second level.

Projects can be organized into folders, or even subfolders.

These sit at the third level.

And then at the top level is an organization node, which encompasses all the projects, folders, and resources in your organization.

It’s important to understand this resource hierarchy because it directly relates to how policies are managed and applied when you use Google Cloud.

A policy is a set of rules that define who can access a resource and what they can do with it.

Policies can be defined at the project, folder, and organization node levels.

Some Google Cloud services can also apply policies to individual resources.

The third level of the Google Cloud resource hierarchy is folders.

Folders let you assign policies to resources at the level of granularity that you choose.

The resources in a folder inherit policies and permissions assigned to that folder.

A folder can contain projects, other folders, or a combination of both.

Now that you understand the structure of the Google Cloud resource hierarchy, let’s explore some additional benefits of using it to control access to cloud resources.

First, the resource hierarchy provides granular access control, meaning you can assign roles and permissions

at different levels of the hierarchy, such as at the folder, project, or individual resource level.

Second, because the resource hierarchy follows inheritance and propagation rules, permissions set at higher levels of the resource hierarchy are automatically inherited by lower-level resources.

For example, if you grant a user access at the folder level, all projects and resources within that folder inherit those permissions by default.

This inheritance simplifies access management and reduces the need for manual configuration at each individual resource level.

Third, the resource hierarchy enhances security and compliance through least privilege principles.

By assigning access permissions at the appropriate level in the hierarchy, you can ensure that users only have the necessary privileges to perform their tasks.

This reduces the risk of unauthorized access and helps maintain regulatory compliance.

Finally, the resource hierarchy provides strong visibility and auditing capabilities.

You can track access permissions and changes across different levels of the hierarchy, which makes it easier to monitor and review access controls.

This improves accountability and helps identify and address potential security issues.


Point 4: Controlling cloud consumption

Organizations want to control cloud consumption for many reasons.

It could be about cost savings by ensuring they’re not overspending on unnecessary resources, increased visibility by providing a better understanding of how

resources are being used and identifying areas to reduce costs, or improved compliance by ensuring your cloud environment is compliant with industry regulations.

Google Cloud offers several tools to help control cloud consumption, including resource quota policies, budget threshold rules, and Cloud Billing reports.

Let’s define each of these terms.

Resource quota policies let you set limits on the amount of resources that can be used by a project or user.

They can help prevent overspending on cloud resources; therefore, they help you ensure that your cloud usage is within your budget.

Then there are budget threshold rules, which let you set alerts to be informed when your cloud costs exceed a certain threshold.

They can act as an early warning for potential cost overruns, and let you take corrective action before costs get out of control.

Both resource quota policies and budget threshold rules are set in the Google Cloud console.

And then there are Cloud Billing reports.

Whereas resource quota policies and budget threshold rules provide proactive means to control cloud consumption, Cloud Billing reports offer a reactive method

to help you track and understand what you’ve already spent on Google Cloud resources and provide ways to help optimize your costs.

You can use Cloud Billing reports to monitor costs by exporting billing data to BigQuery.

This means exporting usage and cost data to a BigQuery dataset, and then using the dataset for detailed analyses.

You can also visualize data with tools like Looker Studio.

After analyzing how you're spending on cloud resources, you might realize that your organization can optimize costs through committed use discounts (CUDs).

If your workloads have predictable resource needs, you can purchase a Google Cloud commitment, which gives you

discounted prices in exchange for your commitment to use a minimum level of resources for a specific term.


Quiz
Passing score: 75%

1.

Which feature lets you set limits on the amount of resources that can be used by a project or user?

Billing reports

Quota policies

Invoicing limits

Committed use discounts

2.

Which feature lets you set alerts for when cloud costs exceed a certain limit?

Budget threshold rules

Cost optimization recommendations

Billing reports

Cost forecasting

3.

Which Google Cloud tool lets you estimate how changes to cloud usage will affect costs?

Google Cloud Pricing Calculator

Cloud Trace

Cloud Billing

Cloud Monitoring

4.

Which term describes a centralized hub within an organization composed of a partnership across finance, technology, and business functions?

Competency center

Center of excellence

Center of innovation

Hub center

5.

Which represents the lowest level in the Google Cloud resource hierarchy?

Projects

Resources

Folders

Organization node

6.

Which offers a reactive method to help you track and understand what you’ve already spent on Google Cloud resources and provide ways to help optimize your costs?

Cost forecasting

Resource usage

Google Cloud Pricing Calculator

Cloud billing reports

7.

Why is it a benefit that the Google Cloud resource hierarchy follows inheritance and propagation rules?

Resources at lower levels can improve the performance of cloud applications.

Faster propagation can simplify a cloud migration.

Inheritance in the hierarchy reduces the overall cost of cloud computing.

Permissions set at higher levels of the resource hierarchy are automatically inherited by lower-level resources.


Section 2: Operational Excellence and reliability at scale

In this section of the course, you explore how to modernize operations with Google Cloud and the importance of designing resilient infrastructure and processes. You also learn about cloud reliability and Google Cloud Customer Care, including the life of a support case.

Introduction

In today's rapidly evolving digital landscape, organizations use cloud technology increasingly to drive innovation, agility, and efficiency.

However, harnessing the true power of the cloud requires a comprehensive understanding of operational excellence and reliability at scale.

Operational excellence and reliability refers to the ability of organizations to optimize their operations and ensure uninterrupted service delivery, even as they handle increasing workloads and complexities in the cloud.

This includes designing robust infrastructure, establishing resilient processes, and employing proactive monitoring and response mechanisms.

Imagine a global ecommerce platform that experiences a sudden surge in traffic during a major sale event.

To meet the increased demand, the platform needs to scale its resources rapidly while ensuring uninterrupted service availability.

Operational excellence here involves efficiently scaling the underlying infrastructure, automating resource provisioning, and implementing load balancing mechanisms.

Reliability focuses on minimizing downtime, employing fault-tolerant systems, and employing disaster recovery strategies.

By excelling in these areas, the ecommerce platform can handle the increased load seamlessly, deliver a consistently positive user experience, and avoid revenue loss or reputational damage.

In this section of the course, you explore modernizing operations by using Google Cloud, designing resilient infrastructure

and processes, the fundamentals of cloud reliability, Google Cloud Customer Care, and the life of a support case.


Point 1: Fundamentals of cloud reliability

Within any IT team, developers are responsible for writing code for systems and applications, and operators are responsible for ensuring that those systems and applications operate reliably.

Developers are expected to be agile and are often pushed to write and deploy code quickly.

Their aim is to release new functions frequently, increase core business value with new features, and release fixes fast for an overall better user experience.

In contrast, operators are expected to keep the system stable, and so they often prefer to work more slowly to ensure reliability and consistency.

Traditionally, developers pushed their code to operators who often had little understanding of how the code would run in a production or live environment.

When problems arise, it can be very difficult for either group to identify the source of the problem and resolve it quickly.

Worse, accountability between the teams isn’t always clear.

DevOps is a software development approach that emphasizes collaboration and communication between development and operations teams to enhance the efficiency, speed, and reliability of software delivery.

It aims to break down silos between these teams and foster a culture of shared responsibility, automation, and continuous improvement.

One particular concept within the DevOps framework is Site Reliability Engineering, or SRE, which ensures the reliability, availability, and efficiency of software systems and services deployed in the cloud.

SRE combines aspects of software engineering and operations to design, build, and maintain scalable and reliable infrastructure.

Monitoring is the foundation of product reliability.

It reveals what needs urgent attention and shows trends in application usage patterns, which can

yield better capacity planning and generally help improve an application client's experience and lessen their pain.

There are “Four Golden Signals” that measure a system’s performance and reliability.

They are latency, traffic, saturation, and errors.

Latency measures how long it takes for a particular part of a system to return a result.

Latency is important because it directly affects the user experience, changes could indicate emerging issues, its

values might be tied to capacity demands, and it can be used to measure system improvements.

Traffic measures how many requests reach your system.

Traffic is important because it’s an indicator of current system demand, its historical trends are used for capacity planning, and it’s a core measure when calculating infrastructure spend.

Saturation measures how close to capacity a system is.

It’s important to note, though, that capacity is often a subjective measure that depends on the underlying service or application.

Saturation is important because it's an indicator of how full the service is, it focuses

on the most constrained resources, and it’s frequently tied to degrading performance as capacity is reached.

And errors are events that measure system failures or other issues.

Errors are often raised when a flaw, failure, or fault in a computer program or system causes it to produce incorrect or unexpected results, or behave in unintended ways.

Errors are important because they can indicate something is failing, configuration or capacity issues, service level objective violations, or that it's time to send an alert.

Three main concepts in site reliability engineering are service-level indicators (SLIs), service-level objectives (SLOs), and service-level agreements (SLAs).

They are all types of targets set for a system’s Four Golden Signal metrics.

Service level indicators are measurements that show how well a system or service is performing.

They’re specific metrics like response time, error rate, or percentage uptime–which is the amount of time a system is available for use–that help us understand the system's behavior and performance.

Service level objectives are the goals that we set for a system's performance based on SLIs.

They define what level of reliability or performance that we want to achieve.

For example, an SLO might state that the system should be available for 9

9.9% of the time in a month.

Service level agreements are agreements between a cloud service provider and its customers.

They outline the promises and guarantees regarding the quality of service.

SLAs include the agreed-upon SLOs, performance metrics, uptime guarantees, and any penalties or remedies if the provider fails to meet those commitments.

This might include refunds or credits when the service has an outage that’s longer than this agreement allows.


Point 2: Designing resilient infrastructure and processes

When infrastructure and processes in a cloud environment are designed, they need to be resilient, fault-tolerant, and scalable, for high availability and disaster recovery.

High availability refers to the ability of a system to remain operational and accessible for users even if hardware or software failures

occur, whereas disaster recovery refers to the process of restoring a system to a functional state after a major disruption or disaster.

Let's explore some of the key design considerations and their significance in more detail.

Redundancy refers to duplicating critical components or resources to provide backup alternatives.

Redundancy can be implemented at various levels, such as hardware, network, or application layers.

For example, having redundant power supplies, network switches, or load balancers ensures that if one fails, the redundant component takes over seamlessly.

Redundancy enhances system reliability and mitigates the impact of single points of failure.

Replication involves creating multiple copies of data or services and distributing them across different servers or locations.

It ensures redundancy and fault tolerance by allowing systems to continue functioning even if certain components or servers fail.

By replicating data across multiple servers, the impact of hardware failures or outages is minimized, and the availability of services is improved.

Cloud service providers offer multiple regions or data center locations spread across different geographic areas.

By distributing resources across regions, businesses can ensure that if an entire region becomes unavailable due

to natural disasters, network issues, or other incidents, their services can continue running from another region.

This approach improves resilience and reduces the risk of prolonged service interruptions.

Building a scalable infrastructure allows organizations to handle varying workloads and accommodate increased demand without compromising performance or availability.

Cloud technologies enable the dynamic allocation and deallocation of resources based on workload fluctuations.

Autoscaling mechanisms can automatically adjust resource capacity to match demand, ensuring that services remain available and responsive during peak periods or sudden spikes in traffic.

Regular backups of critical data and configurations are crucial to ensure that if data loss, hardware failures, or cyber-attacks occur, organizations can restore their systems to a previous state.

Cloud providers often offer backup services, and they let organizations automate backups, store them securely, and easily restore data when needed.

Backups should be stored in geographically separate locations to protect against regional outages or disasters.

These measures improve high availability, allow for rapid recovery from disasters or failures, and minimize downtime and data loss.

It’s important to regularly test and validate these processes to ensure that they function as expected during real-world incidents.

Also, monitoring, alerting, and incident response mechanisms should be implemented to identify and address issues promptly, further enhancing the overall resilience and availability of the cloud infrastructure.


Point 3: Modernizing Operations by using Google cloud

If you've ever worked with on-premises environments, you know that you can physically touch the servers.

If an application becomes unresponsive, someone can physically determine why that happened.

In the cloud though, the servers aren't yours—they belong to the cloud provider—and you can’t physically inspect them.

So the question becomes: how do you know what's happening with your server, database, or application?

The answer is: by using Google’s integrated observability tools.

Observability involves collecting, analyzing, and visualizing data from various sources within a system to gain insights into its performance, health, and behavior.

To achieve this, Google Cloud offers an operations suite, which is a comprehensive set of monitoring, logging, and diagnostics tools.

It offers a unified platform for managing and gaining insights into the performance, availability, and health of applications and infrastructure deployed on Google Cloud.

Let's look at some of the managed services that constitute the operations suite.

Cloud Monitoring provides a comprehensive view of your cloud infrastructure and applications.

It collects metrics, logs, and traces from your applications and infrastructure, and provides you with insights into their performance, health, and availability.

It also lets you create alerting policies to notify you when metrics, health check results, and uptime check results meet specified criteria.

Cloud Logging collects and stores all application and infrastructure logs.

With real-time insights, you can use Cloud Logging to troubleshoot issues, identify trends, and comply with regulations.

Cloud Trace helps identify performance bottlenecks in applications.

It collects latency data from applications, and provides insights into how they’re performing.

Cloud Profiler identifies how much CPU power, memory, and other resources an application uses.

It continuously gathers CPU usage and memory-allocation information from production applications and provides insights into how applications are using resources.

Error Reporting counts, analyzes, and aggregates the crashes in running cloud services in real-time.

A centralized error management interface displays the results with sorting and filtering capabilities.

A dedicated view shows the error details: time chart, occurrences, affected user count, first- and last-seen dates, and a cleaned exception stack trace.

Error Reporting supports email and mobile alerts notification through its API.

Google's integrated observability tools provided by the operations suite offer valuable insights into the performance and health of applications and infrastructure in the cloud.


Point 4: Google cloud customer care

Any cloud adoption program can encounter challenges, so it's important to have an effective and efficient support plan from your cloud provider.

Google Cloud Customer Care can simplify and streamline your support experience with scalable and flexible services built with your business needs at the center.

There are four different service levels, which lets you choose the one that’s right for your organization.

Basic Support is free and is included for all Google Cloud customers.

It provides access to documentation, community support, Cloud Billing Support, and Active Assist recommendations.

Active Assist is the portfolio of tools used in Google Cloud to generate insights and recommendations to help you optimize your cloud projects.

Standard Support is recommended for workloads under development.

You can kickstart your cloud journey with unlimited access to tech support, which lets you troubleshoot, test, and explore.

It offers unlimited individual access to English-speaking support representatives during working hours, 5 days a week.

Standard support also provides access to the Cloud Support API, which lets you integrate Cloud Customer Care with your organization's customer relationship management (CRM) system.

Enhanced Support is designed for workloads in production, with fast response times and additional services to optimize your experience with high-quality, robust support.

Support is available 24/7 in a selection of languages, and initial response times are quicker than those provided by Standard Support.

Enhanced Support also offers technical support escalations and third-party technology support to help you resolve multi-vendor issues.

Premium Support is designed for enterprises with critical workloads.

It features the fastest response time, Customer Aware Support, and a dedicated Technical Account Manager.

Our Premium Support level also offers credit for the Google Cloud Skills Boost training platform, an event

management service for planned peak events, such as a product launch or major sales events, operational health reviews

to help you measure your progress and proactively address blockers to your goals with Google Cloud, and

customer aware support, where Customer Care learns and maintains information about your architecture, partners, and Google Cloud projects.

This information ensures that our support experts can resolve your cases promptly and efficiently.

Both the Enhanced and Premium support plans offer Value-Add Services that are available for additional purchase.

You can learn more about the value-add services and all Google Cloud Customer Care support offerings at cloud.google.com/support.


Point 5: The life of a support case

Any Google Cloud customer on the Standard, Enhanced, or Premium Support plan can use the Google Cloud console to create and manage support cases.

Outside of filing a support case through the Google Cloud console, Customer Care Support also offers

other contact options for live interactions with Support staff such as phone and video call support.

The life of a support case during the Google Cloud Customer Care process typically involves several stages and interactions between the customer and the support team.

Here's an overview of the typical journey of a support case.

First, the customer initiates the support request by creating a case in the Google Cloud Console.

Only users who were assigned the Tech Support Editor role within an organization can do this.

The customer provides relevant details about the issue they are experiencing, including any error messages, logs, or steps to reproduce the problem.

It’s important for the user to select a priority from P4, which means low impact, up

to P1, which means critical impact, because this will influence response times from the Customer Care team.

After the case is created, it goes through a triage process.

The team reviews the information provided by the customer to understand the problem and determine its severity and impact on the customer's business operations.

The team might request additional information or clarification from the customer at this stage.

In many cases, the Customer Care representative will resolve the case, but for more complex issues, the case is assigned to a support engineer with the appropriate level of expertise.

After the case is assigned, the team starts the troubleshooting and investigation process.

They analyze the provided information, review system logs, and conduct various diagnostic tests to identify the root cause of the issue.

Depending on the complexity of the problem, this stage might involve collaboration with other internal teams or experts.

Throughout the investigation, the Customer Care team maintains regular communication with the customer.

They provide updates on the progress, share findings, and request additional information or actions from the customer when needed.

Escalation is meant for flagging process breaks or for the rare occasion that a case is stuck because a

customer and the Customer Care team aren’t fully in sync, despite actively communicating the issue to determine the next steps.

However, it’s important to note that escalation isn’t always the best solution, and with high-impact issues, escalation might not make the case go faster.

This is because escalation can disrupt the workflow of the Customer Care team and lead to delays in other cases.

The best solution for high-impact issues is to ensure that the case is set to the

appropriate priority, ensuring that the case is assigned to the right resources as quickly as possible.

Escalation is a tool that can be used to regain traction on a stuck case.

However, it’s important to use escalation sparingly and only when it’s absolutely necessary.

When the root cause is identified, the team works on resolving the issue or providing a mitigation plan.

They might provide the customer with step-by-step instructions, configuration changes, or workaround suggestions to address the problem.

In some cases, they might consult the issue with higher-level support or engineering teams for further assistance.

The Customer Care team might also need to submit a feature request to the Google Cloud engineering team.

After implementing the resolution or mitigation plan, the Customer Care team collaborates with the customer to validate the effectiveness of the solution.

They might request the customer to perform specific tests or provide feedback on the outcome.

This step ensures that the problem is fully resolved and meets the customer's expectations.

When the customer confirms that the issue is resolved, the support case is closed.

The team provides a summary of the resolution, documents the steps taken, and ensures that the customer is satisfied with the outcome.

If needed, they might also offer recommendations for preventive measures or future best practices to avoid similar issues.

The customer also receives a feedback survey, so the support team can learn what they did well and what needs improvement.

Throughout the entire lifecycle of the support case, Google Cloud’s Customer Care team aims to provide timely and effective assistance to the customer.

They prioritize customer satisfaction, responsiveness, and strive to address the possible technical challenges faced by customers when they use Google Cloud services.


Quiz
Passing score: 75%

1.

Which of these measures should be automated on a regular basis and stored in geographically separate locations to allow for rapid recovery from disasters or failures?

Log files

Backups

Inventory data

Security patches

2.

How does replication help the design of resilient and fault-tolerant infrastructure and processes in a cloud environment?

It creates multiple copies of data or services and distributes them across different servers or locations.

It duplicates critical components or resources to provide backup alternatives.

It scales infrastructure to handle varying workloads and accommodate increased demand.

It monitors and controls incoming and outgoing network traffic based on predetermined security rules.

3.

Whose job is to ensure the reliability, availability, and efficiency of software systems and services deployed in the cloud?

Cloud architect

DevOps engineer

Site reliability engineer

Cloud security engineer

4.

One of the four golden signals is latency. What does latency measure?

How long it takes for a particular part of a system to return a result.

How many requests reach a system.

How close to capacity a system is.

System failures or other issues.

5.

What does the Cloud Profiler tool do?

It counts, analyzes, and aggregates the crashes in running cloud services in real-time.

It collects and stores all application and infrastructure logs.

It identifies how much CPU power, memory, and other resources an application uses.

It provides a comprehensive view of your cloud infrastructure and applications.

6.

Why is escalating a support ticket not always the best course of action when trying to resolve an issue?

It may reduce the number of available virtual machines.

It may disrupt the workflow of the Customer Care team and lead to delays in other cases.

It can result in increased power consumption, impacting carbon neutrality.

It can increase the monthly cost of support plans.

7.

Which metric shows how well a system or service is performing?

Service level agreements

Service level objectives

Service level indicators

Service level contracts

8.

Which Google Cloud Customer Care support level is designed for enterprises with critical workloads and features the fastest response time?

Premium Support

Enhanced Support

Standard Support

Basic Support

9.

Google Cloud’s operations suite provides a comprehensive set of monitoring, logging, and diagnostics tools. Which tool collects latency data from applications and provides insights into how they’re performing?

Cloud Profiler

Cloud Logging

Cloud Trace

Cloud Monitoring


Section 3: Sustainability with Google Cloud

In this section of the course, you'll learn about Google Cloud's commitment to sustainability and how it supports organizations' sustainability goals.


Point 1: Sustainability with Google cloud

As we get closer to the end of this Cloud Digital Leader training, where you’ve explored how cloud computing can help transform

the way you do business, it’s important that we underscore our technology efforts at Google with our commitment to the environment and sustainability.

The virtual world, which includes Google Cloud’s network, is built on physical infrastructure, and all those racks of humming servers use huge amounts of energy.

Altogether, existing data centers use nearly 2% of the world’s electricity.

With this in mind, Google works to make our data centers run as efficiently as possible.

Just like our customers, Google is trying to look after the planet.

We understand that Google Cloud customers have environmental goals of their own, and running their workloads on Google Cloud can be a part of meeting those goals.

Therefore, it’s useful to note that Google's data centers were the first to achieve ISO 14001 certification, which is a

standard that outlines a framework for an organization to enhance its environmental performance through improving resource efficiency and reducing waste.

As an example of how this is being done, here’s Google’s data center in Hamina, Finland.

This facility is one of the most advanced and efficient data centers in the Google fleet.

Its cooling system, which uses sea water from the Bay of Finland, reduces energy use and is the first of its kind anywhere in the world.

In our founding decade, Google became the first major company to be carbon neutral.

In our second decade, we were the first company to achieve 100% renewable energy.

And by 2030, we aim to be the first major company to operate completely carbon free.

We meet the challenges posed by climate change and the need for resource efficiency by working to

empower everyone—businesses, governments, nonprofit organizations, communities, and individuals—to use Google technology to create a more sustainable world.

So, what does that look like in practice?

Let’s explore an example of how one customer, Kaluza, uses Google Cloud technology to launch

smart electric vehicle charging programs that help customers save money while it reduces their carbon footprint.

Electric vehicles already account for one in seven car sales globally, and with new gas and diesel

cars being phased out across the world, global sales are forecast to reach 73 million units by 204

0.

But with power grids becoming increasingly dependent on variable energy sources such as wind and solar, rising

demand from electric vehicles risks overstraining grids at peak times, which can potentially lead to power outages.

Launched by OVO Energy in 2019, Kaluza has taken its deep understanding of the energy market to partner with some of the world’s major energy suppliers and vehicle manufacturers.

With a program called Charge Anytime, customers use Kaluza to smart-charge their electric vehicle, and they pay just about a third of their household electricity rate to do so.

This means that if the customer plugs in their vehicle to charge when they get home

from work at, say,  p.m.—a time when both demand and the carbon intensity on the

grid are at their highest—their vehicle will then be smartly charged at the lowest cost and

greenest periods throughout the night, which leaves it ready for when they need it in the morning.

Behind Kaluza’s smart charging solution lies some sophisticated technology, all of which is built on Google Cloud.

Their core optimization engine gathers real-time data from a wide range of sources, including battery and charging data from

the electric vehicles, and data from the energy suppliers and grid operators, such as the carbon intensity, and price forecasts.

That data is stored in BigQuery where it’s used to train and validate the smart charging optimization models.

These models are then deployed with Google Kubernetes Engine so that whenever a customer plugs in an electric vehicle, data from that vehicle passes in

real-time through their optimization engine to calculate the ideal charging schedule for that vehicle, which ensures that it uses the cheapest, least carbon-intensive energy available.

And as for the grid operators and energy companies, the Kaluza platform lets them visualize how many participating electric vehicles are plugged into the network at any one time.

BigQuery and Looker Studio dashboards provide granular insights, such as how many vehicles are idle, how many are charging, and how well our optimization engine is working.

At Google, we remain committed to sustainability and continue to lead and encourage others, like Kaluza, to join us in improving the health of our planet.



Quiz
Passing score: 66%

1.

What sustainability goal does Google aim to achieve by the year 2030?

To be the first major company to be carbon neutral.

To be the first major company to run its own wind farm.

To be the first major company to achieve 100% renewable energy.

To be the first major company to operate completely carbon free.

2.

Google's data centers were the first to achieve ISO 14001 certification. What is this standard’s purpose?

It’s a framework for an organization to enhance its environmental performance through improving resource efficiency and reducing waste.

It’s a framework for sustainable procurement, which is the process of purchasing goods and services in a way that minimizes environmental and social impacts.

It’s a framework for carbon footprinting that calculates the total amount of greenhouse gas emissions associated with a product, service, or organization.

It’s a framework for identifying, predicting, and evaluating the environmental impacts of a proposed project.

3.

Kaluza is an electric vehicle smart-charging solution. How does it use BigQuery and Looker Studio?

It uses BigQuery and Looker Studio to comply with government regulations.

It uses BigQuery and Looker Studio to build and deploy machine learning models.

It uses BigQuery and Looker Studio to containerize workloads.

It uses BigQuery and Looker Studio to create dashboards that provide granular operational insights.


Chapter 6 summary 

This brings us to the end of the “Scaling with Google Cloud Operations” course.

Let’s do a quick recap.

In the first section of the course titled “Financial governance and managing cloud costs,” you explored the fundamentals of cloud cost

management, cloud financial governance best practices, ways to control access by using the resource hierarchy, and ways to control cloud consumption.

In the second section of the course, titled “Operational excellence and reliability at scale,” you learned about modernizing operations by using Google

Cloud, designing resilient infrastructure and processes, the fundamentals of cloud reliability, Google Cloud Customer Care, and the life of a support case.

And finally, in the third section of the course, titled “Sustainability with Google Cloud,” you

examined how Google Cloud works to reduce our environmental impact and help organizations meet sustainability goals.

Completing this course also concludes the Cloud Digital Leader learning path.

If you’re looking to demonstrate your knowledge of the topics from these six courses, you’re encouraged to take the Cloud Digital Leader certification exam.

For more information about the exam, including additional resources to help continue your preparation, please visit https://cloud.google.com/learn/certification/cloud-digital-leader.

And if you’re looking to further expand your knowledge of Google Cloud products and services, please explore the entire catalog at cloud.google.com/training.


1- Describe cloud computing Module
This module introduces you to cloud computing. It covers things such as cloud concepts, deployment models, and understanding shared responsibility in the cloud.

Learning objectives
Upon completion of this module, you will be able to:

Define cloud computing.

Describe the shared responsibility model.

Define cloud models, including public, private, and hybrid.

Identify appropriate use cases for each cloud model.

Describe the consumption-based model.

Compare cloud pricing models.

1-1 Introduction to cloud computing
In this module, you’ll be introduced to general cloud concepts. You’ll start with an introduction to the cloud in general. Then you'll dive into concepts like shared responsibility, different cloud models, and explore the unique pricing method for the cloud.

If you’re already familiar with cloud computing, this module may be largely review for you.

Learning objectives
After completing this module, you’ll be able to:

Define cloud computing.
Describe the shared responsibility model.
Define cloud models, including public, private, and hybrid.
Identify appropriate use cases for each cloud model.
Describe the consumption-based model.
Compare cloud pricing models.
Next unit: What is cloud computing

What is cloud computing
Cloud computing is the delivery of computing services over the internet. Computing services include common IT infrastructure such as virtual machines, storage, databases, and networking. Cloud services also expand the traditional IT offerings to include things like Internet of Things (IoT), machine learning (ML), and artificial intelligence (AI).

Because cloud computing uses the internet to deliver these services, it doesn’t have to be constrained by physical infrastructure the same way that a traditional datacenter is. That means if you need to increase your IT infrastructure rapidly, you don’t have to wait to build a new datacenter—you can use the cloud to rapidly expand your IT footprint.

This short video provides a quick introduction to cloud computing.
Next unit: Describe the shared responsibility model

Describe the shared responsibility model
You may have heard of the shared responsibility model, but you may not understand what it means or how it impacts cloud computing.

Start with a traditional corporate datacenter. The company is responsible for maintaining the physical space, ensuring security, and maintaining or replacing the servers if anything happens. The IT department is responsible for maintaining all the infrastructure and software needed to keep the datacenter up and running. They’re also likely to be responsible for keeping all systems patched and on the correct version.

With the shared responsibility model, these responsibilities get shared between the cloud provider and the consumer. Physical security, power, cooling, and network connectivity are the responsibility of the cloud provider. The consumer isn’t collocated with the datacenter, so it wouldn’t make sense for the consumer to have any of those responsibilities.

At the same time, the consumer is responsible for the data and information stored in the cloud. (You wouldn’t want the cloud provider to be able to read your information.) The consumer is also responsible for access security, meaning you only give access to those who need it.

Then, for some things, the responsibility depends on the situation. If you’re using a cloud SQL database, the cloud provider would be responsible for maintaining the actual database. However, you’re still responsible for the data that gets ingested into the database. If you deployed a virtual machine and installed an SQL database on it, you’d be responsible for database patches and updates, as well as maintaining the data and information stored in the database.

With an on-premises datacenter, you’re responsible for everything. With cloud computing, those responsibilities shift. The shared responsibility model is heavily tied into the cloud service types (covered later in this learning path): infrastructure as a service (IaaS), platform as a service (PaaS), and software as a service (SaaS). IaaS places the most responsibility on the consumer, with the cloud provider being responsible for the basics of physical security, power, and connectivity. On the other end of the spectrum, SaaS places most of the responsibility with the cloud provider. PaaS, being a middle ground between IaaS and SaaS, rests somewhere in the middle and evenly distributes responsibility between the cloud provider and the consumer.

The following diagram highlights how the Shared Responsibility Model informs who is responsible for what, depending on the cloud service type.
When using a cloud provider, you’ll always be responsible for:

The information and data stored in the cloud
Devices that are allowed to connect to your cloud (cell phones, computers, and so on)
The accounts and identities of the people, services, and devices within your organization
The cloud provider is always responsible for:

The physical datacenter
The physical network
The physical hosts
Your service model will determine responsibility for things like:

Operating systems
Network controls
Applications
Identity and infrastructure
Next unit: Define cloud models

Define cloud models
What are cloud models? The cloud models define the deployment type of cloud resources. The three main cloud models are: private, public, and hybrid.

Private cloud
Let’s start with a private cloud. A private cloud is, in some ways, the natural evolution from a corporate datacenter. It’s a cloud (delivering IT services over the internet) that’s used by a single entity. Private cloud provides much greater control for the company and its IT department. However, it also comes with greater cost and fewer of the benefits of a public cloud deployment. Finally, a private cloud may be hosted from your on site datacenter. It may also be hosted in a dedicated datacenter offsite, potentially even by a third party that has dedicated that datacenter to your company.

Public cloud
A public cloud is built, controlled, and maintained by a third-party cloud provider. With a public cloud, anyone that wants to purchase cloud services can access and use resources. The general public availability is a key difference between public and private clouds.

Hybrid cloud
A hybrid cloud is a computing environment that uses both public and private clouds in an inter-connected environment. A hybrid cloud environment can be used to allow a private cloud to surge for increased, temporary demand by deploying public cloud resources. Hybrid cloud can be used to provide an extra layer of security. For example, users can flexibly choose which services to keep in public cloud and which to deploy to their private cloud infrastructure.

The following table highlights a few key comparative aspects between the cloud models.

Public cloud	Private cloud	Hybrid cloud
No capital expenditures to scale up	Organizations have complete control over resources and security	Provides the most flexibility
Applications can be quickly provisioned and deprovisioned	Data is not collocated with other organizations’ data	Organizations determine where to run their applications
Organizations pay only for what they use	Hardware must be purchased for startup and maintenance	Organizations control security, compliance, or legal requirements
Organizations don’t have complete control over resources and security	Organizations are responsible for hardware maintenance and updates	
Multi-cloud
A fourth, and increasingly likely scenario is a multi-cloud scenario. In a multi-cloud scenario, you use multiple public cloud providers. Maybe you use different features from different cloud providers. Or maybe you started your cloud journey with one provider and are in the process of migrating to a different provider. Regardless, in a multi-cloud environment you deal with two (or more) public cloud providers and manage resources and security in both environments.

Azure Arc
Azure Arc is a set of technologies that helps manage your cloud environment. Azure Arc can help manage your cloud environment, whether it's a public cloud solely on Azure, a private cloud in your datacenter, a hybrid configuration, or even a multi-cloud environment running on multiple cloud providers at once.

Azure VMware Solution
What if you’re already established with VMware in a private cloud environment but want to migrate to a public or hybrid cloud? Azure VMware Solution lets you run your VMware workloads in Azure with seamless integration and scalability.

Next unit: Describe the consumption-based model

Describe the consumption-based model
When comparing IT infrastructure models, there are two types of expenses to consider. Capital expenditure (CapEx) and operational expenditure (OpEx).

CapEx is typically a one-time, up-front expenditure to purchase or secure tangible resources. A new building, repaving the parking lot, building a datacenter, or buying a company vehicle are examples of CapEx.

In contrast, OpEx is spending money on services or products over time. Renting a convention center, leasing a company vehicle, or signing up for cloud services are all examples of OpEx.

Cloud computing falls under OpEx because cloud computing operates on a consumption-based model. With cloud computing, you don’t pay for the physical infrastructure, the electricity, the security, or anything else associated with maintaining a datacenter. Instead, you pay for the IT resources you use. If you don’t use any IT resources this month, you don’t pay for any IT resources.

This consumption-based model has many benefits, including:

No upfront costs.
No need to purchase and manage costly infrastructure that users might not use to its fullest potential.
The ability to pay for more resources when they're needed.
The ability to stop paying for resources that are no longer needed.
With a traditional datacenter, you try to estimate the future resource needs. If you overestimate, you spend more on your datacenter than you need to and potentially waste money. If you underestimate, your datacenter will quickly reach capacity and your applications and services may suffer from decreased performance. Fixing an under-provisioned datacenter can take a long time. You may need to order, receive, and install more hardware. You'll also need to add power, cooling, and networking for the extra hardware.

In a cloud-based model, you don’t have to worry about getting the resource needs just right. If you find that you need more virtual machines, you add more. If the demand drops and you don’t need as many virtual machines, you remove machines as needed. Either way, you’re only paying for the virtual machines that you use, not the “extra capacity” that the cloud provider has on hand.

Compare cloud pricing models
Cloud computing is the delivery of computing services over the internet by using a pay-as-you-go pricing model. You typically pay only for the cloud services you use, which helps you:

Plan and manage your operating costs.
Run your infrastructure more efficiently.
Scale as your business needs change.
To put it another way, cloud computing is a way to rent compute power and storage from someone else’s datacenter. You can treat cloud resources like you would resources in your own datacenter. However, unlike in your own datacenter, when you're done using cloud resources, you give them back. You’re billed only for what you use.

Instead of maintaining CPUs and storage in your datacenter, you rent them for the time that you need them. The cloud provider takes care of maintaining the underlying infrastructure for you. The cloud enables you to quickly solve your toughest business challenges and bring cutting-edge solutions to your users.

Next unit: Knowledge check

Knowledge check
Choose the best response for each question. Then select Check your answers.

Check your knowledge

1. What is cloud computing? 

A- Delivery of computing services over the internet.

B- Delivery of storage services over the internet.

C- Delivery of websites accessible via the internet.

2. Which cloud model uses some datacenters focused on providing cloud services to anyone that wants them, and some data centers that are focused on a single customer? 

A- Public cloud

B- Hybrid cloud

C- Multi-cloud

3. According to the shared responsibility model, which cloud service type places the most responsibility on the customer? 

A- Infrastructure as a Service (IaaS)

B- Software as a Service (SaaS)

C- Platform as a Service (PaaS)
Microsoft Azure Fundamentals

Chapter 2
Describe the benefits of using cloud services

This module introduces you to the benefits cloud computing can offer you or your organization.

Introduction
In this module, you’ll be introduced to some of the benefits that cloud computing offers. You’ll learn how cloud computing can help you meet variable demand while providing a good experience for your customer. You’ll also learn about security, governance, and overall manageability in the cloud.

Learning objectives
After completing this module, you’ll be able to:

Describe the benefits of high availability and scalability in the cloud.
Describe the benefits of reliability and predictability in the cloud.
Describe the benefits of security and governance in the cloud.
Describe the benefits of manageability in the cloud.

Next unit: Describe the benefits of high availability and scalability in the cloud

Describe the benefits of high availability and scalability in the cloud

When building or deploying a cloud application, two of the biggest considerations are uptime (or availability) and the ability to handle demand (or scale).

High availability
When you’re deploying an application, a service, or any IT resources, it’s important the resources are available when needed. High availability focuses on ensuring maximum availability, regardless of disruptions or events that may occur.

When you’re architecting your solution, you’ll need to account for service availability guarantees. Azure is a highly available cloud environment with uptime guarantees depending on the service. These guarantees are part of the service-level agreements (SLAs).

This short video describes Azure SLAs in more detail.


Scalability
Another major benefit of cloud computing is the scalability of cloud resources. Scalability refers to the ability to adjust resources to meet demand. If you suddenly experience peak traffic and your systems are overwhelmed, the ability to scale means you can add more resources to better handle the increased demand.

The other benefit of scalability is that you aren't overpaying for services. Because the cloud is a consumption-based model, you only pay for what you use. If demand drops off, you can reduce your resources and thereby reduce your costs.

Scaling generally comes in two varieties: vertical and horizontal. Vertical scaling is focused on increasing or decreasing the capabilities of resources. Horizontal scaling is adding or subtracting the number of resources.

Vertical scaling
With vertical scaling, if you were developing an app and you needed more processing power, you could vertically scale up to add more CPUs or RAM to the virtual machine. Conversely, if you realized you had over-specified the needs, you could vertically scale down by lowering the CPU or RAM specifications.

Horizontal scaling
With horizontal scaling, if you suddenly experienced a steep jump in demand, your deployed resources could be scaled out (either automatically or manually). For example, you could add additional virtual machines or containers, scaling out. In the same manner, if there was a significant drop in demand, deployed resources could be scaled in (either automatically or manually), scaling in.

Next unit: Describe the benefits of reliability and predictability in the cloud

Describe the benefits of reliability and predictability in the cloud

Reliability and predictability are two crucial cloud benefits that help you develop solutions with confidence.

Reliability
Reliability is the ability of a system to recover from failures and continue to function. It's also one of the pillars of the Microsoft Azure Well-Architected Framework.

The cloud, by virtue of its decentralized design, naturally supports a reliable and resilient infrastructure. With a decentralized design, the cloud enables you to have resources deployed in regions around the world. With this global scale, even if one region has a catastrophic event other regions are still up and running. You can design your applications to automatically take advantage of this increased reliability. In some cases, your cloud environment itself will automatically shift to a different region for you, with no action needed on your part. You’ll learn more about how Azure leverages global scale to provide reliability later in this series.

Predictability
Predictability in the cloud lets you move forward with confidence. Predictability can be focused on performance predictability or cost predictability. Both performance and cost predictability are heavily influenced by the Microsoft Azure Well-Architected Framework. Deploy a solution built around this framework and you have a solution whose cost and performance are predictable.

Performance
Performance predictability focuses on predicting the resources needed to deliver a positive experience for your customers. Autoscaling, load balancing, and high availability are just some of the cloud concepts that support performance predictability. If you suddenly need more resources, autoscaling can deploy additional resources to meet the demand, and then scale back when the demand drops. Or if the traffic is heavily focused on one area, load balancing will help redirect some of the overload to less stressed areas.

Cost
Cost predictability is focused on predicting or forecasting the cost of the cloud spend. With the cloud, you can track your resource use in real time, monitor resources to ensure that you’re using them in the most efficient way, and apply data analytics to find patterns and trends that help better plan resource deployments. By operating in the cloud and using cloud analytics and information, you can predict future costs and adjust your resources as needed. You can even use tools like the Total Cost of Ownership (TCO) or Pricing Calculator to get an estimate of potential cloud spend.

Next unit: Describe the benefits of security and governance in the cloud

Describe the benefits of security and governance in the cloud

Whether you’re deploying infrastructure as a service or software as a service, cloud features support governance and compliance. Things like set templates help ensure that all your deployed resources meet corporate standards and government regulatory requirements. Plus, you can update all your deployed resources to new standards as standards change. Cloud-based auditing helps flag any resource that’s out of compliance with your corporate standards and provides mitigation strategies. Depending on your operating model, software patches and updates may also automatically be applied, which helps with both governance and security.

On the security side, you can find a cloud solution that matches your security needs. If you want maximum control of security, infrastructure as a service provides you with physical resources but lets you manage the operating systems and installed software, including patches and maintenance. If you want patches and maintenance taken care of automatically, platform as a service or software as a service deployments may be the best cloud strategies for you.

And because the cloud is intended as an over-the-internet delivery of IT resources, cloud providers are typically well suited to handle things like distributed denial of service (DDoS) attacks, making your network more robust and secure.

By establishing a good governance footprint early, you can keep your cloud footprint updated, secure, and well managed.

Next unit: Describe the benefits of manageability in the cloud

Describe the benefits of manageability in the cloud

A major benefit of cloud computing is the manageability options. There are two types of manageability for cloud computing that you’ll learn about in this series, and both are excellent benefits.

Management of the cloud
Management of the cloud speaks to managing your cloud resources. In the cloud, you can:

Automatically scale resource deployment based on need.
Deploy resources based on a preconfigured template, removing the need for manual configuration.
Monitor the health of resources and automatically replace failing resources.
Receive automatic alerts based on configured metrics, so you’re aware of performance in real time.
Management in the cloud
Management in the cloud speaks to how you’re able to manage your cloud environment and resources. You can manage these:

Through a web portal.
Using a command line interface.
Using APIs.
Using PowerShell.

Next unit: Knowledge check

Knowledge check

Choose the best response for each question. Then select Check your answers.

Check your knowledge

1. Which type of scaling involves adding or removing resources (such as virtual machines or containers) to meet demand? 

A- Vertical scaling

B- Horizontal scaling

C- Direct scaling

2. What is characterized as the ability of a system to recover from failures and continue to function? 

A- Reliability

B- Predictability

C- Scalability







